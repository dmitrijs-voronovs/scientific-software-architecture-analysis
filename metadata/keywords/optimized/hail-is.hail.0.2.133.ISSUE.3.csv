quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Availability,"ion Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SPHINX-5812109](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-5812109) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-WHEEL-3180413](https://snyk.io/vuln/SNYK-PYTHON-WHEEL-3180413) | `wheel:` <br> `0.30.0 -> 0.38.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. -----",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14108:8846,avail,available,8846,https://hail.is,https://github.com/hail-is/hail/pull/14108,2,['avail'],['available']
Availability,"ion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible.; version 0.2.46-6ef64c08b000. Script is trying to merge 22 vcf.gz into a matrix table.; ```python3; import hail as hl; import sys. hl.init(default_reference='GRCh38'); vcf=""/project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz""; mt=""/project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt""; print(""Converting vcf ""+vcf+"" to mt ""+ mt); hl.import_vcf(vcf,force_bgz=True).write(mt); ```; -----------------------------------------------------------------------------; ```; $ submit ./vcf2mt_all.py; Loading modules; /share/pkg.7/gcc/8.3.0/install/lib64:/share/pkg.7/gcc/8.3.0/install/lib:/share/pkg.7/python3/3.7.7/install/lib:/usr/hdp/2.6.5.0-292/hadoop/lib/native/; Export env vars; Submitting Spark job; 20/08/17 23:24:46 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to /restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/hail-20200817-2324-0.2.46-6ef64c08b000.log; Converting vcf /project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz to mt /project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:1096,avail,available,1096,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['avail'],['available']
Availability,"ion of the</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/blob/master/CHANGES.rst"">aiohttp's changelog</a>.</em></p>; <blockquote>; <h1>3.8.3 (2022-09-21)</h1>; <p>.. attention::</p>; <p>This is the last :doc:<code>aiohttp &lt;index&gt;</code> release tested under; Python 3.6. The 3.9 stream is dropping it from the CI and the; distribution package metadata.</p>; <h2>Bugfixes</h2>; <ul>; <li>; <p>Increased the upper boundary of the :doc:<code>multidict:index</code> dependency; to allow for the version 6 -- by :user:<code>hugovk</code>.</p>; <p>It used to be limited below version 7 in :doc:<code>aiohttp &lt;index&gt;</code> v3.8.1 but; was lowered in v3.8.2 via :pr:<code>6550</code> and never brought back, causing; problems with dependency pins when upgrading. :doc:<code>aiohttp &lt;index&gt;</code> v3.8.3; fixes that by recovering the original boundary of <code>&lt; 7</code>.; <code>[#6950](https://github.com/aio-libs/aiohttp/issues/6950) &lt;https://github.com/aio-libs/aiohttp/issues/6950&gt;</code>_</p>; </li>; </ul>; <hr />; <h1>3.8.2 (2022-09-20, subsequently yanked on 2022-09-21)</h1>; <h2>Bugfixes</h2>; <ul>; <li>; <p>Support registering OPTIONS HTTP method handlers via RouteTableDef.; <code>[#4663](https://github.com/aio-libs/aiohttp/issues/4663) &lt;https://github.com/aio-libs/aiohttp/issues/4663&gt;</code>_</p>; </li>; <li>; <p>Started supporting <code>authority-form</code> and <code>absolute-form</code> URLs on the server-side.; <code>[#6227](https://github.com/aio-libs/aiohttp/issues/6227) &lt;https://github.com/aio-libs/aiohttp/issues/6227&gt;</code>_</p>; </li>; <li>; <p>Fix Python 3.11 alpha incompatibilities by using Cython 0.29.25; <code>[#6396](https://github.com/aio-libs/aiohttp/issues/6396) &lt;https://github.com/aio-libs/aiohttp/issues/6396&gt;</code>_</p>; </li>; <li>; <p>Remove a deprecated usage of",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12296:3395,recover,recovering,3395,https://hail.is,https://github.com/hail-is/hail/pull/12296,1,['recover'],['recovering']
Availability,"ion, So i can't use root permission); openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); Hail version: 0.2.68. **<My workflow>**; 1. Run start-master.sh and start-slaves.sh in spark sbin directory.; 2. (bash) pyspark. I got message below. ![image](https://user-images.githubusercontent.com/78582088/121848873-9f001980-cd25-11eb-9e9d-8854f204c8c3.png); ![image](https://user-images.githubusercontent.com/78582088/121848900-a58e9100-cd25-11eb-9655-58100401e0d3.png); ![image](https://user-images.githubusercontent.com/78582088/121848915-a9baae80-cd25-11eb-90ff-a4f12df5b322.png). How can i set up hail on spark?; Do i need to change java version?. Thank you for your services. My <bashrc>, <conf/spark-defaults.conf> and <./spark-env.sh> are below. **<.bashrc>**; ```; #SPARK; export SPARK_HOME=/home/edu1/tools/spark-2.2.0-bin-hadoop2.6; export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/python:$PATH; export PYTHONPATH=$HAIL_HOME/python:$SPARK_HOME/python:$(echo ${SPARK_HOME}/python/lib/py4j-*-src.zip):$PYTHONPATH. # Hail; export HAIL_HOME=/home/edu1/miniconda2/envs/Hail-on-spark/lib/python3.7/site-packages/hail; export PATH=$PATH:$HAIL_HOME/bin; export PYTHONPATH=$PYTHONPATH:$HAIL_HOME/python; export SPARK_CLASSPATH=$HAIL_HOME/backend/hail-all-spark.jar. # JAVA (I just can modify .bashrc, so This would not apply to java path.); export JAVA_HOME=/home/edu1/tools/jdk-1.8.0_231; export PATH=$PATH:$JAVA_HOME/bin; export CLASSPATH=$JAVA_HOME/lib/tools.jar. # Hadoop; export HADOOP_INSTALL=/home/edu1/tools/hadoop-2.6.0; export PAHT=$PATH:$HADOOP_INSTALL/bin; export LD_LIBRARY_PATH=$HADOOP_INSTALL/lib/native; ```; **</spark/conf/spark-defaults.conf>**; ```; spark.master spark://training.server:7077. spark.serializer org.apache.spark.serializer.KryoSerializer; spark.kryo.registrator is.hail.kryo.HailKryoRegistrator; spark.speculation True. spark.driver.memory 37414m; spark.executor.memory 37414m; spark.execu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590:1369,echo,echo,1369,https://hail.is,https://github.com/hail-is/hail/issues/10590,1,['echo'],['echo']
Availability,"ion.py in eval_expr(expression); 3576 Result of evaluating `expression`.; 3577 """"""; -> 3578 return eval_expr_typed(expression)[0]; 3579; 3580. <decorator-gen-366> in eval_expr_typed(expression). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^. Java stack trace:; is.hail.utils.HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2653:2207,Error,Error,2207,https://hail.is,https://github.com/hail-is/hail/issues/2653,1,['Error'],['Error']
Availability,"ion.py in eval_expr(expression); 3576 Result of evaluating `expression`.; 3577 """"""; -> 3578 return eval_expr_typed(expression)[0]; 3579; 3580. <decorator-gen-366> in eval_expr_typed(expression). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^. Java stack trace:; is.hail.utils.HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2655:2055,Error,Error,2055,https://hail.is,https://github.com/hail-is/hail/issues/2655,1,['Error'],['Error']
Availability,ion: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:8236,ERROR,ERROR,8236,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['ERROR'],['ERROR']
Availability,ion: Timeout on blocking read for 30000000000 NANOSECONDS; E java.lang.IllegalStateException: Timeout on blocking read for 30000000000 NANOSECONDS; E at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:123); E at reactor.core.publisher.Mono.block(Mono.java:1727); E at com.azure.storage.common.implementation.StorageImplUtils.blockWithOptionalTimeout(StorageImplUtils.java:130); E at com.azure.storage.blob.specialized.BlobClientBase.downloadStreamWithResponse(BlobClientBase.java:731); E at is.hail.io.fs.AzureStorageFS$$anon$1.fill(AzureStorageFS.scala:152); E at is.hail.io.fs.FSSeekableInputStream.read(FS.scala:141); E at java.io.DataInputStream.read(DataInputStream.java:149); E at is.hail.utils.richUtils.RichInputStream$.readRepeatedly$extension0(RichInputStream.scala:21); E at is.hail.utils.richUtils.RichInputStream$.readFully$extension1(RichInputStream.scala:12); E at is.hail.io.StreamBlockInputBuffer.readBlock(InputBuffers.scala:546); E at is.hail.io.LZ4SizeBasedCompressingInputBlockBuffer.readBlock(InputBuffers.scala:608); E at is.hail.io.BlockingInputBuffer.readBlock(InputBuffers.scala:382); E at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:388); E at is.hail.io.BlockingInputBuffer.readByte(InputBuffers.scala:405); E at is.hail.io.LEB128InputBuffer.readByte(InputBuffers.scala:217); E at is.hail.io.LEB128InputBuffer.readInt(InputBuffers.scala:223); E at __C173548Compiled.__m174060INPLACE_DECODE_r_binary_TO_r_binary(Emit.scala); E at __C173548Compiled.__m174059DECODE_r_struct_of_r_binaryEND_TO_SBaseStructPointer(Emit.scala); E at __C173548Compiled.__m174058begin_group_0(Emit.scala); E at __C173548Compiled.__m174057begin_group_0(Emit.scala); E at __C173548Compiled.__m173566split_Let(Emit.scala); E at __C173548Compiled.apply(Emit.scala); E at is.hail.expr.ir.lowering.LowerToCDA$.$anonfun$lower$2(LowerToCDA.scala:51); ```. This is already in `isTransientError` but `downloadStreamWithResponse` wasn't getting retried.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12261:2052,down,downloadStreamWithResponse,2052,https://hail.is,https://github.com/hail-is/hail/pull/12261,1,['down'],['downloadStreamWithResponse']
Availability,"ion: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:18516,echo,echo,18516,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['echo'],['echo']
Availability,"ion=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.indels.unfiltered.vcf) mask=(RodBinding name= source=UNBOUND) out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub filterExpression=[FS>200.0, QD<2.0, ReadPosRankSum<-20.0, InbreedingCoeff<-0.8] filterName=[Indel_FS, Indel_QD, Indel_ReadPosRankSum, Indel_InbreedingCoeff] genotypeFilterExpression=[] genotypeFilterName=[] clusterSize=3 clusterWindowSize=0 maskExtension=0 maskName=Mask missingValuesInExpressionsShouldEvaluateAsFailing=false invalidatePreviousFilters=false filter_mismatching_base_and_quals=false""; ##contig=<ID=1,length=248956422>; ##contig=<ID=2,length=242193529>; ##contig=<ID=3,length=198295559>; ##contig=<ID=4,length=190214555>; ##contig=<ID=5,length=181538259>; ##contig=<ID=6,length=170805979>; ##contig=<ID=7,length=159345973>; ##contig=<ID=8,length=145138636>; ##contig=<ID=",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:20364,mask,mask,20364,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['mask'],['mask']
Availability,"ionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever: target kube_event_loop threw exception; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/response.py"", line 601, in _update_chunk_length; self.chunk_left = i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:1449,error,error,1449,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389,1,['error'],['error']
Availability,"ions and get their implementation right, and my hope is that all the non-obvious iterator code will now be concentrated in a small, well tested, component. `FlipbookIterator` solves the confusing behavior where `hasNext` potentially wipes out the current value. (All methods on `FlipbookIterator` and `StagingIterator` should obey the rule that methods defined without trailing `()` do not change the state of the iterator in any way detectable through the API.) The core interface of `FlipbookIterator[A]` consists of the methods. * `isValid: Boolean`; * `value: A`; * `advance(): Unit`. The metaphor is a flipbook, where when you turn the page, you no longer have access to the previous page; where you can read the current page as many times as you want (no need to copy it); and where you only know you've reached the end of the flipbook when you turn the page and find that the next page is empty. In `FlipbookIterator`, `advance()` turns the page, `isValid` asks if the page you are on is non-empty, and `value` gives you the value on the current page (which is an error if the page is empty). `StagingIterator` is a subtype of `FlipbookIterator` which adds a bit of state to each page, together with the methods `consume()` and `stage()`. The bit of state on each page tracks whether the value has been ""consumed"" yet. `consume()` can only be called on a valid page which has not yet been consumed, and marks it as consumed. Note that `consume()` does not actually turn the page, so that the value just consumed is kept alive for the consumer to use. `stage()` brings the iterator forward to the next state in which we are on an unconsumed page: if the current page has been consumed, it flips to the next page, but if the current page has not been consumed it does nothing. My goal with the `StagingIterator` interface was to find something simple—to make it easier to write iterator code that is easily understandable and obviously correct and bug-free—and which covers most of the low-level ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3016:1778,error,error,1778,https://hail.is,https://github.com/hail-is/hail/pull/3016,1,['error'],['error']
Availability,"iosignal 1.3.1 requires frozenlist, which is not installed.; aiodns 2.0.0 requires pycares, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Use of a Broken or Risky Cryptographic Algorithm <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6149518](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6149518) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Uncontrolled Resource Consumption (&#x27;Resource Exhaustion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6157248](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6157248) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiO",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14230:1540,avail,available,1540,https://hail.is,https://github.com/hail-is/hail/pull/14230,1,['avail'],['available']
Availability,ir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:19); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:19); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.methods.VEP$.waitFor(VEP.scala:73); 	at is.hail.methods.VEP.$anonfun$execute$5(VEP.scala:244); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.hasNext(RichContextRDD.scala:77); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at __C1310collect_distributed_array_table_native_writer.apply_region1_87(Unknown Source); 	at __C1310collect_distributed_array_table_native_writer.apply(Unknown Source); 	at __C,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:5989,Error,ErrorHandling,5989,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,['Error'],['ErrorHandling']
Availability,"irectory '/home/edmund/.local/src/hail/hail'; # # If hailtop.aiotools.copy gives you trouble:; # gcloud storage cp -r src/test/resources/\* gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/; # gcloud storage cp -r python/hail/docs/data/\* gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/; python3 -m hailtop.aiotools.copy -vvv 'null' '[\; {""from"":""src/test/resources"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/""},\; {""from"":""python/hail/docs/data"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/""}\; ]' --timeout 600; Traceback (most recent call last):; File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/runpy.py"", line 197, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.local/src/hail/hail/python/hailtop/aiotools/copy.py"", line 211, in <module>; asyncio.run(main()); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/asyncio/runners.py"", line 44, in run; return loop.run_until_complete(main); File ""uvloop/loop.pyx"", line 1517, in uvloop.loop.Loop.run_until_complete; File ""/home/edmund/.local/src/hail/hail/python/hailtop/aiotools/copy.py"", line 182, in main; files = json.loads(args.files); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/json/__init__.py"", line 346, in loads; return _default_decoder.decode(s); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/json/decoder.py"", line 355, in raw_decode; raise JSONDecodeError(""Expecting value"", s, err.value) from None; json.decoder.JSONDecodeError: Expecting value: line 1 column 2 (char 1); make: *** [Makefile:355: upload-remote-test-resources] Error 1; make: Leaving directory '/home/edmund/.local/src/hail/hail'; ```; What am I missing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1887744163:2051,Error,Error,2051,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1887744163,1,['Error'],['Error']
Availability,"is Data Analytics.</li>; </ul>; <h1>1.26.14</h1>; <ul>; <li>api-change:<code>route53</code>: [<code>botocore</code>] Amazon Route 53 now supports the Asia Pacific (Hyderabad) Region (ap-south-2) for latency records, geoproximity records, and private DNS for Amazon VPCs in that region.</li>; </ul>; <h1>1.26.13</h1>; <ul>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow provides a new API called UpdateConnectorRegistration to update a custom connector that customers have previously registered. With this API, customers no longer need to unregister and then register a connector to make an update.</li>; <li>api-change:<code>auditmanager</code>: [<code>botocore</code>] This release introduces a new feature for Audit Manager: Evidence finder. You can now use evidence finder to quickly query your evidence, and add the matching evidence results to an assessment report.</li>; <li>api-change:<code>chime-sdk-voice</code>: [<code>botocore</code>] Amazon Chime Voice Connector, Voice Connector Group and PSTN Audio Service APIs are now available in the Amazon Chime SDK Voice namespace. See <a href=""https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html"">https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html</a> for more details.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/d872f34494e33473126a887499262c6d3139d0f3""><code>d872f34</code></a> Merge branch 'release-1.26.17'</li>; <li><a href=""https://github.com/boto/boto3/commit/c547ba545c4aeb40bc1848e50d9b89f54df8937c""><code>c547ba5</code></a> Bumping version to 1.26.17</li>; <li><a href=""https://github.com/boto/boto3/commit/083655fd0ece1370b4c5100fdb11f1cf11ac3f9a""><code>083655f</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/865ba3450a2408cf04413a2209e8fe4959f162e6""><code>865ba34</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12507:6321,avail,available,6321,https://hail.is,https://github.com/hail-is/hail/pull/12507,1,['avail'],['available']
Availability,"is Data Analytics.</li>; </ul>; <h1>1.26.14</h1>; <ul>; <li>api-change:<code>route53</code>: [<code>botocore</code>] Amazon Route 53 now supports the Asia Pacific (Hyderabad) Region (ap-south-2) for latency records, geoproximity records, and private DNS for Amazon VPCs in that region.</li>; </ul>; <h1>1.26.13</h1>; <ul>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow provides a new API called UpdateConnectorRegistration to update a custom connector that customers have previously registered. With this API, customers no longer need to unregister and then register a connector to make an update.</li>; <li>api-change:<code>auditmanager</code>: [<code>botocore</code>] This release introduces a new feature for Audit Manager: Evidence finder. You can now use evidence finder to quickly query your evidence, and add the matching evidence results to an assessment report.</li>; <li>api-change:<code>chime-sdk-voice</code>: [<code>botocore</code>] Amazon Chime Voice Connector, Voice Connector Group and PSTN Audio Service APIs are now available in the Amazon Chime SDK Voice namespace. See <a href=""https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html"">https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html</a> for more details.</li>; <li>api-change:<code>cloudfront</code>: [<code>botocore</code>] CloudFront API support for staging distributions and associated traffic management policies.</li>; <li>api-change:<code>connect</code>: [<code>botocore</code>] Added AllowedAccessControlTags and TagRestrictedResource for Tag Based Access Control on Amazon Connect Webpage</li>; <li>api-change:<code>dynamodb</code>: [<code>botocore</code>] Updated minor fixes for DynamoDB documentation.</li>; <li>api-change:<code>dynamodbstreams</code>: [<code>botocore</code>] Update dynamodbstreams client to latest version</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds support for copying an Amazon Machine Image'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12498:2007,avail,available,2007,https://hail.is,https://github.com/hail-is/hail/pull/12498,2,['avail'],['available']
Availability,"is a feature release, which includes new features and removes previously deprecated features. The 8.1.x branch is now the supported bugfix branch, the 8.0.x branch will become a tag marking the end of support for that branch. We encourage everyone to upgrade, and to use a tool such as <a href=""https://pypi.org/project/pip-tools/"">pip-tools</a> to pin all dependencies and control upgrades.</p>; <ul>; <li>Changes: <a href=""https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-0"">https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-0</a></li>; <li>Milestone: <a href=""https://github.com/pallets/click/milestone/9?closed=1"">https://github.com/pallets/click/milestone/9?closed=1</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/click/blob/main/CHANGES.rst"">click's changelog</a>.</em></p>; <blockquote>; <h2>Version 8.1.2</h2>; <p>Released 2022-03-31</p>; <ul>; <li>Fix error message for readable path check that was mixed up with the; executable check. :pr:<code>2236</code></li>; <li>Restore parameter order for <code>Path</code>, placing the <code>executable</code>; parameter at the end. It is recommended to use keyword arguments; instead of positional arguments. :issue:<code>2235</code></li>; </ul>; <h2>Version 8.1.1</h2>; <p>Released 2022-03-30</p>; <ul>; <li>Fix an issue with decorator typing that caused type checking to; report that a command was not callable. :issue:<code>2227</code></li>; </ul>; <h2>Version 8.1.0</h2>; <p>Released 2022-03-28</p>; <ul>; <li>; <p>Drop support for Python 3.6. :pr:<code>2129</code></p>; </li>; <li>; <p>Remove previously deprecated code. :pr:<code>2130</code></p>; <ul>; <li><code>Group.resultcallback</code> is renamed to <code>result_callback</code>.</li>; <li><code>autocompletion</code> parameter to <code>Command</code> is renamed to; <code>shell_complete</code>.</li>; <li><code>get_terminal_size</code> is removed, use; <code>shutil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11801:2173,error,error,2173,https://hail.is,https://github.com/hail-is/hail/pull/11801,1,['error'],['error']
Availability,"is filter:; passed=passed.filter_rows((passed.variant_qc.AC>= 10)); Without this filter it runs OK. This file is a merged vcf file from Lumpy. Some sites may have no alternate alleles called (all 0/0 or ./.). ### What went wrong (all error messages here, including the full java stack trace):; [Stage 2:> (0 + 72) / 125]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/hail/lumpy/models.all.py"", line 80, in <module>; print(""Filtered Passed Rows:"",passed.count_rows()); File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2072, in count_rows; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 2.0 failed 4 times, most recent failure: Lost task 30.3 in stage 2.0 (TID 91, scc-q05.scc.bu.edu, executor 9): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:649); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:246); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:219); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$czip$1$$anon$1.next(ContextRDD.scala:322); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3901:1376,failure,failure,1376,https://hail.is,https://github.com/hail-is/hail/issues/3901,1,['failure'],['failure']
Availability,"is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiodns 2.0.0 requires pycares, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Use of a Broken or Risky Cryptographic Algorithm <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6149518](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6149518) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Uncontrolled Resource Consumption (&#x27;Resource Exhaustion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6157248](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6157248) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **451/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.3 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgrad",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14234:1512,avail,available,1512,https://hail.is,https://github.com/hail-is/hail/pull/14234,1,['avail'],['available']
Availability,"is project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - ci/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6050294](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6050294) | `cryptography:` <br> `41.0.7 -> 42.0.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Information Exposure <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6126975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6126975) | `cryptography:` <br> `41.0.7 -> 42.0.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIyMTAxMzNhYS03MjA2LTRmMzQtYTQ2OC1iYjY5YWJmY",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14200:1227,avail,available,1227,https://hail.is,https://github.com/hail-is/hail/pull/14200,1,['avail'],['available']
Availability,"is satisfied. ```; # k describe pod job-5-8hbt5 -n batch-pods; Name: job-5-8hbt5; Namespace: batch-pods; Node: minikube/10.0.2.15; Start Time: Wed, 21 Nov 2018 15:12:01 -0500; Labels: app=batch-job; hail.is/batch-instance=91332f5563704be7a54c56dd334de2ba; uuid=fd1810a5a3cd4fa1b60caeb182eff5e5; Annotations: <none>; Status: Running; IP: 172.17.0.49; Containers:; default:; Container ID: docker://b627d8df102687e50c95272980fbfe0fb634caecd3ace6217e3a6ce92cde1b21; Image: alpine:3.8; Image ID: docker-pullable://alpine@sha256:621c2f39f8133acb8e64023a94dbdf0d5ca81896102b9e57c0dc184cadaf5528; Port: <none>; Host Port: <none>; Command:; echo; left; State: Waiting; Reason: CrashLoopBackOff; Last State: Terminated; Reason: Completed; Exit Code: 0; Started: Wed, 21 Nov 2018 15:13:30 -0500; Finished: Wed, 21 Nov 2018 15:13:30 -0500; Ready: False; Restart Count: 4; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-5-8hbt5 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-lfdr4 (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-lfdr4:; Type: Secret (a volume populated by a Secret); SecretName: default-token-lfdr4; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 2m default-scheduler Successfully assigned job-5-8hbt5 to minikube; Normal SuccessfulMountVolume 2m kubelet, minikube MountVolume.SetUp succeeded for volume ""default-token-lfdr4""; Normal Pulled 53s (x5 over 2m) kubelet, minikube Container image ""alpine:3.8"" already present on machine; Normal Created 53s (x5 over 2m) kubelet, minikube Created container; Normal Started 53s (x5 over 2m) kubelet, minikube Started container; Warning BackOff 38s (x9 over 2m) kubelet, minikube Back-off restarting failed container; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4822:1570,Toler,Tolerations,1570,https://hail.is,https://github.com/hail-is/hail/issues/4822,1,['Toler'],['Tolerations']
Availability,"is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.130-bea04d9c79b5; Error summary: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:19792,Error,Error,19792,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Error'],['Error']
Availability,is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:10214,Error,Error,10214,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['Error'],['Error']
Availability,is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.11-cf54f08305d1; Error summary: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:15529,Error,Error,15529,https://hail.is,https://github.com/hail-is/hail/issues/5718,2,['Error'],['Error']
Availability,isDosage defaults to false if not found in metadata JSON.; Error if wrong type in metadata JSON.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/610:59,Error,Error,59,https://hail.is,https://github.com/hail-is/hail/pull/610,1,['Error'],['Error']
Availability,"iscuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; `devel-17d79be1221e`; ### What you did:; ```; import hail as hl. mt = hl.balding_nichols_model(3, 100, 100); hl.export_vcf(mt, 'foo.vcf.bgz'); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""foo.py"", line 4, in <module>; hl.export_vcf(mt, 'foo.vcf.bgz'); File ""<decorator-gen-878>"", line 2, in export_vcf; File ""/Users/dking/projects/hail/python/hail/typecheck/check.py"", line 546, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/hail/python/hail/methods/impex.py"", line 424, in export_vcf; joption(typ._convert_to_j(metadata))); File ""/Users/dking/borg/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/Users/dking/projects/hail/python/hail/utils/java.py"", line 210, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: in export_vcf: column key must be type 'str', found: [Lis.hail.expr.types.Type;@1e5a1972. Java stack trace:; is.hail.utils.HailException: in export_vcf: column key must be type 'str', found: [Lis.hail.expr.types.Type;@1e5a1972; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.MatrixTable.requireColKeyString(MatrixTable.scala:392); 	at is.hail.io.vcf.ExportVCF$.apply(ExportVCF.scala:202); 	at is.hail.io.vcf.ExportVCF.apply(ExportVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:24",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4283:1117,Error,Error,1117,https://hail.is,https://github.com/hail-is/hail/issues/4283,1,['Error'],['Error']
Availability,"ise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 32 except pyspark.sql.utils.CapturedException as e:; 33 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal. Exit status: 137. Diagnostics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal. Exit status: 137. Diagnostics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.forea",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287:5797,failure,failure,5797,https://hail.is,https://github.com/hail-is/hail/issues/13287,1,['failure'],['failure']
Availability,"issues/2324"">#2324</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; </ul>; <h1>2.18.0 - 2022-04-02</h1>; <h3>Features</h3>; <ul>; <li>Keep <code>GIT_HTTP_PROXY_AUTHMETHOD</code> in git environ.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2272"">#2272</a> PR by <a href=""https://github.com/VincentBerthier""><code>@​VincentBerthier</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2271"">#2271</a> issue by <a href=""https://github.com/VincentBerthier""><code>@​VincentBerthier</code></a>.</li>; </ul>; </li>; <li>Support both <code>cs</code> and <code>coursier</code> executables for coursier hooks.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2293"">#2293</a> PR by <a href=""https://github.com/Holzhaus""><code>@​Holzhaus</code></a>.</li>; </ul>; </li>; <li>Include more information in errors for <code>language_version</code> /; <code>additional_dependencies</code> for languages which do not support them.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2315"">#2315</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; <li>Have autoupdate preferentially pick tags which look like versions when; there are multiple equivalent tags.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2312"">#2312</a> PR by <a href=""https://github.com/mblayman""><code>@​mblayman</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2311"">#2311</a> issue by <a href=""https://github.com/mblayman""><code>@​mblayman</code></a>.</li>; </ul>; </li>; <li>Upgrade <code>ruby-build</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2319"">#2319</a> PR by <a href=""https://github.com/jalessio""><code>@​jalessio</code></a>.</li>; </ul>; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11731:6623,error,errors,6623,https://hail.is,https://github.com/hail-is/hail/pull/11731,1,['error'],['errors']
Availability,"istic <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/issues/3487"">#3487</a> with thanks to <a href=""https://github.com/Skn0tt""><code>@​Skn0tt</code></a></li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fixed ValueError when <code>ff.create_annotated_heatmap</code> passes <code>rgba()</code> colors into <code>to_rgb_color_list</code> <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/issues/3478"">#3478</a> with thanks to <a href=""https://github.com/janosh""><code>@​janosh</code></a></li>; </ul>; <h3>Updated</h3>; <ul>; <li>Updated Plotly.js to from version 2.6.3 to version 2.8.3. See the <a href=""https://github.com/plotly/plotly.js/blob/master/CHANGELOG.md#280----2021-12-10"">plotly.js CHANGELOG</a> for more information. Notable changes include:; <ul>; <li>Horizontal color bars</li>; <li><code>texttemplate</code> for histogram-like and heatmap-like traces</li>; </ul>; </li>; </ul>; <h2>[5.4.0] - 2021-11-15</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed error when serializing dict with mix of string and non-string keys <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/issues/3380"">#3380</a></li>; </ul>; <h3>Updated</h3>; <ul>; <li>The JSON serialization engines no longer sort their keys <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/issues/3380"">#3380</a></li>; <li>Updated Plotly.js to from version 2.4.2 to version 2.6.3. See the <a href=""https://github.com/plotly/plotly.js/blob/master/CHANGELOG.md#263----2021-11-12"">plotly.js CHANGELOG</a> for more information. Notable changes include:; <ul>; <li>New subplot type <code>smith</code> that supports <code>scattersmith</code> trace types for visualizing data in the complex domain</li>; <li>Changes to Plotly.js packaging</li>; </ul>; </li>; </ul>; <h2>[5.3.1] - 2021-08-31</h2>; <h3>Updated</h3>; <ul>; <li>Updated Plotly.js to from version 2.4.1 to version 2.4.2. See the <a href=""https://github.com/plotly/plotly.js/blob/master/CHANGELOG.md#240----2021-08-27"">plotly.js CHANGELOG</a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11535:3103,error,error,3103,https://hail.is,https://github.com/hail-is/hail/pull/11535,1,['error'],['error']
Availability,"it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I j",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1236,down,down,1236,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878,1,['down'],['down']
Availability,"it felt a little weird to directly define a hail function that's like ""throw an error of the specified type"", so I added it as an option to `hl.case()`, but I could break it out explicitly if that would be better.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3984:80,error,error,80,https://hail.is,https://github.com/hail-is/hail/pull/3984,1,['error'],['error']
Availability,"it is stock hail, but I'm running with a new GRCh38 VEP config + files from; gs://hail-common/vep/vep/GRCh38/vep85-GRCh38-init.sh. though I've run it on another VCF and didn't get that error. ; I'll rerun to make sure it wasn't a misconfiguration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301756372:185,error,error,185,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301756372,1,['error'],['error']
Availability,"it is the imperative analog of Code[Unit]. In this analogy, a function returning Code[Unit] becomes a function that takes a CodeBuilder, alternatively, a Code[Unit] can become a CodeLabel: the place to jump to run a given computation. In addition to CodeBuilder, I have a imperative implementation of EmitCode that is similar to the proposal in the dev post: IEmitCode. Under the above analog, the proposal in the dev post would become:. ```; trait IEmitCode {; def apply(missing: (CodeBuilder) => Unit, present: (CodeBuilder, PValue) => Unit): Unit; }; ```. However, I took the additional step of ""defunctionalizing"" this picture by using labels instead of functions of code, giving:. ```; case class IEmitCode(Lmissing: CodeLabel, Lpresent: CodeLabel, pc: PCode) {; ...; }; ```. In this model, the emit function will become: `Emit.emit(cb: CodeBuilder, ...): IEmitCode`. The discipline is after calling `emit`, the contents of the code builder, when executed, will jump to one of `Lmissing` or `Lpresent` (labels which are not defined yet) and the consumer can define those labels, and use the expression `pc` in the code after the `Lpresent` label only. Because obviously I haven't converted everything to the imperative style (yet), I wrote routines to convert them back and forth: `EmitCode.fromI { cb => ... }` provides a CodeBuilder and converts a resulting IEmitCode back to an EmitCode, and EmitCode.toI(cb) does the opposite. There is also `(Emit)CodeBuilder.scoped` that will run a code builder function and collect the code as a Code[Unit]. A second idea introduced in this PR is that some PType operations may only be available on a PSettable/PValue, rather than on PCode. The motivation is the canonical array ref implementation, which involves a lot of duplicate code generation. In the current setup, we can have a compound PValue, and this introduces a PCanonicalIndexableValue that has three fields: the base array address, the length and the elements addresss. FYI @patrick-schultz",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413:2418,avail,available,2418,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413,1,['avail'],['available']
Availability,"it must be some system- or file-system- specific issue -- is there any possibility you could try to use 0.2.8 to read that file, and see if you get the error (or another one)?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5744#issuecomment-479056057:152,error,error,152,https://hail.is,https://github.com/hail-is/hail/issues/5744#issuecomment-479056057,1,['error'],['error']
Availability,it's a bit further down but definitely there,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7644#issuecomment-560944851:19,down,down,19,https://hail.is,https://github.com/hail-is/hail/issues/7644#issuecomment-560944851,1,['down'],['down']
Availability,"it's working, but there are still a number of issues to address. 1. The IndexReader is initialized for each partition (via a path that is passed as a String). This means that the decoder in the IndexReader is compiled on the workers. To make this work, I had to comment out the assertion that FunctionBuilder can only emit code on the master node. If we create the indexReaders on the master node, then we'll have to make them serializable and I was having trouble getting that to work with the decoder and the region in the reader. I could make every problematic field `@transient`, but I'm not sure that actually solves the problem. 2. The underlying iterator that is used in 2 / 3 of the new BgenRecordIterators use iterators from the IndexReader. However, there is no way to specify when to close them. Is `close()` called automatically on the IndexReader when the partition is garbage collected?. 3. The way I wrote this was to make the table that is being joined with the BgenRDD when filtering variants to contain duplicate values if the same key appears in multiple partitions on the BGEN side. This means that the iterator is going through each index in the partition and comparing it to the variants in the filter. Therefore, it is `O(nVariants in Partition)` rather than approximately `O(nVariants in Filter in Partition)`. I think this is okay, since I expect iterating through the keys in the index reader is much faster than downstream operations, but it is something to be aware of. 4. My code for checking whether two BGEN files overlap uses `isDisjoint` on intervals. This means that we don't allow files whose endpoints overlap. I think as a first pass this is fine, but I may need to write more complicated code to test whether the files given by the user are compatible. 5. The MatrixImportBGEN IR in Python is currently wrong -- there's no way to pass the requested type. I didn't dig into this yet since it's not used, but it's still on the list of things that need to be fixed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4291:1552,down,downstream,1552,https://hail.is,https://github.com/hail-is/hail/pull/4291,1,['down'],['downstream']
Availability,"it)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076:1934,Error,Error,1934,https://hail.is,https://github.com/hail-is/hail/issues/2076,1,['Error'],['Error']
Availability,"ite-packages is not writeable; Collecting pip-tools==6.13.0; Downloading pip_tools-6.13.0-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.2/53.2 kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB); Installing collected packages: zipp, wheel, tomli, packaging, click, pyproject_hooks, importlib-metadata, build, pip-tools; Successfully installed build-1.0.3 click-8.1.7 importlib-metadata-6.8.0 packaging-23.2 pip-tools-6.13.0 pyproject_hooks-1.0.0 tomli-2.0.1 wheel-0.41.2 zipp-3.17.0. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; + for package in '$@'; + reqs=python/requirements.txt; + pinned=python/pinned-requirements.txt; ++ mktemp; + new_pinned=/tmp/tmp.YoVBQEw8XF; ++ mktemp; + pinned_no_comments=/tmp/tmp.WRSKGgGEB8; ++ mktemp; + new_pinned_no_comments=/tmp/tmp.C8ggaXDHDt; + PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/aws/puppet/bin/:/home/hadoop/.local/bin:/home/hadoop",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:30226,Down,Downloading,30226,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Down'],['Downloading']
Availability,"ith 1 tasks; 2018-10-09 14:46:43 TaskSetManager: INFO: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4777 bytes); 2018-10-09 14:46:43 Executor: INFO: Running task 0.0 in stage 5.0 (TID 5); 2018-10-09 14:46:43 BlockManager: INFO: Found block rdd_9_0 locally; 2018-10-09 14:46:43 CodeGenerator: INFO: Code generated in 19.341759 ms; 2018-10-09 14:46:43 CodeGenerator: INFO: Code generated in 10.738625 ms; 2018-10-09 14:46:43 SparkContext: INFO: Invoking stop() from shutdown hook; 2018-10-09 14:46:43 AbstractConnector: INFO: Stopped Spark@31b6843e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2018-10-09 14:46:43 SparkUI: INFO: Stopped Spark web UI at http://10.32.119.167:4040; 2018-10-09 14:46:43 DAGScheduler: INFO: Job 3 failed: fold at RVD.scala:361, took 0.107081 s; 2018-10-09 14:46:43 DAGScheduler: INFO: ResultStage 5 (fold at RVD.scala:361) failed in 0.097 s due to Stage cancelled because SparkContext was shut down; 2018-10-09 14:46:43 LiveListenerBus: ERROR: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@58127452); 2018-10-09 14:46:43 LiveListenerBus: ERROR: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(3,1539121603334,JobFailed(org.apache.spark.SparkException: Job 3 cancelled because SparkContext was shut down)); 2018-10-09 14:46:43 MapOutputTrackerMasterEndpoint: INFO: MapOutputTrackerMasterEndpoint stopped!; 2018-10-09 14:46:43 MemoryStore: INFO: MemoryStore cleared; 2018-10-09 14:46:43 BlockManager: INFO: BlockManager stopped; 2018-10-09 14:46:43 BlockManagerMaster: INFO: BlockManagerMaster stopped; 2018-10-09 14:46:43 OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: INFO: OutputCommitCoordinator stopped!; 2018-10-09 14:46:43 SparkContext: INFO: Successfully stopped SparkContext; 2018-10-09 14:46:43 ShutdownHookManager: INFO: Shutdown hook called; 2018-10-09 14:46:43 ShutdownHookManager: INFO: Deleting directory /privat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:48790,down,down,48790,https://hail.is,https://github.com/hail-is/hail/issues/4513,2,"['ERROR', 'down']","['ERROR', 'down']"
Availability,"ithub&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""5da7ca75-f161-4f7a-ae7f-c92bb747eca9"",""prPublicId"":""5da7ca75-f161-4f7a-ae7f-c92bb747eca9"",""dependencies"":[{""name"":""aiohttp"",""from"":""3.8.6"",""to"":""3.9.0""},{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.11.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-AIOHTTP-6091621"",""SNYK-PYTHON-AIOHTTP-6091622"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-JUPYTERSERVER-6099119"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,531,null,null,null,509,384,494,539],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Improper Input Validation](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14074:6928,avail,available,6928,https://hail.is,https://github.com/hail-is/hail/pull/14074,1,['avail'],['available']
Availability,"ithub-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.5</h2>; <p>Maintenance:</p>; <ul>; <li>Publish signed artifacts to Gradle plugin portal</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.4</h2>; <p>Bug fixes:</p>; <ul>; <li>Fix deadlock in <code>DownloadExtension</code> if <code>max-workers</code> equals 1 (thanks to <a href=""https://github.com/beatbrot""><code>@​beatbrot</code></a> for spotting this, see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/205"">#205</a>)</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1b5d69760d19cb7f88cbc837ee46456c494c0696""><code>1b5d697</code></a> Bump up version number to 5.2.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/7d6de83037ca41cd2f2f31830b43e43720e45b3a""><code>7d6de83</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:2339,Mainten,Maintenance,2339,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['Mainten'],['Maintenance']
Availability,"ithub.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Akrassowski+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ameeseeksmachine+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​meeseeksmachine</code></a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/notebook/blob/@jupyter-notebook/tree@7.0.7/CHANGELOG.md"">notebook's changelog</a>.</em></p>; <blockquote>; <h2>7.0.7</h2>; <p>(<a href=""https://github.com/jupyter/notebook/compare/@jupyter-notebook/application-extension@7.0.6...089c78c48fd00b2b0d2f33e4463eb42018e86803"">Full Changelog</a>)</p>; <h3>Enhancements made</h3>; <ul>; <li>Update to JupyterLab 4.0.11 <a href=""https://redirect.github.com/jupyter/notebook/pull/7215"">#7215</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Update ruff config and typing <a href=""https://redirect.github.com/jupyter/notebook/pull/7145"">#7145</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Clean up lint handling <a href=""https://redirect.github.com/jupyter/notebook/pull/7142"">#7142</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Adopt ruff format <a href=""https://redirect.github.com/jupyter/notebook/pull/7132"">#7132</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>[7.0.x] Install stable JupyterLab 4.0 in the releaser hook <a href=""https://redirect.github.com/jupyter/notebook/pull/7183"">#7183</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; <li>Update publish-release workflow for PyPI trusted publisher <a href=""https://redirect.github.com/jupyter/notebook/pull/7176"">#7176</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; </ul>; <h3>Contributors to",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14182:3544,Mainten,Maintenance,3544,https://hail.is,https://github.com/hail-is/hail/pull/14182,1,['Mainten'],['Maintenance']
Availability,"ivate DNS for Amazon VPCs in that region.</li>; </ul>; <h1>1.26.13</h1>; <ul>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow provides a new API called UpdateConnectorRegistration to update a custom connector that customers have previously registered. With this API, customers no longer need to unregister and then register a connector to make an update.</li>; <li>api-change:<code>auditmanager</code>: [<code>botocore</code>] This release introduces a new feature for Audit Manager: Evidence finder. You can now use evidence finder to quickly query your evidence, and add the matching evidence results to an assessment report.</li>; <li>api-change:<code>chime-sdk-voice</code>: [<code>botocore</code>] Amazon Chime Voice Connector, Voice Connector Group and PSTN Audio Service APIs are now available in the Amazon Chime SDK Voice namespace. See <a href=""https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html"">https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html</a> for more details.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/d872f34494e33473126a887499262c6d3139d0f3""><code>d872f34</code></a> Merge branch 'release-1.26.17'</li>; <li><a href=""https://github.com/boto/boto3/commit/c547ba545c4aeb40bc1848e50d9b89f54df8937c""><code>c547ba5</code></a> Bumping version to 1.26.17</li>; <li><a href=""https://github.com/boto/boto3/commit/083655fd0ece1370b4c5100fdb11f1cf11ac3f9a""><code>083655f</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/865ba3450a2408cf04413a2209e8fe4959f162e6""><code>865ba34</code></a> Avoid duplicate serialization in DynamoDB BatchWriter (<a href=""https://github-redirect.dependabot.com/boto/boto3/issues/3504"">#3504</a>)</li>; <li><a href=""https://github.com/boto/boto3/commit/f38ce50a317baf6715870b2706100d43b80b0c73",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12507:6513,avail,available-regions,6513,https://hail.is,https://github.com/hail-is/hail/pull/12507,1,['avail'],['available-regions']
Availability,"ivate DNS for Amazon VPCs in that region.</li>; </ul>; <h1>1.26.13</h1>; <ul>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow provides a new API called UpdateConnectorRegistration to update a custom connector that customers have previously registered. With this API, customers no longer need to unregister and then register a connector to make an update.</li>; <li>api-change:<code>auditmanager</code>: [<code>botocore</code>] This release introduces a new feature for Audit Manager: Evidence finder. You can now use evidence finder to quickly query your evidence, and add the matching evidence results to an assessment report.</li>; <li>api-change:<code>chime-sdk-voice</code>: [<code>botocore</code>] Amazon Chime Voice Connector, Voice Connector Group and PSTN Audio Service APIs are now available in the Amazon Chime SDK Voice namespace. See <a href=""https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html"">https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html</a> for more details.</li>; <li>api-change:<code>cloudfront</code>: [<code>botocore</code>] CloudFront API support for staging distributions and associated traffic management policies.</li>; <li>api-change:<code>connect</code>: [<code>botocore</code>] Added AllowedAccessControlTags and TagRestrictedResource for Tag Based Access Control on Amazon Connect Webpage</li>; <li>api-change:<code>dynamodb</code>: [<code>botocore</code>] Updated minor fixes for DynamoDB documentation.</li>; <li>api-change:<code>dynamodbstreams</code>: [<code>botocore</code>] Update dynamodbstreams client to latest version</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds support for copying an Amazon Machine Image's tags when copying an AMI.</li>; <li>api-change:<code>glue</code>: [<code>botocore</code>] AWSGlue Crawler - Adding support for Table and Column level Comments with database level datatypes for JDBC based crawler.</li>; <li>api-change:<code>io",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12498:2199,avail,available-regions,2199,https://hail.is,https://github.com/hail-is/hail/pull/12498,2,['avail'],['available-regions']
Availability,"ix build issue on 32-bit.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.6.7 - 2022-02-14</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of deserializing almost-empty documents.</li>; <li>Publish arm7l <code>manylinux_2_17</code> wheels to PyPI.</li>; <li>Publish amd4 <code>musllinux_1_1</code> wheels to PyPI.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix build requiring <code>python</code> on <code>PATH</code>.</li>; </ul>; <h2>3.6.6 - 2022-01-21</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing <code>datetime.datetime</code> using <code>tzinfo</code> that; are <code>zoneinfo.ZoneInfo</code>.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix invalid indexing in line and column number reporting in; <code>JSONDecodeError</code>.</li>; <li>Fix <code>orjson.OPT_STRICT_INTEGER</code> not raising an error on; values exceeding a 64-bit integer maximum.</li>; </ul>; <h2>3.6.5 - 2021-12-05</h2>; <h3>Fixed</h3>; <ul>; <li>Fix build on macOS aarch64 CPython 3.10.</li>; <li>Fix build issue on 32-bit.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ijl/orjson/commit/aee8a9fed45f84d227cf2cb7102656aa65a4890a""><code>aee8a9f</code></a> 3.6.7</li>; <li><a href=""https://github.com/ijl/orjson/commit/622cd7b1167262ffe458f6a2c15ec239f015d174""><code>622cd7b</code></a> Add special casing for deserializing empty objects, lists and strings</li>; <li><a href=""https://github.com/ijl/orjson/commit/5da14a00fed93dc55a5e01e4eba0e3d77b0a89fc""><code>5da14a0</code></a> Add benchmark for loading empty objects</li>; <li><a href=""https://github.com/ijl/orjson/commit/12b867c7bfbd9c6404b2f2e859c134822af05e73""><code>12b867c</code></a> cargo update, nightly-2022-02-13</li>; <li><a href=""https://github.com/ijl/orjson/commit/ab633b6d0fa064b0c4b248bee8dc106",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11572:2114,error,error,2114,https://hail.is,https://github.com/hail-is/hail/pull/11572,1,['error'],['error']
Availability,ix_ir(SparkBackend.scala:687); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5137,Error,ErrorHandling,5137,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Error'],['ErrorHandling']
Availability,"ixtable.py in repartition(self, n_partitions, shuffle); 2505 Repartitioned dataset.; 2506 """"""; -> 2507 jvds = self._jvds.coalesce(n_partitions, shuffle); 2508 return MatrixTable(jvds); 2509 ; /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:2254,failure,failure,2254,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['failure'],['failure']
Availability,"ixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:2681,Error,Error,2681,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Error'],['Error']
Availability,"j/java_gateway.py in __call__(self, *args); 1302 ; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306 . /databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw); 115 def deco(*a, **kw):; 116 try:; --> 117 return f(*a, **kw); 118 except py4j.protocol.Py4JJavaError as e:; 119 converted = convert_exception(e.java_exception). /databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client); 325 if answer[1] == REFERENCE_TYPE:; --> 326 raise Py4JJavaError(; 327 ""An error occurred while calling {0}{1}{2}.\n"".; 328 format(target_id, ""."", name), value). Py4JJavaError: An error occurred while calling o504.pyPersistTable.; : is.hail.utils.HailException: 1 samples and 12 covariates (including x) implies -11 degrees of freedom.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:11); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:11); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.methods.LinearRegressionRowsSingle.execute(LinearRegression.scala:51); 	at is.hail.expr.ir.functions.WrappedMatrixToTableFunction.execute(RelationalFunctions.scala:51); 	at is.hail.expr.ir.TableToTableApply.execute(TableIR.scala:2936); 	at is.hail.expr.ir.TableIR.analyzeAndExecute(TableIR.scala:57); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:27); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyPersistTable$2(SparkBackend.scala:502); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteCont",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11413:3788,Error,ErrorHandling,3788,https://hail.is,https://github.com/hail-is/hail/issues/11413,1,['Error'],['ErrorHandling']
Availability,"java(self._jbackend.pyPersistTable(storage_level, self._to_java_table_ir(t._tir))); 286 ; 287 def unpersist_table(self, t):. /fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 14 import pyspark; 15 try:; ---> 16 return f(*args, **kwargs); 17 except py4j.protocol.Py4JJavaError as e:; 18 s = e.java_exception.toString(). /fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 334 raise Py4JError(; 335 ""An error occurred while calling {0}{1}{2}"".; --> 336 format(target_id, ""."", name)); 337 else:; 338 type = answer[1]. Py4JError: An error occurred while calling o1.pyPersistTable; --------------- end of error -------------------. The above is for running the GWAS tutorial. While running my own GWAS, I have encountered a similar error when running GWAS with the line :. gwas = hl.agg.linreg(hl.float(mt.phenos.height),; [1.0, mt.hapcounts0.x, mt.anc1dos.x, mt.anc2dos.x]). The error message is as follows:. -------------- start of error ------------------. ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38197); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 977, in _get_connection; connection = self.deque.pop(); IndexError: pop from an empty deque. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1115, in start; self.socket.connect((self.address, self",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9939:7638,error,error,7638,https://hail.is,https://github.com/hail-is/hail/issues/9939,1,['error'],['error']
Availability,java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1003:90,error,error,90,https://hail.is,https://github.com/hail-is/hail/issues/1003,1,['error'],['error']
Availability,"java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""domain"": ""global"",; ""reason"": ""forbidden""; }; ]; }; }. 	at is.hail.relocated.com.google.cloud.storage.StorageException.translate(StorageException.java:165) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:298) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.open(HttpStora",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:10585,error,error,10585,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['error'],['error']
Availability,java:1294); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343); 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911); 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); 	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); 	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); 	at java.lang.Thread.run(Thread.java:748); 	Error: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.; 	This stopped SparkContext was created at:; 	; 	org.apache.spark.SparkContext.getOrCreate(SparkContext.scala); 	sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	java.lang.reflect.Method.invoke(Method.java:498); 	sparklyr.Invoke.invoke(invoke.scala:139); 	sparklyr.StreamHandler.handleMethodCall(stream.scala:123); 	sparklyr.StreamHandler.read(stream.scala:66); 	sparklyr.BackendHandler.channelRead0(handler.scala:51); 	sparklyr.BackendHandler.channelRead0(handler.scala:4); 	io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105); 	io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357); 	io.netty.channel.AbstractChannelHandlerContext.invok,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:6735,Error,Error,6735,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['Error'],['Error']
Availability,java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:2909,Heartbeat,HeartbeatReceiver,2909,https://hail.is,https://github.com/hail-is/hail/issues/4780,1,['Heartbeat'],['HeartbeatReceiver']
Availability,"jobs/126. ```; io/test/test_batch.py::test_always_run_job_private_instance_cancel ; -------------------------------- live log setup --------------------------------; 2023-09-06T21:45:24 INFO test.conftest conftest.py:14:log_before_after starting test; 2023-09-06T21:45:24 INFO hailtop.aiocloud.aioazure.credentials credentials.py:99:default_credentials using credentials file /test-gsa-key/key.json; -------------------------------- live log call ---------------------------------; 2023-09-06T21:45:25 INFO azure.identity.aio._internal.get_token_mixin get_token_mixin.py:93:get_token ClientSecretCredential.get_token succeeded; 2023-09-06T21:45:25 INFO batch_client.aioclient aioclient.py:809:_submit created batch 191; submit job bunches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 1/1 0:00:00 0:00:00; 2023-09-06T21:47:17 WARNING hailtop.utils utils.py:842:retry_transient_errors_with_debug_string A transient error occured. We will automatically retry. Do not be alarmed. We have thus far seen 2 transient errors (next delay: 3.794s). The most recent error was <class 'asyncio.exceptions.TimeoutError'> . . +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ~~~~~~~~~~~~~~~~~~~~~ Stack of asyncio_0 (140387515627072) ~~~~~~~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++; FAILED; _________________ test_always_run_job_private_instance_cancel __________________. client = <hailtop.batch_client.client.BatchClient object at 0x7fae899806a0>. def test_always_run_job_private_instance_cancel(client: BatchClient):; b = create_batch(c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13582:1071,error,errors,1071,https://hail.is,https://github.com/hail-is/hail/issues/13582,1,['error'],['errors']
Availability,json parsing error on hl.literal([float('nan')]).value,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3785:13,error,error,13,https://hail.is,https://github.com/hail-is/hail/issues/3785,1,['error'],['error']
Availability,"jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **429/1000** <br/> **Why?** Has a fix available, CVSS 4.3 | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **501/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 4.3 | Generation of Error Message Containing Sensitive Information <br/>[SNYK-PYTHON-JUPYTERSERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14109:3526,avail,available,3526,https://hail.is,https://github.com/hail-is/hail/pull/14109,1,['avail'],['available']
Availability,"k broadcast_0_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 434.4 MiB); 2022-05-14 12:09:09 BlockManagerInfo: INFO: Added broadcast_0_piece0 in memory on 10.40.3.21:33951 (size: 3.2 KiB, free: 434.4 MiB); 2022-05-14 12:09:09 SparkContext: INFO: Created broadcast 0 from broadcast at SparkBackend.scala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:1639,Error,ErrorHandling,1639,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Error'],['ErrorHandling']
Availability,"k.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 18:18:30 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583:2172,Error,Error,2172,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583,1,['Error'],['Error']
Availability,"k.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5777683) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813750](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5914629](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Missing Cryptographic Step <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6036192](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6036192) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:7146,avail,available,7146,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"k.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5777683) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813750](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5914629](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Missing Cryptographic Step <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6036192](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6036192) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:7138,avail,available,7138,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,"kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB); Installing collected packages: zipp, wheel, tomli, packaging, click, pyproject_hooks, importlib-metadata, build, pip-tools; Successfully installed build-1.0.3 click-8.1.7 importlib-metadata-6.8.0 packaging-23.2 pip-tools-6.13.0 pyproject_hooks-1.0.0 tomli-2.0.1 wheel-0.41.2 zipp-3.17.0. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; + for package in '$@'; + reqs=python/requirements.txt; + pinned=python/pinned-requirements.txt; ++ mktemp; + new_pinned=/tmp/tmp.YoVBQEw8XF; ++ mktemp; + pinned_no_comments=/tmp/tmp.WRSKGgGEB8; ++ mktemp; + new_pinned_no_comments=/tmp/tmp.C8ggaXDHDt; + PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/aws/puppet/bin/:/home/hadoop/.local/bin:/home/hadoop/.local/bin; + pip-compile --quiet python/requirements.txt python/pinned-requirements.txt --output-file=/tmp/tmp.YoVBQEw8XF; WARNING: the legacy dependency resolver is",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:30400,Down,Downloading,30400,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Down'],['Downloading']
Availability,kBackend._execute(SparkBackend.scala:600); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:636); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:631); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:630); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:630); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88); 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); 	at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83); 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82); 	at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:822); 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); 	at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:794); 	at sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:199); 	at sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:544); 	at sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:509); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.130-bea04d9c79b5; Error summary: IllegalArgumentException: requirement failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14529:12837,Error,Error,12837,https://hail.is,https://github.com/hail-is/hail/issues/14529,1,['Error'],['Error']
Availability,"kUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'foo.py', '--cluster=dk', '--files=', '--py-files=/var/folders/cq/p_l4jm3x72j7wkxqxswccs180000gq/T/pyscripts_yg_wzlu0.zip', '--properties=']' ret",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:1475,avail,available,1475,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815,1,['avail'],['available']
Availability,"kage - Surefire/Maven - JDK 11 fails (Krishnan Mahadevan); Fixed: GITHUB:2788: TestResult.isSuccess() is TRUE when test fails due to expectedExceptions (Krishnan Mahadevan); Fixed: GITHUB-2800: Running Test Classes with Inherited <a href=""https://github.com/Factory""><code>@​Factory</code></a> and <a href=""https://github.com/DataProvider""><code>@​DataProvider</code></a> Annotated Non-Static Methods Fail (Krishnan Mahadevan); New: Ability to provide custom error message for assertThrows\expectThrows methods (Anatolii Yuzhakov); Fixed: GITHUB-2780: Use SpotBugs instead of abandoned FindBugs; Fixed: GITHUB-2801: JUnitReportReporter is too slow; Fixed: GITHUB-2807: buildStackTrace should be fail-safe (Sergey Chernov); Fixed: GITHUB-2830: TestHTMLReporter parameter toString should be fail-safe (Sergey Chernov); Fixed: GITHUB-2798: Parallel executions coupled with retry analyzer results in duplicate retry analyzer instances being created (Krishnan Mahadevan)</p>; <p>7.6.1; Fixed: GITHUB-2761: Exception: ERROR java.nio.file.NoSuchFileException: /tmp/testngXmlPathInJar-15086412835569336174 (Krishnan Mahadevan); 7.6.0; Fixed: GITHUB-2741: Show fully qualified name of the test instead of just the function name for better readability of test output.(Krishnan Mahadevan); Fixed: GITHUB-2725: Honour custom attribute values in TestNG default reports (Krishnan Mahadevan); Fixed: GITHUB-2726: <a href=""https://github.com/AfterClass""><code>@​AfterClass</code></a> config method is executed for EACH <a href=""https://github.com/Test""><code>@​Test</code></a> method when parallel == methods (Krishnan Mahadevan); Fixed: GITHUB-2752: TestListener is being lost when implenting both IClassListener and ITestListener (Krishnan Mahadevan); New: GITHUB-2724: DataProvider: possibility to unload dataprovider class, when done with it (Dzmitry Sankouski); Fixed: GITHUB-217: Configure TestNG to fail when there's a failure in data provider (Krishnan Mahadevan); Fixed: GITHUB-2743: SuiteRunner could not be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:12359,ERROR,ERROR,12359,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['ERROR'],['ERROR']
Availability,kingOutputStream.scala:23); 	at is.hail.io.index.IndexWriterUtils.close(IndexWriter.scala:225); 	at __C1756collect_distributed_array_table_native_writer.apply_region99_120(Unknown Source); 	at __C1756collect_distributed_array_table_native_writer.apply_region5_223(Unknown Source); 	at __C1756collect_distributed_array_table_native_writer.apply(Unknown Source); 	at __C1756collect_distributed_array_table_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$16(BackendUtils.scala:91); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90); 	at is.hail.backend.service.Worker$.$anonfun$main$9(Worker.scala:172); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:171); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.service.Worker$.main(Worker.scala:169); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.125-6e6f46797aed; Error summary: NullPointerException: null; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13937:10129,Error,Error,10129,https://hail.is,https://github.com/hail-is/hail/issues/13937,1,['Error'],['Error']
Availability,"kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which internal namespaces (dev and PR) are currently active. This requires relatively small changes to CI to track active namespaces but overall is a pretty small change. Note that this does not introduce a depend",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:3789,avail,available,3789,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['avail'],['available']
Availability,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1680,Error,Error,1680,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914,3,"['Error', 'error']","['Error', 'error']"
Availability,"kubernetes/kubernetes#105405</a>, <a href=""https://github.com/verb""><code>@​verb</code></a>)</li>; <li>Fix kube-proxy regression on UDP services because the logic to detect stale connections was not considering if the endpoint was ready. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106163"">kubernetes/kubernetes#106163</a>, <a href=""https://github.com/aojea""><code>@​aojea</code></a>) [SIG API Machinery, Apps, Architecture, Auth, Autoscaling, CLI, Cloud Provider, Contributor Experience, Instrumentation, Network, Node, Release, Scalability, Scheduling, Storage, Testing and Windows]</li>; <li>If a conflict occurs when creating an object with <code>generateName</code>, the server now returns an &quot;AlreadyExists&quot; error with a retry option. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104699"">kubernetes/kubernetes#104699</a>, <a href=""https://github.com/vincepri""><code>@​vincepri</code></a>)</li>; <li>Implement support for recovering from volume expansion failures (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106154"">kubernetes/kubernetes#106154</a>, <a href=""https://github.com/gnufied""><code>@​gnufied</code></a>) [SIG API Machinery, Apps and Storage]</li>; <li>In kubelet, log verbosity and flush frequency can also be configured via the configuration file and not just via command line flags. In other commands (kube-apiserver, kube-controller-manager), the flags are listed in the &quot;Logs flags&quot; group and not under &quot;Global&quot; or &quot;Misc&quot;. The type for <code>-vmodule</code> was made a bit more descriptive (<code>pattern=N,...</code> instead of <code>moduleSpec</code>). (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106090"">kubernetes/kubernetes#106090</a>, <a href=""https://github.com/pohly""><code>@​pohly</code></a>) [SIG API Machinery, Architecture, CLI, Cluster Lifecycle, Instrumentation, Node and Scheduling]</li>; <li>Introd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:4581,recover,recovering,4581,https://hail.is,https://github.com/hail-is/hail/pull/11957,2,"['failure', 'recover']","['failures', 'recovering']"
Availability,"kup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1164, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ---------------------------------------------------------------------------; Py4JError Traceback (most recent call last); <ipython-input-3-9b1033d0ac55> in <module>; ----> 1 t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]). <decorator-gen-1332> in linear_regression_rows(y, x, covariates, block_size, pass_through). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/methods/statgen.py in linear_regression_rows(y, x, covariates, block_size, pass_through); 434 ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}); 435 ; --> 436 return ht_result.persist(); 437 ; 438 . <decorator-gen-918> in persist(self, storage_level). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5559:2416,Error,Error,2416,https://hail.is,https://github.com/hail-is/hail/issues/5559,1,['Error'],['Error']
Availability,"l fixtures and tests to run in the same event loop, but have async fixtures that are set up and torn down for each module. If you're affected by this issue, please continue using the v0.21 release, until it is resolved.</p>; <h2>pytest-asyncio 0.23.4</h2>; <h1>0.23.4 (2024-01-28)</h1>; <ul>; <li>pytest-asyncio no longer imports additional, unrelated packages during test collection <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/729"">#729</a></li>; <li>Addresses further issues that caused an internal pytest error during test collection</li>; <li>Declares incompatibility with pytest 8 <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/737"">#737</a></li>; </ul>; <h2>pytest-asyncio 0.23.4a2</h2>; <h1>0.23.4 (UNRELEASED)</h1>; <ul>; <li>pytest-asyncio no longer imports additional, unrelated packages during test collection <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/729"">#729</a></li>; <li>Addresses further issues that caused an internal pytest error during test collection</li>; </ul>; <h2>Known issues</h2>; <p>As of v0.23, pytest-asyncio attaches an asyncio event loop to each item of the test suite (i.e. session, packages, modules, classes, functions) and allows tests to be run in those loops when marked accordingly. Pytest-asyncio currently assumes that async fixture scope is correlated with the new event loop scope. This prevents fixtures from being evaluated independently from the event loop scope and breaks some existing test suites (see <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/706"">#706</a>). For example, a test suite may require all fixtures and tests to run in the same event loop, but have async fixtures that are set up and torn down for each module. If you're affected by this issue, please continue using the v0.21 release, until it is resolved.</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14507:6001,error,error,6001,https://hail.is,https://github.com/hail-is/hail/pull/14507,1,['error'],['error']
Availability,"l last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<decorator-gen-468>"", line 2, in eval_typed; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 199, in eval_typed; return (Env.backend().execute(expression._ir), expression.dtype); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 86, in _to_java_ir; code = r(ir); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 511, in render; return '(ArrayLen {})'.format(r(self.a)); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 2003, in render; return f'(Literal {self._typ._parsable_string()} ' \; File ""/Users/wang/code/hail/hail/python/hail/utils/java.py"", line 159, in escape_str; return Env.jutils().escapePyString(s); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/Users/wang/code/hail/hail/python/hail/utils/java.py"", line 215, in deco; return f(*args, **kwargs); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 327, in get_return_value; py4j.protocol.Py4JError: An error occurred while calling o33.escapePyString; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:3928,error,error,3928,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184,1,['error'],['error']
Availability,"l-0g3aqft5/pyspark/; Complete output (47 lines):; Could not import pypandoc - required to package PySpark; WARNING: The wheel package is not available.; ERROR: Command errored out with exit status 1:; command: /Users/spascal/.pyenv/versions/3.7.9/bin/python3.7 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""'; __file__='""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-ggmq8ipk; cwd: /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/; Complete output (8 lines):; no pandoc found, building platform unspecific wheel...; use 'python setup.py download_pandoc' to download pandoc.; usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]; or: setup.py --help [cmd1 cmd2 ...]; or: setup.py --help-commands; or: setup.py cmd --help; ; error: invalid command 'bdist_wheel'; ----------------------------------------; ERROR: Failed building wheel for pypandoc; ERROR: Failed to build one or more wheels; Traceback (most recent call last):; File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/site-packages/setuptools/installer.py"", line 128, in fetch_build_egg; subprocess.check_call(cmd); File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/subprocess.py"", line 363, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/Users/spascal/.pyenv/versions/3.7.9/bin/python3.7', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/tmpspi2awj3', '--quiet', 'pypandoc']' returned non-zero exit status 1.; ; During handling of the above",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9742:2046,down,download,2046,https://hail.is,https://github.com/hail-is/hail/issues/9742,1,['down'],['download']
Availability,"l.backend.service.Main$.main(Main.scala:15); at is.hail.backend.service.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```; </details>. The code is simple and clearly is running against a path that does not already exist:; ```; if not hl.hadoop_exists(get_aou_util_path('mt_sample_qc')):; print('Run sample qc MT.....'); mt = hl.read_matrix_table(ACAF_MT_PATH); mt = mt.filter_rows(mt.locus.in_autosome()); # mt = mt.filter_rows(mt.locus.contig == 'chr1'); ht = hl.sample_qc(mt, name='mt_sample_qc'); ht.write(get_aou_util_path('mt_sample_qc'), overwrite=args.overwrite); ```. Job log: https://batch.hail.is/batches/8058522/jobs/171029. <details>; <summary>The last TableIR logged</summary>. ```; 2023-10-13 02:14:44.213 : INFO: after optimize: darrayLowerer, after LowerAndExecuteShuffles: IR size 232: . !ht = TableRead [Table{global:Struct{},key:[locus,alleles],row:Struct{locus:Locus(GRCh38),alleles:Array[String],filters:Set[String],a_index:Int32,was_split:Boolean,variant_qc:Struct{gq_stats:Struct{mean:Float64,stdev:Float64,min:Float64,max:Float6",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:6995,Error,Error,6995,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['Error'],['Error']
Availability,"l.log</summary>. ```; 2018-10-09 14:46:38 Hail: INFO: SparkUI: http://10.32.119.167:4040; 2018-10-09 14:46:38 Hail: INFO: Running Hail version devel-e7552fd55a9d; 2018-10-09 14:46:38 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 14:46:38 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 14:46:38 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@28f0ac7{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@49a30f89{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4495af6e{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6baf9f3b{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@562ad221{/static/sql,null,AVAILABLE,@Spark}; 2018-10-09 14:46:39 StateStoreCoordinatorRef: INFO: Registered StateStoreCoordinator endpoint; 2018-10-09 14:46:39 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:39 SparkSqlParser: INFO: Parsing command: SHOW TABLES; 2018-10-09 14:46:40 SparkContext: INFO: Starting job: collect at utils.scala:44; 2018-10-09 14:46:40 DAGScheduler: INFO: Got job 0 (collect at utils.scala:44) with 1 output partitions; 2018-10-09 14:46:40 DAGScheduler: INFO: Final stage: ResultStage 0 (collect at utils.scala:44); 2018-10-09 14:46:40 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 14:46:40 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 14:46:40 DAGSche",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:31900,AVAIL,AVAILABLE,31900,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['AVAIL'],['AVAILABLE']
Availability,"l/lib/python3.6/site-packages/hail/matrixtable.py"", line 2371, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:2517,failure,failure,2517,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['failure'],['failure']
Availability,"l/share/google/dataproc/bdutil/components/shared/docker.sh; /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/docker-ce.done; /tmp/dataproc/components/pre-uninstall/docker-ce.running; /tmp/dataproc/components/pre-uninstall/docker-ce.done; /etc/apt/preferences.d/docker-ce.pref; /etc/apt/preferences.d/docker-ce-cli.pref; /etc/apt/sources.list.d/docker.list; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_InRelease; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_stable_binary-amd64_Packages; ```. </details>. There is a `/run/docker.sock` but notice it is not `/var/run/...`. However, if I install Docker by hand into this worker of a *non-Hail* Dataproc cluster, it just works. ---. I also tried to replicate the failure using an initialization action, but that also just worked.; ```; gcloud dataproc clusters create dk-test2 --initialization-actions=gs://hail-common/dk-test.sh; ```; `gs://hail-common/dk-test.sh`:; ```; apt-get update; apt-get -y install \; apt-transport-https \; ca-certificates \; curl \; gnupg2 \; software-properties-common \; tabix; curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -; sudo add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable""; apt-get update; apt-get install -y --allow-unauthenticated docker-ce; ```. ---. Our users often report this error. In my experience, it has happened in 2/8 test_dataproc steps that I have run myself or seen run. The more workers you have, the higher the chance at least one worker fails. As @bpblanken suggested [here](https://github.com/hail-is/hail/issues/12936#issuecomment-1589956412), restarting docker on a failed worker works. Docker starts fine. However, I missed a subtlety: ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:13347,failure,failure,13347,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['failure'],['failure']
Availability,"l::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int8<16>; T = simdpp::arch_avx2::uint16<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int8<16>; T = simdpp::arch_avx2::uint16<8>]’; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:58:35: required from ‘simdpp::arch_avx2::int8<16>::int8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint16<8, simdpp::arch_avx2::expr_bit_or<simdpp::arch_avx2::uint16<8, simdpp::arch_avx2::uint16<8> >, simdpp::arch_avx2::uint16<8, simdpp::arch_avx2::uint16<8> > > >]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:47:36: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int8<16>’ with ‘private’ member ‘simdpp::arch_avx2::int8<16>::d_’ from an array of ‘const class simdpp::arch_avx2::uint16<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:33:7: note: ‘class simdpp::arch_avx2::int8<16>’ declared here; class int8<16, void> : public any_int8<16, int8<16,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::int8<32>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:60389,error,error,60389,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"lContext(); input_vcf = ""gs://seqr-hail/reference_data/GRCh38/1kg/ALL.GRCh38_sites.20170504.vcf.gz""; vds = hc.import_vcf(input_vcf, npartitions=1000, force=True); ```. causes. ```; FatalErrorTraceback (most recent call last); <ipython-input-4-5e86630fbae5> in <module>(); ----> 1 vds = hc.import_vcf(input_vcf, npartitions=1000, force=True). <decorator-gen-291> in import_vcf(self, path, force, force_bgz, header_file, npartitions, sites_only, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields). /home/hail/pyhail-hail-is-master-ebabd77.zip/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: IllegalArgumentException: Size exceeds Integer.MAX_VALUE. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, seqr-pipeline-cluster-grch38-w-0.c.seqr-project.internal): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitions",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806:1040,failure,failure,1040,https://hail.is,https://github.com/hail-is/hail/issues/1806,1,['failure'],['failure']
Availability,"l_fields, reference_genome, contig_recoding, array_elements_required, skip_invalid_loci); 1893 skip_invalid_loci,; 1894 force_bgz,; -> 1895 force; 1896 ); 1897 return MatrixTable(jmt). ~/bin/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.la",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775:2445,Error,ErrorHandling,2445,https://hail.is,https://github.com/hail-is/hail/issues/4775,1,['Error'],['ErrorHandling']
Availability,"l_idx, mt._row_idx]); ); # [col_idx, [row_idx]]; .group_by(lambda x: x[0]); # [[col_idx, row_idx), (col_idx, row_idx), ...], ...]; .values(); # [[row_idx, row_idx, ...], ...]; .map(lambda x: x.map(lambda y: y[0])); .flatmap(lambda x: hl.range(0, hl.len(x)); .flatmap(lambda i1: hl.range(i1 + 1, hl.len(x)); .map(lambda i2: hl.tuple([x[i1], x[i2]])))); ); ). mt.describe(). ht = mt.annotate_rows(; variant_pairs=hl.agg.take(mt.variant_pairs_entry, 1)[0]; ).rows(). ht = ht.explode('variant_pairs'); ht = ht.key_by(v1_idx=ht.variant_pairs[0], v2_idx=ht.variant_pairs[1]); return ht.select(). ht = variant_pairs_ht(mt, ['gene']; ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'v1_idx': int32 ; 'v2_idx': int32 ; ----------------------------------------; Key: ['v1_idx', 'v2_idx']; ----------------------------------------; ht.show(). ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-53-73b5a6c78295>"", line 1, in <module>; ht.show(); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1169, in show; print(self._show(n,width, truncate, types)); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1172, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/laurent/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3744:1473,error,error,1473,https://hail.is,https://github.com/hail-is/hail/issues/3744,1,['error'],['error']
Availability,"l` module. It's a collection of, well, experimental tools that we're working on but which are not yet ready for broad use. A couple years ago, the functionality which you seek was moved to [`hail.vds`](https://hail.is/docs/0.2/vds/index.html):; - `hl.vds.lgt_to_get`; - Densify has become `hl.vds.to_dense_mt`. You're not the first person to report some surprise at the disappearance of `hail.experimental` items. I've made a note to clarify the description of the `hail.experimental` module. If you do need the docs for the old version of Hail you can find them [in the code](https://github.com/hail-is/hail/blob/0.2.119/hail/python/hail/experimental/vcf_combiner/densify.py) though I understand that's a more frustrating viewing experience. Since you're referencing the old GVCF combiner, it sounds like you might be using what we call the old ""merged sparse representation"". In this representation, the reference and variant data is stored together in one Hail Matrix Table. We have found this representation error prone. Analysts often accidentally forget to properly handle the reference blocks. This representation is also slightly large than our new split representation: VDS. You can convert back and forth between VDS and the ""merged sparse representation"":; 1. VDS -> merged sparse representation: [`hl.vds.to_merged_sparse_representation(vds)`](https://hail.is/docs/0.2/vds/hail.vds.to_merged_sparse_mt.html#hail.vds.to_merged_sparse_mt) (but please note that the VDS does not necessarily store a reference allele for each reference block, so you might need to provide one [see details here](https://discuss.hail.is/t/vds-to-merged-sparse-matrix/3478/3)); 2. [`hl.vds.VariantDataset.from_merged_representation(sparse_mt)`](https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html#hail.vds.VariantDataset.from_merged_representation). We recommend that new projects use the VDS. It is a high-quality, durable solution which we've used to combine and analyze as many as 250k genomes and also ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13377#issuecomment-1666132086:1206,error,error,1206,https://hail.is,https://github.com/hail-is/hail/issues/13377#issuecomment-1666132086,1,['error'],['error']
Availability,"lable, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `7.34.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Access Control Bypass <br/>[SNYK-PYTHON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13718:2092,avail,available,2092,https://hail.is,https://github.com/hail-is/hail/pull/13718,3,['avail'],['available']
Availability,"lable, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `7.34.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Access Control Bypass <br/>[SNYK-PYTHON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13873:2160,avail,available,2160,https://hail.is,https://github.com/hail-is/hail/pull/13873,2,['avail'],['available']
Availability,"last); Cell In[32], line 1; ----> 1 hl.experimental.load_dataset(""gnomad_pca_variant_loadings"", version='2.1', reference_genome='GRCh38'). File ~/.local/lib/python3.8/site-packages/hail/experimental/datasets.py:115, in load_dataset(name, version, reference_genome, region, cloud); 107 raise ValueError(f'Region {repr(region)} not available for dataset'; 108 f' {repr(name)} on cloud platform {repr(cloud)}.\n'; 109 f'Available regions: {regions}.'); 111 path = [dataset['url'][cloud][region]; 112 for dataset in datasets[name]['versions']; 113 if all([dataset['version'] == version,; 114 dataset['reference_genome'] == reference_genome])]; --> 115 assert len(path) == 1; 116 path = path[0]; 117 if path.startswith('s3://'):. AssertionError: ; ```. I'm a new Hail user and don't have the full context here, but it seems like there are at least three problems:. 1. An assert failed in production code, which indicates either the presence of a bug or an incorrect use of assert (e.g. using assert to check for value errors).; 2. The assert has no corresponding error message, so the user learns that something has gone wrong but can't easily tell what.; 3. The assert is bare. Bare asserts can get optimized out of code in ways that are difficult to foresee in advance, and are generally deprecated in favor of the `if error_condition: raise AssertionError(...)` pattern (see: https://discuss.python.org/t/stop-ignoring-asserts-when-running-in-optimized-mode/13132). **The Big Picture**. The bare assert pattern is used over 3k times in Hail. To be fair, many of these usages occur in test directories, where they're fine. But they also occur in application code, and often in the dangerous form `assert(expr1, expr2)` which will never fail (because a tuple with two falsy elements is truthy in python). These asserts are never actually getting checked. . Fixing all of them would be a heavy lift. One compromise solution might be to add a bare assert rule to the linter (e.g. https://pypi.org/project/fl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12952:1257,error,errors,1257,https://hail.is,https://github.com/hail-is/hail/issues/12952,1,['error'],['errors']
Availability,"latform to convert from plink bed format to hail matrix format. . https://github.com/shengqh/warp/blob/develop/pipelines/vumc_biostatistics/genotype/VUMCBed2HailMatrix.wdl. code is pretty simple:. ```; import hail as hl. hl.init(spark_conf={""spark.driver.memory"": ""~{memory_gb}g""}). #contig_recoding is hard coded for human only; dsplink = hl.import_plink(bed=""~{source_bed}"",; bim=""~{source_bim}"",; fam=""~{source_fam}"",; reference_genome=""~{reference_genome}"",; contig_recoding={; '1': 'chr1',; '2': 'chr2',; '3': 'chr3',; '4': 'chr4',; '5': 'chr5',; '6': 'chr6',; '7': 'chr7',; '8': 'chr8',; '9': 'chr9',; '10': 'chr10',; '11': 'chr11',; '12': 'chr12',; '13': 'chr13',; '14': 'chr14',; '15': 'chr15',; '16': 'chr16',; '17': 'chr17',; '18': 'chr18',; '19': 'chr19',; '20': 'chr20',; '21': 'chr21',; '22': 'chr22',; 'X': 'chrX',; 'Y': 'chrY',; 'MT': 'chrM'}). dsplink.write(""~{target_prefix}"", overwrite=True); ```. When I tested it on the chr12 with 34523 samples and 18377527 variants from one of my dataset in Terra (100 g was allocated for this task), it failed with error message:. ```; java.lang.NegativeArraySizeException: null; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.resize(IdentityObjectIntMap.java:542); at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:306); at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:300); at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:162); at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:307); at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:300); at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:162); at com.esotericsoftware.kryo.util.MapReferenceResolver.addWrittenObject(MapReferenceResolver.java:41); at com.esotericsoftware.kryo.Kryo.writeReferenceOrNull(Kryo.java:681); at com.esotericsoftware.kryo.Kryo.write",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:1146,error,error,1146,https://hail.is,https://github.com/hail-is/hail/issues/14168,1,['error'],['error']
Availability,"ld 1: PCStruct{locus:PCLocus(GRCh37),alleles:PCArray[PCString],rsid:PCString,qual:PFloat64,filters:PCSet[PCString],info:PCStruct{NEGATIVE_TRAIN_SITE:PBoolean,HWP:PFloat64,AC:PCArray[PInt32],culprit:PCString,MQ0:PInt32,ReadPosRankSum:PFloat64,AN:PInt32,InbreedingCoeff:PFloat64,AF:PCArray[PFloat64],GQ_STDDEV:PFloat64,FS:PFloat64,DP:PInt32,GQ_MEAN:PFloat64,POSITIVE_TRAIN_SITE:PBoolean,VQSLOD:PFloat64,ClippingRankSum:PFloat64,BaseQRankSum:PFloat64,MLEAF:PCArray[PFloat64],MLEAC:PCArray[PInt32],MQ:PFloat64,QD:PFloat64,END:PInt32,DB:PBoolean,HaplotypeScore:PFloat64,MQRankSum:PFloat64,CCC:PInt32,NCC:PInt32,DS:PBoolean},s:PCString,GT:PCCall,AD:PCArray[+PInt32],DP:PInt32,GQ:PInt32,PL:PCArray[+PInt32]}. Child 2: PCStruct{locus:PCLocus(GRCh37),alleles:PCArray[PCString],rsid:PCString,qual:PFloat64,filters:PCSet[PCString],info:PCStruct{NEGATIVE_TRAIN_SITE:PBoolean,HWP:PFloat64,AC:PCArray[PInt32],culprit:PCString,MQ0:PInt32,ReadPosRankSum:PFloat64,AN:PInt32,InbreedingCoeff:PFloat64,AF:PCArray[PFloat64],GQ_STDDEV:PFloat64,FS:PFloat64,DP:PInt32,GQ_MEAN:PFloat64,POSITIVE_TRAIN_SITE:PBoolean,VQSLOD:PFloat64,ClippingRankSum:PFloat64,BaseQRankSum:PFloat64,MLEAF:PCArray[PFloat64],MLEAC:PCArray[PInt32],MQ:PFloat64,QD:PFloat64,END:PInt32,DB:PBoolean,HaplotypeScore:PFloat64,MQRankSum:PFloat64,CCC:PInt32,NCC:PInt32,DS:PBoolean},s:PCString,GT:PCCall,AD:PCArray[PInt32],DP:PInt32,GQ:PInt32,PL:PCArray[PInt32]}. No problems doing:. ```python; mt1 = hl.import_vcf(resource('sample.vcf'), array_elements_required=True); mt2 = hl.import_vcf(resource('sample_missing_pl2.vcf'), array_elements_required=False); mt1.entries().union(mt2.entries()).count(); ```. Also shows me that array_elements_required=True really does force requiredeness to be passed on the virtual type; by setting the array_elements_required=True on the vcf file with missing PL values, I can also trigger an error LoweringPipeline, suggesting that requiredeness (and other type information) is not overridden by reading over the data first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8008#issuecomment-581048722:2447,error,error,2447,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-581048722,1,['error'],['error']
Availability,"ld infrastructure current. The; Python versions supported for this release are 3.8-3.11.</p>; <h2>Contributors</h2>; <p>A total of 7 people contributed to this release. People with a &quot;+&quot; by; their names contributed a patch for the first time.</p>; <ul>; <li><a href=""https://github.com/DWesl""><code>@​DWesl</code></a></li>; <li>Aayush Agrawal +</li>; <li>Adam Knapp +</li>; <li>Charles Harris</li>; <li>Navpreet Singh +</li>; <li>Sebastian Berg</li>; <li>Tania Allard</li>; </ul>; <h2>Pull requests merged</h2>; <p>A total of 10 pull requests were merged for this release.</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22489"">#22489</a>: TST, MAINT: Replace most setup with setup_method (also teardown)</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22490"">#22490</a>: MAINT, CI: Switch to cygwin/cygwin-install-action@v2</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22494"">#22494</a>: TST: Make test_partial_iteration_cleanup robust but require leak...</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22592"">#22592</a>: MAINT: Ensure graceful handling of large header sizes</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22593"">#22593</a>: TYP: Spelling alignment for array flag literal</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22594"">#22594</a>: BUG: Fix bounds checking for <code>random.logseries</code></li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22595"">#22595</a>: DEV: Update GH actions and Dockerfile for Gitpod</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22596"">#22596</a>: CI: Only fetch in actions/checkout</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22597"">#22597</a>: BUG: Decrement ref count in gentype_reduce if allocated memory...</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12515:1426,robust,robust,1426,https://hail.is,https://github.com/hail-is/hail/pull/12515,1,['robust'],['robust']
Availability,"le, dest_reference_genome); 255; 256 def remove_liftover(self, name, dest_reference_genome):. /opt/conda/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py in __call__(self, *args); 1319; 1320 answer = self.gateway_client.send_command(command); -> 1321 return_value = get_return_value(; 1322 answer, self.gateway_client, self.target_id, self.name); 1323. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 63 tpl = Env.jutils().handleForPython(e.java_exception); 64 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 65 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 66 except pyspark.sql.utils.CapturedException as e:; 67 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: Chain file 'grch37_to_grch38.over.chain.gz' does not exist. Java stack trace:; is.hail.utils.HailException: Chain file 'grch37_to_grch38.over.chain.gz' does not exist.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.variant.ReferenceGenome.addLiftover(ReferenceGenome.scala:407); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$2(SparkBackend.scala:613); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$2$adapted(SparkBackend.scala:612); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$1(SparkBackend.scala:347); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$1(SparkBackend.scala:612); 	",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13993:3418,Error,ErrorHandling,3418,https://hail.is,https://github.com/hail-is/hail/issues/13993,1,['Error'],['ErrorHandling']
Availability,"leIR.scala:856); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:846); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:846); 	at is.hail.expr.ir.TableKeyBy.execute(TableIR.scala:237); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:838); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:42); 	at is.hail.table.Table.x$3$lzycompute(Table.scala:211); 	at is.hail.table.Table.x$3(Table.scala:211); 	at is.hail.table.Table.rvd$lzycompute(Table.scala:211); 	at is.hail.table.Table.rvd(Table.scala:211); 	at is.hail.table.Table.toDF(Table.scala:455); 	at sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.8-70304a52d33d; Error summary: AssertionError: assertion failed: type mismatch:; name: global; actual: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__uid_882:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; expect: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__cols:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5212:8060,Error,Error,8060,https://hail.is,https://github.com/hail-is/hail/issues/5212,1,['Error'],['Error']
Availability,"leQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-63d60cc; Error summary: HailException: invalid allele ""<DEL>"". ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413:9738,Error,Error,9738,https://hail.is,https://github.com/hail-is/hail/issues/3413,1,['Error'],['Error']
Availability,lect(RDD.scala:935); 	at is.hail.io.RichRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:847); 	at is.hail.variant.MatrixTable.write(MatrixTable.scala:2712); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: found non-left aligned variant: 18:76051965:C:G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.methods.SplitMultiPartitionContext.splitRow(SplitMulti.scala:98); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:226); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:225); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedJoinDistinctIterator.advanceRight1(OrderedJoinDistinctIterator.scala:36); 	at is.hail.sparkextras.OrderedJoinDistinctIterator.advanceRight(OrderedJoinDistinctIterator.scala:42); 	at is.hail.spa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3040:9697,Error,ErrorHandling,9697,https://hail.is,https://github.com/hail-is/hail/issues/3040,1,['Error'],['ErrorHandling']
Availability,"lectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```. 2. gs://hail-common/gencode_and_production_intervals.merged.hg19.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683:2843,Error,ErrorHandling,2843,https://hail.is,https://github.com/hail-is/hail/issues/1683,1,['Error'],['ErrorHandling']
Availability,led while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:234); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(OrderedRDD.scala:202); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(OrderedRDD.scala:195); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2407:12518,Error,ErrorHandling,12518,https://hail.is,https://github.com/hail-is/hail/issues/2407,1,['Error'],['ErrorHandling']
Availability,"leles'); return mt. def mwzj_hts_by_tree(all_hts, temp_dir, globals_for_col_key, debug=False, inner_mode = 'overwrite',; repartition_final: int = None):; chunk_size = int(len(all_hts) ** 0.5) + 1; outer_hts = []. checkpoint_kwargs = {inner_mode: True}; if repartition_final is not None:; intervals = get_n_even_intervals(repartition_final); checkpoint_kwargs['_intervals'] = intervals. if debug: print(f'Running chunk size {chunk_size}...'); for i in range(chunk_size):; if i * chunk_size >= len(all_hts): break; hts = all_hts[i * chunk_size:(i + 1) * chunk_size]; if debug: print(f'Going from {i * chunk_size} to {(i + 1) * chunk_size} ({len(hts)} HTs)...'); try:; if isinstance(hts[0], str):; hts = list(map(lambda x: hl.read_table(x), hts)); ht = hl.Table.multi_way_zip_join(hts, 'row_field_name', 'global_field_name'); except:; if debug:; print(f'problem in range {i * chunk_size}-{i * chunk_size + chunk_size}'); _ = [ht.describe() for ht in hts]; raise; outer_hts.append(ht.checkpoint(f'{temp_dir}/temp_output_{i}.ht', **checkpoint_kwargs)); ht = hl.Table.multi_way_zip_join(outer_hts, 'row_field_name_outer', 'global_field_name_outer'); ht = ht.transmute(inner_row=hl.flatmap(lambda i:; hl.cond(hl.is_missing(ht.row_field_name_outer[i].row_field_name),; hl.range(0, hl.len(ht.global_field_name_outer[i].global_field_name)); .map(lambda _: hl.null(ht.row_field_name_outer[i].row_field_name.dtype.element_type)),; ht.row_field_name_outer[i].row_field_name),; hl.range(hl.len(ht.global_field_name_outer)))); ht = ht.transmute_globals(inner_global=hl.flatmap(lambda x: x.global_field_name, ht.global_field_name_outer)); mt = ht._unlocalize_entries('inner_row', 'inner_global', globals_for_col_key); return mt. all_hts = list(map(lambda x: unify_saige_ht_schema(hl.read_table(x)), all_variant_outputs)); mt = join_pheno_hts_to_mt(all_hts, row_keys, col_keys, pheno_dict, f'{temp_bucket}/{pop}/variant',; inner_mode=inner_mode, repartition_final=20000); mt = mt.annotate_cols(saige_heritability=heri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8338:1734,checkpoint,checkpoint,1734,https://hail.is,https://github.com/hail-is/hail/issues/8338,1,['checkpoint'],['checkpoint']
Availability,"ler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:13513,AVAIL,AVAILABLE,13513,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"ler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.comp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6721,AVAIL,AVAILABLE,6721,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,let's meet tomorrow either before lab meeting (~9:40) or after lunch to make a plan? There's definitely a problem with the way something is getting rebuilt. The extra failures should be things we can resolve as well,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4585#issuecomment-434121212:167,failure,failures,167,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-434121212,1,['failure'],['failures']
Availability,"level); 1677 Persisted table.; 1678 """"""; -> 1679 return Env.backend().persist_table(self, storage_level); 1680 ; 1681 def unpersist(self) -> 'Table':. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py in persist_table(self, t, storage_level); 107 ; 108 def persist_table(self, t, storage_level):; --> 109 return Table._from_java(self._to_java_ir(t._tir).pyPersist(storage_level)); 110 ; 111 def unpersist_table(self, t):. ~/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/utils/java.py in deco(*args, **kwargs); 213 import pyspark; 214 try:; --> 215 return f(*args, **kwargs); 216 except py4j.protocol.Py4JJavaError as e:; 217 s = e.java_exception.toString(). ~/.conda/envs/hail/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 334 raise Py4JError(; 335 ""An error occurred while calling {0}{1}{2}"".; --> 336 format(target_id, ""."", name)); 337 else:; 338 type = answer[1]. Py4JError: An error occurred while calling o51.pyPersist; ```. ```; (hail) -bash:uger-r7-c001:~ 1037 $ find /lib /lib64/ -regex '.*blas.*' -exec ls -al \{\} \;; lrwxrwxrwx. 1 root root 16 Dec 13 2017 /lib64/libblas.so.3.4 -> libblas.so.3.4.2; lrwxrwxrwx. 1 root root 16 Dec 13 2017 /lib64/libblas.so.3 -> libblas.so.3.4.2; lrwxrwxrwx. 1 root root 16 Dec 13 2017 /lib64/libblas.so -> libblas.so.3.4.2; -rwxr-xr-x. 1 root root 364808 Mar 2 2017 /lib64/libblas.so.3.4.2; ```. ```; (hail) -bash:uger-r7-c001:~ 1034 $ objdump -T /lib64/liblapack.so.3 | grep dgemm; 0000000000000000 DF *UND*	0000000000000000 dgemm_; (hail) -bash:uger-r7-c001:~ 1035 $ objdump -T /lib64/libblas.so.3 | grep dgemm; 0000000000018850 g DF .text	0000000000000935 Base dgemm_; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5559:5012,error,error,5012,https://hail.is,https://github.com/hail-is/hail/issues/5559,2,['error'],['error']
Availability,level.txt'; adding 'hail-0.2.120.dist-info/RECORD'; removing build/bdist.linux-x86_64/wheel; python3 -m pip install 'pip-tools==6.13.0' && bash ../check_pip_requirements.sh python; Requirement already satisfied: pip-tools==6.13.0 in /usr/local/lib/python3.8/site-packages (6.13.0); Requirement already satisfied: build in /usr/local/lib/python3.8/site-packages (from pip-tools==6.13.0) (0.10.0); Requirement already satisfied: click>=8 in /usr/local/lib/python3.8/site-packages (from pip-tools==6.13.0) (8.1.6); Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.8/site-packages (from pip-tools==6.13.0) (23.2.1); Requirement already satisfied: setuptools in /usr/lib/python3.8/site-packages (from pip-tools==6.13.0) (38.4.0); Requirement already satisfied: wheel in /usr/local/lib/python3.8/site-packages (from pip-tools==6.13.0) (0.41.1); Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.8/site-packages (from build->pip-tools==6.13.0) (23.1); Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.8/site-packages (from build->pip-tools==6.13.0) (1.0.0); Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.8/site-packages (from build->pip-tools==6.13.0) (2.0.1); WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; + for package in '$@'; + reqs=python/requirements.txt; + pinned=python/pinned-requirements.txt; ++ mktemp; + new_pinned=/tmp/tmp.aWUFJ1BMnP; ++ mktemp; + pinned_no_comments=/tmp/tmp.9UoEuYguOd; ++ mktemp; + new_pinned_no_comments=/tmp/tmp.88KxU992pQ; + PATH=/sbin:/bin:/usr/sbin:/usr/bin:/root/.local/bin; + pip-compile --quiet python/requirements.txt python/pinned-requirements.txt --output-file=/tmp/tmp.aWUFJ1BMnP; ../check_pip_requirements.sh: line 13: pip-compile: command not found; make: *** [check-pip-lockfile] Error 127; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13445:15240,Error,Error,15240,https://hail.is,https://github.com/hail-is/hail/issues/13445,1,['Error'],['Error']
Availability,"lf.name); 1258 ; 1259 for temp_arg in temp_args:. ~/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 225 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 226 'Hail version: %s\n'; --> 227 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 228 except pyspark.sql.utils.CapturedException as e:; 229 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed: type mismatch:; name: global; actual: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__uid_882:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; expect: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__cols:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}. Java stack trace:; is.hail.utils.HailException: Error while typechecking IR:; (MakeStruct; (bn; (GetField bn; (Ref global)))); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:11); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:16); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:45); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:32); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:77); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:856); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:846); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:846); 	at is.hail.expr.ir.TableKeyBy.execute(TableIR.scala:237); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:838); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.Interpret$.apply(Interpret.sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5212:3156,Error,Error,3156,https://hail.is,https://github.com/hail-is/hail/issues/5212,1,['Error'],['Error']
Availability,"lhttp_ to v8.1.1 -- by :user:<code>webknjaz</code>; and :user:<code>Dreamsorcerer</code>.</p>; <p>Thanks to :user:<code>sethmlarson</code> for reporting this and providing us with; comprehensive reproducer, workarounds and fixing details! For more; information, see; <a href=""https://github.com/aio-libs/aiohttp/security/advisories/GHSA-45c4-8wx5-qw6w"">https://github.com/aio-libs/aiohttp/security/advisories/GHSA-45c4-8wx5-qw6w</a>.</p>; <p>.. _llhttp: <a href=""https://llhttp.org"">https://llhttp.org</a></p>; <p><code>[#7346](https://github.com/aio-libs/aiohttp/issues/7346) &lt;https://github.com/aio-libs/aiohttp/issues/7346&gt;</code>_</p>; </li>; </ul>; <h2>Features</h2>; <ul>; <li>; <p>Added information to C parser exceptions to show which character caused the error. -- by :user:<code>Dreamsorcerer</code></p>; <p><code>[#7366](https://github.com/aio-libs/aiohttp/issues/7366) &lt;https://github.com/aio-libs/aiohttp/issues/7366&gt;</code>_</p>; </li>; </ul>; <h2>Bugfixes</h2>; <ul>; <li>; <p>Fixed a transport is :data:<code>None</code> error -- by :user:<code>Dreamsorcerer</code>.</p>; <p><code>[#3355](https://github.com/aio-libs/aiohttp/issues/3355) &lt;https://github.com/aio-libs/aiohttp/issues/3355&gt;</code>_</p>; </li>; </ul>; <hr />; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/9c13a52c21c23dfdb49ed89418d28a5b116d0681""><code>9c13a52</code></a> Bump aiohttp to v3.8.5 a security release</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/7c02129567bc4ec59be467b70fc937c82920948c""><code>7c02129</code></a>  Bump pypa/cibuildwheel to v2.14.1</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/135a45e9d655d56e4ebad78abe84f1cb7b5c62dc""><code>135a45e</code></a> Improve error messages from C parser (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7366"">#7366</a>) (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7380"">#7380</a>)</li>; <li><a href",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13270:2805,error,error,2805,https://hail.is,https://github.com/hail-is/hail/pull/13270,5,['error'],['error']
Availability,"li>1904_, [Windows]: <code>OpenProcess</code> fails with <code>ERROR_SUCCESS</code> due to; <code>GetLastError()</code> called after <code>sprintf()</code>. (patch by alxchk)</li>; <li>1913_, [Linux]: <code>wait_procs()</code>_ should catch <code>subprocess.TimeoutExpired</code>; exception.</li>; <li>1919_, [Linux]: <code>sensors_battery()</code>_ can raise <code>TypeError</code> on PureOS.</li>; <li>1921_, [Windows]: <code>swap_memory()</code>_ shows committed memory instead of swap.</li>; <li>1940_, [Linux]: psutil does not handle <code>ENAMETOOLONG</code> when accessing process; file descriptors in procfs. (patch by Nikita Radchenko)</li>; <li>1948_, <strong>[critical]</strong>: <code>memoize_when_activated</code> decorator is not thread-safe.; (patch by Xuehai Pan)</li>; <li>1953_, [Windows], <strong>[critical]</strong>: <code>disk_partitions()</code>_ crashes due to; insufficient buffer len. (patch by MaWe2019)</li>; <li>1965_, [Windows], <strong>[critical]</strong>: fix &quot;Fatal Python error: deallocating None&quot;; when calling <code>users()</code>_ multiple times.</li>; <li>1980_, [Windows]: 32bit / WoW64 processes fails to read <code>Process.name()</code>_ longer</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/giampaolo/psutil/commit/f1a54ad88527e0706fb8a88ad7daae80686acc62""><code>f1a54ad</code></a> pre-release</li>; <li><a href=""https://github.com/giampaolo/psutil/commit/d81e75e94a1dd2b8d64caa0e72c771a7196a5d15""><code>d81e75e</code></a> HISTORY.rst add hyperlinks pointing to psutil API doc (<a href=""https://github-redirect.dependabot.com/giampaolo/psutil/issues/2042"">#2042</a>)</li>; <li><a href=""https://github.com/giampaolo/psutil/commit/a7165b5c02670cb7c4886425dd3911dd1b1116a6""><code>a7165b5</code></a> add sponsorhips / supporters <a href=""https://github.com/indeedeng""><code>@​indeedeng</code></a> and <a href=""https://github.com/PySi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11459:3450,error,error,3450,https://hail.is,https://github.com/hail-is/hail/pull/11459,1,['error'],['error']
Availability,"li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1da8f078e22412475b694ce07b890148b8a5e4fc""><code>1da8f07</code></a> Add comment</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/9703f764df56c52626f7d6f44bca8b1d51312389""><code>9703f76</code></a> Use pooling connection manager instead of basic one</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/306172e4c6532e185c8a6a9998bca7d22d2d0c63""><code>306172e</code></a> Bump up version number to 5.2.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b9df0c0daa080450772c365f16a9406fe0ca607a""><code>b9df0c0</code></a> Document eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/05a4433770f7020ff845add9348bdc12c82793dd""><code>05a4433</code></a> Add eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/09d1eca91afbf21ace3672be24c68d9028ee1e33""><code>09d1eca</code></a> Document runAsync method</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/800e3df1647c5ce65bffdd25c3240dfa5244e6c5""><code>800e3df</code></a> Add runAsync method to download extension</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/80f04c6a46fe7df053ac55bcfc6f90ff74c4b873""><code>80f04c6</code></a> Bump up version number to 5.1.3</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/3.2.0...5.2.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=3.2.0&new-version=5.2.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:4277,down,download-task,4277,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['down'],['download-task']
Availability,"li>Remove kubescheduler.config.k8s.io/v1alpha1 (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89298"">kubernetes/kubernetes#89298</a>, <a href=""https://github.com/gavinfish""><code>@​gavinfish</code></a>) [SIG Scheduling]</li>; <li>Reserve plugins that fail to reserve will trigger the unreserve extension point (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92391"">kubernetes/kubernetes#92391</a>, <a href=""https://github.com/adtac""><code>@​adtac</code></a>) [SIG Scheduling and Testing]</li>; <li>Resolve regression in <code>metadata.managedFields</code> handling in update/patch requests submitted by older API clients (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91748"">kubernetes/kubernetes#91748</a>, <a href=""https://github.com/apelisse""><code>@​apelisse</code></a>)</li>; <li>Scheduler: optionally check for available storage capacity before scheduling pods which have unbound volumes (alpha feature with the new <code>CSIStorageCapacity</code> feature gate, only works for CSI drivers and depends on support for the feature in a CSI driver deployment) (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92387"">kubernetes/kubernetes#92387</a>, <a href=""https://github.com/pohly""><code>@​pohly</code></a>) [SIG API Machinery, Apps, Auth, Scheduling, Storage and Testing]</li>; <li>Seccomp support has graduated to GA. A new <code>seccompProfile</code> field is added to pod and container securityContext objects. Support for <code>seccomp.security.alpha.kubernetes.io/pod</code> and <code>container.seccomp.security.alpha.kubernetes.io/...</code> annotations is deprecated, and will be removed in v1.22. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91381"">kubernetes/kubernetes#91381</a>, <a href=""https://github.com/pjbgf""><code>@​pjbgf</code></a>) [SIG Apps, Auth, Node, Release, Scheduling and Testing]</li>; <li>ServiceAppProtocol feature gate is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:12776,avail,available,12776,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['avail'],['available']
Availability,"lib/python3.12/site-packages/hail/__init__.py"", line 2, in <module>; import pkg_resources; ModuleNotFoundError: No module named 'pkg_resources'; ```. It looks like in Python 3.12, the bundled setuptools was removed and new virtual environments will not have setuptools in them, it needs to be specifically installed through pip: https://github.com/python/cpython/issues/95299. This could be fixed either by adding `setuptools` to hail's requirements so that it will be installed when users install hail, or hail could remove usage of setuptools & its associated modules (`pkg_resources`) at runtime, as some other projects have done: https://github.com/TDAmeritrade/stumpy/issues/950. At a glance, the cleanest thing to do here may be to move off of the deprecated `pkg_resources` and to the recommended `importlib` if it has what you need: https://setuptools.pypa.io/en/latest/pkg_resources.html. I also have to admit that I discovered this while playing around with hail on a Raspberry Pi 4, so it is possible that something else broken caused this failure, but I believe I understand what's happening. Here's my full `pip freeze` for reference:. ```; (venv) (py312) alex@rpi400:~/hail $ pip freeze; aiodns==2.0.0; aiohttp==3.9.3; aiosignal==1.3.1; attrs==23.2.0; avro==1.11.3; azure-common==1.1.28; azure-core==1.30.1; azure-identity==1.15.0; azure-mgmt-core==1.4.0; azure-mgmt-storage==20.1.0; azure-storage-blob==12.19.1; bokeh==3.4.0; boto3==1.34.73; botocore==1.34.73; cachetools==5.3.3; certifi==2024.2.2; cffi==1.16.0; charset-normalizer==3.3.2; click==8.1.7; commonmark==0.9.1; contourpy==1.2.0; cryptography==42.0.5; decorator==4.4.2; Deprecated==1.2.14; dill==0.3.8; frozenlist==1.4.1; google-auth==2.29.0; google-auth-oauthlib==0.8.0; hail==0.2.128; humanize==1.1.0; idna==3.6; isodate==0.6.1; janus==1.0.0; Jinja2==3.1.3; jmespath==1.0.1; jproperties==2.1.1; MarkupSafe==2.1.5; msal==1.28.0; msal-extensions==1.1.0; msrest==0.7.1; multidict==6.0.5; nest-asyncio==1.6.0; numpy==1.26.4; oa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14428:1542,failure,failure,1542,https://hail.is,https://github.com/hail-is/hail/issues/14428,1,['failure'],['failure']
Availability,"lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8390:1612,down,download,1612,https://hail.is,https://github.com/hail-is/hail/issues/8390,1,['down'],['download']
Availability,"lib3/pull/3139&gt;</code>_)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/c9016bf464751a02b7e46f8b86504f47d4238784""><code>c9016bf</code></a> Release 1.26.17</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/01220354d389cd05474713f8c982d05c9b17aafb""><code>0122035</code></a> Backport GHSA-v845-jxx5-vc9f (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3139"">#3139</a>)</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/e63989f97d206e839ab9170c8a76e3e097cc60e8""><code>e63989f</code></a> Fix installing <code>brotli</code> extra on Python 2.7</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2e7a24d08713a0131f0b3c7197889466d645cc49""><code>2e7a24d</code></a> [1.26] Configure OS for RTD to fix building docs</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/57181d6ea910ac7cb2ff83345d9e5e0eb816a0d0""><code>57181d6</code></a> [1.26] Improve error message when calling urllib3.request() (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3058"">#3058</a>)</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/3c0148048a523325819377b23fc67f8d46afc3aa""><code>3c01480</code></a> [1.26] Run coverage even with failed jobs</li>; <li>See full diff in <a href=""https://github.com/urllib3/urllib3/compare/1.26.16...1.26.17"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.16&new-version=1.26.17)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13762:2063,error,error,2063,https://hail.is,https://github.com/hail-is/hail/pull/13762,5,['error'],['error']
Availability,"libs/hail-all-spark.jar --conf=spark.executor.extraClassPath=./hail-all-spark.jar; WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).; WARNING: Running pyspark from user-defined location.; Python 2.7.5 (default, Aug 4 2017, 00:39:18) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 17/10/19 08:45:43 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.2.0.cloudera1; /_/. Using Python version 2.7.5 (default, Aug 4 2017 00:39:18); SparkSession available as 'spark'.; >>> import hail; >>> hc = hail.HailContext(); log4j:ERROR setFile(null,false) call failed.; java.io.FileNotFoundException: hail.log (Permission denied); 	at java.io.FileOutputStream.open0(Native Method); 	at java.io.FileOutputStream.open(FileOutputStream.java:270); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:213); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:133); 	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294); 	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165); 	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104); 	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842); 	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768); 	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648); 	at org.apa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198:1176,ERROR,ERROR,1176,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198,1,['ERROR'],['ERROR']
Availability,"lient = <hailtop.batch_client.client.BatchClient object at 0x7f9cbc23b610>. async def test_callback(client):; import nest_asyncio # pylint: disable=import-outside-toplevel; ; nest_asyncio.apply(); ; app = web.Application(); callback_bodies = []; callback_event = asyncio.Event(); ; def url_for(uri):; host = os.environ['HAIL_BATCH_WORKER_IP']; port = os.environ['HAIL_BATCH_WORKER_PORT']; return f'http://{host}:{port}{uri}'; ; async def callback(request):; body = await request.json(); callback_bodies.append(body); callback_event.set(); return web.Response(); ; app.add_routes([web.post('/test', callback)]); runner = web.AppRunner(app); await runner.setup(); site = web.TCPSite(runner, '0.0.0.0', 5000); await site.start(); ; try:; token = secrets.token_urlsafe(32); b = create_batch(; client, callback=url_for('/test'), attributes={'foo': 'bar', 'name': 'test_callback'}, token=token; ); head = b.create_job('alpine:3.8', command=['echo', 'head']); b.create_job('alpine:3.8', command=['echo', 'tail'], parents=[head]); b.submit(); await asyncio.wait_for(callback_event.wait(), 5 * 60); callback_body = callback_bodies[0]; ; # verify required fields present; callback_body.pop('cost'); callback_body.pop('msec_mcpu'); callback_body.pop('time_created'); callback_body.pop('time_closed'); callback_body.pop('time_completed'); callback_body.pop('duration'); callback_body.pop('duration_ms'); callback_body.pop('cost_breakdown'); > assert callback_body == {; 'id': b.id,; 'user': 'test',; 'billing_project': 'test',; 'token': token,; 'state': 'success',; 'complete': True,; 'closed': True,; 'n_jobs': 2,; 'n_completed': 2,; 'n_succeeded': 2,; 'n_failed': 0,; 'n_cancelled': 0,; 'attributes': {'foo': 'bar', 'name': 'test_callback'},; }, callback_body; E AssertionError: {'attributes': {'client_job': '8051758-182', 'foo': 'bar', 'name': 'test_callback'}, 'billing_project': 'test', 'closed': True, 'complete': True, ...}; E assert {'id': 260, 'user': 'test', 'billing_project': 'test', 'token': 'dL_z32",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13739#issuecomment-1739224427:1178,echo,echo,1178,https://hail.is,https://github.com/hail-is/hail/pull/13739#issuecomment-1739224427,1,['echo'],['echo']
Availability,liftover error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5174:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/issues/5174,1,['error'],['error']
Availability,"likely that whole stage codegen has either (1) changed memory management in a way that uses more memory or (2) is newly lowering code that exposes a latent issue in memory management that uses too much (or leaks) memory.]. Reported by Ben Weisburd and Julia Goodrich. [Ben is] running the first step of readviz for gnomAD v4 and we are hitting a 137 error on a partition that includes a site that has 27374 alleles. His code is [here](https://github.com/broadinstitute/gnomad-readviz/blob/step1_optimizations/step1__select_samples.py). I was testing his code out on just that failing partition (just added mt = vds.variant_data._filter_partitions([41229])) and I was able to recreate the error using Hail 0.2.119 (this is what Ben was using when he hit the error on the full dataset). However, the first time I tried to recreate the error I was accidentally using a different version of Hail and it ran with no memory error. It seems that 0.2.117 runs without error, but 0.2.118 and 0.2.119 both hit the 137 error. I am currently rerunning these tests so I can get logs:. Test with Hail 0.2.118:. Cluster:; ```; hailctl dataproc start readviz-118 \; --requester-pays-allow-all \; --packages=""git+https://github.com/broadinstitute/gnomad_methods.git@main"",""git+https://github.com/broadinstitute/gnomad_qc.git@main"" \; --autoscaling-policy=max-20 \; --master-machine-type n1-highmem-16 \; --no-off-heap-memory \; --worker-machine-type n1-highmem-8 \; --max-idle 560m \; --labels gnomad_release=gnomad_v4,gnomad_v4_testing=readviz_test_118; ```; Command:; ```; hailctl dataproc submit readviz-118 /Users/jgoodric/PycharmProjects/gnomad-readviz/step1__select_samples.py --sample-metadata-tsv gs://gnomad-readviz/v4.0/gnomad.exomes.v4.0.metadata.tsv.gz --output-ht-path gs://gnomad-tmp/julia/readviz/gnomad.exomes.v4.0.readviz_crams.part_41229.hail_118.ht; Job Link: https://console.cloud.google.com/dataproc/jobs/4db24eb6f93b491f8f07babc25c0d9c9/monitoring?region=us-central1&project=broad-mpg-gnomad; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13248:1168,error,error,1168,https://hail.is,https://github.com/hail-is/hail/issues/13248,1,['error'],['error']
Availability,"line 555, in _async_execute; _, resp, timings = await self._rpc(; File ""/Users/rye/opt/anaconda3/lib/python3.9/site-packages/hail/backend/service_backend.py"", line 487, in _rpc; result_bytes = await retry_transient_errors(self._read_output, ir, iodir + '/out', iodir + '/in'); File ""/Users/rye/opt/anaconda3/lib/python3.9/site-packages/hailtop/utils/utils.py"", line 779, in retry_transient_errors; return await retry_transient_errors_with_debug_string('', 0, f, *args, **kwargs); File ""/Users/rye/opt/anaconda3/lib/python3.9/site-packages/hailtop/utils/utils.py"", line 792, in retry_transient_errors_with_debug_string; return await f(*args, **kwargs); File ""/Users/rye/opt/anaconda3/lib/python3.9/site-packages/hail/backend/service_backend.py"", line 515, in _read_output; raise reconstructed_error.maybe_user_error(ir); hail.utils.java.FatalError: GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/wes-bipolar-tmp-4day/o/bge-wave-1-VQSR%2FparallelizeAndComputeWithIndex%2FgCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=%2Fresult.2706?alt=media; No such object: wes-bipolar-tmp-4day/bge-wave-1-VQSR/parallelizeAndComputeWithIndex/gCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=/result.2706. Java stack trace:; is.hail.relocated.com.google.cloud.storage.StorageException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/wes-bipolar-tmp-4day/o/bge-wave-1-VQSR%2FparallelizeAndComputeWithIndex%2FgCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=%2Fresult.2706?alt=media; No such object: wes-bipolar-tmp-4day/bge-wave-1-VQSR/parallelizeAndComputeWithIndex/gCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=/result.2706; 	at is.hail.relocated.com.google.cloud.storage.StorageException.translate(StorageException.java:163); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:297); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.load(HttpStorageRpc.java:730); 	at is.hail.relocate",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409:4369,down,download,4369,https://hail.is,https://github.com/hail-is/hail/issues/13409,1,['down'],['download']
Availability,"lize/rich#2875</a></li>; <li>Fix rich.pretty.install breakage in iPython <a href=""https://redirect.github.com/Textualize/rich/issues/3013"">Textualize/rich#3013</a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/Textualize/rich/blob/master/CHANGELOG.md"">rich's changelog</a>.</em></p>; <blockquote>; <h2>[13.6.0] - 2023-09-30</h2>; <h3>Added</h3>; <ul>; <li>Added Python 3.12 to classifiers.</li>; </ul>; <h2>[13.5.3] - 2023-09-17</h2>; <h3>Fixed</h3>; <ul>; <li>Markdown table rendering issue with inline styles and links <a href=""https://redirect.github.com/Textualize/rich/issues/3115"">Textualize/rich#3115</a></li>; <li>Fix Markdown code blocks on a light background <a href=""https://redirect.github.com/Textualize/rich/issues/3123"">Textualize/rich#3123</a></li>; </ul>; <h2>[13.5.2] - 2023-08-01</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs assertion error</li>; </ul>; <h2>[13.5.1] - 2023-07-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fix tilde character (<code>~</code>) not included in link regex when printing to console <a href=""https://redirect.github.com/Textualize/rich/issues/3057"">Textualize/rich#3057</a></li>; </ul>; <h2>[13.5.0] - 2023-07-29</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs not expanding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13758:3351,error,error,3351,https://hail.is,https://github.com/hail-is/hail/pull/13758,2,['error'],['error']
Availability,"ll last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 608, in mark_complete\n json.dumps(pod_status.to_dict()))\n File \""/usr/lib/python3.6/json/__init__.py\"", line 231, in dumps\n return _default_encoder.encode(obj)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 199, in encode\n chunks = self.iterencode(o, _one_shot=True)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 257, in iterencode\n return _iterencode(o, 0)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 180, in default\n o.__class__.__name__)\nTypeError: Object of type 'datetime' is not JSON serializable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:21:06,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:21:41,499"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6707:2983,ERROR,ERROR,2983,https://hail.is,https://github.com/hail-is/hail/issues/6707,1,['ERROR'],['ERROR']
Availability,"ll last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 608, in mark_complete\n json.dumps(pod_status.to_dict()))\n File \""/usr/lib/python3.6/json/__init__.py\"", line 231, in dumps\n return _default_encoder.encode(obj)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 199, in encode\n chunks = self.iterencode(o, _one_shot=True)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 257, in iterencode\n return _iterencode(o, 0)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 180, in default\n o.__class__.__name__)\nTypeError: Object of type 'datetime' is not JSON serializable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:26:01,694"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: Object of type 'datetime' is not JSON serializable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 608, in mark_complete\n json.dumps(pod_status.to_dict()))\n File \""/usr/lib/python3.6/json/__init__.py\"", line 231, in dumps\n return _default_encoder.encode(obj)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 199, in encode\n chunks = self.iterencode(o, _one_shot=Tru",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6707:7648,ERROR,ERROR,7648,https://hail.is,https://github.com/hail-is/hail/issues/6707,1,['ERROR'],['ERROR']
Availability,"llelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test. vdf'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 4 times, most recent failure: Lost task 6.3 in stage 2.0 (TID 2 53, scc-q15.scc.bu.edu, executor 1): java.io.IOException: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:2537,Error,Error,2537,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['Error'],['Error']
Availability,"lm0m0000gn/T//ccuBKQk1.s:1675:no such instruction: `vmovaps %xmm0, (%r11,%rax)'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1686:no such instruction: `vpxor -16(%rsi), %xmm2,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1699:no such instruction: `vpsrlq $1, %xmm1,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1713:no such instruction: `vpor %xmm1, %xmm0,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1725:no such instruction: `vmovaps %xmm0, (%r14,%rax)'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1901:no such instruction: `vmovdqa LC1(%rip), %xmm2'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1916:no such instruction: `vpxor (%rdx), %xmm2,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1929:no such instruction: `vpsrlq $1, %xmm1,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1953:no such instruction: `vpor %xmm1, %xmm0,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1970:no such instruction: `vmovaps %xmm0, (%r8,%rax)'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1981:no such instruction: `vpxor -16(%r11), %xmm2,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1994:no such instruction: `vpsrlq $1, %xmm1,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:2008:no such instruction: `vpor %xmm1, %xmm0,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:2020:no such instruction: `vmovaps %xmm0, (%r9,%rax)'; make: *** [lib/darwin/libibs.dylib] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 12.99 secs; Johns-MacBook-Pro-7:hail farrell$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274:10935,Error,Error,10935,https://hail.is,https://github.com/hail-is/hail/issues/1274,2,"['Error', 'FAILURE']","['Error', 'FAILURE']"
Availability,"load/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **/1000** <br/> **Why?** | NULL Pointer Dereference <br/>[SNYK-PYTHON-NUMPY-2321964](https://snyk.io/vuln/SNYK-PYTHON-NUMPY-2321964) | `numpy:` <br> `1.21.3 -> 1.22.2` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **/1000** <br/> **Why?** | Buffer Overflow <br/>[SNYK-PYTHON-NUMPY-2321966](https://snyk.io/vuln/SNYK-PYTHON-NUMPY-2321966) | `numpy:` <br> `1.21.3 -> 1.22.2` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **/1000** <br/> **Why?** | Denial of Service (DoS) <br/>[SNYK-PYTHON-NUMPY-2321970](https://snyk.io/vuln/SNYK-PYTHON-NUMPY-2321970) | `numpy:` <br> `1.21.3 -> 1.22.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13871:3241,avail,available,3241,https://hail.is,https://github.com/hail-is/hail/pull/13871,1,['avail'],['available']
Availability,loading a second HailContext() does not produce an error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1137:51,error,error,51,https://hail.is,https://github.com/hail-is/hail/issues/1137,1,['error'],['error']
Availability,"local notebook works fine for me as well, looks to be just dataproc that's not working as expected. submitting that test command as a script finished in 36.2s. notebook is currently still hanging with this output (it's been 11 minutes):; ```; BokehJS 3.2.2 successfully loaded.; Initializing Hail with default parameters...; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; SPARKMONITOR_LISTENER: Started SparkListener for Jupyter Notebook; SPARKMONITOR_LISTENER: Port obtained from environment: 55989; SPARKMONITOR_LISTENER: Application Started: application_1695402030462_0001 ...Start Time: 1695402594764; Running on Apache Spark version 3.3.0; SparkUI available at http://notebook-slowdown-repro-m.c.broad-ctsa.internal:43055/; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.124-ee7fef6fc40d; LOGGING: writing to /home/hail/hail-20230922-1709-0.2.124-ee7fef6fc40d.log; [Stage 0:> (0 + 2) / 2]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13690#issuecomment-1731791216:738,avail,available,738,https://hail.is,https://github.com/hail-is/hail/issues/13690#issuecomment-1731791216,1,['avail'],['available']
Availability,lock down ci,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6091:5,down,down,5,https://hail.is,https://github.com/hail-is/hail/pull/6091,1,['down'],['down']
Availability,"looked into the failing tests concerning randomness, and tracked down the source of the failures:. - `ApplySeeded` is an `AbstractApplyNode`. this means the interpreter will try to ""memoize"" the function definition so as to generate it only once even if you call it in multiple places in the IR. ; - memoization is based on referential equality, so they need to be the exact same IR object in order for reuse to trigger; - the ""seeded function"" implementations used by the test suite will create a new randomness state per generated function; - the Interpret pipeline used to rewrite each ApplySeeded node to be different objects, but with your changes they are the same, so their functions get memoized, thus sharing the same state. im not entirely sure which of these was doing the wrong thing, but IMO your changes in this PR should not have introduced any problems",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7567#issuecomment-556266727:65,down,down,65,https://hail.is,https://github.com/hail-is/hail/pull/7567#issuecomment-556266727,2,"['down', 'failure']","['down', 'failures']"
Availability,looking at the failures.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3414#issuecomment-383174303:15,failure,failures,15,https://hail.is,https://github.com/hail-is/hail/pull/3414#issuecomment-383174303,1,['failure'],['failures']
Availability,looking into test failure,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2531#issuecomment-350003512:18,failure,failure,18,https://hail.is,https://github.com/hail-is/hail/pull/2531#issuecomment-350003512,1,['failure'],['failure']
Availability,"looks good to me, I downloaded the docs and poked around. Much improved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7047#issuecomment-530825854:20,down,downloaded,20,https://hail.is,https://github.com/hail-is/hail/pull/7047#issuecomment-530825854,1,['down'],['downloaded']
Availability,"looks like a compile error in TestUtils.scala. The rest looks good, will approve when tests pass",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5077#issuecomment-452945059:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/pull/5077#issuecomment-452945059,1,['error'],['error']
Availability,looks like a random failure in test_batch. I retried the build,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10058#issuecomment-781729645:20,failure,failure,20,https://hail.is,https://github.com/hail-is/hail/pull/10058#issuecomment-781729645,1,['failure'],['failure']
Availability,looks like a rebase error?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7134#issuecomment-558211252:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-558211252,1,['error'],['error']
Availability,"ls.CapturedException as e:; 230 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 20 times, most recent failure: Lost task 0.19 in stage 24.0 (TID 1813, lfrani-sw-hqb8.c.broad-mpg-gnomad.internal, executor 159): is.hail.utils.HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:2862,failure,failure,2862,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['failure'],['failure']
Availability,"ls.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:339); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:483); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:482); 	at sun.reflect.GeneratedMethodAccessor88.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.107-2387bb00ceee; Error summary: SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:. ```; [hail-20230803-1955-0.2.107-2387bb00ceee.log](https://github.com/hail-is/hail/files/12254716/hail-20230803-1955-0.2.107-2387bb00ceee.log); Log file attached.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:10846,Error,Error,10846,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,3,"['Error', 'failure']","['Error', 'failure']"
Availability,"lso saw it in test_dataproc. Cal also reported it.; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 56 in stage 4.0 failed 20 times, most recent failure: Lost task 56.19 in stage 4.0 (TID 48622) (jsealock-schema-sw-43bq.c.daly-neale-sczmeta.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 125; VEP Error output:; docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.; See 'docker run --help'. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.methods.VEP$.waitFor(VEP.scala:73); 	at is.hail.methods.VEP.$anonfun$execute$5(VEP.scala:231); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.hasNext(RichContextRDD.scala:69); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:1153,Error,ErrorHandling,1153,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['Error'],['ErrorHandling']
Availability,"lt_alleles()` which is actually fine. ```; In [13]: import hail as hl ; ...: mt = hl.balding_nichols_model(2, 5, 5) ; ...: mt2 = hl.balding_nichols_model(2, 5, 5) ; ...: mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af) ; Initializing Hail with default parameters...; 2020-07-28 10:40:36 WARN Utils:66 - Your hostname, wm06b-953 resolves to a loopback address: 127.0.0.1; using 192.168.0.54 instead (on interface en0); 2020-07-28 10:40:36 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2020-07-28 10:40:37 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 2020-07-28 10:40:37 WARN Hail:37 - This Hail JAR was compiled for Spark 2.4.5, running with Spark 2.4.1.; Compatibility is not guaranteed.; Running on Apache Spark version 2.4.1; SparkUI available at http://192.168.0.54:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.49-c6975678edc4; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20200728-1040-0.2.49-c6975678edc4.log; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; Traceback (most recent call last):; File ""<ipython-input-13-f638f6c0399a>"", line 4, in <module>; mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 1988, in __mul__; return self._bin_op_numeric(""*"", other); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/base_expression.py"", line 567, in _bin_op_numeric; return me._bin_op(name, other, ret_type); File ""/Users/dking/projects/hail/hail/python/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9163:1159,avail,available,1159,https://hail.is,https://github.com/hail-is/hail/issues/9163,1,['avail'],['available']
Availability,"luded in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - ci/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **471/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.7 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `41.0.2 -> 41.0.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **551/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `41.0.2 -> 41.0.3` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **471/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.7 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813750](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `41.0.2 -> 41.0.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13365:1258,avail,available,1258,https://hail.is,https://github.com/hail-is/hail/pull/13365,1,['avail'],['available']
Availability,"ly created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - batch/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiodocker 0.21.0 requires aiohttp, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14035:1072,avail,available,1072,https://hail.is,https://github.com/hail-is/hail/pull/14035,1,['avail'],['available']
Availability,"ly created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; jupyter 1.0.0 requires notebook, which is not installed.; beautifulsoup4 4.12.2 requires soupsieve, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Information Exposure Through Sent Data <br/>[SNYK-PYTHON-URLLIB3-5926907](https://snyk.io/vuln/SNYK-PYTHON-URLLIB3-5926907) | `urllib3:` <br> `1.26.16 -> 1.26.17` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJiNzM0YmY4Ni1mNzQyLTQyMjMtOWVlYS1lNGU3ZjN",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13772:1059,avail,available,1059,https://hail.is,https://github.com/hail-is/hail/pull/13772,1,['avail'],['available']
Availability,ly$20.apply(ContextRDD.scala:280); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-76c42fe; Error summary: ClassCastException: is.hail.codegen.generated.C29 cannot be cast to is.hail.asm4s.AsmFunction5; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:9561,Error,Error,9561,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,1,['Error'],['Error']
Availability,"ly(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:922); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:915); at is.hail.utils.package$.using(package.scala:577); at is.h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:11951,Error,ErrorHandling,11951,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['Error'],['ErrorHandling']
Availability,"ly(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/b09ec92a-49f4-4d16-ad6d-efc5a5805e92/05_variant_qc.py"", line 201, in <module>; cumcounts = {'step0': rt.aggregate(hl.agg.sum(hl.cond(rt.qccum.step0, 1, 0))),; File ""<decorator-gen-519>"", line 2, in aggregate; File ""/home/hail/hail.zip/hail/utils/java.py"", line 191, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3063:11910,failure,failure,11910,https://hail.is,https://github.com/hail-is/hail/issues/3063,1,['failure'],['failure']
Availability,"lying Javascript Engine; * Chromium: V8: libevent: https://stackoverflow.com/questions/25750884/are-there-significant-differences-between-the-chrome-browser-event-loop-versus-t; * Firefox: Spidermonkey: ?; * https://developer.mozilla.org/en-US/docs/Web/JavaScript/EventLoop#Event_loop. #### Using callbacks; ```js. # Callback-based; function asyncCall(arg, cb => {; const (err, result) = someSynchronousOperation();. cb(err, result);; }. asyncCall(arg,(r, err) => { if(err){ throw new Error(err); doSomething(r)} ); ```. #### Using async/await; Deeply nested callbacks are hard to follow. This is called ""callback hell"". To help combat this, JS, in both NodeJS and Web context, developed Promises. Promises flatten the callback tree. ```js; function asyncPromise(arg) {; return new Promise((resolve, reject) => {; const (err, result) = someSynchronousOperation();; ; if(err) {; reject(err);; return;; } ; ; resolve(result);; });; }. asyncPromise(arg).then( r => doSomething(r) ).catch( err => throw new Error(err) ); ```. This has one problem. Chaining promises leads to a potentially hard to follow chain of `.then` `.catch`. As in many other languages, the solution to ""transforming"" async call syntax to sync ones, is to color async functions with a ""async"" and ""await"" clauses. This can be used with any functions that return promises (but not those that just return a callback). Luckily again, JS libraries have been moving towards the Promise-land (sorry) for ~5 years, before Promises were in stdlib (bluebird). ```js; async function usePromise() {; const arg = someSyncOperation();; ; let result;; try {; result = await asyncPromise(arg);; } catch(e) {; // without wrapping catch, will just throw on reject(), unwinding the call stack; doSomethingWIthError(e) ; }; ; doStuffWithResult(result);; }; ```; ### React; What is a react component? A function that returns JSX. React components accept props (HTML attributes `<Component propName={propValue} />`); Stateless vs stateful components; ``",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:5641,Error,Error,5641,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['Error'],['Error']
Availability,"m 2.8.1 to 2.8.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/dateutil/dateutil/releases"">python-dateutil's releases</a>.</em></p>; <blockquote>; <h2>2.8.2</h2>; <h1>Version 2.8.2 (2021-07-08)</h1>; <h2>Data updates</h2>; <ul>; <li>Updated tzdata version to 2021a. (gh pr <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/1128"">#1128</a>)</li>; </ul>; <h2>Bugfixes</h2>; <ul>; <li>Fixed a bug in the parser where non-<code>ValueError</code> exceptions would be raised; during exception handling; this would happen, for example, if an; <code>IllegalMonthError</code> was raised in <code>dateutil</code> code. Fixed by Mark Bailey.; (gh issue <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/981"">#981</a>, pr <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/987"">#987</a>).</li>; <li>Fixed the custom <code>repr</code> for <code>dateutil.parser.ParserError</code>, which was not; defined due to an indentation error. (gh issue <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/991"">#991</a>, gh pr <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/993"">#993</a>)</li>; <li>Fixed a bug that caused <code>b'</code> prefixes to appear in parse_isodate exception; messages. Reported and fixed by Paul Brown (<a href=""https://github.com/pawl""><code>@​pawl</code></a>) (gh pr <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/1122"">#1122</a>)</li>; <li>Make <code>isoparse</code> raise when trying to parse times with inconsistent use of; <code>:</code> separator. Reported and fixed by <a href=""https://github.com/mariocj89""><code>@​mariocj89</code></a> (gh pr <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/1125"">#1125</a>).</li>; <li>Fixed <code>tz.gettz()</code> not returning local time when passed an empty string.; Reported by <a href=""https://github.com/labrys""><code>@​labrys</code><",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11518:1095,error,error,1095,https://hail.is,https://github.com/hail-is/hail/pull/11518,1,['error'],['error']
Availability,"m Poterba; * 038a6de7c - (45 minutes ago) refresh from batch (#4670) - Daniel King; ```. #4586 was never tested against 75d5e7560. This is bad. We can look at the log of statuses posted to GitHub:; ```; # curl -sSL api.github.com/repos/hail-is/hail/commits/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/statuses | less; ```; [46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt](https://github.com/hail-is/hail/files/2531246/46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt). Before the merge status goes in we see this one:; ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728320639,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MzIwNjM5"",; ""state"": ""success"",; ""description"": ""successful build"",; ""target_url"": ""https://storage.googleapis.com/hail-ci-0-1/ci/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/038a6de7ce33b218b8d45160f224cae1feaf1c5a/index.html"",; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:51:09Z"",; ""updated_at"": ""2018-10-30T18:51:09Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. and before that:. ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728220065,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MjIwMDY1"",; ""state"": ""pending"",; ""description"": ""build 38 pending. target: 038a6de7ce33"",; ""target_url"": null,; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:36:31Z"",; ""updated_at"": ""2018-10-30T18:36:31Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. I don't understand what this Kubernetes state is, but this EOF thing is being incorrectly reported by batch to CI as an error. I cannot get the old batch pod logs.; ```; # k logs -l app=batch -p ; Error from server (BadRequest): previous terminated container ""batch"" in pod ""batch-deployment-769554dd84-kszwm"" not found. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4685:2247,error,error,2247,https://hail.is,https://github.com/hail-is/hail/issues/4685,2,"['Error', 'error']","['Error', 'error']"
Availability,"m an array of ‘const class simdpp::arch_avx2::int16<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:24,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:91:7: note: ‘class simdpp::arch_avx2::uint32<8>’ declared here; class uint32<8, void> : public any_int32<8, uint32<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int8<16>; T = simdpp::arch_avx2::uint8<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int8<16>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int8<16>; T = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:62:35: required from ‘simdpp::arch_avx2::int8<16>& simdpp::arch_avx2::int8<16>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_zbytes16.h:48:11: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int8<16>’ with ‘private’ member ‘simdpp::arch_avx2::int8<16>::d_’ from an array of ‘const class simdpp::arch_avx2::uint8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:88701,Mask,MaskCastOverride,88701,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"m an array of ‘const class simdpp::arch_avx2::int16<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:104:7: note: ‘class simdpp::arch_avx2::uint16<8>’ declared here; class uint16<8, void> : public any_int16<8, uint16<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int8<16>; T = simdpp::arch_avx2::uint16<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int8<16>; T = simdpp::arch_avx2::uint16<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int8<16>; T = simdpp::arch_avx2::uint16<8>]’; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:58:35: required from ‘simdpp::arch_avx2::int8<16>::int8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint16<8, simdpp::arch_avx2::expr_bit_or<simdpp::arch_avx2::uint16<8, simdpp::arch_avx2::uint16<8> >, simdpp::arch_avx2::uint16<8, simdpp::arch_avx2::uint16<8> > > >]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:47:36: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int8<16>’ with ‘private’ member ‘simdpp::arch_avx2::int8<16>::d_’ from an array of ‘const class simdpp::arch_avx2::uint16<8>’; use assignment or copy-ini",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:59611,Mask,MaskCastOverride,59611,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"m an array of ‘const class simdpp::arch_avx2::int32<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:104:7: note: ‘class simdpp::arch_avx2::uint32<4>’ declared here; class uint32<4, void> : public any_int32<4, uint32<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::int32<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::int32<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::int32<8>]’; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:114:36: required from ‘simdpp::arch_avx2::uint32<8>& simdpp::arch_avx2::uint32<8>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int32<8, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_avg.h:250:8: required from ‘V simdpp::arch_avx2::detail::insn::v_emul_avg_i32(const V&, const V&) [with V = simdpp::arch_avx2::int32<8>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_avg.h:211:31: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint32<8>’ with ‘private’ member ‘simdpp::arch_avx2::uint32<8>::d_’",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:116240,Mask,MaskCastOverride,116240,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"m an array of ‘const class simdpp::arch_avx2::int64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:26,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:102:7: note: ‘class simdpp::arch_avx2::uint64<2>’ declared here; class uint64<2, void> : public any_int64<2, uint64<2,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::uint64<2>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::uint64<2>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::uint64<2>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:58:35: required from ‘simdpp::arch_avx2::int32<4>::int32(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint64<2>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:272:34: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int32<4>’ with ‘private’ member ‘simdpp::arch_avx2::int32<4>::d_’ from an array of ‘const class simdpp::arch_avx2::uint64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:72447,Mask,MaskCastOverride,72447,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"m an array of ‘const class simdpp::arch_avx2::int8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: ‘class simdpp::arch_avx2::uint8<16>’ declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::int16<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::int16<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::int16<8>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:129:36: required from ‘simdpp::arch_avx2::uint32<4>::uint32(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int16<8>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_reduce_mul.h:154:30: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint32<4>’ with ‘private’ member ‘simdpp::arch_avx2::uint32<4>::d_’ from an array of ‘const class simdpp::arch_avx2::int16<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/si",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:85157,Mask,MaskCastOverride,85157,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"m an array of ‘const class simdpp::arch_avx2::int8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:104:7: note: ‘class simdpp::arch_avx2::uint16<8>’ declared here; class uint16<8, void> : public any_int16<8, uint16<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::int16<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::int16<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::int16<8>]’; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:133:36: required from ‘simdpp::arch_avx2::uint16<8>& simdpp::arch_avx2::uint16<8>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int16<8, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:42:36: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint16<8>’ with ‘private’ member ‘simdpp::arch_avx2::uint16<8>::d_’ from an array of ‘const class simdpp::arch_avx2::int16<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:57781,Mask,MaskCastOverride,57781,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"m outside palette <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7515"">#7515</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Seek past the data when skipping a PSD layer <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7483"">#7483</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>ImageMath: Inline <code>isinstance</code> check <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7623"">#7623</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>; <li>Update actions/upload-artifact action to v4 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7619"">#7619</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Import plugins relative to the module <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7576"">#7576</a> [<a href=""https://github.com/deliangyang""><code>@​deliangyang</code></a>]</li>; <li>Translate encoder error codes to strings; deprecate <code>ImageFile.raise_oserror()</code> <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7609"">#7609</a> [<a href=""https://github.com/bgilbert""><code>@​bgilbert</code></a>]</li>; <li>Updated readthedocs to latest version of Python <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7611"">#7611</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Support reading BC4U and DX10 BC1 images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/6486"">#6486</a> [<a href=""https://github.com/REDxEYE""><code>@​REDxEYE</code></a>]</li>; <li>Optimize ImageStat.Stat.extrema <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7593"">#7593</a> [<a href=""https://github.com/florath""><code>@​florath</code></a>]</li>; <li>Handle pathlib.Path in FreeTypeFont <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7578"">#7578</a> [<a href=""https://github.com/radarhere""><c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:6569,error,error,6569,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['error'],['error']
Availability,"m.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.908 [ERROR] [system.err] Ran 7 tests in 83.523s. I noticed that a previous issue #209 from early last year had the exact same issue in a different context where the function `breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;` didn't exist. However, when I use spark 2.0.2, it is able to run almost al",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419:1401,ERROR,ERROR,1401,https://hail.is,https://github.com/hail-is/hail/issues/1419,1,['ERROR'],['ERROR']
Availability,"m/elastic/elasticsearch-hadoop) from 8.0.0 to 8.4.3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-30_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.4.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html</a></p>; <h2>Elasticsearch Hadoop 8.4.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html</a></p>; <h2>Elasticsearch Hadoop 8.4.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:1038,down,downloads,1038,https://hail.is,https://github.com/hail-is/hail/pull/12319,1,['down'],['downloads']
Availability,"m/elastic/elasticsearch-hadoop) from 8.4.3 to 8.6.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-20_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.6.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elast",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12601:1038,down,downloads,1038,https://hail.is,https://github.com/hail-is/hail/pull/12601,1,['down'],['downloads']
Availability,"m/elastic/elasticsearch-hadoop) from 8.4.3 to 8.6.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-20_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.6.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.1.html</a></p>; <h2>Elasticsearch Hadoop 8.6.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12623:1038,down,downloads,1038,https://hail.is,https://github.com/hail-is/hail/pull/12623,1,['down'],['downloads']
Availability,"m/jmsmkn""><code>@​jmsmkn</code></a> and <a href=""https://github.com/adamchainz""><code>@​adamchainz</code></a>!</li>; <li>Fix incorrect documentation for <a href=""https://www.curlylint.org/docs/rules/no_autofocus""><code>no_autofocus</code></a> and <a href=""https://www.curlylint.org/docs/rules/tabindex_no_positive""><code>tabindex_no_positive</code></a>.</li>; </ul>; <h2>v0.13.0 – Quality-of-life improvements</h2>; <h2><a href=""https://github.com/thibaudcolas/curlylint/releases/tag/v0.13.0"">v0.13.0</a> 2021-04-24</h2>; <p>This release comes with a blog post! Read on <a href=""https://www.curlylint.org/blog/quality-of-life-improvements"">Quality-of-life improvements</a>.</p>; <h3>Added</h3>; <ul>; <li>Implement --template-tags CLI flag (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/25"">#25</a>, <a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/77"">#77</a>).</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Add more descriptive error message for missing whitespace between HTML attributes (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/23#issuecomment-700622837"">#23 (comment)</a>, <a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/68"">#68</a>).</li>; <li>Move development dependencies from extras to separate <code>requirements.txt</code> (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/68"">#68</a>).</li>; <li>Declare support for Python 3.9.</li>; <li>Tentatively declare support for Python 3.10 (tested with <code>Python 3.10.0a6+</code>).</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix Python 3.10 deprecation warning by importing Iterable from collections.abc (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/68"">#68</a>).</li>; </ul>; <h2>v0.12.2</h2>; <h2><a href=""https://github.com/thibaudcolas/curlylint/releases/tag/v0.12.2"">v0.12.2</a> 2021-03-06</h2>; <h3>Fixed</h3>; <ul>; <li>The <code>image_alt</code> rule no longer ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11713:1697,error,error,1697,https://hail.is,https://github.com/hail-is/hail/pull/11713,1,['error'],['error']
Availability,make gs available in annotateglobal.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/552:8,avail,available,8,https://hail.is,https://github.com/hail-is/hail/issues/552,1,['avail'],['available']
Availability,make_table().to_pandas() error related to globals,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5212:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/issues/5212,1,['error'],['error']
Availability,"mand(Command.scala:259); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:91); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.utils.package$.time(package.scala:119); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:114); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:108); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:108); at org.broadinstitute.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 4, localhost): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1260:6218,failure,failure,6218,https://hail.is,https://github.com/hail-is/hail/issues/1260,1,['failure'],['failure']
Availability,mand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.71-f3a54b530979; Error summary: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:15660,Error,Error,15660,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['Error'],['Error']
Availability,"mary>; <p><em>Sourced from <a href=""https://github.com/michel-kraemer/gradle-download-task/releases"">de.undercouch.download's releases</a>.</em></p>; <blockquote>; <h2>5.3.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Downgrade slf4j to fix warning on console about missing slf4j provider</li>; <li>Allow <code>download</code> and <code>verify</code> extensions to be created on demand in custom tasks, so these tasks can be made compatible with Gradle's configuration cache (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/284"">#284</a>). Thanks to <a href=""https://github.com/liblit""><code>@​liblit</code></a> for testing!</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; <li>Improve documentation</li>; <li>Add integration tests for Gradle 6.9.3 and 7.6</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a0374fc7c895ae53309ea351e989571204e0ea5f""><code>a0374fc</code></a> Bump up version number to 5.3.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/612f57a382b8640cc730dc5e75d1c809e3e772bd""><code>612f57a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/291"">#291</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/53af1049f5514afe58e884d487d7c57dae47759d""><code>53af104</code></a> Bump http-cache-semantics from 4.1.0 to 4.1.1 in /screencast</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/398c14c05c6448b380ac35c6095598299c5e23c5""><code>398c14c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/15cf7eecfbc17d2466143828b9b69494c6cb6f2b""><code>15cf7ee</code></a> Bump up version number to 5.3.1-SNAPSHOT</li>; <li><a href=""https://git",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:1102,down,download-task,1102,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['down'],['download-task']
Availability,match error on ReadMatrix,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3840#issuecomment-401624633:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/pull/3840#issuecomment-401624633,1,['error'],['error']
Availability,"matically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - gear/requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; kubernetes-asyncio 19.15.1 requires aiohttp, which is not installed.; aiohttp-session 2.12.0 requires aiohttp, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Improper Limitation of a Pathname to a Restricted Directory (&#x27;Path Traversal&#x27;) <br/>[SNYK-PYTHON-AIOHTTP-6209406](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6209406) | `aiohttp:` <br> `3.8.6 -> 3.9.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **718/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-AIOHTTP-6209407](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6209407) | `aiohttp:` <br> `3.8.6 -> 3.9.2` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencie",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14220:1052,avail,available,1052,https://hail.is,https://github.com/hail-is/hail/pull/14220,1,['avail'],['available']
Availability,"matically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiohttp 3.8.5 requires frozenlist, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **611/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 6.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5914629](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `41.0.3 -> 41.0.4` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJmOTk4OWJlMC0yOWQ3LTQyYTctYTAzMC04NzljMTRmO",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13700:1052,avail,available,1052,https://hail.is,https://github.com/hail-is/hail/pull/13700,1,['avail'],['available']
Availability,maybe we can raise an error on array iteration instead of unsupported operation exception,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4081#issuecomment-410359446:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/issues/4081#issuecomment-410359446,1,['error'],['error']
Availability,"maybe we should check the result type in anything that could localize an unrealizable python data structure, and error? Places like collect/take/aggregate?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1694#issuecomment-365623725:113,error,error,113,https://hail.is,https://github.com/hail-is/hail/issues/1694#issuecomment-365623725,1,['error'],['error']
Availability,maybe we should throw a better error here?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7797#issuecomment-570617965:31,error,error,31,https://hail.is,https://github.com/hail-is/hail/issues/7797#issuecomment-570617965,1,['error'],['error']
Availability,"me and space since we're eventually going to deprecate those tables anyways. Because I don't touch those tables, we don't need to worry about modifying the client code and how the current billing information is calculated. How this migration works is there are 5 phases:; 1. Compute the expected number of attempts to process for format version < 3. ; 2. Divide the search space into chunks of size 100 attempts (empirically determined this was the best chunk size) and randomize the order of the chunks.; 3. Serially process 5000 chunks with only 10 out of the 100 records as a ""burn in period"" to avoid the birthday problem when trying to insert records in parallel.; 4. In parallel, process all the chunks with 10 way parallelism (empirically determined to max CPU for a 4 core db instance); 5. Do an audit of the results to make sure the attempt resources now has the correct number of rows and the billing is within $0.001 per job with the old way and new way of computing the billing. The tolerance of $0.001 was empirically determined. At a threshold of $0.0001, 33/30,000,000 attempts failed. I think this is good enough as there's always going to be rounding errors. I did not do an explicit audit in the code to make sure the other aggregated_*_resources tables did not change. I spot checked this was correct in my test database. To do the complete audit during the actual migration would take more time. I made sure all the inserts were idempotent. Please double check this. The inserts use a temporary table with an isolation level of read committed. The reason for this is because `INSERT INTO ... SELECT` locks the next gap lock if the isolation level is not read committed. Maybe what I did is overkill and it's no longer a problem with the new burn in period. https://dev.mysql.com/doc/refman/8.0/en/innodb-locks-set.html I'd be interested to hear @danking feedback on what the best query here is to allow parallelism. There are ~30 million attempts that need to be processed for hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11990:1863,toler,tolerance,1863,https://hail.is,https://github.com/hail-is/hail/pull/11990,1,['toler'],['tolerance']
Availability,"me directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/sub",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1580,avail,available,1580,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,1,['avail'],['available']
Availability,"mem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: All done; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. Notice:; 1. The total memory available on the machine is less than 52 GiB (= 53,248 MiB), indeed it is a full 1025 MiB below the advertised amount.; 2. Once all the components of the Dataproc cluster have started (but before any Hail Query jobs are submitted) the total memory available is already depleted to 42760 MiB. Recall that Hail allocates 41 GiB (= 41,984 MiB) to its JVM. This leaves the Python process and all other daemons on the system only 776 MiB of excess RAM. For reference `python3 -c 'import hail'` needs 206 MiB. ---. We must address this situation. It seems safe to assume that the system daemons will use a constant 9.5 GiB of RAM. Moreover the advertised RAM amount is at least 1 GiB larger than reality. I propose:; 1. The driver memory calculation in `hailctl dataproc` should take the advertised RAM amount, subtract 10.5 GiB, and then use 90% of the remaining value. For an n1-highmem-8, that reduces our allocation from 41 GiB to 37 GiB yielding an additional 4GiB to Python and deamon memory fluctuations.; 2. AoU RWB needs to review its memory settings for Spark driver nodes to ensure that the JVM is set to an appropriate maximum heap size. For what it's worth, I think the reason we didn't get an outcry",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790:2431,avail,available,2431,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790,1,['avail'],['available']
Availability,"ment>;; const good3 = () => <span><div>OK</div><span>GOOD!</span></span>;; ```. #### JSX naming conventions; 1. Lowercase components are just built-in html elements. i.e `<span>` is a an HTML `<span>` on output.; 2. Uppercase components are javascript functions. This makes composing components really simple. ```jsx; const CoolComponent = () => <span>Hello World</span>;. export default () => <CoolComponent>;; ```. You can pass state to these user-defined components, much like you would in HTML, using attributes. These attributes can have arbitrary names, except they must start with a lowercase letter, and follow camel-case convention. These attributes are called `props`. ```jsx; const CoolComponent = (props) => <span>Hello {props.name}!</span>;. export default () => <CoolComponent name=""Alex"">; #mounts <span>Hello Alex!</span> in DOM; ```. #### PureComponent / shallow watch; React's reconciler is triggered whenever this.setState is called, resulting in a walk down the descendent nodes, based on either the presence of that state variable as a ""prop"" (i.e `<MyComponent name={this.state.name}/>`), or its use directly within the component (i.e `{this.state.name === 'Alex' ? <div>Do stuff</div> : <div>Do other stuff</div>). To give the reconciler less work to do, when accepting objects as props, use a `<PureComponent>`. This will tell React to check the reference for diff, rather than deep value compare. Obviously much faster to do the latter. You can do even better than `PureComponent`. Use a regular `Component`, and specific a `shouldComponentUpdate() { }` method in that component. Within that method, write whatever checks needed, so that when a prop, or state changes, you return `true`, otherwise `false`. When true, the component will re-render. However, this allows you to react in a more fine-grained way, i.e instead of checking reference, check for the update of a specific property, or don't react to that object changing at all. Behind the scenes, PureComponent is in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:9143,down,down,9143,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['down'],['down']
Availability,"mespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); INFO | 2019-06-25 12:37:07,703 | batch.py | mark_complete:501 | no logs for batch-2554-job-4-main-cc8d4 due to previous error, rescheduling pod; INFO | 2019-06-25 12:37:07,730 | batch.py | _create_pod:205 | created pod name: batch-2554-job-4-main-vsk7h for job (2554, 4), task main; INFO | 2019-06-25 12:37:07,788 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instan",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:11043,error,error,11043,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['error'],['error']
Availability,monitor time-to-running for pods by tolerations,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6680:36,toler,tolerations,36,https://hail.is,https://github.com/hail-is/hail/issues/6680,1,['toler'],['tolerations']
Availability,more robust VCF parser,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/36:5,robust,robust,5,https://hail.is,https://github.com/hail-is/hail/issues/36,1,['robust'],['robust']
Availability,moved header down to use /header without recreating directory,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2133:13,down,down,13,https://hail.is,https://github.com/hail-is/hail/pull/2133,1,['down'],['down']
Availability,moved to https://discuss.hail.is/t/error-building-jar-for-hail-0-2-11/912,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5659#issuecomment-475210838:35,error,error-building-jar-for-hail-,35,https://hail.is,https://github.com/hail-is/hail/issues/5659#issuecomment-475210838,1,['error'],['error-building-jar-for-hail-']
Availability,"mt = mt.annotate_cols(**{f""PC{i + 1}"":hl.rand_unif(0, 10) for i in range(10)}); mt = mt.annotate_entries(x = hl.rand_unif(0, 3)); mt = mt.key_rows_by(mt.gene_set, mt.consequence_category); mt = mt.key_cols_by(mt.col_id); return mt. def permute_phenotypes(np_pheno, n_perms):. np_pheno[np_pheno == None] = 2; np_pheno = np_pheno.astype(int); np_pheno_mat = np.repeat(np_pheno, n_perms).reshape(np_pheno.size, n_perms); for i in range(np_pheno_mat.shape[1]):; np.random.shuffle(np_pheno_mat[:,i]). return(np_pheno_mat). def run_regressions_perm(mt, phenotypes, n_perms):. mt = mt.add_col_index(). init = True; for phenotype in phenotypes:. mt = mt.annotate_globals(; pheno_perm = permute_phenotypes(np.array(mt.phenotype_boolean[phenotype].collect()), n_perms)); mt = mt.annotate_globals(; pheno_perm = mt.pheno_perm.map(; lambda x: { hl.array([False, True, hl.null(hl.tbool)])[hl.int(x)] }; )); return mt. mt = make_fake_data(); res = run_regressions_perm(mt, phenotypes_BP, 100); res.show(); ```. error:. ```; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:331); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8325:1507,error,error,1507,https://hail.is,https://github.com/hail-is/hail/issues/8325,1,['error'],['error']
Availability,"n = self.deque.pop(); IndexError: pop from an empty deque. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1115, in start; self.socket.connect((self.address, self.port)); ConnectionRefusedError: [Errno 111] Connection refused; ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38197); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 977, in _get_connection; connection = self.deque.pop(); IndexError: pop from an empty deque. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1115, in start; self.socket.connect((self.address, self.port)); ConnectionRefusedError: [Errno 111] Connection refused; ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38197); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 977, in _get_connection; connection = self.deque.pop(); IndexError: pop from an empty deque. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1115, in start; self.socket.connect((self.address, self.port)); ConnectionRefusedError: [Errno 111] Connection refused; ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38197); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 977, in _get_connection; connection = self.deque.pop(); IndexError: pop from an empty deque",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9939:9360,ERROR,ERROR,9360,https://hail.is,https://github.com/hail-is/hail/issues/9939,2,['ERROR'],['ERROR']
Availability,"n array of ‘const class simdpp::arch_avx2::uint16<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:91:7: note: ‘class simdpp::arch_avx2::uint8<32>’ declared here; class uint8<32, void> : public any_int8<32, uint8<32,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint32<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint32<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint32<4>]’; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:133:36: required from ‘simdpp::arch_avx2::uint16<8>& simdpp::arch_avx2::uint16<8>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint32<4, simdpp::arch_avx2::expr_bit_and<simdpp::arch_avx2::uint32<4, simdpp::arch_avx2::uint16<8> >, simdpp::arch_avx2::uint32<4, simdpp::arch_avx2::uint32<4> > > >]’; libsimdpp-2.0-rc2/simdpp/detail/insn/unzip_lo.h:80:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint16<8>’ with ‘private’ member ‘simdpp::arch_avx2::uint16<8>::d_’ from an array of ‘const class simdpp::arch_avx",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:31784,Mask,MaskCastOverride,31784,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"n array of ‘const class simdpp::arch_avx2::uint16<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:22,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:33:7: note: ‘class simdpp::arch_avx2::int16<16>’ declared here; class int16<16, void> : public any_int16<16, int16<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int64<2>; T = simdpp::arch_avx2::uint64<2>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int64<2>; T = simdpp::arch_avx2::uint64<2>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int64<2>; T = simdpp::arch_avx2::uint64<2>]’; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:60:35: required from ‘simdpp::arch_avx2::int64<2>& simdpp::arch_avx2::int64<2>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint64<2>]’; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:53:73: required from ‘simdpp::arch_avx2::int64<2>::int64(const simdpp::arch_avx2::uint64<2, E>&) [with E = void]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:277:25: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int64<2>’ with ‘private’ member ‘simdpp::arch_avx2::int64<2>::d_’ from an array of ‘const class simdpp::arch_avx2::uin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:108091,Mask,MaskCastOverride,108091,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"n array of ‘const class simdpp::arch_avx2::uint16<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:24,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:91:7: note: ‘class simdpp::arch_avx2::uint32<8>’ declared here; class uint32<8, void> : public any_int32<8, uint32<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint32<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint32<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint32<4>]’; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:129:36: required from ‘simdpp::arch_avx2::uint64<2>& simdpp::arch_avx2::uint64<2>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint32<4, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:157:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint64<2>’ with ‘private’ member ‘simdpp::arch_avx2::uint64<2>::d_’ from an array of ‘const class simdpp::arch_avx2::uint32<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:39395,Mask,MaskCastOverride,39395,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"n array of ‘const class simdpp::arch_avx2::uint16<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: ‘class simdpp::arch_avx2::uint8<16>’ declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint8<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:129:36: required from ‘simdpp::arch_avx2::uint16<8>::uint16(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:69:58: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint16<8>’ with ‘private’ member ‘simdpp::arch_avx2::uint16<8>::d_’ from an array of ‘const class simdpp::arch_avx2::uint8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:6751,Mask,MaskCastOverride,6751,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"n array of ‘const class simdpp::arch_avx2::uint32<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: ‘class simdpp::arch_avx2::uint8<16>’ declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint8<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:129:36: required from ‘simdpp::arch_avx2::uint32<4>::uint32(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:101:58: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint32<4>’ with ‘private’ member ‘simdpp::arch_avx2::uint32<4>::d_’ from an array of ‘const class simdpp::arch_avx2::uint8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:10287,Mask,MaskCastOverride,10287,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"n array of ‘const class simdpp::arch_avx2::uint32<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:26,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:102:7: note: ‘class simdpp::arch_avx2::uint64<2>’ declared here; class uint64<2, void> : public any_int64<2, uint64<2,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint64<2>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint64<2>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint64<2>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:133:36: required from ‘simdpp::arch_avx2::uint32<4>& simdpp::arch_avx2::uint32<4>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint64<2, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:159:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint32<4>’ with ‘private’ member ‘simdpp::arch_avx2::uint32<4>::d_’ from an array of ‘const class simdpp::arch_avx2::uint64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:41231,Mask,MaskCastOverride,41231,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"n array of ‘const class simdpp::arch_avx2::uint64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: ‘class simdpp::arch_avx2::uint8<16>’ declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint8<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:125:36: required from ‘simdpp::arch_avx2::uint64<2>::uint64(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:150:58: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint64<2>’ with ‘private’ member ‘simdpp::arch_avx2::uint64<2>::d_’ from an array of ‘const class simdpp::arch_avx2::uint8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:13824,Mask,MaskCastOverride,13824,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"n array of ‘const class simdpp::arch_avx2::uint64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:104:7: note: ‘class simdpp::arch_avx2::uint32<4>’ declared here; class uint32<4, void> : public any_int32<4, uint32<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint32<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint32<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint32<8>]’; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:114:36: required from ‘simdpp::arch_avx2::uint64<4>& simdpp::arch_avx2::uint64<4>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint32<8, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:179:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint64<4>’ with ‘private’ member ‘simdpp::arch_avx2::uint64<4>::d_’ from an array of ‘const class simdpp::arch_avx2::uint32<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:43067,Mask,MaskCastOverride,43067,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"n array of ‘const class simdpp::arch_avx2::uint8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:104:7: note: ‘class simdpp::arch_avx2::uint16<8>’ declared here; class uint16<8, void> : public any_int16<8, uint16<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint32<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint32<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint32<4>]’; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:129:36: required from ‘simdpp::arch_avx2::uint8<16>::uint8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint32<4>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:101:42: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint8<16>’ with ‘private’ member ‘simdpp::arch_avx2::uint8<16>::d_’ from an array of ‘const class simdpp::arch_avx2::uint32<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:8519,Mask,MaskCastOverride,8519,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"n array of ‘const class simdpp::arch_avx2::uint8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:104:7: note: ‘class simdpp::arch_avx2::uint32<4>’ declared here; class uint32<4, void> : public any_int32<4, uint32<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint64<2>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint64<2>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint64<2>]’; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:129:36: required from ‘simdpp::arch_avx2::uint8<16>::uint8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint64<2>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:150:42: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint8<16>’ with ‘private’ member ‘simdpp::arch_avx2::uint8<16>::d_’ from an array of ‘const class simdpp::arch_avx2::uint64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:12056,Mask,MaskCastOverride,12056,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"n array of ‘const class simdpp::arch_avx2::uint8<32>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:27,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:91:7: note: ‘class simdpp::arch_avx2::uint64<4>’ declared here; class uint64<4, void> : public any_int64<4, uint64<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint16<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint16<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint16<16>]’; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:114:36: required from ‘simdpp::arch_avx2::uint8<32>& simdpp::arch_avx2::uint8<32>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint16<16, simdpp::arch_avx2::expr_bit_and<simdpp::arch_avx2::uint16<16, simdpp::arch_avx2::uint8<32> >, simdpp::arch_avx2::uint16<16, simdpp::arch_avx2::uint16<16> > > >]’; libsimdpp-2.0-rc2/simdpp/detail/insn/unzip_lo.h:56:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint8<32>’ with ‘private’ member ‘simdpp::arch_avx2::uint8<32>::d_’ from an array of ‘const class simdpp::arc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:29818,Mask,MaskCastOverride,29818,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"n issue in our environment when opening spark-shell because hail-all-spark.jar is bundled with scala-reflect that has missing classes. The ideal solution is to remove scala language packages as part of the shadow jar because these already come with the Spark distribution, but proposing to have them in-sync as a workaround for now. ```; $ spark-shell; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; ...; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Exception in thread ""main"" java.lang.NoSuchMethodError: 'scala.reflect.internal.settings.MutableSettings scala.reflect.internal.settings.MutableSettings$.SettingsOps(scala.reflect.internal.settings.MutableSettings)'; at scala.tools.nsc.interpreter.ILoop.$anonfun$chooseReader$1(ILoop.scala:914); at scala.tools.nsc.interpreter.ILoop.mkReader$1(ILoop.scala:920); at scala.tools.nsc.interpreter.ILoop.$anonfun$chooseReader$4(ILoop.scala:926); at scala.tools.nsc.interpreter.ILoop.$anonfun$chooseReader$3(ILoop.scala:926); at scala.tools.nsc.interpreter.ILoop.chooseReader(ILoop.scala:926); at org.apache.spark.repl.SparkILoop.$anonfun$process$1(SparkILoop.scala:138); ...; ```. The error is caused by the change in between 2.12.13 and 2.12.15 in the scala-reflect package that has an additional implicit class:; https://github.com/scala/scala/blob/v2.12.13/src/reflect/scala/reflect/internal/settings/MutableSettings.scala; https://github.com/scala/scala/blob/v2.12.15/src/reflect/scala/reflect/internal/settings/MutableSettings.scala#L88. See also this thread in zulipchat that I posted a while back:; https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/scala.20class.20files.20bundled.20in.20hail-all-spark.2Ejar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12951:1797,error,error,1797,https://hail.is,https://github.com/hail-is/hail/pull/12951,1,['error'],['error']
Availability,"n n1-highmem-8 driver. The cluster is created by hailctl with no custom driver settings; <details><summary>template for hailctl dataproc start</summary>. [Source](https://github.com/broadinstitute/gatk/blob/ah_var_store/scripts/variantstore/wdl/extract/run_in_hail_cluster.py#L36C1-L48C1). ```; hailctl dataproc start ; --autoscaling-policy={autoscaling_policy}; --worker-machine-type {worker_machine_type}; --region {region}; --project {gcs_project}; --service-account {account}; --num-master-local-ssds 1; --num-worker-local-ssds 1 ; --max-idle=60m; --max-age=1440m; --subnet=projects/{gcs_project}/regions/{region}/subnetworks/subnetwork; {cluster_name}; ```. </details>. I have the driver node syslogs as well as the Hail log file. For some reason all logs other than the Hail logs are missing from this file. We separately need to determine why all the Spark logs etc. are missing. Based on the syslog, after system start up and just before the Jupyter notebook starts, the system is already using ~8,500MiB:; ```; Nov 22 14:29:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 43808 of 52223 MiB (83.89%), swap free: 0 of 0 MiB ( 0.00%); ```; So, the effective maximum memory that Hail could possibly use is around 43808MiB. After the Notebook and Spark initialize we're down to 42,700 MiB (about ~1000MiB more in use).; ```; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. `hailctl` sets the VM RAM limit to 80% of the instance type's memory, so 80% * 52GiB = 42598MiB. This means the JVM is permitted to effectively use all the remaining memory. At time of sigkill the total memory allocated by the JVM was about 2000MiB below the max heap size. Note that the heap is contained within all memory allocated by the JVM.; ```; Nov 22 15:31:05 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 43 of 52223 MiB ( 0.08%), swap free: 0 of 0 MiB ( 0.00%); Nov 22 15:31:09 vds-cluster-91f3f4c1-b737-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1832531419:1192,avail,avail,1192,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832531419,1,['avail'],['avail']
Availability,"n$apply$2$$anonfun$apply$3.apply(RowStore.scala:767); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:766); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:766); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:763); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:763); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-abac611; Error summary: NumberFormatException: For input string: ""-66.2667,0,-25.4754""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:16237,Error,Error,16237,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Error'],['Error']
Availability,n$execute$4(SparkBackend.scala:546); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:542); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:541); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$3(SparkBackend.scala:368); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:364); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:541); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:51); 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); 	at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83); 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82); 	at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:822); 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); 	at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:794); 	at sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:199); 	at sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:544); 	at sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:509); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.125-c4e2880b3279; Error summary: FileNotFoundException: File not found: gs://danking/chr*.vcf; ```. ### Version. 0.2.124. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13915:7820,Error,Error,7820,https://hail.is,https://github.com/hail-is/hail/issues/13915,1,['Error'],['Error']
Availability,"n, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][""loadings""],; pca_af=pcloadings[sibs.row_key][""pca_af""]; ). mt = mt.filter_rows(hl.is_defined(mt.pca_loadings) & hl.is_defined(mt.pca_af) &; (mt.pca_af > 0) & (mt.pca_af < 1)). gt_norm = (mt.GT.n_alt_alleles() - 2 * mt.pca_af) / hl.sqrt(n_variants * 2 * mt.pca_af * (1 - mt.pca_af)). mt = mt.annotate_cols(scores=hl.agg.array_sum(mt.pca_loadings * gt_norm)). related_scores = mt.cols().select('scores'); ```. ### What went wrong (all error messages here, including the full java stack trace):; No error messages, but my pipeline craps out at the following lines:; ```; pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(); ```. `.count()` works instantaneously up until that `pcloadings.annotate()` line. After that line, it gets stuck at 0 of 1 tasks for ~20 minutes before I give up and cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3953:4374,error,error,4374,https://hail.is,https://github.com/hail-is/hail/issues/3953,2,['error'],['error']
Availability,"n-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 32.4/32.4 MB 48.4 MB/s eta 0:00:00; 919 | amazon-ebs: Preparing metadata (setup.py): started; 920 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 921 | amazon-ebs: Requirement already satisfied: boto3<2.0,>=1.17 in /usr/local/lib/python3.7/site-packages (1.24.78); 922 | amazon-ebs: Requirement already satisfied: botocore<2.0,>=1.20 in /usr/local/lib/python3.7/site-packages (1.27.78); 923 | amazon-ebs: Collecting decorator<5; 924 | amazon-ebs: Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); 925 | amazon-ebs: Collecting Deprecated<1.3,>=1.2.10; 926 | amazon-ebs: Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB); 927 | amazon-ebs: Collecting dill<0.4,>=0.3.1.1; 928 | amazon-ebs: Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB); 929 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.8/95.8 kB 15.3 MB/s eta 0:00:00; 930 | amazon-ebs: Collecting google-auth==1.27.0; 931 | amazon-ebs: Downloading google_auth-1.27.0-py2.py3-none-any.whl (135 kB); 932 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.6/135.6 kB 30.6 MB/s eta 0:00:00; 933 | amazon-ebs: Collecting google-cloud-storage==1.25.*; 934 | amazon-ebs: Downloading google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); 935 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 22.1 MB/s eta 0:00:00; 936 | amazon-ebs: Collecting humanize==1.0.0; 937 | amazon-ebs: Downloading humanize-1.0.0-py2.py3-none-any.whl (51 kB); 938 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.9/51.9 kB 14.6 MB/s eta 0:00:00; 939 | amazon-ebs: Collecting hurry.filesize==0.9; 940 | amazon-ebs: Downloading hurry.filesize-0.9.tar.gz (2.8 kB); 941 | amazon-ebs: Preparing metadata (setup.py): started; 942 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 943 | amazon-ebs: Collecting janus<1.1,>=0.6; 944 | amazon-ebs: Downloading janus-1.0.0-py3-none-any.whl (6.9 kB); 945 | amazon-ebs: Req",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:4401,Down,Downloading,4401,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"n.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-544bf8f; Error summary: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:16167,Error,Error,16167,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['Error'],['Error']
Availability,"n: devel-544bf8f. ### What you did: import vcf from delly; ```; import hail as hl; hl.init(default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); ```. ### What went wrong (all error messages here, including the full java stack trace):. The CN field is an integer. When the integer is -1 for CN, an error is generated. ./.:0,0,0:0:LowQual:0:0:0:**-1**:0:0:0:0. The header defines CN with this:. ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Read-depth based copy-number estimate for autosomal sites"">. ```; Hail version: devel-544bf8f; Error summary: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; ```; Here is the full log and exception..... ```; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-544bf8f; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(414 + 2) / 416]2018-04-15 14:38:32 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 34) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-552>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/matrixtable.py"", line 1935, in write; self._jvds.write(output, overwrite, _codec_spec); ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:1244,avail,available,1244,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['avail'],['available']
Availability,"n=2.10.0, branch=HEAD, revision=d20e84d0fb64aff2f62a977adc8cfb656da4e286)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:323 build_context=""(go=go1.12.5, user=root@a49185acd9b0, date=20190525-12:28:13)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:324 host_details=""(Linux 4.14.127+ #1 SMP Tue Jun 18 18:32:10 PDT 2019 x86_64 prometheus-0 (none))""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:325 fd_limits=""(soft=1048576, hard=1048576)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:326 vm_limits=""(soft=unlimited, hard=unlimited)""; level=info ts=2019-07-31T15:45:51.993Z caller=main.go:645 msg=""Starting TSDB ...""; level=info ts=2019-07-31T15:45:51.994Z caller=web.go:417 component=web msg=""Start listening for connections"" address=0.0.0.0:9090; level=info ts=2019-07-31T15:45:51.996Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563105600000 maxt=1563170400000 ulid=01DFTDRJHCX1S9B0KPJTG8CRGW; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563170400000 maxt=1563235200000 ulid=01DFWBK0336Z71ZCRRKS79T18P; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563235200000 maxt=1563300000000 ulid=01DFY9C92NRA1S7FDVHFRFMFPF; level=info ts=2019-07-31T15:45:51.998Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:3572,repair,repair,3572,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['repair'],['repair']
Availability,nAlleles assertion in htsjdkrecordReader has bad error msg,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/952:49,error,error,49,https://hail.is,https://github.com/hail-is/hail/issues/952,1,['error'],['error']
Availability,"n[T] -> (U -> Boolean) -> Gen[Unit]`; - A `Gen[Unit]` is a bit artificial because the test framework halts execution (presumably with an exception) when a counter-example is found. I instead prefer that `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Boolean]`; - Now `Prop.forAll` has the same type as `Gen.flatMap[Boolean]`. It seems the difference between `forAll` and `flatMap` is that `forAll` conceptually preforms a product operation while `flatMap` performs a sampling. However, I think they are, in reality, the same operation: sampling. The implementation for `GenProp3` looks like:. ``` scala; for (i <- 0 until p.count) {; val v1 = g1(p); val v2 = g2(p); val v3 = g3(p); val r = f(v1, v2, v3); if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; }; ```. Which could be re-written as:. ``` scala; for (i <- 0 until p.count) {; (for (v1 <- g1; v2 <- g2; v3 <- g3) {; if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; })(p); }; ```. The primary difference between `flatMap` and `forAll` seems to be in error reporting. We can fix this by noting `Gen[T]` is currently a Reader monad on `Parameters`. If we add a ""forAll stack"" to `Parameters` we could implement `forAll` as:. ``` scala; def forAll[T,U](gt: Gen[T], gu: T -> Gen[U]): Gen[U] =; for (t <- gt; u <- local(pushQuantified(t), gu(t)) yield u. def pushQuantified(x: Any)(Parameters p): Paramters =; new Parameters(p.rng, p.size, p.count, (x :: p.quanitifed)); ```. We complete the Reader monad transformation by adding the `local` operation to `class Gen[T]`. ``` scala; // in class Gen; def local(modify: Parameters -> Parameters, gu: Gen[U]): Gen[U] =; Gen { p => gu(modify(p)) }; ```. Finally, the `check` method can access this stack of quantified variables to provide a useful error message. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/400#issuecomment-238901220:1436,error,error,1436,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238901220,2,['error'],['error']
Availability,"nagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:4174,AVAIL,AVAILABLE,4174,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,"name, dest_reference_genome):. /opt/conda/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py in __call__(self, *args); 1319; 1320 answer = self.gateway_client.send_command(command); -> 1321 return_value = get_return_value(; 1322 answer, self.gateway_client, self.target_id, self.name); 1323. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 63 tpl = Env.jutils().handleForPython(e.java_exception); 64 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 65 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 66 except pyspark.sql.utils.CapturedException as e:; 67 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: Chain file 'grch37_to_grch38.over.chain.gz' does not exist. Java stack trace:; is.hail.utils.HailException: Chain file 'grch37_to_grch38.over.chain.gz' does not exist.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.variant.ReferenceGenome.addLiftover(ReferenceGenome.scala:407); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$2(SparkBackend.scala:613); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$2$adapted(SparkBackend.scala:612); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$1(SparkBackend.scala:347); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$1(SparkBackend.scala:612); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$1$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13993:3481,Error,ErrorHandling,3481,https://hail.is,https://github.com/hail-is/hail/issues/13993,1,['Error'],['ErrorHandling']
Availability,nce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGSched,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5501,Error,ErrorHandling,5501,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Error'],['ErrorHandling']
Availability,"nce_genome):. /opt/conda/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py in __call__(self, *args); 1319; 1320 answer = self.gateway_client.send_command(command); -> 1321 return_value = get_return_value(; 1322 answer, self.gateway_client, self.target_id, self.name); 1323. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 63 tpl = Env.jutils().handleForPython(e.java_exception); 64 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 65 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 66 except pyspark.sql.utils.CapturedException as e:; 67 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: Chain file 'grch37_to_grch38.over.chain.gz' does not exist. Java stack trace:; is.hail.utils.HailException: Chain file 'grch37_to_grch38.over.chain.gz' does not exist.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.variant.ReferenceGenome.addLiftover(ReferenceGenome.scala:407); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$2(SparkBackend.scala:613); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$2$adapted(SparkBackend.scala:612); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$1(SparkBackend.scala:347); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$1(SparkBackend.scala:612); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$1$adapted(SparkBacke",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13993:3502,Error,ErrorHandling,3502,https://hail.is,https://github.com/hail-is/hail/issues/13993,1,['Error'],['ErrorHandling']
Availability,"ncies and control upgrades.</p>; <ul>; <li>Changes: <a href=""https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-0"">https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-0</a></li>; <li>Milestone: <a href=""https://github.com/pallets/click/milestone/9?closed=1"">https://github.com/pallets/click/milestone/9?closed=1</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/click/blob/main/CHANGES.rst"">click's changelog</a>.</em></p>; <blockquote>; <h2>Version 8.1.3</h2>; <p>Released 2022-04-28</p>; <ul>; <li>Use verbose form of <code>typing.Callable</code> for <code>@command</code> and; <code>@group</code>. :issue:<code>2255</code></li>; <li>Show error when attempting to create an option with; <code>multiple=True, is_flag=True</code>. Use <code>count</code> instead.; :issue:<code>2246</code></li>; </ul>; <h2>Version 8.1.2</h2>; <p>Released 2022-03-31</p>; <ul>; <li>Fix error message for readable path check that was mixed up with the; executable check. :pr:<code>2236</code></li>; <li>Restore parameter order for <code>Path</code>, placing the <code>executable</code>; parameter at the end. It is recommended to use keyword arguments; instead of positional arguments. :issue:<code>2235</code></li>; </ul>; <h2>Version 8.1.1</h2>; <p>Released 2022-03-30</p>; <ul>; <li>Fix an issue with decorator typing that caused type checking to; report that a command was not callable. :issue:<code>2227</code></li>; </ul>; <h2>Version 8.1.0</h2>; <p>Released 2022-03-28</p>; <ul>; <li>; <p>Drop support for Python 3.6. :pr:<code>2129</code></p>; </li>; <li>; <p>Remove previously deprecated code. :pr:<code>2130</code></p>; <ul>; <li><code>Group.resultcallback</code> is renamed to <code>result_callback</code>.</li>; <li><code>autocompletion</code> parameter to <code>Command</code> is renamed to; <code>shell_complete</code>.</li>; <li><code>get_terminal_size</code> is removed, use; <code>shutil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11808:3003,error,error,3003,https://hail.is,https://github.com/hail-is/hail/pull/11808,1,['error'],['error']
Availability,"ncies on Cygwin <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7175"">#7175</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Improved checks in font_render <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7218"">#7218</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Change <code>grabclipboard()</code> to use PNG compression on macOS <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7219"">#7219</a> [<a href=""https://github.com/abey79""><code>@​abey79</code></a>]</li>; <li>Added PyPy 3.10 and removed PyPy 3.8 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7216"">#7216</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Added in_place argument to ImageOps.exif_transpose() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7092"">#7092</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Corrected error code <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7177"">#7177</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use &quot;not in&quot; <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7174"">#7174</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Only call text_layout once in getmask2 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7206"">#7206</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed calling putpalette() on L and LA images before load() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7187"">#7187</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Removed unused INT64 definition <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7180"">#7180</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Updated xz to 5.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:4115,error,error,4115,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['error'],['error']
Availability,"ndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho= with scratch directory '/batch/1c00c7157d4d41bcbf508f12d75329b1'; 2023-09-13 16:37:36.617 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-13 16:37:36.821 services: WARN: A limited retry error has occured. We will automatically retry 4 more times. Do not be alarmed. (next delay: 1938). The most recent error was javax.net.ssl.SSLException: Connection reset.; 2023-09-13 16:37:38.893 WorkerTimer$: INFO: readInputs took 2278.496020 ms.; 2023-09-13 16:37:38.893 : INFO: RegionPool: initialized for thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 : INFO: TaskReport: stage=0, partition=38854, attempt=0, peakBytes=65536, peakBytesReadable=64.00 KiB, chunks requested=0, cache hits=0; 2023-09-13 16:37:38.903 : INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Cau",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:2337,ERROR,ERROR,2337,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,1,['ERROR'],['ERROR']
Availability,"nder.java:165); 	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104); 	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842); 	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768); 	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648); 	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514); 	at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:440); 	at is.hail.HailContext$.configureLogging(HailContext.scala:132); 	at is.hail.HailContext$.apply(HailContext.scala:159); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-289>"", line 2, in __init__; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); File ""/opt/Software/hail/python/hail/java.py"", line 42, in hc; raise EnvironmentError('no Hail context initialized, create one first'); EnvironmentError: no Hail context initialized, create one first; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198:3372,Error,Error,3372,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198,1,['Error'],['Error']
Availability,ndering if the problem is actually workload-dependent and is based on the number of jobs / number of files. The GCS best practices states the initial capacity is 5000 read requests / second per bucket including list operations until the bucket has time to scale up its capacity. https://cloud.google.com/storage/docs/request-rate#best-practices. ```. ==============================================================================; DIAGNOSTIC RESULTS ; ==============================================================================. ------------------------------------------------------------------------------; Latency ; ------------------------------------------------------------------------------; Operation Size Trials Mean (ms) Std Dev (ms) Median (ms) 90th % (ms); ========= ========= ====== ========= ============ =========== ===========; Delete 0 B 5 43.1 6.4 40.9 50.9 ; Delete 1 KiB 5 44.2 12.7 42.5 58.1 ; Delete 100 KiB 5 44.7 10.4 42.8 56.3 ; Delete 1 MiB 5 41.5 3.7 40.2 45.7 ; Download 0 B 5 74.6 7.9 73.2 84.0 ; Download 1 KiB 5 84.3 15.9 80.6 103.4 ; Download 100 KiB 5 81.9 16.0 82.7 99.6 ; Download 1 MiB 5 90.6 6.5 94.5 96.8 ; Metadata 0 B 5 23.6 2.7 23.6 26.3 ; Metadata 1 KiB 5 25.5 2.1 26.9 27.4 ; Metadata 100 KiB 5 26.2 3.6 27.3 29.9 ; Metadata 1 MiB 5 24.0 3.7 23.3 28.4 ; Upload 0 B 5 98.1 16.6 95.5 117.9 ; Upload 1 KiB 5 116.7 21.8 115.5 142.1 ; Upload 100 KiB 5 116.5 17.8 115.1 135.1 ; Upload 1 MiB 5 168.2 18.5 179.6 185.6 . ------------------------------------------------------------------------------; Write Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Write throughput: 977.7 Mibit/s.; Parallelism strategy: both. ------------------------------------------------------------------------------; Read Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 G,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:1284,Down,Download,1284,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['Down'],['Download']
Availability,"ne 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 401, in read; raise IncompleteRead(self._fp_bytes_read, self.length_remaining); File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__; self.gen.throw(type, value, traceback)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8390:1843,down,download,1843,https://hail.is,https://github.com/hail-is/hail/issues/8390,1,['down'],['download']
Availability,"ne-any.whl (7.4 kB); 965 | amazon-ebs: Collecting requests==2.25.1; 966 | amazon-ebs: Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB); 967 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 15.6 MB/s eta 0:00:00; 968 | amazon-ebs: Requirement already satisfied: scipy<1.8,>1.2 in /usr/local/lib64/python3.7/site-packages (1.7.3); 969 | amazon-ebs: Requirement already satisfied: sortedcontainers==2.4.0 in /usr/local/lib/python3.7/site-packages (2.4.0); 970 | amazon-ebs: Collecting tabulate==0.8.9; 971 | amazon-ebs: Downloading tabulate-0.8.9-py3-none-any.whl (25 kB); 972 | amazon-ebs: Requirement already satisfied: tqdm==4.* in /usr/local/lib/python3.7/site-packages (4.64.1); 973 | amazon-ebs: Collecting uvloop==0.16.0; 974 | amazon-ebs: Downloading uvloop-0.16.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.8 MB); 975 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 113.0 MB/s eta 0:00:00; 976 | ==> amazon-ebs: ERROR: Ignored the following versions that require a different python version: 1.22.0 Requires-Python >=3.8; 1.22.0rc1 Requires-Python >=3.8; 1.22.0rc2 Requires-Python >=3.8; 1.22.0rc3 Requires-Python >=3.8; 1.22.1 Requires-Python >=3.8; 1.22.2 Requires-Python >=3.8; 1.22.3 Requires-Python >=3.8; 1.22.4 Requires-Python >=3.8; 1.23.0 Requires-Python >=3.8; 1.23.0rc1 Requires-Python >=3.8; 1.23.0rc2 Requires-Python >=3.8; 1.23.0rc3 Requires-Python >=3.8; 1.23.1 Requires-Python >=3.8; 1.23.2 Requires-Python >=3.8; 1.23.3 Requires-Python >=3.8; 1.4.0 Requires-Python >=3.8; 1.4.0rc0 Requires-Python >=3.8; 1.4.1 Requires-Python >=3.8; 1.4.2 Requires-Python >=3.8; 1.4.3 Requires-Python >=3.8; 1.4.4 Requires-Python >=3.8; 1.5.0 Requires-Python >=3.8; 1.5.0rc0 Requires-Python >=3.8; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11; 1.9.0 Require",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:7852,ERROR,ERROR,7852,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['ERROR'],['ERROR']
Availability,"ne: 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20) 	at is.hail.utils.package$.fatal(package.scala:26) 	at is.hail.utils.Context.wrapException(Context.scala:19) 	at is.hail.utils.WithContext.foreach(Context.scala:51) 	at is.hail.utils.TextTableReader$$anonfun$5$$anonfun$apply$2.apply(TextTableReader.scala:126) 	at is.hail.utils.TextTableReader$$anonfun$5$$anonfun$apply$2.apply(TextTableReader.scala:126) 	at scala.collection.Iterator$class.foreach(Iterator.scala:893) 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) 	at is.hail.utils.TextTableReader$$anonfun$5.apply(TextTableReader.scala:126) 	at is.hail.utils.TextTableReader$$anonfun$5.apply(TextTableReader.scala:122) 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797) 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:108) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) Caused by: is.hail.utils.HailException: expected 13 fields, but found 1 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9) 	at is.hail.utils.package$.fatal(package.scala:26) 	at is.hail.utils.TextTableReader$$anonfun$5$$anonfun$apply$2$$anonfun$apply$3.apply(TextTableReader.scala:129) 	at is.hail.utils.TextTableReader$$anonfun$5$$anonfun$apply$2$$anonfun$apply$3.apply(TextTableReader.scala:126) 	at is.hail.utils.WithContext.foreach(Context.scala:49) 	... 17 more; --. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4100:2069,Error,ErrorHandling,2069,https://hail.is,https://github.com/hail-is/hail/issues/4100,2,['Error'],['ErrorHandling']
Availability,"nect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg St",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5068,error,errors,5068,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,1,['error'],['errors']
Availability,"nect to the Java server (127.0.0.1:38197); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 977, in _get_connection; connection = self.deque.pop(); IndexError: pop from an empty deque. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1115, in start; self.socket.connect((self.address, self.port)); ConnectionRefusedError: [Errno 111] Connection refused; ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38197); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 977, in _get_connection; connection = self.deque.pop(); IndexError: pop from an empty deque. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1115, in start; self.socket.connect((self.address, self.port)); ConnectionRefusedError: [Errno 111] Connection refused; ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38197); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 977, in _get_connection; connection = self.deque.pop(); IndexError: pop from an empty deque. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1115, in start; self.socket.connect((self.address, self.port)); ConnectionRefusedError: [Errno 111] Connection refused; --------------- end of error -------------------. Again, it's related to java. Could you please help? Thanks so much!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9939:10656,ERROR,ERROR,10656,https://hail.is,https://github.com/hail-is/hail/issues/9939,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"ned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:58:35: required from ‘simdpp::arch_avx2::int32<4>::int32(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/detail/extract128.h:40:58: required from ‘simdpp::arch_avx2::int32x4 simdpp::arch_avx2::detail::extract128(const int32x8&) [with unsigned int s = 0; simdpp::arch_avx2::int32x4 = simdpp::arch_avx2::int32<4>; simdpp::arch_avx2::int32x8 = simdpp::arch_avx2::int32<8>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_reduce_add.h:356:49: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int32<4>’ with ‘private’ member ‘simdpp::arch_avx2::int32<4>::d_’ from an array of ‘const class simdpp::arch_avx2::uint8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:33:7: note: ‘class simdpp::arch_avx2::int32<4>’ declared here; class int32<4, void> : public any_int32<4, int32<4,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int64<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:123238,error,error,123238,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"ned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int64<2>; T = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:56:35: required from ‘simdpp::arch_avx2::int64<2>::int64(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/detail/extract128.h:42:58: required from ‘simdpp::arch_avx2::int64x2 simdpp::arch_avx2::detail::extract128(const int64x4&) [with unsigned int s = 0; simdpp::arch_avx2::int64x2 = simdpp::arch_avx2::int64<2>; simdpp::arch_avx2::int64x4 = simdpp::arch_avx2::int64<4>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_reduce_max.h:478:40: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int64<2>’ with ‘private’ member ‘simdpp::arch_avx2::int64<2>::d_’ from an array of ‘const class simdpp::arch_avx2::uint8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:26,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:33:7: note: ‘class simdpp::arch_avx2::int64<2>’ declared here; class int64<2, void> : public any_int64<2, int64<2,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int32<16>; T = simdpp::arch_avx2::uint32<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:127352,error,error,127352,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"ned?. [reporter's note: IIRC, exit code 137 indicates that the ""container"" in which the worker JVM was executing exceeded memory limits. It seems likely that whole stage codegen has either (1) changed memory management in a way that uses more memory or (2) is newly lowering code that exposes a latent issue in memory management that uses too much (or leaks) memory.]. Reported by Ben Weisburd and Julia Goodrich. [Ben is] running the first step of readviz for gnomAD v4 and we are hitting a 137 error on a partition that includes a site that has 27374 alleles. His code is [here](https://github.com/broadinstitute/gnomad-readviz/blob/step1_optimizations/step1__select_samples.py). I was testing his code out on just that failing partition (just added mt = vds.variant_data._filter_partitions([41229])) and I was able to recreate the error using Hail 0.2.119 (this is what Ben was using when he hit the error on the full dataset). However, the first time I tried to recreate the error I was accidentally using a different version of Hail and it ran with no memory error. It seems that 0.2.117 runs without error, but 0.2.118 and 0.2.119 both hit the 137 error. I am currently rerunning these tests so I can get logs:. Test with Hail 0.2.118:. Cluster:; ```; hailctl dataproc start readviz-118 \; --requester-pays-allow-all \; --packages=""git+https://github.com/broadinstitute/gnomad_methods.git@main"",""git+https://github.com/broadinstitute/gnomad_qc.git@main"" \; --autoscaling-policy=max-20 \; --master-machine-type n1-highmem-16 \; --no-off-heap-memory \; --worker-machine-type n1-highmem-8 \; --max-idle 560m \; --labels gnomad_release=gnomad_v4,gnomad_v4_testing=readviz_test_118; ```; Command:; ```; hailctl dataproc submit readviz-118 /Users/jgoodric/PycharmProjects/gnomad-readviz/step1__select_samples.py --sample-metadata-tsv gs://gnomad-readviz/v4.0/gnomad.exomes.v4.0.metadata.tsv.gz --output-ht-path gs://gnomad-tmp/julia/readviz/gnomad.exomes.v4.0.readviz_crams.part_41229.hail_118.ht; Jo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13248:993,error,error,993,https://hail.is,https://github.com/hail-is/hail/issues/13248,2,['error'],['error']
Availability,needs a bump. There's also a weird failure in the tutorial docs - seems like something is generating an `inf` that Python doesn't expect,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6742#issuecomment-519546529:35,failure,failure,35,https://hail.is,https://github.com/hail-is/hail/pull/6742#issuecomment-519546529,1,['failure'],['failure']
Availability,"needs bump, looks like random failure (shuffler was last test run)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9223#issuecomment-670168023:30,failure,failure,30,https://hail.is,https://github.com/hail-is/hail/pull/9223#issuecomment-670168023,1,['failure'],['failure']
Availability,"neric way. Tim's work in #7792 does a good job introducing behavior like this this specifically for `ArrayRef` nodes, but I want to add three things on top of that:. 1. I don't want to have to do as much custom per IR node work; 2. I don't want to send the entire python stack trace over py4j to scala for every node; 3. I don't want the user to see a Java stack trace in this scenarios. This first PR is a proof of concept that adds this behavior for the `Die` node, which will catch any errors generated by uses of `CaseBuilder.or_error`. Follow up PRs should change `ArrayRef` to work this way, as well as catch things like looking up a key in a dictionary but not finding it. In an ideal future, we'd bolt on some extra mechanism to give types to these errors, and we could throw a proper `IndexError` in the `ArrayRef` case or `KeyError` in the dictionary case. . It feels a little bit messy right now, open to suggestions. I don't love using `-1` as the ""no error"" situation, but I thought it was probably easier than dealing with optionals between python and scala. . To give an example of what it looks like, the error message for this script:. ```; import hail as hl. ht = hl.utils.range_table(10); ht = ht.annotate(foo = hl.nd.array([[1], [2], [3]])); ht = ht.annotate(bar = ht.foo[0:4, 12]); ht.collect(); ```. is. ```; Traceback (most recent call last):; File ""better_error_test.py"", line 6, in <module>; ht.collect(); File ""<decorator-gen-1103>"", line 2, in collect; File ""/Users/johnc/Code/hail/hail/python/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/johnc/Code/hail/hail/python/hail/table.py"", line 1903, in collect; return Env.backend().execute(e._ir); File ""/Users/johnc/Code/hail/hail/python/hail/backend/spark_backend.py"", line 325, in execute; raise HailUserError(message_and_trace) from None; hail.utils.java.HailUserError: Error summary: HailException: Index 12 is out of bounds for axis 1 with size 1; ------------; H",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9398:1148,error,error,1148,https://hail.is,https://github.com/hail-is/hail/pull/9398,1,['error'],['error']
Availability,"nformation. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very worried about that. I think it's fine and it helps simplify the architecture. It avoids entangling the monitor with the pools and the JPIM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:1357,down,downward,1357,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358,1,['down'],['downward']
Availability,"nfun$7$$anonfun$apply$8.apply(RVD.scala:218); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-e6de08e; Error summary: ClassCastException: is.hail.codegen.generated.C14 cannot be cast to is.hail.asm4s.AsmFunction2; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [3c5f402fed564ccd85257c0919d4bffb] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""pyhail.py"", line 128, in <module>; main(args, pass_through_args); File ""pyhail.py"", line 109, in main; subprocess.check_output(job); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 573, in check_output; raise CalledProcessError(retcode, cmd, output=output); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', '/Users/gtiao/gnomad_qc/hail/sample_qc/assign_subpops.py', '--cluster', 'gt1', '--files=gs://hail-common/builds/devel/jars/hail-devel-38dbf156b630-Spark-2.2.0.jar', '--py-files=gs://hail-common/builds/devel/python/hail-devel-38dbf156b630.zip,/var/folders/rn/t2xcx1ps4h96txll46qkkfsj2q8bnl/T/pyscripts_fYVAte.zip', '--p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:7465,Error,Error,7465,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['Error'],['Error']
Availability,"ng (XSS) <br/>[SNYK-PYTHON-NBCONVERT-2979829](https://snyk.io/vuln/SNYK-PYTHON-NBCONVERT-2979829) | `nbconvert:` <br> `5.6.1 -> 6.3.0b0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **434/1000** <br/> **Why?** Has a fix available, CVSS 4.4 | Open Redirect <br/>[SNYK-PYTHON-NOTEBOOK-1041707](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-1041707) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Information Exposure <br/>[SNYK-PYTHON-NOTEBOOK-2441824](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2441824) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **449/1000** <br/> **Why?** Has a fix available, CVSS 4.7 | Access Restriction Bypass <br/>[SNYK-PYTHON-NOTEBOOK-2928995](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2928995) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **696/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-1086606](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1086606) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-PYGMENTS-1088505](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1088505) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](ht",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:4383,avail,available,4383,https://hail.is,https://github.com/hail-is/hail/pull/13717,2,['avail'],['available']
Availability,"ng (XSS) <br/>[SNYK-PYTHON-NBCONVERT-2979829](https://snyk.io/vuln/SNYK-PYTHON-NBCONVERT-2979829) | `nbconvert:` <br> `5.6.1 -> 6.3.0b0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **434/1000** <br/> **Why?** Has a fix available, CVSS 4.4 | Open Redirect <br/>[SNYK-PYTHON-NOTEBOOK-1041707](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-1041707) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Information Exposure <br/>[SNYK-PYTHON-NOTEBOOK-2441824](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2441824) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **449/1000** <br/> **Why?** Has a fix available, CVSS 4.7 | Access Restriction Bypass <br/>[SNYK-PYTHON-NOTEBOOK-2928995](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2928995) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Race Condition <br/>[SNYK-PYTHON-PROMPTTOOLKIT-6141120](https://snyk.io/vuln/SNYK-PYTHON-PROMPTTOOLKIT-6141120) | `prompt-toolkit:` <br> `1.0.18 -> 3.0.13` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **696/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-1086606](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1086606) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | Proof of Concept ; ![high severity",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:5075,avail,available,5075,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['avail'],['available']
Availability,"ng Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; ```; ----------------------------; ```; >>> rdd = sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; ```; ----------------------------------; ```; >>> vds = hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no files; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071:2493,Error,ErrorHandling,2493,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071,2,['Error'],"['Error', 'ErrorHandling']"
Availability,"ng hostnames in supported PyPy releases (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3087"">#3087</a>)</li>; </ul>; <h2>2.0.3</h2>; <ul>; <li>Allowed alternative SSL libraries such as LibreSSL, while still issuing a warning as we cannot help users facing issues with implementations other than OpenSSL. <a href=""https://redirect.github.com/urllib3/urllib3/issues/3020"">#3020</a></li>; <li>Deprecated URLs which don't have an explicit scheme <a href=""https://redirect.github.com/urllib3/urllib3/pull/2950"">#2950</a></li>; <li>Fixed response decoding with Zstandard when compressed data is made of several frames. <a href=""https://redirect.github.com/urllib3/urllib3/issues/3008"">#3008</a></li>; <li>Fixed <code>assert_hostname=False</code> to correctly skip hostname check. <a href=""https://redirect.github.com/urllib3/urllib3/issues/3051"">#3051</a></li>; </ul>; <h2>2.0.2</h2>; <ul>; <li>Fixed <code>HTTPResponse.stream()</code> to continue yielding bytes if buffered decompressed data was still available to be read even if the underlying socket is closed. This prevents a compressed response from being truncated. (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3009"">urllib3/urllib3#3009</a>)</li>; </ul>; <h2>2.0.1</h2>; <ul>; <li>Fixed a socket leak when fingerprint or hostname verifications fail. (<a href=""https://redirect.github.com/urllib3/urllib3/issues/2991"">#2991</a>)</li>; <li>Fixed an error when <code>HTTPResponse.read(0)</code> was the first <code>read</code> call or when the internal response body buffer was otherwise empty. (<a href=""https://redirect.github.com/urllib3/urllib3/issues/2998"">#2998</a>)</li>; </ul>; <h2>2.0.0</h2>; <p>Read the <a href=""https://urllib3.readthedocs.io/en/latest/v2-migration-guide.html"">v2.0 migration guide</a> for help upgrading to the latest version of urllib3.</p>; <h1>Removed</h1>; <ul>; <li>Removed support for Python 2.7, 3.5, and 3.6 (<a href=""https://redirect.github.com/urllib3/urllib3/issues/883"">#883</a>, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13768:2575,avail,available,2575,https://hail.is,https://github.com/hail-is/hail/pull/13768,3,['avail'],['available']
Availability,"ng pip_tools-6.13.0-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.2/53.2 kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB); Installing collected packages: zipp, wheel, tomli, packaging, click, pyproject_hooks, importlib-metadata, build, pip-tools; Successfully installed build-1.0.3 click-8.1.7 importlib-metadata-6.8.0 packaging-23.2 pip-tools-6.13.0 pyproject_hooks-1.0.0 tomli-2.0.1 wheel-0.41.2 zipp-3.17.0. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; + for package in '$@'; + reqs=python/requirements.txt; + pinned=python/pinned-requirements.txt; ++ mktemp; + new_pinned=/tmp/tmp.YoVBQEw8XF; ++ mktemp; + pinned_no_comments=/tmp/tmp.WRSKGgGEB8; ++ mktemp; + new_pinned_no_comments=/tmp/tmp.C8ggaXDHDt; + PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/aws/puppet/bin/:/home/hadoop/.local/bin:/home/hadoop/.local/bin; + pip-compile --quiet python/requirements.txt python/pinne",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:30317,Down,Downloading,30317,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Down'],['Downloading']
Availability,"ngelog</a>.</em></p>; <blockquote>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h2>3.9.11 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ijl/orjson/commit/a348f59f0b55d92a1364523560f52f5b3cf9c12a""><code>a348f59</code></a> 3.9.15</li>; <li><a href=""https://github.com/ijl/orjson/commit/b0e4d2c06ce06c6e63981bf0276e4b7c74e5845e""><code>b0e4d2c</code></a> yyjson 0eca326, recursion limit</li>; <li><a href=""https://github.com/ijl/orjson/commit/5067eadc84cf516e4eb33bcb09ad756bb59dc42e""><code>5067ead</code></a> impl_escape_unchecked() byte exact read</li>; <li><a href=""https://github.com/ijl/orjson/commit/e04ea735b087742b6cee738aa295d8b835c3a195""><code>e04ea73</code></a> cargo update, build misc</li>; <li><a href=""htt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14357:2657,failure,failure,2657,https://hail.is,https://github.com/hail-is/hail/pull/14357,3,['failure'],['failure']
Availability,"ngs 21.2.0 requires cffi, which is not installed.; aiohttp-devtools 1.1 requires watchfiles, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJhNGNiNTQzMi0zM2VmLTQ3ZmQtYmYzMy1lZGU2YzJlNDJ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14042:1680,avail,available,1680,https://hail.is,https://github.com/hail-is/hail/pull/14042,1,['avail'],['available']
Availability,"ning a frozenset of functions; registered to the router that the event would be called on.; [Issue <a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/74"">#74</a>](<a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/74"">brettcannon/gidgethub#74</a>).</li>; <li>Add support for GitHub Actions Environment Files with <code>gidgethub.actions.setenv</code>; and <code>gidgethub.actions.addpath</code>.; [Issue <a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/137"">#137</a>](<a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/132"">brettcannon/gidgethub#132</a>).</li>; <li>Make router callback execution order non-deterministic to avoid relying on; registration order.; [Issue <a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/74"">#74</a>](<a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/74"">brettcannon/gidgethub#74</a>).</li>; <li>Fix mypy errors in <code>gidgethub.httpx.GitHubAPI._request</code>[Issue <a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/133"">#133</a>](<a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/133"">brettcannon/gidgethub#133</a>).</li>; <li>Make the minimum version of PyJWT be v2.0.0.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/brettcannon/gidgethub/blob/main/docs/changelog.rst"">gidgethub's changelog</a>.</em></p>; <blockquote>; <h2>5.2.1</h2>; <ul>; <li>; <p>Fix cgi and importlib_resources deprecations.; (<code>PR [#185](https://github.com/brettcannon/gidgethub/issues/185) &lt;https://github.com/brettcannon/gidgethub/pull/185&gt;_</code>)</p>; </li>; <li>; <p>Add support for Python 3.11 and drop EOL Python 3.6; (<code>PR [#184](https://github.com/brettcannon/gidgethub/issues/184) &lt;https://github.com/brettcannon/gidgethub/pull/184&gt;_</code>)</p>; </li>; </ul>; <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12328:3764,error,errors,3764,https://hail.is,https://github.com/hail-is/hail/pull/12328,1,['error'],['errors']
Availability,"nnector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 975, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.128.0.11:5000 ssl:default [Connection refused]; INFO	2022-03-02 19:06:35,620	pool.py	create_instances:244	pool standard n_instances 1 {'pending': 0, 'active': 1, 'inactive': 0, 'deleted': 0} free_cores 0.0 live_free_cores 8.0 ready_cores 11.0; INFO	2022-03-02 19:06:35,620	pool.py	create_instances_from_ready_cores:206	creating 1 new instances; INFO	2022-03-02 19:06:35,848	pool.py	create_instances:244	pool highmem n_instances 1 {'pending': 0, 'active': 1, 'inactive': 0, 'deleted': 0} free_cores 4.0 live_free_cores 4.0 ready_cores 0.0; ERROR	2022-03-02 19:06:37,336	job.py	schedule_job:473	error while scheduling job (94, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\n File ""/usr/local/li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:9851,ERROR,ERROR,9851,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['ERROR'],['ERROR']
Availability,"nnotate_all` in the joiner for `index_entries` AST:; ```; def joiner(left: MatrixTable):; localized = Table(self._jvds.localizeEntries(row_uid)); src_cols_indexed = self.cols().add_index(col_uid); src_cols_indexed = src_cols_indexed.annotate(**{col_uid: hl.int32(src_cols_indexed[col_uid])}); left = left._annotate_all(row_exprs = {row_uid: localized.index(*row_exprs)[row_uid]},; col_exprs = {col_uid: src_cols_indexed.index(*col_exprs)[col_uid]}); return left.annotate_entries(**{uid: left[row_uid][left[col_uid]]}); ```. ### Hail version:; master; b1ac051d34bcc4c26fe9dea58aeac53038f2963e. ### What you did:. ```; mt = hl.utils.range_matrix_table(4, 4); mt2 = hl.utils.range_matrix_table(4, 4); mt2 = mt2.annotate_entries(x=mt2.row_idx + mt2.col_idx); mt.select_entries(a=mt2[mt.row_idx, mt.col_idx].x,; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Error; Traceback (most recent call last):; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 59, in testPartExecutor; yield; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 605, in run; testMethod(); File ""/Users/jbloom/hail/python/hail/tests/test_api.py"", line 1557, in test_force_bug; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 1171, in select_entries; return self._select_entries(""MatrixTable.select_entries"", hl.struct(**entry)); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2844, in _select_entries; base, cleanup = self._process_joins(s); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2503, in _process_joins; return process_joins(self, exprs, broadcast_f); File ""/Users/jbloom/hail/python/hail/utils/misc.py"", line 356, in process_joins; left = j.join_func(left); File ""/User",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3763:1229,Error,Error,1229,https://hail.is,https://github.com/hail-is/hail/issues/3763,1,['Error'],['Error']
Availability,no error when vep.location refers to directory,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/838:3,error,error,3,https://hail.is,https://github.com/hail-is/hail/issues/838,1,['error'],['error']
Availability,"no need to apologize for this! there was extra code hanging around that just added noise, so more work was definitely required. It's also good for my own development to slow down and be more attentive to that kind of thing sometimes :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2901#issuecomment-367128376:174,down,down,174,https://hail.is,https://github.com/hail-is/hail/pull/2901#issuecomment-367128376,1,['down'],['down']
Availability,nobody's complained about this in a long time. our errors are pretty good here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/258#issuecomment-301789487:51,error,errors,51,https://hail.is,https://github.com/hail-is/hail/issues/258#issuecomment-301789487,1,['error'],['errors']
Availability,"nomad --requester-pays-allow-buckets gnomad-public-requester-pays \; 	--master-machine-type=n1-highmem-8 --worker-machine-type=n1-highmem-8 \; 	--num-workers=300	--num-secondary-workers=0 \; 	--worker-boot-disk-size=1000 \; 	--properties=dataproc:dataproc.logging.stackdriver.enable=true,dataproc:dataproc.monitoring.stackdriver.enable=true; ```; We are currently receiving a spark error when using this cluster for our larger dataset. ```; [Stage 10:=====> (69 + 656) / 729]; raise err; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 98, in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); File ""/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1304, in __call__; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 31, in deco; raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.project-.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.gbsc-project.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGSchedul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:1402,failure,failure,1402,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['failure'],['failure']
Availability,nonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1014); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.RVD$$anonfun$4$$anon$1.hasNext(RVD.scala:226); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1014); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:471); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-f2b0dca9f506; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4114:9204,Error,Error,9204,https://hail.is,https://github.com/hail-is/hail/issues/4114,1,['Error'],['Error']
Availability,"not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.5</h2>; <p>Maintenance:</p>; <ul>; <li>Publish signed artifacts to Gradle plugin portal</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.4</h2>; <p>Bug fixes:</p>; <ul>; <li>Fix deadlock in <code>DownloadExtension</code> if <code>max-workers</code> equals 1 (thanks to <a href=""https://github.com/beatbrot""><code>@​beatbrot</code></a> for spotting this, see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/205"">#205</a>)</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1b5d69760d19cb7f88cbc837ee46456c494c0696""><code>1b5d697</code></a> Bump up version number to 5.2.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/7d6de83037ca41cd2f2f31830b43e43720e45b3a""><code>7d6de83</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1da8f078e22412475b694ce07b890148b8a5e4fc""><code>1da8f07</code></a> Add comment</li>; <li><a href=""https://github.com/michel-krae",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:2526,Down,DownloadExtension,2526,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['Down'],['DownloadExtension']
Availability,"note this doesn't add the errors everywhere, just the infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11641#issuecomment-1075436071:26,error,errors,26,https://hail.is,https://github.com/hail-is/hail/pull/11641#issuecomment-1075436071,1,['error'],['errors']
Availability,"notebook.py"", line 101, in <module>; user_table = Table(); File ""/notebook/table.py"", line 40, in __init__; self.connection_params = Table.get_secrets(); File ""/notebook/table.py"", line 24, in get_secrets; res = k8s.read_namespaced_secret('get-users', 'default'); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 19441, in read_namespaced_secret; (data) = self.read_namespaced_secret_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 19532, in read_namespaced_secret_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); kubernetes.client.rest.ApiException: (403); Reason: Forbidden; HTTP response headers: HTTPHeaderDict({'Audit-Id': 'b3dac876-d327-4129-84ca-30c0e1cb7e49', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'Date': 'Tue, 23 Jul 2019 18:36:54 GMT', 'Content-Length': '345'}); HTTP response body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""secrets \""get-users\"" is forbidden: User \""system:serviceaccount:pr-6531-default-p84s51gcoauw:notebook\"" cannot get resource \""secrets\"" in API group \""\"" in the namespace \""default\"""",""reason"":""Forbidden"",""details"":{""name"":""get-users"",""kind"":""secrets""},""code"":403}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6718:1842,Failure,Failure,1842,https://hail.is,https://github.com/hail-is/hail/issues/6718,1,['Failure'],['Failure']
Availability,"nother exception occurred:; ; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/setup.py"", line 224, in <module>; 'Programming Language :: Python :: Implementation :: PyPy']; File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/site-packages/setuptools/__init__.py"", line 143, in setup; _install_setup_requires(attrs); File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/site-packages/setuptools/__init__.py"", line 138, in _install_setup_requires; dist.fetch_build_eggs(dist.setup_requires); File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/site-packages/setuptools/dist.py"", line 698, in fetch_build_eggs; replace_conflicting=True,; File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 783, in resolve; replace_conflicting=replace_conflicting; File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 1066, in best_match; return self.obtain(req, installer); File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 1078, in obtain; return installer(requirement); File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/site-packages/setuptools/dist.py"", line 754, in fetch_build_egg; return fetch_build_egg(self, req); File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/site-packages/setuptools/installer.py"", line 130, in fetch_build_egg; raise DistutilsError(str(e)); distutils.errors.DistutilsError: Command '['/Users/spascal/.pyenv/versions/3.7.9/bin/python3.7', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/tmpspi2awj3', '--quiet', 'pypandoc']' returned non-zero exit status 1.; ----------------------------------------; ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9742:4622,error,errors,4622,https://hail.is,https://github.com/hail-is/hail/issues/9742,3,"['ERROR', 'error']","['ERROR', 'errored', 'errors']"
Availability,"nspace-data</code>: [<code>botocore</code>] Add new APIs for managing Users and Permission Groups.</li>; <li>api-change:<code>amplify</code>: [<code>botocore</code>] Add repositoryCloneMethod field for hosting an Amplify app. This field shows what authorization method is used to clone the repo: SSH, TOKEN, or SIGV4.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for the following FSx for OpenZFS features: snapshot lifecycle transition messages, force flag for deleting file systems with child resources, LZ4 data compression, custom record sizes, and unsetting volume quotas and reservations.</li>; <li>api-change:<code>fis</code>: [<code>botocore</code>] This release adds logging support for AWS Fault Injection Simulator experiments. Experiment templates can now be configured to send experiment activity logs to Amazon CloudWatch Logs or to an S3 bucket.</li>; <li>api-change:<code>route53-recovery-cluster</code>: [<code>botocore</code>] This release adds a new API option to enable overriding safety rules to allow routing control state updates.</li>; <li>api-change:<code>amplifyuibuilder</code>: [<code>botocore</code>] We are adding the ability to configure workflows and actions for components.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/67b84e02c185294c54a8e49510d4cb962e89cee2""><code>67b84e0</code></a> Merge branch 'release-1.21.13'</li>; <li><a href=""https://github.com/boto/boto3/commit/99acd545b20fe30ffa2f589a674c5a7ad74c266b""><code>99acd54</code></a> Bumping version to 1.21.13</li>; <li><a href=""https://github.com/boto/boto3/commit/83a8f662655bada44d442df7f33cb20d71ead257""><code>83a8f66</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/261b0f2ffe079b6940d683657fcad358195f882e""><code>261b0f2</code></a> Merge branch 'release-1.21.12'</li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11504:5132,recover,recovery-cluster,5132,https://hail.is,https://github.com/hail-is/hail/pull/11504,1,['recover'],['recovery-cluster']
Availability,"nspace-data</code>: [<code>botocore</code>] Add new APIs for managing Users and Permission Groups.</li>; <li>api-change:<code>amplify</code>: [<code>botocore</code>] Add repositoryCloneMethod field for hosting an Amplify app. This field shows what authorization method is used to clone the repo: SSH, TOKEN, or SIGV4.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for the following FSx for OpenZFS features: snapshot lifecycle transition messages, force flag for deleting file systems with child resources, LZ4 data compression, custom record sizes, and unsetting volume quotas and reservations.</li>; <li>api-change:<code>fis</code>: [<code>botocore</code>] This release adds logging support for AWS Fault Injection Simulator experiments. Experiment templates can now be configured to send experiment activity logs to Amazon CloudWatch Logs or to an S3 bucket.</li>; <li>api-change:<code>route53-recovery-cluster</code>: [<code>botocore</code>] This release adds a new API option to enable overriding safety rules to allow routing control state updates.</li>; <li>api-change:<code>amplifyuibuilder</code>: [<code>botocore</code>] We are adding the ability to configure workflows and actions for components.</li>; <li>api-change:<code>athena</code>: [<code>botocore</code>] This release adds support for updating an existing named query.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds support for new AMI property 'lastLaunchedTime'</li>; <li>api-change:<code>servicecatalog-appregistry</code>: [<code>botocore</code>] AppRegistry is deprecating Application and Attribute-Group Name update feature. In this release, we are marking the name attributes for Update APIs as deprecated to give a heads up to our customers.</li>; </ul>; <h1>1.21.8</h1>; <ul>; <li>api-change:<code>elasticache</code>: [<code>botocore</code>] Doc only update for ElastiCache</li>; <li>api-change:<code>panorama</code>: [<code>botocore</code>] Added",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11486:3685,recover,recovery-cluster,3685,https://hail.is,https://github.com/hail-is/hail/pull/11486,1,['recover'],['recovery-cluster']
Availability,"nswer, self.gateway_client, self.target_id, self.name); File ""/Users/cseed/hail/python/hail/utils/java.py"", line 200, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: OrderedRVD error! Unexpected key in partition 7; Range bounds for partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Key should be in partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Invalid key: [0.9986274705095608]. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 2.0 failed 1 times, most recent failure: Lost task 7.0 in stage 2.0 (TID 23, localhost, executor driver): is.hail.utils.HailException: OrderedRVD error! Unexpected key in partition 7; Range bounds for partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Key should be in partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Invalid key: [0.9986274705095608]; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1031); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1011); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.io.RichContextRDDRegionValue$.writeRowsPartition(RowStore.scala:1071); 	at is.hail.io.RichContextRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:1096); 	at is.hail.io.RichContextRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:1096); 	at is.hail.utils.richUtils.RichContextRDD$$anonfun$1.apply(RichContextRDD.scala:42); 	at is.hail.utils.richUtils.RichContextRDD$$anonfun$1.apply(RichContextRDD.scala:27); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22.apply(ContextRDD.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4096:1670,Error,ErrorHandling,1670,https://hail.is,https://github.com/hail-is/hail/issues/4096,1,['Error'],['ErrorHandling']
Availability,"nt = <py4j.java_gateway.GatewayClient object at 0x11098bc50>, target_id = 'z:is.hail.HailContext', name = 'apply'. def get_return_value(answer, gateway_client, target_id=None, name=None):; """"""Converts an answer received from the Java gateway into a Python object.; ; For example, string representation of integers are converted to Python; integer, string representation of objects are converted to JavaObject; instances, etc.; ; :param answer: the string returned by the Java gateway; :param gateway_client: the gateway client used to communicate with the Java; Gateway. Only necessary if the answer is a reference (e.g., object,; list, map); :param target_id: the name of the object from which the answer comes from; (e.g., *object1* in `object1.hello()`). Optional.; :param name: the name of the member from which the answer comes from; (e.g., *hello* in `object1.hello()`). Optional.; """"""; if is_error(answer)[0]:; if len(answer) > 1:; type = answer[1]; value = OUTPUT_CONVERTER[type](answer[2:], gateway_client); if answer[1] == REFERENCE_TYPE:; raise Py4JJavaError(; ""An error occurred while calling {0}{1}{2}.\n"".; format(target_id, ""."", name), value); else:; raise Py4JError(; ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; > format(target_id, ""."", name, value)); E py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; E py4j.Py4JException: Method apply([null, class java.lang.String, class scala.Some, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist; E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339); E 	at py4j.Gateway.invoke(Gateway.java:276); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCom",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:3870,error,error,3870,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928,1,['error'],['error']
Availability,"nt call last):; File ""foo.py"", line 4, in <module>; hl.export_vcf(mt, 'foo.vcf.bgz'); File ""<decorator-gen-878>"", line 2, in export_vcf; File ""/Users/dking/projects/hail/python/hail/typecheck/check.py"", line 546, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/hail/python/hail/methods/impex.py"", line 424, in export_vcf; joption(typ._convert_to_j(metadata))); File ""/Users/dking/borg/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/Users/dking/projects/hail/python/hail/utils/java.py"", line 210, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: in export_vcf: column key must be type 'str', found: [Lis.hail.expr.types.Type;@1e5a1972. Java stack trace:; is.hail.utils.HailException: in export_vcf: column key must be type 'str', found: [Lis.hail.expr.types.Type;@1e5a1972; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.MatrixTable.requireColKeyString(MatrixTable.scala:392); 	at is.hail.io.vcf.ExportVCF$.apply(ExportVCF.scala:202); 	at is.hail.io.vcf.ExportVCF.apply(ExportVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-17d79be122",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4283:1508,Error,ErrorHandling,1508,https://hail.is,https://github.com/hail-is/hail/issues/4283,1,['Error'],['ErrorHandling']
Availability,"nt working directory. ; `vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`. However, I get the following error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(Thread.java:745)`. `Hail version: 0.1-4238176`; `Error summar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2070:1052,Error,ErrorHandling,1052,https://hail.is,https://github.com/hail-is/hail/issues/2070,1,['Error'],['ErrorHandling']
Availability,"nt.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""domain"": ""global"",; ""reason"": ""forbidden""; }; ]; }; }. 	at is.hail.relocated.com.google.cloud.storage.StorageException.translate(StorageException.java:165) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:298) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.open(HttpStorageRpc.java:1029) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.ResumableMedia.lambda$startUploadForBlobInfo$0(ResumableMedia.java:40) ~[gs:__hail-query-ger0g_j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:10829,error,errors,10829,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['error'],['errors']
Availability,"nt: examples] Apply blackbody example label edits (<a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/12534"">#12534</a>) to the ts example as well</li>; </ul>; </li>; </ul>; <h2>2022-10-30 3.0:</h2>; <ul>; <li>bugfixes:; <ul>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/5046"">#5046</a> Webgl problem in stream app with multiple glyphs</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/6669"">#6669</a> [component: bokehjs] BoxAnnotation does not appear to handle formal NumberSpec</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8168"">#8168</a> [component: bokehjs] Strange behavior with BoxSelectTool when click+dragging on toolbar</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8332"">#8332</a> [component: bokehjs] Autohide toolbar quirks</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8346"">#8346</a> [component: bokehjs] update datasource cause error with webgl backend</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8469"">#8469</a> Modifying a child element in a tab causes the whole tab to rerender</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8531"">#8531</a> [component: bokehjs] Save tool in gridplot initiates multiple downloads</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8684"">#8684</a> Allow at least partial alignment of fixed sized frames</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9113"">#9113</a> [component: bokehjs] Empty group widgets don't size properly once populated</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9133"">#9133</a> [BUG] Tabs ignore explicitly set dimensions</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9208"">#9208</a> [component: bokehjs] [BUG] sizing_mode='stretch_width' makes plot too wide if scrollbar is sho",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12454:2300,error,error,2300,https://hail.is,https://github.com/hail-is/hail/pull/12454,1,['error'],['error']
Availability,"ntainer exit](https://user-images.githubusercontent.com/106194/44540204-293de580-a6d4-11e8-90b8-1ecaaec45443.png); ![Job 12 page show mapPartitionsWithIndex running](https://user-images.githubusercontent.com/106194/44540205-293de580-a6d4-11e8-9893-af8edc382906.png); ![executors page showing 202 active executors, 100 dead](https://user-images.githubusercontent.com/106194/44540178-15927f00-a6d4-11e8-99e1-78755df3756c.png). This `mapPartitionsWithIndex` call is in the tree aggregate of context rdd. Judging from the fact that it's the first one, this is probably the first step of the aggregation.; ; ### What went wrong (all error messages here, including the full java stack trace):; Workers exceed memory limits, e.g. (from hail.log):; ```; 2018-08-22 17:28:37 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container killed by YARN for exceeding memory limits. 12.2 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; 2018-08-22 17:28:37 YarnScheduler: ERROR: Lost executor 431 on pca-sw-pd61.c.daly-ibd.internal: Container killed by YARN for exceeding memory limits. 12.2 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; 2018-08-22 17:28:37 TaskSetManager: WARN: Lost task 1047.0 in stage 16.0 (TID 20251, pca-sw-pd61.c.daly-ibd.internal, executor 431): ExecutorLostFailure (executor 431 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 12.2 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; 2018-08-22 17:28:37 TaskSetManager: WARN: Lost task 1046.0 in stage 16.0 (TID 20250, pca-sw-pd61.c.daly-ibd.internal, executor 431): ExecutorLostFailure (executor 431 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 12.2 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; 2018-08-22 17:28:37 TaskSetManager: WARN: Lost task 1049.0 in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4202:1216,ERROR,ERROR,1216,https://hail.is,https://github.com/hail-is/hail/issues/4202,1,['ERROR'],['ERROR']
Availability,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:6460,outage,outages,6460,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['outage'],['outages']
Availability,"nterfaces, types, classes). 9. Really amazing debug, tooling. React debug tools natively included in Chrome for instance. First class support for JSX (React-wrapped HTML) in popular IDE's, most obviously Visual Studio Code. ## Tech TL;DR; Mainly React. React should take about a day to learn well enough to make contributions. Guide: https://reactjs.org/docs/introducing-jsx.html. ```jsx; # Renders Hello World; # Biggest annoyance (may go away in 2019) is that ""class"" is not a valid tag (reserved by React); export default function SomePage() {; const name = 'Alex'. # Renders ""Hello Alex""; return (; <div className='some-class'> Hello {name} </div>; ); }; ```. ## Challenges; 1. Auth ; Authentication is tricky, but not for any reason specific to React, Next, Node. Server-side rendered apps tie the web app to the resource server; as such it's easier to hide sensitive information. . Mobile and desktop apps have dealt with this for 2 decades. We should build a robust infrastructure, and not one that requires server-rendered web pages for security. Currently it seems Auth0 may not be the best choice: it does not interface for us with third-party API's; requires us to either insecurely store 3rd party access tokens (with at least 1 extra round trip), or altogether proxy all third-party requests through our own resource server... Firebase Auth seems to avoid these limitations. ## TODO:; 1. Create a structured description of this pull request; 2. Incorporate Firebase Auth in place of Auth0 for 3rd party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:6287,robust,robust,6287,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['robust'],['robust']
Availability,"ntext$.$anonfun$scoped$2(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:63); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:350); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:495); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:494); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182); 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.120-f00f916faf78; Error summary: HailException: zip: length mismatch: 62164, 104; ```. > One of the input tables (context_ht) has 8061724269 rows and 62164 partitions, and another input_table (mutation_ht) has 104 rows and 104 partitions. If I add “mutation_ht = mutation_ht = mutation_ht.repartition(1)” at line 37, the script runs fine. If I add “mutation_ht = mutation_ht.naive_coalesce(1)” it gives the same error. I've obtained a log from a failed run and do see us zipping the two contexts together without making sure they're the same length.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13486:6821,Error,Error,6821,https://hail.is,https://github.com/hail-is/hail/issues/13486,2,"['Error', 'error']","['Error', 'error']"
Availability,"ny.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723e4b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-10-19T03:09:40Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.2' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.124' >> src/main/resources/build-info.properties; creating env/HAIL_DEBUG_MODE which does not exist; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; make -C src/main/c prebuilt; make[1]: Entering directory `/mnt/tmp/hail/hail/src/main/c'; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:3866,echo,echo,3866,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,"nyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; jupyter 1.0.0 requires notebook, which is not installed.; beautifulsoup4 4.12.2 requires soupsieve, which is not installed.; argon2-cffi-bindings 21.2.0 requires cffi, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `7.34.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Access Control Bypass <br/>[SNYK-PYTHON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13836:1133,avail,available,1133,https://hail.is,https://github.com/hail-is/hail/pull/13836,3,['avail'],['available']
Availability,"nyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; msal-extensions 1.0.0 requires portalocker, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiodns 2.0.0 requires pycares, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14041:1143,avail,available,1143,https://hail.is,https://github.com/hail-is/hail/pull/14041,1,['avail'],['available']
Availability,"nyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2928995) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Race Condition <br/>[SNYK-PYTHON-PROMPTTOOLKIT-6141120](https://snyk.io/vuln/SNYK-PYTHON-PROMPTTOOLKIT-6141120) | `prompt-toolkit:` <br> `1.0.18 -> 3.0.13` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **696/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-1086606](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1086606) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-PYGMENTS-1088505](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1088505) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-5750273](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-5750273) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **519/1000** <br/> **Why?** Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:6190,avail,available,6190,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['avail'],['available']
Availability,"nylinux_2_24_x86_64.whl (249 kB); 951 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 249.9/249.9 kB 45.1 MB/s eta 0:00:00; 952 | amazon-ebs: Requirement already satisfied: pandas<1.5.0,>=1.3.0 in /usr/local/lib64/python3.7/site-packages (1.3.5); 953 | amazon-ebs: Collecting parsimonious<0.9; 954 | amazon-ebs: Downloading parsimonious-0.8.1.tar.gz (45 kB); 955 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.1/45.1 kB 10.2 MB/s eta 0:00:00; 956 | amazon-ebs: Preparing metadata (setup.py): started; 957 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 958 | amazon-ebs: Collecting plotly<5.11,>=5.5.0; 959 | amazon-ebs: Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB); 960 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/15.2 MB 57.8 MB/s eta 0:00:00; 961 | amazon-ebs: Collecting PyJWT; 962 | amazon-ebs: Downloading PyJWT-2.5.0-py3-none-any.whl (20 kB); 963 | amazon-ebs: Collecting python-json-logger==2.0.2; 964 | amazon-ebs: Downloading python_json_logger-2.0.2-py3-none-any.whl (7.4 kB); 965 | amazon-ebs: Collecting requests==2.25.1; 966 | amazon-ebs: Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB); 967 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 15.6 MB/s eta 0:00:00; 968 | amazon-ebs: Requirement already satisfied: scipy<1.8,>1.2 in /usr/local/lib64/python3.7/site-packages (1.7.3); 969 | amazon-ebs: Requirement already satisfied: sortedcontainers==2.4.0 in /usr/local/lib/python3.7/site-packages (2.4.0); 970 | amazon-ebs: Collecting tabulate==0.8.9; 971 | amazon-ebs: Downloading tabulate-0.8.9-py3-none-any.whl (25 kB); 972 | amazon-ebs: Requirement already satisfied: tqdm==4.* in /usr/local/lib/python3.7/site-packages (4.64.1); 973 | amazon-ebs: Collecting uvloop==0.16.0; 974 | amazon-ebs: Downloading uvloop-0.16.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.8 MB); 975 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 113.0 MB/s eta 0:00:00; 976 ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:6828,Down,Downloading,6828,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"o give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: n ; WARNING: Please verify service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com has the role ""roles/storage.objectAdmin"" or both ""roles/storage.objectViewer"" and ""roles/storage.objectCreator"" roles for bucket hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. Not existing user-specified remote tmpdir:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://my-bucket/foo/bar; ERROR: You do not have sufficient permissions to get information about bucket my-bucket or it does not exist. If the bucket exists, ask a project administrator to give you the permission ""storage.buckets.get"" or assign you the StorageAdmin role in Google Cloud Storage.; Aborted.; ```. Existing remote tmpdir in wrong region:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/bar/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-oxmmp.; Which region do you want your j",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:4063,ERROR,ERROR,4063,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['ERROR'],['ERROR']
Availability,"o pick up CI's credentials instead of the test account's credentials. ```; E hail.utils.java.FatalError: GoogleJsonResponseException: 403 Forbidden; E GET https://storage.googleapis.com/storage/v1/b/hail-test-requester-pays-fds32/o/zero-to-nine?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata&userProject=hail-vdc; E {; E ""code"": 403,; E ""errors"": [; E {; E ""domain"": ""global"",; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist)."",; E ""reason"": ""forbidden""; E }; E ],; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist).""; E }; E ; E Java stack trace:; E java.io.IOException: Error accessing gs://hail-test-requester-pays-fds32/zero-to-nine; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1986); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1882); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystemImpl.getFileInfoInternal(GoogleCloudStorageFileSystemImpl.java:861); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystemImpl.getFileInfo(GoogleCloudStorageFileSystemImpl.java:833); E 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem.getFileStatus(GoogleHadoopFileSystem.java:724); E 	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:115); E 	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:349); E 	at org.apache.hadoop.fs.Globber.glob(Globber.java:202); E 	at org.apache.hadoop.fs.FileSystem.globStat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236:1561,Error,Error,1561,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236,1,['Error'],['Error']
Availability,"o/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5914629](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Missing Cryptographic Step <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6036192](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6036192) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6092044](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6092044) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **519/1000** <br/> **Why?** Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:8282,avail,available,8282,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"o/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Missing Cryptographic Step <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6036192](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6036192) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6050294](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6050294) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6092044](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6092044) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Information Exposure <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6126975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6126975) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **489/1000** <br/> **Why?** Has a fix available, CVSS 5.5 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14329:8651,avail,available,8651,https://hail.is,https://github.com/hail-is/hail/pull/14329,1,['avail'],['available']
Availability,"o/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Missing Cryptographic Step <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6036192](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6036192) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6050294](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6050294) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6092044](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6092044) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Information Exposure <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6126975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6126975) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **561/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.5 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `3.3.2 -> 42.0.2` ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:8643,avail,available,8643,https://hail.is,https://github.com/hail-is/hail/pull/14327,1,['avail'],['available']
Availability,"o; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:768); at is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:3452,Error,ErrorHandling,3452,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['Error'],['ErrorHandling']
Availability,"oad-task/commit/e3c65ffcb49b9c5a33fde5f31fb63043dbf21134""><code>e3c65ff</code></a> Allow extensions to be created from tasks</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/34e2dd41477f18b1ae3d6d5a71dca5449d6cd1e0""><code>34e2dd4</code></a> Downgrade slf4j to fix warning on console about missing slf4j provider</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b3fa29f9ffb4d4544e13ef84601e371fb2778ddf""><code>b3fa29f</code></a> Revert &quot;Update Apache HttpClient to 5.2.1&quot;</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/01f05e046be0dca18f506723c79e88f208336e71""><code>01f05e0</code></a> Add integration tests for Gradle 6.9.3 and 7.6</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a998a544908a8b39f713f4526f717fcb328c06eb""><code>a998a54</code></a> Upgrade Gradle to 7.6</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/5.3.0...5.3.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=5.3.0&new-version=5.3.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:3183,down,download-task,3183,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['down'],['download-task']
Availability,"oads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html</a></p>; <h2>Elasticsearch Hadoop 8.2.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html</a></p>; <h2>Elasticsearch Hadoop 8.2.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html</a></p>; <h2>Elasticsearch Hadoop 8.1.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/a7873eb46985d849286ab89fa0a51f8b5374e02e""><code>a7873eb</code></a> Update index.adoc (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2000"">#2000</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2002"">#2002</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/9f44fe66d0ff82f18a13a38cae6abf3f72183a94""><code>9f44fe6</code></a> Bump to version 8.4.3</li>; <li><a href=""https://github",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:4009,down,downloads,4009,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"oads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html</a></p>; <h2>Elasticsearch Hadoop 8.2.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html</a></p>; <h2>Elasticsearch Hadoop 8.2.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html</a></p>; <h2>Elasticsearch Hadoop 8.1.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/a7873eb46985d849286ab89fa0a51f8b5374e02e""><code>a7873eb</code></a> Update index.adoc (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-had",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:3683,down,downloads,3683,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"oads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html</a></p>; <h2>Elasticsearch Hadoop 8.2.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html</a></p>; <h2>Elasticsearch Hadoop 8.2.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html</a></p>; <h2>Elasticsearch Hadoop 8.1.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a></p>; <!-- raw HTML omitted --",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:3357,down,downloads,3357,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"oads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html</a></p>; <h2>Elasticsearch Hadoop 8.2.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html</a></p>; <h2>Elasticsearch Hadoop 8.2.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:3031,down,downloads,3031,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"oads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html</a></p>; <h2>Elasticsearch Hadoop 8.2.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:2705,down,downloads,2705,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"oads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:2379,down,downloads,2379,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"oads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:2053,down,downloads,2053,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"oads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html</a></p>; <h2>Elasticsearch Hadoop 8.4.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:1727,down,downloads,1727,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"oads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html</a></p>; <h2>Elasticsearch Hadoop 8.4.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html</a></p>; <h2>Elasticsearch Hadoop 8.4.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:1401,down,downloads,1401,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"oads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/da4f3c3f209aea47d69c4faf90029a6933bd3a60""><code>da4f3c3</code></a> [DOCS] Add 8.5.3 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2045"">#2045</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2050"">#2050</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/79d592abdce1cd90845d153afab8b66069e2a172""><code>79d592a</code></a> [DOCS] Add 8.5.2 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2042"">#2042</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2049"">#2049</a>)<",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12601:1727,down,downloads,1727,https://hail.is,https://github.com/hail-is/hail/pull/12601,1,['down'],['downloads']
Availability,"oads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/e665cd918aa8d0ba7806b92e16881edb96180d48""><code>e665cd9</code></a> Update Spark to 3.2.3 (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2056"">#2056</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2057"">#2057</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/07380b0e17c7d908d50d59fc69ac2953adfa5a0d""><code>07380b0</code></a> Use DRA repository for build-tools dependencies</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/77bce30bfefb39c39bd34a6f147b17fb0df4701c""><code>77bce30</code></a> Bump to version 8.6.1</li>; <li><a href=""https://github",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12623:2053,down,downloads,2053,https://hail.is,https://github.com/hail-is/hail/pull/12623,1,['down'],['downloads']
Availability,"oads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/da4f3c3f209aea47d69c4faf90029a6933bd3a60""><code>da4f3c3</code></a> [DOCS] Add 8.5.3 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2045"">#2045</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2050"">#2050</a>)</li>; <li><a href=""https://github.com/elastic/elas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12601:1401,down,downloads,1401,https://hail.is,https://github.com/hail-is/hail/pull/12601,1,['down'],['downloads']
Availability,"oads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/e665cd918aa8d0ba7806b92e16881edb96180d48""><code>e665cd9</code></a> Update Spark to 3.2.3 (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2056"">#2056</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2057"">#2057</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12623:1727,down,downloads,1727,https://hail.is,https://github.com/hail-is/hail/pull/12623,1,['down'],['downloads']
Availability,"oads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.1.html</a></p>; <h2>Elasticsearch Hadoop 8.6.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/e665cd918aa8d0ba7806b92e16",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12623:1401,down,downloads,1401,https://hail.is,https://github.com/hail-is/hail/pull/12623,1,['down'],['downloads']
Availability,"ob:; ```python; utils.py	retry_long_running:923	in delete_prev_cancelled_job_group_cancellable_resources_records	; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 915, in retry_long_running; return await f(*args, **kwargs)\n File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 959, in loop; await f(*args, **kwargs)\n File ""/usr/local/lib/python3.9/dist-packages/batch/driver/main.py"", line 1485, in delete_prev_cancelled_job_group_cancellable_resources_records; async for target in targets:\n File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 334, in execute_and_fetchall; async for row in tx.execute_and_fetchall(sql, args, query_name):\n File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 257, in execute_and_fetchall; await cursor.execute(sql, args)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 469, in query; await self._read_query_result(unbuffered=unbuffered)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 683, in _read_query_result; await result.read()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 1164, in read; first_packet = await self.connection._read_packet()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 652, in _read_packet; packet.raise_for_error()\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/protocol.py"", line 219, in raise_for_error; err.raise_mysql_exception(self._data)\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/err.py"", line 150, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.OperationalError: (1054, ""Unknown column 'cancelled.id' in 'on clause'""); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2349752340:1929,error,errorclass,1929,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2349752340,1,['error'],['errorclass']
Availability,"ocus\"":{\""contig\"":\""chr20\"",\""position\"":17994753},\""alleles\"":[\""A\"",\""<NON_REF>\""]},\""includeStart\"":true,\""includeEnd\"":true},{\""start\"":{\""locus\"":{\""contig\"":\""chr20\"",\""position\"":17994753},\""alleles\"":[\""A\"",\""<NON_REF>\""]},\""end\"":{\""locus\"":{\""cont...""] ; !s20 = ToStream(%29) [False]; !s21 = StreamMap(!s20) { (%elt10) =>; SelectFields(%elt10) [; (partitionCounts distinctlyKeyed firstKey; lastKey)]; }; !37 = ToArray(!s21); !38 = WriteMetadata(!37) [""{\""name\"":\""TableSpecWriter\"",\""path\"":\""/tmp/foo.ht\"",\""typ\"":{\""rowType\"":\""Struct{locus:Locus(GRCh38),alleles:Array[String],data:Array[Struct{}]}\"",\""key\"":[\""locus\"",\""alleles\""],\""globalType\"":\""Struct{new_globals:Array[Struct{}]}\""},\""rowRelPath\"":\""rows\"",\""globalRelPath\"":\""globals\"",\""refRelPath\"":\""references\"",\""log\"":true}""]; !39 = Begin(!34, !36, !38); WriteMetadata(!39) [""{\""name\"":\""RelationalWriter\"",\""path\"":\""/tmp/foo.ht\"",\""overwrite\"":true,\""maybeRefs\"":{\""references\"":[\""GRCh38\""]}}""]. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:23); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:23); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:17); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:29); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:205); 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:249); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$2(LocalBackend.scala:314); 	at is.hail.utils.ExecutionTimer.time(ExecutionTi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245:15581,Error,ErrorHandling,15581,https://hail.is,https://github.com/hail-is/hail/issues/14245,1,['Error'],['ErrorHandling']
Availability,"ode>] Adding support for ImageProperties feature to detect dominant colors and image brightness, sharpness, and contrast, inclusion and exclusion filters for labels and label categories, new fields to the API response, &quot;aliases&quot; and &quot;categories&quot;</li>; <li>api-change:<code>securityhub</code>: [<code>botocore</code>] Documentation updates for Security Hub</li>; <li>api-change:<code>ssm-incidents</code>: [<code>botocore</code>] RelatedItems now have an ID field which can be used for referencing them else where. Introducing event references in TimelineEvent API and increasing maximum length of &quot;eventData&quot; to 12K characters.</li>; </ul>; <h1>1.26.7</h1>; <ul>; <li>api-change:<code>autoscaling</code>: [<code>botocore</code>] This release adds a new price capacity optimized allocation strategy for Spot Instances to help customers optimize provisioning of Spot Instances via EC2 Auto Scaling, EC2 Fleet, and Spot Fleet. It allocates Spot Instances based on both spare capacity availability and Spot Instance price.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds a new price capacity optimized allocation strategy for Spot Instances to help customers optimize provisioning of Spot Instances via EC2 Auto Scaling, EC2 Fleet, and Spot Fleet. It allocates Spot Instances based on both spare capacity availability and Spot Instance price.</li>; <li>api-change:<code>ecs</code>: [<code>botocore</code>] This release adds support for task scale-in protection with updateTaskProtection and getTaskProtection APIs. UpdateTaskProtection API can be used to protect a service managed task from being terminated by scale-in events and getTaskProtection API to get the scale-in protection status of a task.</li>; <li>api-change:<code>es</code>: [<code>botocore</code>] Amazon OpenSearch Service now offers managed VPC endpoints to connect to your Amazon OpenSearch Service VPC-enabled domain in a Virtual Private Cloud (VPC). This feature allows y",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12458:2202,avail,availability,2202,https://hail.is,https://github.com/hail-is/hail/pull/12458,2,['avail'],['availability']
Availability,"ode>html_style</code> to an iterable of strings.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10366"">#10366</a>: std domain: Add support for emphasising placeholders in :rst:dir:<code>option</code>; directives through a new :confval:<code>option_emphasise_placeholders</code> configuration; option.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10439"">#10439</a>: std domain: Use the repr of some variables when displaying warnings,; making whitespace issues easier to identify.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10571"">#10571</a>: quickstart: Reduce content in the generated <code>conf.py</code> file. Patch by; Pradyun Gedam.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10648"">#10648</a>: LaTeX: CSS-named-alike additional :ref:<code>'sphinxsetup' &lt;latexsphinxsetup&gt;</code>; keys allow to configure four separate border-widths, four paddings, four; corner radii, a shadow (possibly inset), colours for border, background, shadow; for each of the code-block, topic, attention, caution, danger, error and warning; directives.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10655"">#10655</a>: LaTeX: Explain non-standard encoding in LatinRules.xdy</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10599"">#10599</a>: HTML Theme: Wrap consecutive footnotes in an <code>&lt;aside&gt;</code> element when; using Docutils 0.18 or later, to allow for easier styling. This matches the; behaviour introduced in Docutils 0.19. Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10518"">#10518</a>: config: Add <code>include_patterns</code> as the opposite of <code>exclude_patterns</code>.; Patch by Adam Turner.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <su",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12165:4308,error,error,4308,https://hail.is,https://github.com/hail-is/hail/pull/12165,1,['error'],['error']
Availability,"ode>metadata.selfLink</code> field can no longer be populated by kube-apiserver; it was deprecated in 1.16 and has not been populated by default since 1.20+. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/107527"">kubernetes/kubernetes#107527</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>)</li>; <li>Kubelet external Credential Provider feature is moved to Beta. Credential Provider Plugin and Credential Provider Config API's updated from v1alpha1 to v1beta1 with no API changes. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108847"">kubernetes/kubernetes#108847</a>, <a href=""https://github.com/adisky""><code>@​adisky</code></a>)</li>; <li>Make STS available replicas optional again. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/109241"">kubernetes/kubernetes#109241</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>)</li>; <li>MaxUnavailable for StatefulSets, allows faster RollingUpdate by taking down more than 1 pod at a time. The number of pods you want to take down during a RollingUpdate is configurable using maxUnavailable parameter. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/82162"">kubernetes/kubernetes#82162</a>, <a href=""https://github.com/krmayankk""><code>@​krmayankk</code></a>)</li>; <li>Non-graceful node shutdown handling is enabled for stateful workload failovers (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108486"">kubernetes/kubernetes#108486</a>, <a href=""https://github.com/sonasingh46""><code>@​sonasingh46</code></a>)</li>; <li>Omit enum declarations from the static openapi file captured at <a href=""https://git.k8s.io/kubernetes/api/openapi-spec"">https://git.k8s.io/kubernetes/api/openapi-spec</a>. This file is used to generate API clients, and use of enums in those generated clients (rather than strings) can break forward compatibility with addi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12196:9804,down,down,9804,https://hail.is,https://github.com/hail-is/hail/pull/12196,1,['down'],['down']
Availability,"ode>{.interpreted-text role=&quot;ref&quot;}.</li>; </ul>; <h2>7.0.0</h2>; <h1>pytest 7.0.0 (2022-02-03)</h1>; <p>(<strong>Please see the full set of changes for this release also in the 7.0.0rc1 notes below</strong>)</p>; <h2>Deprecations</h2>; <ul>; <li>; <p><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9488"">#9488</a>: If custom subclasses of nodes like <code>pytest.Item</code>{.interpreted-text role=&quot;class&quot;} override the; <code>__init__</code> method, they should take <code>**kwargs</code>. See; <code>uncooperative-constructors-deprecated</code>{.interpreted-text role=&quot;ref&quot;} for details.</p>; <p>Note that a deprection warning is only emitted when there is a conflict in the; arguments pytest expected to pass. This deprecation was already part of pytest; 7.0.0rc1 but wasn't documented.</p>; </li>; </ul>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9355"">#9355</a>: Fixed error message prints function decorators when using assert in Python 3.8 and above.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9396"">#9396</a>: Ensure <code>pytest.Config.inifile</code>{.interpreted-text role=&quot;attr&quot;} is available during the <code>pytest_cmdline_main &lt;_pytest.hookspec.pytest_cmdline_main&gt;</code>{.interpreted-text role=&quot;func&quot;} hook (regression during <code>7.0.0rc1</code>).</li>; </ul>; <h2>Improved Documentation</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9404"">#9404</a>: Added extra documentation on alternatives to common misuses of [pytest.warns(None)]{.title-ref} ahead of its deprecation.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9505"">#9505</a>: Clarify where the configuration files are located. To avoid confusions documentation mentions; that configuration file is located in the root of the repository.</li>; </ul>; <h2>Trivia",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:2668,error,error,2668,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['error'],['error']
Availability,"odified the `VirtualHost` set up to use [name-based VirtualHost discrimination](https://httpd.apache.org/docs/2.4/vhosts/name-based.html) based on information from the [TeamCity wiki](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1246,Avail,Available,1246,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,1,['Avail'],['Available']
Availability,"ods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=roles"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=Role""; Name: ""batch-pods-admin"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""Role"" ""metadata"":map[""name"":""batch-pods-admin"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""rules"":[map[""apiGroups"":[""""] ""resources"":[""pods""] ""verbs"":[""get"" ""list"" ""watch"" ""create"" ""update"" ""patch"" ""delete""]] map[""apiGroups"":[""""] ""resources"":[""pods/log""] ""verbs"":[""get""]]]]}; from server for: ""deployment.yaml"": roles.rbac.authorization.k8s.io ""batch-pods-admin"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get roles.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=rolebindings"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=RoleBinding""; Name: ""batch-pods-admin-binding"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""RoleBinding"" ""metadata"":map[""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""name"":""batch-pods-admin-binding"" ""namespace"":""batch-pods""] ""roleRef"":map[""apiGroup"":"""" ""kind"":""Role"" ""name"":""batch-pods-admin""] ""subjects"":[map[""kind"":""ServiceAccount"" ""name"":""batch-svc"" ""namespace"":""default""]]]}; from server for: ""deployment.yaml"": rolebindings.rbac.authorization.k8s.io ""batch-pods-admin-binding"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get rolebindings.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:2234,Error,Error,2234,https://hail.is,https://github.com/hail-is/hail/issues/4609,2,"['Error', 'error']","['Error', 'error']"
Availability,"odule>; File ""</home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/decorator.py:decorator-gen-1058>"", line 2, in write; File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/matrixtable.py"", line 2508, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Hail only supports 8-bit probabilities, found 16. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8545:1728,Error,Error,1728,https://hail.is,https://github.com/hail-is/hail/issues/8545,1,['Error'],['Error']
Availability,"ogLevel(newLevel).; Running on Apache Spark version 2.2.3; SparkUI available at http://10.200.100.39:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /home/unix/dking/hail-20190307-1908-0.2.11-cf54f08305d1.log; 2019-03-07 19:08:30 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 100 variants...; ^[[A; In [3]: t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]) ; [Stage 0:============================================> (6 + 1) / 8]2019-03-07 19:08:39 Hail: INFO: Coerced sorted dataset; 2019-03-07 19:08:40 Hail: INFO: linear_regression_rows: running on 100 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; /broad/software/free/Linux/redhat_7_x86_64/pkgs/jdk1.8.0_181/bin/java: symbol lookup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1164, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ---------------------------------------------------------------------------; Py4JError Traceback (most recent call last); <ipython-input-3-9b1033d0ac55> in <module>; ----> 1 t = hl.linear",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5559:1622,ERROR,ERROR,1622,https://hail.is,https://github.com/hail-is/hail/issues/5559,1,['ERROR'],['ERROR']
Availability,"ogleapis/java-storage/issues/2184"">#2184</a>) (<a href=""https://github.com/googleapis/java-storage/commit/d9859768081ea6f872097851d3e318b5bad384d9"">d985976</a>)</li>; <li>Initial CLI for SSB integration and Workload 1 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2166"">#2166</a>) (<a href=""https://github.com/googleapis/java-storage/commit/a349735e7fe108e623a330afec0c8cd608ebeef9"">a349735</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>A resumable session without a Range header should be interpreted as 0 length (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2182"">#2182</a>) (<a href=""https://github.com/googleapis/java-storage/commit/53022011d83e6a8515a5ba008fc45fc2dae39cea"">5302201</a>)</li>; <li>Update User-Agent handling for resumable uploads (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2168"">#2168</a>) (<a href=""https://github.com/googleapis/java-storage/commit/665b714f421d3c13b557d0ff71460c328c010856"">665b714</a>)</li>; <li>Update version resolution logic to be more resilient (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2169"">#2169</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c89d27508039a014ea5a6dd8d4889f63d07db73f"">c89d275</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update actions/checkout action to v4 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2188"">#2188</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c10267e176bda21cd5755dfb0e96d0504fbc1d54"">c10267e</a>)</li>; <li>Update actions/checkout action to v4 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2189"">#2189</a>) (<a href=""https://github.com/googleapis/java-storage/commit/5c048c499eef224dade8f4409dfae732cb5a7017"">5c048c4</a>)</li>; <li>Update actions/checkout action to v4 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2190"">#2190</a>) (<a href=""https://github.com/googleapis/java-storage/commit/45e66e89373",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13624:1878,resilien,resilient,1878,https://hail.is,https://github.com/hail-is/hail/pull/13624,2,['resilien'],['resilient']
Availability,"oh, crap - the lowering rule for MatrixEntriesTable has the same problem:; ```; E Current key: [1:10000,[A,G],sample_001]; E Previous key: [1:10000,[A,G],sample_500]; E This error can occur after a split_multi if the dataset; E contains both multiallelic variants and duplicated loci.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6267#issuecomment-499296452:174,error,error,174,https://hail.is,https://github.com/hail-is/hail/pull/6267#issuecomment-499296452,1,['error'],['error']
Availability,"oh, man, this is super exciting. 3x on the combiner? yes please!. We can probably make incremental performance improvements to the LIR method splitting code to bring the compile and execute back down, and that one I consider a little less critical anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8963#issuecomment-650837044:195,down,down,195,https://hail.is,https://github.com/hail-is/hail/pull/8963#issuecomment-650837044,1,['down'],['down']
Availability,"oh, that's it. You can't do that. This is still a bad error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4081#issuecomment-410356589:54,error,error,54,https://hail.is,https://github.com/hail-is/hail/issues/4081#issuecomment-410356589,1,['error'],['error']
Availability,"oint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. ### Error No. 2; ```python; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 343 method = get_real_method(obj, self.print_method); 344 if method is not None:; --> 345 return method(); 346 return None; 347 else:. /usr/local/lib/python3.6/site-packages/hail/matrixtable.py in _repr_html_(self); 2524 ; 2525 def _repr_html_(self):; -> 2526 s = self.table_show._repr_html_(); 2527 if self.displayed_n_cols != self.actual_n_cols:; 2528 s += '<p style=""background: #fdd; padding: 0.4em;"">'. /usr/local/lib/python3.6/site-packages/hail/table.py in _repr_html_(self); 1256 ; 1257 def _repr_html_(self):; -> 1258 return self._html_str(); 1259 ; 1260 def _ascii_str(self):. /usr/local/lib/python3.6/site-packages/hail/table.py in _html_str(self); 1342 types = self.types; 1343 ; -> 1344 rows, has_more, d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:26179,Error,Error,26179,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['Error'],['Error']
Availability,"ok, so back to the same error. I think your Spark cluster must be configured incorrectly, which is something we can't really help with. You should be able to do:; ```; pyspark; ```; and then:; ```python; sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); ```; without error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321224012:24,error,error,24,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321224012,2,['error'],['error']
Availability,"ok. Figured it out. The issue is that the build of the new image finally succeeded after 18 minutes, but the test timed out and batch must have been shut down already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8440#issuecomment-614882682:154,down,down,154,https://hail.is,https://github.com/hail-is/hail/pull/8440#issuecomment-614882682,1,['down'],['down']
Availability,"ok1 totally goes away, we can probably remove the ""services"" permission. Before:; ```yaml; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services; apiGroup: """"; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchang",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5746:1666,error,error,1666,https://hail.is,https://github.com/hail-is/hail/pull/5746,1,['error'],['error']
Availability,"okay, now ready. In addition to the bugfix and expr array features, fixed a lingering stupid problem in our error traversal method",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/574#issuecomment-239036784:108,error,error,108,https://hail.is,https://github.com/hail-is/hail/pull/574#issuecomment-239036784,1,['error'],['error']
Availability,"okay, so this makes no sense to me, and i don't understand gradle at all really, but i tried reproducing the issue with each recent release until i found the one where it started presenting (0.2.123), then tried it on every commit in between the previous release and that one, and found that the issue started presenting after https://github.com/hail-is/hail/pull/13551 merged. i tried reverting that commit on the current `main` and confirmed the issue stopped showing up. i also tried downgrading just the `google-cloud-storage` version back to 2.17.1, since that was bumped in that commit, but the issue still presented.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13690#issuecomment-1737808187:487,down,downgrading,487,https://hail.is,https://github.com/hail-is/hail/issues/13690#issuecomment-1737808187,1,['down'],['downgrading']
Availability,"okay, so this must be some problem with the code, not the file format. Can you reproduce the error in the current version from the new files?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743#issuecomment-358467186:93,error,error,93,https://hail.is,https://github.com/hail-is/hail/issues/2743#issuecomment-358467186,1,['error'],['error']
Availability,oke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Hail version: devel-9a5678f; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3516:7837,Error,Error,7837,https://hail.is,https://github.com/hail-is/hail/issues/3516,1,['Error'],['Error']
Availability,"ol). For example:. ```; sema = asyncio.Semaphore(50); async with sema:; await copy(sema, ...); ```. Then, to run a set of operations in parallel, subject to the global parallelism bound, use bounded_gather2:. ```; await bounded_gather2(sema, *aws); ```. The naive implementation of bounded_gather2 doesn't work: bounded_gather2 cannot spawn a task for each awaitable and have it try to acquiare the semaphore, because this can lead to deadlock: if 50 tasks launch and wait for children, but none of those children can run because the parents have all the threads of control, the algorithm will deadlock. The key is to make sure at least one child can always be running in a bounded_gather2. But the caller of bounded_gather2 had a reserved thread of execution and it is blocking, so that thread of execution should be transferred to the children while the bounded_gather2 is blocked. This is what bounded_gather2 does. There is also an ""online"" version of bounded_gather2 which lets you schedule an unbounded number of children (potentially generated asynchronously). OnlineBoundedGather2 is used in parallelizing file transfers generated by directory listings, for example, which are enumerated via an async generator, and are potentially very large. I will try to replace bounded_gather and the async worker pool with this mechanism in a future PR. The parameters will likely need additional tuning. I have done some rough timing, and already this is beating gsutil:. - Transfer 10GB spread over 40K files (times in ms):. {'upload': 95803,; 'download': 55240,; 'compare': 54117,; 'clean file': 632,; 'clean gs': 117263,; 'total': 323061}. vs the gsutil transfer:. real	11m14.153s; user	14m28.789s; sys	1m29.090s. and gsutil cleanup (removing 40K files in gs://):. real	10m12.236s; user	3m33.382s; sys	0m55.450s. - Transfer 10GB in one file takes ~20s (up) and ~30s (down) for copy vs ~1m for gsutil. I'm still working on the benchmark harness and will post more complete comparisons in a future PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9934:2942,down,download,2942,https://hail.is,https://github.com/hail-is/hail/pull/9934,2,['down'],"['down', 'download']"
Availability,ollection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collect,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5365,Error,Error,5365,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Error'],['Error']
Availability,ollection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.ste,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:18691,Error,ErrorHandling,18691,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Error'],['ErrorHandling']
Availability,"ollowing error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(Thread.java:745)`. `Hail version: 0.1-4238176`; `Error summary: HailException: arguments refer to no files`. It's probably something quick, but I can't seem to figure it out?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2070:2050,Error,Error,2050,https://hail.is,https://github.com/hail-is/hail/issues/2070,1,['Error'],['Error']
Availability,"om an array of ‘const class simdpp::arch_avx2::int32<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:26,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:102:7: note: ‘class simdpp::arch_avx2::uint64<2>’ declared here; class uint64<2, void> : public any_int64<2, uint64<2,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int32<8>; T = simdpp::arch_avx2::int64<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int32<8>; T = simdpp::arch_avx2::int64<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int32<8>; T = simdpp::arch_avx2::int64<4>]’; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:55:35: required from ‘simdpp::arch_avx2::int32<8>& simdpp::arch_avx2::int32<8>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int64<4>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:291:13: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int32<8>’ with ‘private’ member ‘simdpp::arch_avx2::int32<8>::d_’ from an array of ‘const class simdpp::arch_avx2::int64<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:76036,Mask,MaskCastOverride,76036,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"om an array of ‘const class simdpp::arch_avx2::int32<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:91:7: note: ‘class simdpp::arch_avx2::uint8<32>’ declared here; class uint8<32, void> : public any_int8<32, uint8<32,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::uint8<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:58:35: required from ‘simdpp::arch_avx2::int32<4>::int32(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/detail/extract128.h:40:58: required from ‘simdpp::arch_avx2::int32x4 simdpp::arch_avx2::detail::extract128(const int32x8&) [with unsigned int s = 0; simdpp::arch_avx2::int32x4 = simdpp::arch_avx2::int32<4>; simdpp::arch_avx2::int32x8 = simdpp::arch_avx2::int32<8>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_reduce_add.h:356:49: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int32<4>’ wi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:122319,Mask,MaskCastOverride,122319,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"om an array of ‘const class simdpp::arch_avx2::int32<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:24,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:91:7: note: ‘class simdpp::arch_avx2::uint32<8>’ declared here; class uint32<8, void> : public any_int32<8, uint32<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::uint32<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::uint32<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::uint32<4>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:62:35: required from ‘simdpp::arch_avx2::int32<4>& simdpp::arch_avx2::int32<4>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint32<4>]’; libsimdpp-2.0-rc2/simdpp/core/split.h:96:8: required from ‘void simdpp::arch_avx2::split(const simdpp::arch_avx2::int32<N>&, simdpp::arch_avx2::int32<(N / 2)>&, simdpp::arch_avx2::int32<(N / 2)>&) [with unsigned int N = 8]’; libsimdpp-2.0-rc2/simdpp/detail/insn/to_int64.h:67:20: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int32<4>’ with ‘private’ member ‘simdpp::arch_avx2:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:118244,Mask,MaskCastOverride,118244,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"om an array of ‘const class simdpp::arch_avx2::int32<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:27,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:91:7: note: ‘class simdpp::arch_avx2::uint64<4>’ declared here; class uint64<4, void> : public any_int64<4, uint64<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int32<8>; T = simdpp::arch_avx2::uint64<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int32<8>; T = simdpp::arch_avx2::uint64<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int32<8>; T = simdpp::arch_avx2::uint64<4>]’; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:55:35: required from ‘simdpp::arch_avx2::int32<8>& simdpp::arch_avx2::int32<8>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint64<4, simdpp::arch_avx2::expr_bit_and<simdpp::arch_avx2::uint64<4, simdpp::arch_avx2::int32<8> >, simdpp::arch_avx2::uint64<4, simdpp::arch_avx2::uint64<4> > > >]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:305:32: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int32<8>’ with ‘private’ member ‘simdpp::arch_avx2::int32<8>::d_’ from an array of ‘const class simdpp::arch_avx2::uin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:81377,Mask,MaskCastOverride,81377,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"om an array of ‘const class simdpp::arch_avx2::int64<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:91:7: note: ‘class simdpp::arch_avx2::uint8<32>’ declared here; class uint8<32, void> : public any_int8<32, uint8<32,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int64<2>; T = simdpp::arch_avx2::uint8<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int64<2>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int64<2>; T = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:56:35: required from ‘simdpp::arch_avx2::int64<2>::int64(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/detail/extract128.h:42:58: required from ‘simdpp::arch_avx2::int64x2 simdpp::arch_avx2::detail::extract128(const int64x4&) [with unsigned int s = 0; simdpp::arch_avx2::int64x2 = simdpp::arch_avx2::int64<2>; simdpp::arch_avx2::int64x4 = simdpp::arch_avx2::int64<4>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_reduce_max.h:478:40: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int64<2>’ wi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:126433,Mask,MaskCastOverride,126433,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"om an array of ‘const class simdpp::arch_avx2::int64<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:27,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:91:7: note: ‘class simdpp::arch_avx2::uint64<4>’ declared here; class uint64<4, void> : public any_int64<4, uint64<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int64<8>; T = simdpp::arch_avx2::uint64<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int64<8>; T = simdpp::arch_avx2::uint64<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int64<8>; T = simdpp::arch_avx2::uint64<8>]’; libsimdpp-2.0-rc2/simdpp/types/int64.h:49:35: required from ‘simdpp::arch_avx2::int64<N>& simdpp::arch_avx2::int64<N>::operator=(const simdpp::arch_avx2::any_vec<(N * 8), V>&) [with V = simdpp::arch_avx2::uint64<8>; unsigned int N = 8]’; libsimdpp-2.0-rc2/simdpp/types/int64.h:42:73: required from ‘simdpp::arch_avx2::int64<N>::int64(const simdpp::arch_avx2::uint64<N, E>&) [with E = void; unsigned int N = 8]’; libsimdpp-2.0-rc2/simdpp/core/combine.h:80:69: required from ‘simdpp::arch_avx2::int64<(N * 2)> simdpp::arch_avx2::combine(const simdpp::arch_avx2::int64<N, E>&, const simdpp::arch_avx2::int64<N, E2>&) [with unsigned int N = 4; E1 = void; E2 = void]’; libsimdpp-2.0-rc2/simdpp/detail/insn/to_int64.h:70:2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:135143,Mask,MaskCastOverride,135143,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"om an array of ‘const class simdpp::arch_avx2::int8<32>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:91:7: note: ‘class simdpp::arch_avx2::uint8<32>’ declared here; class uint8<32, void> : public any_int8<32, uint8<32,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::int32<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::int32<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::int32<4>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:133:36: required from ‘simdpp::arch_avx2::uint32<4>& simdpp::arch_avx2::uint32<4>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int32<4, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_avg.h:250:8: required from ‘V simdpp::arch_avx2::detail::insn::v_emul_avg_i32(const V&, const V&) [with V = simdpp::arch_avx2::int32<4>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_avg.h:199:31: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint32<4>’ with ‘private’ member ‘simdpp::arch_avx2::uint32<4>::d_’",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:114235,Mask,MaskCastOverride,114235,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"om an array of ‘const class simdpp::arch_avx2::uint16<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:33:7: note: ‘class simdpp::arch_avx2::int16<8>’ declared here; class int16<8, void> : public any_int16<8, int16<8,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int16<16>; T = simdpp::arch_avx2::uint16<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int16<16>; T = simdpp::arch_avx2::uint16<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int16<16>; T = simdpp::arch_avx2::uint16<16>]’; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:56:36: required from ‘simdpp::arch_avx2::int16<16>& simdpp::arch_avx2::int16<16>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint16<16>]’; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:49:75: required from ‘simdpp::arch_avx2::int16<16>::int16(const simdpp::arch_avx2::uint16<16, E>&) [with E = void]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:64:29: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int16<16>’ with ‘private’ member ‘simdpp::arch_avx2::int16<16>::d_’ from an array of ‘const class simdpp::arc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:106124,Mask,MaskCastOverride,106124,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"om an array of ‘const class simdpp::arch_avx2::uint8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:26,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:33:7: note: ‘class simdpp::arch_avx2::int64<2>’ declared here; class int64<2, void> : public any_int64<2, int64<2,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int32<16>; T = simdpp::arch_avx2::uint32<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int32<16>; T = simdpp::arch_avx2::uint32<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int32<16>; T = simdpp::arch_avx2::uint32<16>]’; libsimdpp-2.0-rc2/simdpp/types/int32.h:50:35: required from ‘simdpp::arch_avx2::int32<N>& simdpp::arch_avx2::int32<N>::operator=(const simdpp::arch_avx2::any_vec<(N * 4), V>&) [with V = simdpp::arch_avx2::uint32<16>; unsigned int N = 16]’; libsimdpp-2.0-rc2/simdpp/types/int32.h:43:73: required from ‘simdpp::arch_avx2::int32<N>::int32(const simdpp::arch_avx2::uint32<N, E>&) [with E = void; unsigned int N = 16]’; libsimdpp-2.0-rc2/simdpp/core/combine.h:73:69: required from ‘simdpp::arch_avx2::int32<(N * 2)> simdpp::arch_avx2::combine(const simdpp::arch_avx2::int32<N, E>&, const simdpp::arch_avx2::int32<N, E2>&) [with unsigned int N = 8; E1 = void; E2 = void]’; libsimdpp-2.0-rc2/simdpp/detail/insn/to_int32.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:128489,Mask,MaskCastOverride,128489,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"om an array of ‘const class simdpp::arch_avx2::uint8<32>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:32:7: note: ‘class simdpp::arch_avx2::int8<32>’ declared here; class int8<32, void> : public any_int8<32, int8<32,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::float32<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::float32<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::float32<4>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:129:36: required from ‘simdpp::arch_avx2::uint32<4>::uint32(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::float32<4>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/test_bits.h:64:39: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint32<4>’ with ‘private’ member ‘simdpp::arch_avx2::uint32<4>::d_’ from an array of ‘const class simdpp::arch_avx2::float32<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:92299,Mask,MaskCastOverride,92299,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"ommand(None, pargs); 380 ; 381 def index_bgen(self, path):. /Users/tpoterba/hail/python/pyhail/context.pyc in run_command(self, vds, pargs); 43 jstate = self._jstate(vds.jvds if vds != None else None); 44 result = cmd.run(jstate,; ---> 45 cmd_args); 46 return VariantDataset(self, result.vds()); 47 . /Users/tpoterba/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 811 answer = self.gateway_client.send_command(command); 812 return_value = get_return_value(; --> 813 answer, self.gateway_client, self.target_id, self.name); 814 ; 815 for temp_arg in temp_args:. /Users/tpoterba/spark-1.6.2-bin-hadoop2.6/python/pyspark/sql/utils.pyc in deco(*a, **kw); 43 def deco(*a, **kw):; 44 try:; ---> 45 return f(*a, **kw); 46 except py4j.protocol.Py4JJavaError as e:; 47 s = e.java_exception.toString(). /Users/tpoterba/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 306 raise Py4JJavaError(; 307 ""An error occurred while calling {0}{1}{2}.\n"".; --> 308 format(target_id, ""."", name), value); 309 else:; 310 raise Py4JError(. Py4JJavaError: An error occurred while calling o92.run.; : org.broadinstitute.hail.utils.package$FatalException: arguments refer to no files; 	at org.broadinstitute.hail.utils.package$.fatal(package.scala:27); 	at org.broadinstitute.hail.driver.VCFImporter$class.globAllVcfs(ImportVCF.scala:17); 	at org.broadinstitute.hail.driver.ImportVCF$.globAllVcfs(ImportVCF.scala:33); 	at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:83); 	at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:33); 	at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:258); 	at org.broadinstitute.hail.driver.Command.run(Command.scala:263); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1096:1491,error,error,1491,https://hail.is,https://github.com/hail-is/hail/issues/1096,1,['error'],['error']
Availability,"ommand); -> 1321 return_value = get_return_value(; 1322 answer, self.gateway_client, self.target_id, self.name); 1324 for temp_arg in temp_args:; 1325 temp_arg._detach(). File /private/tmp/hail/hail/python/hail/backend/py4j_backend.py:35, in handle_java_exception.<locals>.deco(*args, **kwargs); 33 tpl = Env.jutils().handleForPython(e.java_exception); 34 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 35 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 36 except pyspark.sql.utils.CapturedException as e:; 37 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 38 'Hail version: %s\n'; 39 'Error summary: %s' % (e.desc, e.stackTrace, hail.__version__, e.desc)) from None. FatalError: ClassCastException: class is.hail.types.virtual.TStruct cannot be cast to class is.hail.types.virtual.TIterable (is.hail.types.virtual.TStruct and is.hail.types.virtual.TIterable are in unnamed module of loader 'app'). Java stack trace:; java.lang.RuntimeException: typ: inference failure:; 	at is.hail.expr.ir.IR.typ(IR.scala:38); 	at is.hail.expr.ir.IR.typ$(IR.scala:33); 	at is.hail.expr.ir.ToStream.typ(IR.scala:300); 	at is.hail.expr.ir.IRParser$.$anonfun$ir_value_expr_1$81(Parser.scala:1111); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_value_ir$1(Parser.scala:2157); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:2153); 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:2157); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_value_ir$2(SparkBackend.scala:691); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.annotations.RegionPool$.scop",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13699:3493,failure,failure,3493,https://hail.is,https://github.com/hail-is/hail/issues/13699,1,['failure'],['failure']
Availability,"ommit/398c14c05c6448b380ac35c6095598299c5e23c5""><code>398c14c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/15cf7eecfbc17d2466143828b9b69494c6cb6f2b""><code>15cf7ee</code></a> Bump up version number to 5.3.1-SNAPSHOT</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/e3c65ffcb49b9c5a33fde5f31fb63043dbf21134""><code>e3c65ff</code></a> Allow extensions to be created from tasks</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/34e2dd41477f18b1ae3d6d5a71dca5449d6cd1e0""><code>34e2dd4</code></a> Downgrade slf4j to fix warning on console about missing slf4j provider</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b3fa29f9ffb4d4544e13ef84601e371fb2778ddf""><code>b3fa29f</code></a> Revert &quot;Update Apache HttpClient to 5.2.1&quot;</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/01f05e046be0dca18f506723c79e88f208336e71""><code>01f05e0</code></a> Add integration tests for Gradle 6.9.3 and 7.6</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a998a544908a8b39f713f4526f717fcb328c06eb""><code>a998a54</code></a> Upgrade Gradle to 7.6</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/5.3.0...5.3.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=5.3.0&new-version=5.3.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-auto",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:2787,down,download-task,2787,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['down'],['download-task']
Availability,"on 100 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; /broad/software/free/Linux/redhat_7_x86_64/pkgs/jdk1.8.0_181/bin/java: symbol lookup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1164, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ---------------------------------------------------------------------------; Py4JError Traceback (most recent call last); <ipython-input-3-9b1033d0ac55> in <module>; ----> 1 t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]). <decorator-gen-1332> in linear_regression_rows(y, x, covariates, block_size, pass_through). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/methods/statgen.py in linear_regression_rows(y, x, covariates, block_size, pass_through); 434 ht_result = ht_result.annotate(**{f: ht_result[f][0] for f ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5559:2332,Error,Error,2332,https://hail.is,https://github.com/hail-is/hail/issues/5559,1,['Error'],['Error']
Availability,"on dataproc and trying to import a vcf from the local filesystem; run(""wget ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh{0}/clinvar.vcf.gz -O /tmp/clinvar.vcf.gz"".format(args.genome_version)); run(""ls -l /tmp/clinvar.vcf.gz""); vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); The output is; ls -l /tmp/clinvar.vcf.gz; -rw-r--r-- 1 root root 16218805 Jun 14 20:21 /tmp/clinvar.vcf.gz; Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:1322,failure,failure,1322,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['failure'],['failure']
Availability,"on-autosomes for `concordance`. Importing two VCFs, splitting multi, and `describe()`, `count()`, and `_force_count_rows()` all behave as expected. ```; import hail as hl. giab_gs_path = 'gs://xxxxxxxx'; giab_ds = hl.import_vcf(giab_gs_path, reference_genome='GRCh38'); giab_sm = hl.SplitMulti(giab_ds); giab_ds = giab_sm.result(); giab_ds.describe(); gaib_ds.count(); gaib_ds._force_count_rows(); # all good. sent_gs_path = 'gs://xxxxxxxxxxx'; sent_ds = hl.import_vcf(sent_gs_path, reference_genome='GRCh38'); sent_sm = hl.SplitMulti(sent_ds); sent_ds = sent_sm.result(); sent_ds.describe(); sent_ds.count(); sent_ds._force_count_rows(); # all good. # then this gives the stack trace below. giab_22_ds = hl.filter_intervals(giab_ds, [hl.parse_locus_interval('chr22', reference_genome='GRCh38')]); ```. Stack trace:. ```; FatalError: ClassCastException: is.hail.codegen.generated.C29 cannot be cast to is.hail.asm4s.AsmFunction5. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 20 times, most recent failure: Lost task 0.19 in stage 19.0 (TID 312, gilson-validation-test-2-w-4.c.perfect-atrium-179917.internal, executor 2): java.lang.ClassCastException: is.hail.codegen.generated.C29 cannot be cast to is.hail.asm4s.AsmFunction5; 	at is.hail.expr.TableMapRows$$anonfun$execute$5.apply(Relational.scala:1641); 	at is.hail.expr.TableMapRows$$anonfun$execute$5.apply(Relational.scala:1637); 	at is.hail.sparkextras.ContextRDD$$anonfun$mapPartitions$1.apply(ContextRDD.scala:151); 	at is.hail.sparkextras.ContextRDD$$anonfun$mapPartitions$1.apply(ContextRDD.scala:151); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19$$anonfun$apply$20.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.Con",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:1086,failure,failure,1086,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,1,['failure'],['failure']
Availability,"on-ebs: Collecting google-cloud-storage==1.25.*; 934 | amazon-ebs: Downloading google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); 935 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 22.1 MB/s eta 0:00:00; 936 | amazon-ebs: Collecting humanize==1.0.0; 937 | amazon-ebs: Downloading humanize-1.0.0-py2.py3-none-any.whl (51 kB); 938 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.9/51.9 kB 14.6 MB/s eta 0:00:00; 939 | amazon-ebs: Collecting hurry.filesize==0.9; 940 | amazon-ebs: Downloading hurry.filesize-0.9.tar.gz (2.8 kB); 941 | amazon-ebs: Preparing metadata (setup.py): started; 942 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 943 | amazon-ebs: Collecting janus<1.1,>=0.6; 944 | amazon-ebs: Downloading janus-1.0.0-py3-none-any.whl (6.9 kB); 945 | amazon-ebs: Requirement already satisfied: Jinja2==3.0.3 in /usr/local/lib/python3.7/site-packages (3.0.3); 946 | amazon-ebs: Collecting nest_asyncio==1.5.4; 947 | amazon-ebs: Downloading nest_asyncio-1.5.4-py3-none-any.whl (5.1 kB); 948 | amazon-ebs: Requirement already satisfied: numpy<2 in /usr/local/lib64/python3.7/site-packages (1.21.6); 949 | amazon-ebs: Collecting orjson==3.6.4; 950 | amazon-ebs: Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB); 951 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 249.9/249.9 kB 45.1 MB/s eta 0:00:00; 952 | amazon-ebs: Requirement already satisfied: pandas<1.5.0,>=1.3.0 in /usr/local/lib64/python3.7/site-packages (1.3.5); 953 | amazon-ebs: Collecting parsimonious<0.9; 954 | amazon-ebs: Downloading parsimonious-0.8.1.tar.gz (45 kB); 955 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.1/45.1 kB 10.2 MB/s eta 0:00:00; 956 | amazon-ebs: Preparing metadata (setup.py): started; 957 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 958 | amazon-ebs: Collecting plotly<5.11,>=5.5.0; 959 | amazon-ebs: Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB); 960 | amazon-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:5565,Down,Downloading,5565,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"on.; o CODEC-289: Base32/Base64 Input/OutputStream: Added strict decoding property; to control handling of trailing bits. Default lenient mode; discards them without error. Strict mode raise an exception.; o Update tests from JUnit 4.12 to 4.13. Thanks to Gary Gregory.; o Update actions/checkout from v1 to v2.3.2 <a href=""https://github-redirect.dependabot.com/apache/commons-codec/issues/50"">#50</a>, <a href=""https://github-redirect.dependabot.com/apache/commons-codec/issues/56"">#56</a>.; Thanks to Dependabot.; o Update actions/setup-java from v1.4.0 to v1.4.1 <a href=""https://github-redirect.dependabot.com/apache/commons-codec/issues/57"">#57</a>.; Thanks to Dependabot.</p>; <p>For complete information on Apache Commons Codec, including instructions on how; to submit bug reports, patches, or suggestions for improvement, see the; Apache Commons Codec website:</p>; <p><a href=""https://commons.apache.org/proper/commons-codec/"">https://commons.apache.org/proper/commons-codec/</a></p>; <p>Download page: <a href=""https://commons.apache.org/proper/commons-codec/download_codec.cgi"">https://commons.apache.org/proper/commons-codec/download_codec.cgi</a></p>; <hr />; <pre><code> Apache Commons Codec 1.14 RELEASE NOTES; December 30 2019; </code></pre>; <p>The Apache Commons Codec package contains simple encoder and decoders for; various formats such as Base64 and Hexadecimal. In addition to these; widely used encoders and decoders, the codec package also maintains a</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/apache/commons-codec/commit/c89d2af770f05457fbefa5fb4713c888bf177fb2""><code>c89d2af</code></a> Prepare for 1.15 release</li>; <li><a href=""https://github.com/apache/commons-codec/commit/ba81ed5dc06661d931a4bb8f7abaa51ee5300396""><code>ba81ed5</code></a> Use gav=true for the maven central redirect</li>; <li><a href=""https://github.com/apache/commons-codec/commi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12385:2218,Down,Download,2218,https://hail.is,https://github.com/hail-is/hail/pull/12385,1,['Down'],['Download']
Availability,"on/dev/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; jupyter 1.0.0 requires qtconsole, which is not installed.; jupyter 1.0.0 requires notebook, which is not installed.; curlylint 0.13.1 requires pathspec, which is not installed.; beautifulsoup4 4.12.2 requires soupsieve, which is not installed.; astroid 2.15.8 requires lazy-object-proxy, which is not installed.; argon2-cffi-bindings 21.2.0 requires cffi, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `7.34.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Access Control Bypass <br/>[SNYK-PYTHON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14211:1383,avail,available,1383,https://hail.is,https://github.com/hail-is/hail/pull/14211,1,['avail'],['available']
Availability,"on3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in persist(self, storage_level); 2110 Persisted table.; 2111 """"""; -> 2112 return Env.backend().persist(self); 2113 ; 2114 def unpersist(self) -> 'Table':. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py in persist(self, dataset); 167 from hail.context import TemporaryFilename; 168 tempfile = TemporaryFilename(prefix=f'persist_{type(dataset).__name__}'); --> 169 persisted = dataset.checkpoint(tempfile.__enter__()); 170 self._persisted_locations[persisted] = (tempfile, dataset); 171 return persisted. <decorator-gen-1330> in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals); 1329 ; 1330 if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; -> 1331 self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); 1332 _assert_type = self._type; 1333 _load_refs = False. <decorator-gen-1332> in write(self, output, overwrite, stage_locally, _codec_spec). /opt/conda/miniconda3/lib/python3.10/site-package",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:3802,checkpoint,checkpoint,3802,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,1,['checkpoint'],['checkpoint']
Availability,"on=4.4.11 -Daws.java.sdk.version=1.12.446 -Daws.kinesis.client.version=1.12.0 -Daws.kinesis.producer.version=0.12.9 -Dscala.version=2.12.15 -DrecompileMode=all -Dmaven.deploy.plugin.version=2.8.2 -Dmaven.scaladoc.skip -Pyarn -Phadoop-3.2 -Phive -Phive-thriftserver -Psparkr -Pspark-ganglia-lgpl -Pnetlib-lgpl -Pscala-2.12 -Pkubernetes -Pvolcano -Pkinesis-asl -DskipTests; ```; I still did not found why scala is downgraded to 2.12.13. <details><summary>Hail logs</summary>; <p>; # Build Hail #; WARNING: Package(s) not found: hail; REVISION is set to ""13536b531342a263b24a7165bfeec7bd02723e4b"" which is different from old value """"; printf ""13536b531342a263b24a7165bfeec7bd02723e4b"" > env/REVISION; echo 13536b531342a263b24a7165bfeec7bd02723e4b > python/hail/hail_revision; SHORT_REVISION is set to ""13536b531342"" which is different from old value """"; printf ""13536b531342"" > env/SHORT_REVISION; HAIL_PIP_VERSION is set to ""0.2.124"" which is different from old value """"; printf ""0.2.124"" > env/HAIL_PIP_VERSION; echo 0.2.124-13536b531342 > python/hail/hail_version; echo 0.2.124 > python/hail/hail_pip_version; cp -f python/hail/hail_version python/hailtop/hail_version; printf 'hail_version=""0.2.124-13536b531342"";' > python/hail/docs/_static/hail_version.js; printf 'hail_pip_version=""0.2.124""' >> python/hail/docs/_static/hail_version.js; cloud_base is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_note",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:2076,echo,echo,2076,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,"one more change required to error message in AST:. ```; s""""""Tried to access index [$i] on array ${ JsonMethods.compact(localT.toJSON(a)) } of length ${ a.length }; | Hint: All arrays in Hail are zero-indexed (`array[0]' is the first element); | Hint: For accessing `A'-numbered info fields in split variants, `va.info.field[va.aIndex]' is correct"""""".stripMargin); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/658#issuecomment-242136648:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/pull/658#issuecomment-242136648,1,['error'],['error']
Availability,"one-any.whl|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-standard-8', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--labels=creator=weisburd_broadinstitute_org', '--max-idle=12h']' returned non-zero exit status 1.; ```. Then looking at the error log; ```; $ gsutil cat gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output; pip packages are ['setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclient==2.0.0', 'websocket-client', 'sklearn', 'tabulate', 'statsmodels', 'scikit-learn', 'hdbscan', 'matplotlib']; b""Double requirement given: tabulate (already in tabulate==0.8.3, name='tabulate')\nYou are using pip version 10.0.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n""; Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 16, in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6634:4840,error,error,4840,https://hail.is,https://github.com/hail-is/hail/issues/6634,1,['error'],['error']
Availability,"onents/initialize/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/post-install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/activate/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh; /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/docker-ce.done; /tmp/dataproc/components/pre-uninstall/docker-ce.running; /tmp/dataproc/components/pre-uninstall/docker-ce.done; /etc/apt/preferences.d/docker-ce.pref; /etc/apt/preferences.d/docker-ce-cli.pref; /etc/apt/sources.list.d/docker.list; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_InRelease; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_stable_binary-amd64_Packages; ```. </details>. There is a `/run/docker.sock` but notice it is not `/var/run/...`. However, if I install Docker by hand into this worker of a *non-Hail* Dataproc cluster, it just works. ---. I also tried to replicate the failure using an initialization action, but that also just worked.; ```; gcloud dataproc clusters create dk-test2 --initialization-actions=gs://hail-common/dk-test.sh; ```; `gs://hail-common/dk-test.sh`:; ```; apt-get update; apt-get -y install \; apt-transport-https \; ca-certificates \; curl \; gnupg2 \; software-properties-common \; tabix; curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -; sudo add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable""; apt-get update; apt-get install -y --allow-unauthenticated docker-ce; ```. ---. Our users often report this error. In my experience, it has hap",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:13049,down,download,13049,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['down'],['download']
Availability,"ons.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:24.240 : INFO: RegionPool: REPORT_THRESHOLD: 4.3M allocated (2.2M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:24.240 GoogleStorageFS$: INFO: createNoCompression: gs://aou_tmp/tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89; 2023-09-24 01:58:24.305 GoogleStorageFS$: INFO: close: gs://aou_tmp/tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89; 2023-09-24 01:58:51.513 : INFO: TaskReport: stage=0, partition=9571, attempt=0, peakBytes=4507648, peakBytesReadable=4.30 MiB, chunks requested=51, cache hits=0; 2023-09-24 01:58:51.513 : INFO: RegionPool: FREE: 4.3M allocated (2.2M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:51.515 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Cau",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721:3798,ERROR,ERROR,3798,https://hail.is,https://github.com/hail-is/hail/issues/13721,1,['ERROR'],['ERROR']
Availability,"onsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [...]; 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:478); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:488); 	at is.hail.rvd.OrderedRVD$.coerce(OrderedRVD.scala:556); 	at is.hail.rvd.OrderedRVD$.coerce(OrderedRVD.scala:514); 	at is.hail.table.Table.toOrderedRVD(Table.scala:1152); 	at is.hail.table.Table.distinctByKey(Table.scala:540); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); ```; @gtiao got a similar error running ld_prune:; ```; Traceback (most recent call last):; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/assign_subpops.py"", line 157, in <module>; main(args); File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/assign_subpops.py"", line 98, in main; pca_mt = hl.ld_prune(pca_mt, r2=0.1, n_cores=args.num_cores); File ""<decorator-gen-788>"", line 2, in ld_prune; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/hail-devel-38dbf156b630.zip/hail/typecheck/check.py"", line 490, in _typecheck; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/hail-devel-38dbf156b630.zip/hail/methods/statgen.py"", line 2918, in ld_prune; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/hail-devel-38dbf156b630.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: ClassCastException: is.hail.codegen.generated.C14 cannot be cast to is.hail.asm4s.AsmFunction2; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446:2155,error,error,2155,https://hail.is,https://github.com/hail-is/hail/issues/3446,1,['error'],['error']
Availability,"ook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1445,toler,tolerate,1445,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878,1,['toler'],['tolerate']
Availability,"ool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:310); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:449); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:448); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.98-f8833c1ae16b; Error summary: SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.project.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:; ```; We also tried using a similar cluster configuration with dynamic scaling and received the same error. Do you have a recommended cluster configuration to run king() on this many samples?. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:7823,Error,Error,7823,https://hail.is,https://github.com/hail-is/hail/issues/12290,5,"['Error', 'error', 'failure', 'heartbeat']","['Error', 'error', 'failure', 'heartbeat']"
Availability,"oop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^. Java stack trace:; is.hail.utils.HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2655:2435,Error,ErrorHandling,2435,https://hail.is,https://github.com/hail-is/hail/issues/2655,1,['Error'],['ErrorHandling']
Availability,oops my fault.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4394#issuecomment-423657075:8,fault,fault,8,https://hail.is,https://github.com/hail-is/hail/issues/4394#issuecomment-423657075,1,['fault'],['fault']
Availability,oops. thanks for preempting the test failure!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4380#issuecomment-423268427:37,failure,failure,37,https://hail.is,https://github.com/hail-is/hail/pull/4380#issuecomment-423268427,1,['failure'],['failure']
Availability,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457:1719,FAILURE,FAILURE,1719,https://hail.is,https://github.com/hail-is/hail/issues/457,1,['FAILURE'],['FAILURE']
Availability,op.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 1 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 1 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 1; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 2 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.Future,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:75536,ERROR,ERROR,75536,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,op.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 5 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 5 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 5; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 10 on scc-q20.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.Future,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:98443,ERROR,ERROR,98443,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,op.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 BlockManagerMasterEndpoint: INFO: Trying to remove executor 9 from BlockManagerMaster.; 2019-01-22 13:11:55 BlockManagerMaster: INFO: Removal of executor 9 requested; 2019-01-22 13:11:55 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 9; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 3 on scc-q09.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.Future,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:126080,ERROR,ERROR,126080,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,"or('%s\n\nJava stack trace:\n%s\n'; 199 'Hail version: %s\n'; --> 200 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 201 except pyspark.sql.utils.CapturedException as e:; 202 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: RuntimeException: Cannot find row in Map(). Java stack trace:; is.hail.utils.HailException: Error while typechecking IR:; (MakeStruct; (titv; (ApplyBinaryPrimOp FloatingPointDivide; (GetField n_ti; (Ref Struct{rank_id:String,snv:Boolean,bi_allelic:Boolean,singleton:Boolean,bin:Int32,min_score:Float64,max_score:Float64,n_ti:Int64,n_tv:Int64,model:String} row)); (GetField n_tv; (Ref Struct{rank_id:String,snv:Boolean,bi_allelic:Boolean,singleton:Boolean,bin:Int32,min_score:Float64,max_score:Float64,n_ti:Int64,n_tv:Int64,model:String} row)))); (min_score; (GetField `0`; (In Struct{`0`:Float64,`1`:Float64} 0))); (max_score; (GetField `1`; (In Struct{`0`:Float64,`1`:Float64} 0)))); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:11); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:11); 	at is.hail.expr.ir.Emit$.emit(Emit.scala:42); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:28); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:51); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:31); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:74); 	at is.hail.expr.ir.TableKeyByAndAggregate.execute(TableIR.scala:843); 	at is.hail.table.Table.value$lzycompute(Table.scala:215); 	at is.hail.table.Table.value(Table.scala:213); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:218); 	at is.hail.table.Table.x$5(Table.scala:218); 	at is.hail.table.Table.rvd$lzycompute(Table.scala:218); 	at is.hail.table.Table.rvd(Table.scala:218); 	at is.hail.table.Table.take(Table.scala:649); 	at is.hail.table.Table.showString(Table.scala:685); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAcc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4110:3294,Error,ErrorHandling,3294,https://hail.is,https://github.com/hail-is/hail/issues/4110,1,['Error'],['ErrorHandling']
Availability,"or.run(DefaultThreadFactory.java:144); 	at java.lang.Thread.run(Thread.java:748); </details>. <details>; <summary>Working hail.log</summary>. ```; 2018-10-09 15:04:33 Hail: INFO: SparkUI: http://10.32.119.167:4040; 2018-10-09 15:04:33 Hail: INFO: Running Hail version devel-17a988f2a628; 2018-10-09 15:04:33 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 15:04:33 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 15:04:33 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@16ba3696{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2780d0b8{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7cea1161{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@696b1f0{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@14d32b0c{/static/sql,null,AVAILABLE,@Spark}; 2018-10-09 15:04:34 StateStoreCoordinatorRef: INFO: Registered StateStoreCoordinator endpoint; 2018-10-09 15:04:34 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:34 SparkSqlParser: INFO: Parsing command: SHOW TABLES; 2018-10-09 15:04:36 SparkContext: INFO: Starting job: collect at utils.scala:44; 2018-10-09 15:04:36 DAGScheduler: INFO: Got job 0 (collect at utils.scala:44) with 1 output partitions; 2018-10-09 15:04:36 DAGScheduler: INFO: Final stage: ResultStage 0 (collect at utils.scala:44); 2018-10-09 15:04:36 DAGScheduler: INFO: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:14258,AVAIL,AVAILABLE,14258,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['AVAIL'],['AVAILABLE']
Availability,"orage, the ServiceBackend submits the one-job driver batch to Hail; Batch. It then waits for the driver job to complete. When the driver job is finished, it reads the; outputs from google cloud storage and returns the results to the user. All the meaningful changes to batch are in the worker. The worker starts one jvm per core on; startup. The mainclass is a new class called `JVMEntryway`. This entryway starts a UNIX socket and; speaks a very simple binary protocol. It accepts only one type of message:. ```; int32 the number of strings to expect; (; int32 the number of bytes in the next string; byte* UTF-8 string; )*; ```. The array of strings is interpreted as:. ```; comma-spearated-classpath; main-class-name; arg0; arg1; ...; ```. The entryway constructs a URLClassLoader with the given classpath, reflectively allocates an; instance of the mainclass and invokes the `main` method with the remaining arguments. This is; obviously a security risk. The system bans JARs from locations not controlled (and locked down) by; Hail Team. You should require me to hardcode the mainclass as; `is.hail.backend.service.ServiceBackendSocketAPI2` before we merge; however, this flexibility was; useful during development. The JVMEntryway will eventually be useful because we will keep a ClassLoader full of a bunch of; JIT-optimized Hail classes. I did not include that in this PR because we need to finish eliminating; global state used by Hail. Currently, two executions would try to re-use compiled class names for; different code, leading to very weird errors. # Changes to File Systems. Hail has three four file system interfaces:. | File System Interface | Public | Language | Async |; | ----------------------- | ------ | -------- | ----- |; | hail.utils.hadoop_utils | Yes | Python | no |; | hail.fs | Yes | Python | no |; | hailtop.aiotools.fs | No | Python | yes |; | is.hail.io.fs | No | Scala | no |. `hail.fs` is technically in the public API (via `hl.current_backend().fs`), but I doubt ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194:3701,down,down,3701,https://hail.is,https://github.com/hail-is/hail/pull/11194,1,['down'],['down']
Availability,"orczynski""><code>@​DavidKorczynski</code></a> in <a href=""https://github-redirect.dependabot.com/python-jsonschema/jsonschema/pull/965"">python-jsonschema/jsonschema#965</a></li>; <li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/python-jsonschema/jsonschema/pull/967"">python-jsonschema/jsonschema#967</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.6.0...v4.6.1"">https://github.com/python-jsonschema/jsonschema/compare/v4.6.0...v4.6.1</a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/python-jsonschema/jsonschema/blob/main/CHANGELOG.rst"">jsonschema's changelog</a>.</em></p>; <blockquote>; <h2>v4.6.1</h2>; <ul>; <li>Gut the (incomplete) implementation of <code>recursiveRef</code> on draft 2019. It; needs completing, but for now can lead to recursion errors (e.g. <a href=""https://github-redirect.dependabot.com/python-jsonschema/jsonschema/issues/847"">#847</a>).</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python-jsonschema/jsonschema/commit/12c793cf1f3c11d038b1c097d560871180976138""><code>12c793c</code></a> v4.6.1 -&gt; CHANGELOG</li>; <li><a href=""https://github.com/python-jsonschema/jsonschema/commit/19595b5e19098240fa82ba0e85bc9c3a6c674aa7""><code>19595b5</code></a> Gut the meat of the recursiveRef implementation for draft2019</li>; <li><a href=""https://github.com/python-jsonschema/jsonschema/commit/5fea5596f9768a08a4ab4deb4bc5682b42e10ca4""><code>5fea559</code></a> Temporarily remove the future-keywords tests which were removed upstream.</li>; <li><a href=""https://github.com/python-jsonschema/jsonschema/commit/2f5b4d63ec82e5ec1b55f73d84f09be7a62fd32c""><code>2f5b4d6</code></a> Merge commit '118726fe2085ea58d9b3c1bd4764b389e8df8842'</li>; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11981:1665,error,errors,1665,https://hail.is,https://github.com/hail-is/hail/pull/11981,1,['error'],['errors']
Availability,"order as listed"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; ##INFO=<ID=BaseQRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities"">; ##INFO=<ID=DB,Number=0,Type=Flag,Description=""dbSNP Membership"">; ##INFO=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth; some reads may have been filtered"">; ##INFO=<ID=DS,Number=0,Type=Flag,Description=""Were any of the samples downsampled?"">; ##INFO=<ID=Dels,Number=1,Type=Float,Description=""Fraction of Reads Containing Spanning Deletions"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##INFO=<ID=FS,Number=1,Type=Float,Description=""Phred-scaled p-value using Fisher's exact test to detect strand bias"">; ##INFO=<ID=HaplotypeScore,Number=1,Type=Float,Description=""Consistency of the site with at most two segregating haplotypes"">; ##INFO=<ID=InbreedingCoeff,Number=1,Type=Float,Description=""Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation"">; ##INFO=<ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">; ##INFO=<ID=MQ0,Number=1,Type=Integer,Description=""Total Mapping Quality Zero Reads"">; ##INFO=<ID=MQRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities"">; ##INFO=<ID=OriginalContig,Number=1,Type=String,Description=""The name of the source contig/chromosome prior to liftover."">; ##INFO=<ID=OriginalStart,Number=1,Type=String,Description=""The p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:6991,down,downsampled,6991,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['down'],['downsampled']
Availability,orderedRVD assertion error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3959:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/issues/3959,1,['error'],['error']
Availability,"original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 142 ; 143 edges = t.key_by().select('i', 'j'); --> 144 nodes_in_set = Env.hail().utils.Graph.maximalIndependentSet(edges._jt.collect(), node_t._jtype, joption(tie_breaker_str)); 145 ; 146 nt = Table._from_java(nodes._jt.annotateGlobal(nodes_in_set, hl.tset(node_t)._jtype, 'nodes_in_set')). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: MatchError: cmg_exomes (of class java.lang.String). Java stack trace:; scala.MatchError: cmg_exomes (of class java.lang.String); 	at is.hail.annotations.RegionValueBuilder.addAnnotation(RegionValueBuilder.scala:537); 	at is.hail.annotations.RegionValueBuilder.addRow(RegionValueBuilder.scala:298); 	at is.hail.annotations.RegionValueBuilder.addAnnotation(RegionValueBuilder.scala:541); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:60); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:54); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:54); 	at is.hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4857:2696,Error,Error,2696,https://hail.is,https://github.com/hail-is/hail/issues/4857,1,['Error'],['Error']
Availability,"orization(A, W, p)`, and `R = U_1 S V'_1` is an SVD, then `W' f(A'A) W` is well-approximated by `V_1[:k, :] f(S^2) V_1[:k, :]'`, and this is exact if `f` is a degree-p polynomial. We can combine the above into an estimator for the p-th spectral moment, `𝜇_p = ∑_i 𝜆_i^p`. We get an unbiased estimator by generating a random vector `v`, and using `E(v' (A'A)^p v) = tr((A'A)^p) = 𝜇_p`. The estimator can be computed exactly using the Krylov factorization as above, i.e. `v' (A'A)^p v = V_1[0, :] S^{2p} V_1[0, :]'`. But this estimator has large variance, so we can just average over many independent estimators. We combine `k` random vectors into a random matrix `V_0`, compute `_krylov_factorization(A, V_0, p)`, and then `V'_0 (A'A)^p V_0 = V_1[:k, :] f(S^2) V_1[:k, :]'`, from which we can average the individual estimates. This spectral moments estimator is implemented in `KrylovFactorization.spectral_moments`. # SVD and moments; Finally, in practice we may need to average over too many estimators to get the desired accuracy, so we use one more trick to bring down the variance. This depends on the following property:; * If `Q` is an orthonormal matrix, then `tr(X) = tr(QQ'XQQ') + tr((I-QQ')X(I-QQ'))`, i.e. the trace decomposes into a sum of the traces on the projection onto the subspace spanned by `Q`, and the projection onto its complement. To see this, use the additivity and cyclicity of the trace (`tr(X+Y) = tr(X) + tr(Y)`, `tr(XY) = tr(YX)`). In particular, cyclicity implies `tr(QQ'XQQ') = tr(QQ'X) = tr(XQQ')`. Now we do two passes. In the first pass, we use some method (such as a Krylov factorization) to compute an orthonormal basis `V` for a subspace which approximates the dominant subspace. Even if this is not a very accurate approximation, the result is that `tr(QQ' X^p QQ')` is much larger than the complement `tr((I-QQ')X^p(I-QQ'))`. But the former we can compute exactly, and all the error comes from our estimate of the much smaller piece. This variance reduction tr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11045:3324,down,down,3324,https://hail.is,https://github.com/hail-is/hail/pull/11045,1,['down'],['down']
Availability,"ormat):; ...; ```. The command decorator replaces the function with one that takes a `List[str]` of command line parameters, parses them, and calls the original function. The option options are pretty self-explanatory. - To access an argument to a group (like `dataproc --beta`) in a (sub)command, use the decorator `click.pass_context` to pass the click context which allows you to access parent group parameters. `dataproc start` is an example:. ```; @dataproc.command(; help=""Start a Dataproc cluster configured for Hail.""); @click.argument('cluster_name'); ...; @click.pass_context; def start(ctx, cluster_name, ...):; beta = ctx.parent.params['beta']; ```. The help output for a group looks like this:. ```; $ hailctl dataproc --help; Usage: hailctl dataproc [OPTIONS] COMMAND [ARGS]... Manage and monitor Hail deployments. Options:; --beta Force use of `beta` in gcloud commands; --help Show this message and exit. Commands:; connect Connect to a running Dataproc cluster; describe Gather information about a Hail (Table or MatrixTable) file...; diagnose Diagnose problems in a Dataproc cluster.; list List Dataproc clusters.; modify; start Start a Dataproc cluster configured for Hail.; stop Shut down a Dataproc cluster.; submit Submit a Python script to a running Dataproc cluster.; ```. The help output for a command looks like:. ```; $ hailctl batch get --help; Usage: hailctl batch get [OPTIONS] BATCH_ID. Get a particular batch's info. Options:; -o, --output-format [yaml|json]; Specify output format [default: yaml]; --help Show this message and exit.; ```. I also made `BatchClient` a context manager and made the default limit unbounded in `BatchClient.list_batches`. I have marked this WIP until we are happy with the interface changes and how we're going to communicate them to the users. CHANGELOG: Rewrote the hailctl argument parsing code, which made some incompatible changes to the way hailctl handles arguments. For more details, see [the URL of this PR]. Spice level: medium.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842:3194,down,down,3194,https://hail.is,https://github.com/hail-is/hail/pull/9842,1,['down'],['down']
Availability,"ors</li>; <li><a href=""https://github.com/tqdm/tqdm/commit/8fb3d91f561e2a286a7fda13291eda16613dac39""><code>8fb3d91</code></a> fix ipywidgets&gt;=8 display</li>; <li><a href=""https://github.com/tqdm/tqdm/commit/05e3d32a5fc8559e133e6d627d44afda93018637""><code>05e3d32</code></a> fix jupyterlab display</li>; <li><a href=""https://github.com/tqdm/tqdm/commit/4f208e72552c4d916aa4fe6a955349ee8b2ed353""><code>4f208e7</code></a> bump version, merge branch 'slack'</li>; <li><a href=""https://github.com/tqdm/tqdm/commit/1d29dec4b07de3dab34d3557baa9520cd9d46e38""><code>1d29dec</code></a> add <code>[slack]</code> extra dependency</li>; <li><a href=""https://github.com/tqdm/tqdm/commit/4a1d10e19fdca00db47fd50725715dc5e4aa68e6""><code>4a1d10e</code></a> consistent ordering</li>; <li><a href=""https://github.com/tqdm/tqdm/commit/bf6c960f60f8a390b47ac55d2ece3ffc419e5dcd""><code>bf6c960</code></a> emoji bars</li>; <li><a href=""https://github.com/tqdm/tqdm/commit/7994aa8285743b351cf1a3b36275335d8d0730b7""><code>7994aa8</code></a> warn once on error</li>; <li><a href=""https://github.com/tqdm/tqdm/commit/a1d4401f186dc5a79b4ad452f38cae75e1f2e6da""><code>a1d4401</code></a> remove unneeded variable</li>; <li>Additional commits viewable in <a href=""https://github.com/tqdm/tqdm/compare/v4.42.1...v4.64.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tqdm&package-manager=pip&previous-version=4.42.1&new-version=4.64.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12260:5424,error,error,5424,https://hail.is,https://github.com/hail-is/hail/pull/12260,1,['error'],['error']
Availability,"ort](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""514b5ede-26fa-4106-8310-c2ceed7c08a9"",""prPublicId"":""514b5ede-26fa-4106-8310-c2ceed7c08a9"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.11.2""},{""name"":""setuptools"",""from"":""40.5.0"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-JUPYTERSERVER-6099119"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,null,509,384,494,539],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14365:6985,avail,available,6985,https://hail.is,https://github.com/hail-is/hail/pull/14365,1,['avail'],['available']
Availability,"ort](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""bc93468d-68e9-4fac-a335-f87261706f48"",""prPublicId"":""bc93468d-68e9-4fac-a335-f87261706f48"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.11.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-JUPYTERSERVER-6099119"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,null,509,384,494,539],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14211:6915,avail,available,6915,https://hail.is,https://github.com/hail-is/hail/pull/14211,1,['avail'],['available']
Availability,"ortalocker, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiodns 2.0.0 requires pycares, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **554/1000** <br/> **Why?** Has a fix available, CVSS 6.8 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CERTIFI-3164749](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-3164749) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![critical severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/c.png ""critical severity"") | **704/1000** <br/> **Why?** Has a fix available, CVSS 9.8 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3172287](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3172287) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **454/1000** <br/> **Why?** Has a fix available, CVSS 4.8 | Expected Behavior Violation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3314966](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3314966) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:1493,avail,available,1493,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"ortalocker, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiodns 2.0.0 requires pycares, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **554/1000** <br/> **Why?** Has a fix available, CVSS 6.8 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CERTIFI-3164749](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-3164749) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![critical severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/c.png ""critical severity"") | **704/1000** <br/> **Why?** Has a fix available, CVSS 9.8 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3172287](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3172287) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **454/1000** <br/> **Why?** Has a fix available, CVSS 4.8 | Expected Behavior Violation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3314966](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3314966) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:1485,avail,available,1485,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,"ose data to malicious users with access to a shared system. See for more information <a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1617"">#1617</a></p>; <p>4a4024a97 Fix temporary directory hijacking or temporary directory information disclosure (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1621"">#1621</a>); 9fd0ecf21 Disable codecov until we can fix the uploader (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1622"">#1622</a>); 347c0ac57 Fix EdgeReadIterator (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1616"">#1616</a>); d15a5bacb Added ULTIMA and ELEMENT as valid value for RG-PL according to SAM spec. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1619"">#1619</a>)</p>; <h2>3.0.0</h2>; <p>Htsjdk 3.0.0: Revenge of the Simple Allele</p>; <p>This is the first htsjdk with a major version increase in a long time. We bumped it to indicate there are some breaking changes that will potentially require downstream code changes. Notably, <code>Allele</code> became an interface instead of a concrete class. <code>SimpleAllele</code> may be used as a replacement if you have classes which previously subclassed allele.</p>; <p>New Plugin Infrastructure:; 6a60de7c2 Move API marker annotations into new annotation package. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1558"">#1558</a>); 7ac95d5f7 Plugin framework and interfaces for versioned file format codecs (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1525"">#1525</a>); d40fe5412 Beta implementation of Bundles. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1546"">#1546</a>)</p>; <p>CRAM; 489c4192d Support CRAM reference regions. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1605"">#1605</a>); 22aec6782 Fix decoding of CRAM Scores read feature during normalization. (<a href=""https://github-redirect.dependabot.c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12229:1380,down,downstream,1380,https://hail.is,https://github.com/hail-is/hail/pull/12229,1,['down'],['downstream']
Availability,"ot installed.; aiohttp-session 2.12.0 requires aiohttp, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Improper Limitation of a Pathname to a Restricted Directory (&#x27;Path Traversal&#x27;) <br/>[SNYK-PYTHON-AIOHTTP-6209406](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6209406) | `aiohttp:` <br> `3.8.6 -> 3.9.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **718/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-AIOHTTP-6209407](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6209407) | `aiohttp:` <br> `3.8.6 -> 3.9.2` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI2NDVjMDg3ZS00NzIwLTRkZTgtYmI0NC00MWNkOTY0NTBmZj",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14220:1516,avail,available,1516,https://hail.is,https://github.com/hail-is/hail/pull/14220,1,['avail'],['available']
Availability,"ot.com/inveniosoftware/dictdiffer/issues/160"">#160</a>)</li>; <li>Adds <code>assert_no_diff</code> helper to assist pytest users (<a href=""https://github.com/joesolly""><code>@​joesolly</code></a>) (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/153"">#153</a>)</li>; <li>Migrates CI to gh-actions (<a href=""https://github.com/ParthS007""><code>@​ParthS007</code></a> <a href=""https://github.com/diegodelemos""><code>@​diegodelemos</code></a>) (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/145"">#145</a>)</li>; <li>Removes dependency on pkg_resources (<a href=""https://github.com/eldruin""><code>@​eldruin</code></a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/inveniosoftware/dictdiffer/blob/master/CHANGES"">dictdiffer's changelog</a>.</em></p>; <blockquote>; <h1>Changes</h1>; <p>Version 0.9.0 (released 2021-07-22)</p>; <ul>; <li>Adds absolute tolerance feature for floats (<a href=""https://github.com/adrien-berchet""><code>@​adrien-berchet</code></a>) (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/152"">#152</a>)</li>; <li>Drops support of Python&lt;3.5 (<a href=""https://github.com/adrien-berchet""><code>@​adrien-berchet</code></a>) (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/160"">#160</a>)</li>; <li>Adds <code>assert_no_diff</code> helper to assist pytest users (<a href=""https://github.com/joesolly""><code>@​joesolly</code></a>) (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/153"">#153</a>)</li>; <li>Migrates CI to gh-actions (<a href=""https://github.com/ParthS007""><code>@​ParthS007</code></a> <a href=""https://github.com/diegodelemos""><code>@​diegodelemos</code></a>) (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/145"">#145</a>)</li>; <li>Removes dependency on pkg_resources (<a href=""ht",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11485:1691,toler,tolerance,1691,https://hail.is,https://github.com/hail-is/hail/pull/11485,1,['toler'],['tolerance']
Availability,"ote>; <h2>3.8.5</h2>; <h2>Security bugfixes</h2>; <ul>; <li>; <p>Upgraded the vendored copy of llhttp_ to v8.1.1 -- by :user:<code>webknjaz</code>; and :user:<code>Dreamsorcerer</code>.</p>; <p>Thanks to :user:<code>sethmlarson</code> for reporting this and providing us with; comprehensive reproducer, workarounds and fixing details! For more; information, see; <a href=""https://github.com/aio-libs/aiohttp/security/advisories/GHSA-45c4-8wx5-qw6w"">https://github.com/aio-libs/aiohttp/security/advisories/GHSA-45c4-8wx5-qw6w</a>.</p>; <p>.. _llhttp: <a href=""https://llhttp.org"">https://llhttp.org</a></p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7346"">#7346</a>)</p>; </li>; </ul>; <h2>Features</h2>; <ul>; <li>; <p>Added information to C parser exceptions to show which character caused the error. -- by :user:<code>Dreamsorcerer</code></p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7366"">#7366</a>)</p>; </li>; </ul>; <h2>Bugfixes</h2>; <ul>; <li>; <p>Fixed a transport is :data:<code>None</code> error -- by :user:<code>Dreamsorcerer</code>.</p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/3355"">#3355</a>)</p>; </li>; </ul>; <hr />; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/blob/v3.8.5/CHANGES.rst"">aiohttp's changelog</a>.</em></p>; <blockquote>; <h1>3.8.5 (2023-07-19)</h1>; <h2>Security bugfixes</h2>; <ul>; <li>; <p>Upgraded the vendored copy of llhttp_ to v8.1.1 -- by :user:<code>webknjaz</code>; and :user:<code>Dreamsorcerer</code>.</p>; <p>Thanks to :user:<code>sethmlarson</code> for reporting this and providing us with; comprehensive reproducer, workarounds and fixing details! For more; information, see; <a href=""https://github.com/aio-libs/aiohttp/security/advisories/GHSA-45c4-8wx5-qw6w"">https://github.com/aio-libs/aiohttp/security/advisories/GHSA-45c4-8wx5-qw6w</a>.</p>; <p>.. _llhttp: <a href=""https://llhttp.org",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13270:1286,error,error,1286,https://hail.is,https://github.com/hail-is/hail/pull/13270,5,['error'],['error']
Availability,other versions at your own risk. > Task :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; WARNING: The wheel package is not available.; WARNING: The wheel package is not available.; installing to build/bdist.linux-x86_64/wheel; creating build/bdist.linux-x86_64/wheel/hail-0.2.124.dist-info/WHEEL; creating 'dist/hail-0.2.124-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it; adding 'hail/__init__.py'; adding 'hail/builtin_references.py'; adding 'hail/conftest.py'; adding 'hail/context.py'; adding 'hail/hail_logging.py'; adding 'hail/hail_pip_version'; adding 'hail/hail_revision'; adding 'hail/hail_version'; adding 'hail/matrixtable.py'; adding 'hail/table.py'; adding 'hail/backend/__init__.py'; adding 'hail/backend/backend.py'; adding 'hail/backend/hail-all-spark.jar'; adding 'hail/backend/local_backend.py'; adding 'hail/backend/py4j_backend.py'; adding 'hail/backend/service_backend.py'; adding 'hail/backend/spark_backend.py'; adding 'hail/experimental/__init__.py'; adding 'hail/experimental/codec.py'; adding 'hail/experimental/compile.py'; adding 'hail/experimental/datasets.json'; adding 'hail/experimental/datasets.py'; a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:17006,avail,available,17006,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['avail'],['available']
Availability,"otocol); * In IntelliJ, go to File->Open, and choose the hail root directory; * When the project is open, go to File->Project Structure; * in the Project pane, set an sdk (8 or 11), and set the language level to 8; * in the Modules pane, delete the existing root module, click the plus sign -> Import Module, choose the `hail/` subdirectory, and choose ""Import module from external model"" and `BSP`; * you should see a progress bar at the bottom as it imports the project; * when it's done, quit and reopen IntelliJ. There should now be a bsp icon (two bars with two arrows between them) on the right, where the gradle elephant used to be. Just like before, sometimes you'll need to click the ""reload"" icon in there if things get wonky.; * if it says ""scalafmt configuration detected"", go ahead and enable the formatter. ## Metals setup. * delete any `.metals` directories; * open the hail repo in VSCode (even if you won't use VSCode, this seems to be the best way to get metals set up initially); * it should ask you to import a Mill build; * when that finishes, at the bottom it should say it's connected to a Bloop build server. In general, I think using Mill as the BSP directly will work best, but I don't have much experience to say for sure. To switch, run `Metals: switch build server` from the command palette. ## Debug and release builds. As before, debug mode adds some (fairly expensive) checking to our native memory system. But now there are a few other differences:; * treat warnings as errors only in release mode, so you can still compile, run tests, etc. during development without fixing all warnings; * enable optimization in scalac only in release mode. The intention is that we use debug mode during development, and release mode ony for published artifacts, or performance profiling. Mill will use debug mode by default. To enable release mode, define `HAIL_RELEASE_MODE` in your environment. Note changing this will invalidate all mill intermediates and rebuild from scratch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147:5324,error,errors,5324,https://hail.is,https://github.com/hail-is/hail/pull/14147,1,['error'],['errors']
Availability,"oud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud; -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir; -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3\n ""; image: google/cloud-sdk:237.0.0-alpine; imagePullPolicy: IfNotPresent; name: setup; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; nodeName: gke-vdc-non-preemptible-pool-0106a51b-qz7f; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: batch-output-pod; serviceAccountName: batch-output-pod; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: wang-gsa-key; - name: batch-gsa-key; secret:; defaultMode: 420; secretName: batch-gsa-key; - name: batch-12728-job-287-742170; persistentVolumeClaim:; claimName: batch-12728-job-287-742170; - name: batch-output-pod-token-8pkmz; secret:; defaultMode: 420; secretName: batch-output-pod-token-8pkmz; status:; conditions:; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with incomplete status: [setup]'; reason: ContainersNotInitialized; status: ""False""; type: Initialized; - lastProbeTime: null; lastTransitionTime:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7016:9633,toler,tolerations,9633,https://hail.is,https://github.com/hail-is/hail/issues/7016,1,['toler'],['tolerations']
Availability,"ourced from <a href=""https://github.com/pallets/jinja/blob/master/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 2.11.3</h2>; <p>Released 2021-01-31</p>; <ul>; <li>Improve the speed of the <code>urlize</code> filter by reducing regex; backtracking. Email matching requires a word character at the start; of the domain part, and only word characters in the TLD. :pr:<code>1343</code></li>; </ul>; <h2>Version 2.11.2</h2>; <p>Released 2020-04-13</p>; <ul>; <li>Fix a bug that caused callable objects with <code>__getattr__</code>, like; :class:<code>~unittest.mock.Mock</code> to be treated as a; :func:<code>contextfunction</code>. :issue:<code>1145</code></li>; <li>Update <code>wordcount</code> filter to trigger :class:<code>Undefined</code> methods; by wrapping the input in :func:<code>soft_str</code>. :pr:<code>1160</code></li>; <li>Fix a hang when displaying tracebacks on Python 32-bit.; :issue:<code>1162</code></li>; <li>Showing an undefined error for an object that raises; <code>AttributeError</code> on access doesn't cause a recursion error.; :issue:<code>1177</code></li>; <li>Revert changes to :class:<code>~loaders.PackageLoader</code> from 2.10 which; removed the dependency on setuptools and pkg_resources, and added; limited support for namespace packages. The changes caused issues; when using Pytest. Due to the difficulty in supporting Python 2 and; :pep:<code>451</code> simultaneously, the changes are reverted until 3.0.; :pr:<code>1182</code></li>; <li>Fix line numbers in error messages when newlines are stripped.; :pr:<code>1178</code></li>; <li>The special <code>namespace()</code> assignment object in templates works in; async environments. :issue:<code>1180</code></li>; <li>Fix whitespace being removed before tags in the middle of lines when; <code>lstrip_blocks</code> is enabled. :issue:<code>1138</code></li>; <li>:class:<code>~nativetypes.NativeEnvironment</code> doesn't evaluate; intermediate strings during rendering. This prevents ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10209:3622,error,error,3622,https://hail.is,https://github.com/hail-is/hail/pull/10209,2,['error'],['error']
Availability,out of memory error when writing on the cloud,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186:14,error,error,14,https://hail.is,https://github.com/hail-is/hail/issues/1186,1,['error'],['error']
Availability,"output in how-to/fixtures.rst</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/fadfb4f3463bc828535f86682300907a30f240e9""><code>fadfb4f</code></a> Prepare release version 7.1.3</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/ab96ea88e829af05e1491c30214b924c9553697b""><code>ab96ea8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/10258"">#10258</a> from pytest-dev/backport-10252-to-7.1.x</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/fc0e024b118fa63e84637bd5c9242b2b382e58fd""><code>fc0e024</code></a> [7.1.x] Fix regendoc</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/8f5088f4126b61ff76ac9809d5eb27cdbc31f07b""><code>8f5088f</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/10249"">#10249</a> from pytest-dev/backport-10231-to-7.1.x</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/aae93d6127c43a7f9036556ba7482019d389e21d""><code>aae93d6</code></a> Ignore type-errors related to attr.asdict</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/71b79fcda5313624544ae501a2045186c1c72244""><code>71b79fc</code></a> [7.1.x] Ignore editable installation modules</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/89f7518cb131579608387936c55f96cd4d3e9d3f""><code>89f7518</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/10222"">#10222</a> from pytest-dev/backport-10171-to-7.1.x</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/88fc45bd57d217a9dc6f53e419c94d28e2d5392f""><code>88fc45b</code></a> [7.1.x] Update fixtures.rst w/ finalizer order</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/d0b53d6ba7b41dcb200a35af5f5733f591cd929b""><code>d0b53d6</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/10221"">#10221</a> from pytest-dev/backport-10217-to-7.1.x</li>; <li>Additi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12187:4849,error,errors,4849,https://hail.is,https://github.com/hail-is/hail/pull/12187,1,['error'],['errors']
Availability,"ow.get(UnsafeRow.scala:254); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:871); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:860); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:637); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:631); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:631); 	at is.hail.io.RichRDDRegionValue$.writeRowsPartition(RowStore.scala:510); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-df17cef; Error summary: IndexOutOfBoundsException: 3; ```; (NB: a custom VEP/LOFTEE, but that shouldn't matter - ran same thing on `devel-cd48e11` and it worked fine)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:7097,Error,Error,7097,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['Error'],['Error']
Availability,"ow/lib/python3.7/site-packages/bokeh/core/templates.py:43: in <module>; from jinja2 import Environment, Markup, FileSystemLoader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); [error] 	at java.lang.Thread.run(Thread.java:748); [error] (hail / hailtest) java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; ```. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:1543,error,error,1543,https://hail.is,https://github.com/hail-is/hail/issues/11705,1,['error'],['error']
Availability,"p version number to 5.2.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b9df0c0daa080450772c365f16a9406fe0ca607a""><code>b9df0c0</code></a> Document eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/05a4433770f7020ff845add9348bdc12c82793dd""><code>05a4433</code></a> Add eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/09d1eca91afbf21ace3672be24c68d9028ee1e33""><code>09d1eca</code></a> Document runAsync method</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/800e3df1647c5ce65bffdd25c3240dfa5244e6c5""><code>800e3df</code></a> Add runAsync method to download extension</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/80f04c6a46fe7df053ac55bcfc6f90ff74c4b873""><code>80f04c6</code></a> Bump up version number to 5.1.3</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/3.2.0...5.2.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=3.2.0&new-version=5.2.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:4851,down,download-task,4851,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['down'],['download-task']
Availability,"p-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::int16<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::int16<8>]’; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:133:36: required from ‘simdpp::arch_avx2::uint16<8>& simdpp::arch_avx2::uint16<8>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int16<8, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:42:36: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint16<8>’ with ‘private’ member ‘simdpp::arch_avx2::uint16<8>::d_’ from an array of ‘const class simdpp::arch_avx2::int16<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:104:7: note: ‘class simdpp::arch_avx2::uint16<8>’ declared here; class uint16<8, void> : public any_int16<8, uint16<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int8<16>; T = simdpp::arch_avx2::uint16<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:58470,error,error,58470,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"p/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 20 times, most recent failure: Lost task 8.19 in stage 1.0 (TID 2899) (hail-test-w-1.australia-southeast1-a.c.pb-dev-312200.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:4145,Error,Error,4145,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Error'],['Error']
Availability,"p>; <blockquote>; <h2>v7.4.6</h2>; <h2>7.4.6</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.4.5...3394591f161be4a19f9e61c66ba510d7e29afd59"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Reconcile connection information <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/879"">#879</a> (<a href=""https://github.com/kevin-bates""><code>@​kevin-bates</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2022-11-10&amp;to=2022-11-15&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ameeseeksmachine+updated%3A2022-11-10..2022-11-15&amp;type=Issues""><code>@​meeseeksmachine</code></a></p>; <h2>v7.4.5</h2>; <h2>7.4.5</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.4.4...d27c8a497c6cbb1a232fbbe75cb1fd0f53faa9b0"">Full Changelog</a>)</p>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>[7.x] Handle Jupyter Core Warning <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/875"">#875</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Clean up 7.x workflows <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/865"">#865</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2022-10-25&amp;to=2022-11-10&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2022-10-25..2022-11-10&amp;type=Issues""><code>@​blink1073</code></a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_client/blob/v7.4.6/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12467:1259,Mainten,Maintenance,1259,https://hail.is,https://github.com/hail-is/hail/pull/12467,1,['Mainten'],['Maintenance']
Availability,"p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; msal-extensions 1.0.0 requires portalocker, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiodns 2.0.0 requires pycares, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **556/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.4 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-JINJA2-6150717](https://snyk.io/vuln/SNYK-PYTHON-JINJA2-6150717) | `jinja2:` <br> `3.1.2 -> 3.1.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI0M2ViODJhZS04ZDkwLTRjZWUtYjIzMS01ZDMyYmZiZWM4OWEiLCJldmVudC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14136:1117,avail,available,1117,https://hail.is,https://github.com/hail-is/hail/pull/14136,1,['avail'],['available']
Availability,"p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>; <p>This release contains refinements, improvements, and bug fixes, with highlights listed below.</p>; <h2>Core</h2>; <ul>; <li>Backport EventEngine Forkables. (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/30605"">#30605</a>)</li>; </ul>; <h2>Release v1.48.0</h2>; <p>This is release 1.48.0 (<a href=""https://github.com/grpc/grpc/blob/master/doc/g_stands_for.md"">garum</a>) of gRPC Core.</p>; <p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>; <p>This release contains refinements, improvements, and bug fixes, with highlights listed below.</p>; <h2>Core</h2>; <ul>; <li>Upgrade Abseil to LTS 20220623.0 . (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/30155"">#30155</a>)</li>; <li>Call: Send cancel op down the stack even when no ops are sent. (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/30004"">#30004</a>)</li>; <li>FreeBSD system roots implementation. (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/29436"">#29436</a>)</li>; <li>xDS: Workaround to get gRPC clients working with istio. (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/29841"">#29841</a>)</li>; </ul>; <h2>Python</h2>; <ul>; <li>Set Correct Platform Tag in Wheels on Mac OS with Python 3.10. (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/29857"">#29857</a>)</li>; <li>[Aio] Ensure Core channel closes when deallocated. (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/29797"">#29797</a>)</li>; <li>[Aio] Fix the wait_for_termination return value. (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/29795"">#29795</a>)</li>; </ul>; <h2>Ruby</h2>; <ul>; <li>Make the gem build on TruffleRuby. (<a href=""https://github-redirect.dep",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12201:1398,down,down,1398,https://hail.is,https://github.com/hail-is/hail/pull/12201,1,['down'],['down']
Availability,"p[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""hail.is/sha"":""1c6dbf20333a"" ""app"":""batch""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""1c6dbf20333a""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:4b4139c73fe9be3bee6c2895aa74059e157eb861d2bdac7d2304ba44b5421f88"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""labels"":map[""app"":""batch""] ""name"":""batch""] ""spec"":map[""ports"":[map[""port"":'P' ""protocol"":""TCP"" ""targetPort"":'\u1388']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; make: *** [deploy-batch] Error 1; Makefile:45: recipe for target 'deploy-batch' failed; ```; [deploy.log](https://github.com/hail-is/hail/files/2504429/deploy.log). Service accounts:; ```; error: the server doesn't have a resource type ""service-accounts""; # kubectl get serviceaccounts ; NAME SECRETS AGE; batch-svc 1 9h; default 1 113d; # kubectl get serviceaccounts ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:4331,Error,Error,4331,https://hail.is,https://github.com/hail-is/hail/issues/4609,2,"['Error', 'error']","['Error', 'error']"
Availability,pache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 1 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPool,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:52482,ERROR,ERROR,52482,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,pache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 6 on scc-q18.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPool,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:60953,ERROR,ERROR,60953,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,pache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 7 on scc-q21.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPool,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:68097,ERROR,ERROR,68097,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,"packages/hail/context.py](https://localhost:8080/#) in init_spark(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, gcs_requester_pays_configuration); 425 app_name = app_name or 'Hail'; 426 gcs_requester_pays_project, gcs_requester_pays_buckets = convert_gcs_requester_pays_configuration_to_hadoop_conf_style(gcs_requester_pays_configuration); --> 427 backend = SparkBackend(; 428 idempotent, sc, spark_conf, app_name, master, local, log,; 429 quiet, append, min_block_size, branching_factor, tmpdir, local_tmpdir,. TypeError: SparkBackend__init__() got an unexpected keyword argument 'gcs_requester_pays_project'`. - Hail was able to initialize by itself; `import hail as hl; print(hl.version()); hl.init()`; `0.2.112-31ceff2fb5fd; Running on Apache Spark version 3.3.2; SparkUI available at http://57ba6dc5f4a9:4040/; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.112-31ceff2fb5fd; LOGGING: writing to /content/hail-20230410-0617-0.2.112-31ceff2fb5fd.log`. ### Version. 0.2.112. ### Relevant log output. ```shell; using variant-spark jar at '/usr/local/lib/python3.9/dist-packages/varspark/jars/variant-spark_2.12-0.5.2-all.jar'; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-8-3d2ff0083f18> in <cell line: 3>(); 1 import hail as hl; 2 import varspark.hail as vshl; ----> 3 vshl.init(). 4 frames; <decorator-gen-1907> in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, backend, driver_cores, driver_memory, worker_cores, worker_memory, gcs_requester_pays_configuration, regions). <decorator-gen-1909> in init_spark(sc, app_name, mast",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12859:2319,avail,available,2319,https://hail.is,https://github.com/hail-is/hail/issues/12859,1,['avail'],['available']
Availability,"park.rdd.RDD.iterator(RDD.scala:283). Konrad Karczewski @konradjk 16:24; this should work, so i think it's a bug. but in the short run, you could hdfs dfs -cp file:///tmp/clinvar.vcf.gz / and then just load /clinvar.vcf.gz; copy to hdfs; (you shouldn't have to, but ¯\_(ツ)_/¯). bw2 @bw2 16:27; that worked. thanks!. ### What went wrong (all error messages here, including the full java stack trace):. Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); 	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); 	at org.apache.spark.rdd.HadoopRDD$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:5157,failure,failure,5157,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['failure'],['failure']
Availability,"particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-lim",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1368,error,error,1368,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006,1,['error'],['error']
Availability,partition error on GCP related to parquet.block.size,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1161:10,error,error,10,https://hail.is,https://github.com/hail-is/hail/issues/1161,1,['error'],['error']
Availability,pass hidden read args from checkpoint,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8113:27,checkpoint,checkpoint,27,https://hail.is,https://github.com/hail-is/hail/pull/8113,1,['checkpoint'],['checkpoint']
Availability,"pc/pull/29797"">#29797</a>)</li>; <li>[Aio] Fix the wait_for_termination return value. (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/29795"">#29795</a>)</li>; </ul>; <h2>Ruby</h2>; <ul>; <li>Make the gem build on TruffleRuby. (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/27660"">#27660</a>)</li>; <li>Support for prebuilt Ruby binary on x64-mingw-ucrt platform. (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/29684"">#29684</a>)</li>; <li>[Ruby] Add ruby_abi_version to exported symbols. (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/28976"">#28976</a>)</li>; </ul>; <h2>Objective-C</h2>; <p>First developer preview of XCFramework binary distribution via Cocoapod (<a href=""https://github-redirect.dependabot.com/grpc/grpc/issues/28749"">#28749</a>).</p>; <p>This brings in significant speed up to local compile time and includes support for Apple Silicon build.</p>; <ul>; <li>The following binary pods are made available for ObjC V1 &amp; V2 API</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/grpc/grpc/commit/d52ed193d11dee797c0d51dc8db06032998b33f4""><code>d52ed19</code></a> Bump version to v1.48.1 (<a href=""https://github-redirect.dependabot.com/grpc/grpc/issues/30782"">#30782</a>)</li>; <li><a href=""https://github.com/grpc/grpc/commit/2c7ba6410e5f9849a4848ff2e23839d6f8339f1e""><code>2c7ba64</code></a> xDS interop: Python LB tests build and use the python server (<a href=""https://github-redirect.dependabot.com/grpc/grpc/issues/30637"">#30637</a>) (<a href=""https://github-redirect.dependabot.com/grpc/grpc/issues/30655"">#30655</a>)</li>; <li><a href=""https://github.com/grpc/grpc/commit/d0f491285f74f8b07dec7f1c745a6636f3a691de""><code>d0f4912</code></a> Bump version to 1.48.1-pre1 (on v1.48.x branch) (<a href=""https://github-redirect.dependabot.com/grpc/grpc/issues/30627"">#30627</a>)</li>; <li><a href=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12201:3084,avail,available,3084,https://hail.is,https://github.com/hail-is/hail/pull/12201,1,['avail'],['available']
Availability,peCheck$.apply(TypeCheck.scala:56); 	at is.hail.expr.ir.TypeCheck$.is$hail$expr$ir$TypeCheck$$check$1(TypeCheck.scala:17); 	at is.hail.expr.ir.TypeCheck$$anonfun$apply$18.apply(TypeCheck.scala:174); 	at is.hail.expr.ir.TypeCheck$$anonfun$apply$18.apply(TypeCheck.scala:174); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:174); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:9); 	at is.hail.expr.ir.Emit$.emit(Emit.scala:42); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:28); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:51); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:31); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:74); 	at is.hail.expr.ir.TableKeyByAndAggregate.execute(TableIR.scala:843); 	at is.hail.table.Table.value$lzycompute(Table.scala:215); 	at is.hail.table.Table.value(Table.scala:213); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:218); 	at is.hail.table.Table.x$5(Table.scala:218); 	at is.hail.table.Table.rvd$lzycompute(Table.scala:218); 	at is.hail.table.Table.rvd(Table.scala:218); 	at is.hail.table.Table.take(Table.scala:649); 	at is.hail.table.Table.showString(Table.scala:685); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-f2b0dca9f506; Error summary: RuntimeException: Cannot find row in Map(); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4110:7448,Error,Error,7448,https://hail.is,https://github.com/hail-is/hail/issues/4110,1,['Error'],['Error']
Availability,"pected hail to be able to handle data with dimensions 10x3202, which is not too large. Data was downloaded from the 1000 Genomes ftp site: [link](https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/working/20201028_3202_phased/). ```; # load data; vcf_path = "".../1000G_b38_20201028_3202_phased/CCDG_14151_B01_GRM_WGS_2020-08-05_chr*.filtered.shapeit2-duohmm-phased.vcf.gz""; mt = hl.import_vcf(vcf_path, force_bgz=True). # select a few random variants; n_selected_variants = 10; selected_variants = np.random.choice(mt.rsid.collect(), n_selected_variants); selected_variants = hl.array(list(selected_variants)). (; mt.filter_rows(selected_variants.contains(mt.rsid)); .select_rows('rsid'); .select_entries('GT'); ).count(); ```; ```; [Stage 18:=====================================================>(255 + 1) / 256]; (10, 3202); ```. Trying to convert to pandas dataframe `.make_table().to_pandas()`, or even just taking 1 row `.make_table().take(1)` results in the following error:; ```; (; mt.filter_rows(selected_variants.contains(mt.rsid)); .select_rows('rsid'); .select_entries('GT'); ).make_table().take(1); ```; ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); Cell In[10], [line 5](vscode-notebook-cell:?execution_count=10&line=5); [1](vscode-notebook-cell:?execution_count=10&line=1) (; [2](vscode-notebook-cell:?execution_count=10&line=2) mt.filter_rows(selected_variants.contains(mt.rsid)); [3](vscode-notebook-cell:?execution_count=10&line=3) .select_rows('rsid'); [4](vscode-notebook-cell:?execution_count=10&line=4) .select_entries('GT'); ----> [5](vscode-notebook-cell:?execution_count=10&line=5) ).make_table().take(1). File .../python3.10/site-packages/decorator.py:232, in decorate.<locals>.fun(*args, **kw); [230](.../python3.10/site-packages/decorator.py:230) if not kwsyntax:; [231](.../python3.10/site-packages/decorator.py:231) args, kw = fix(args, kw, sig); --> [232",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14362:1254,error,error,1254,https://hail.is,https://github.com/hail-is/hail/issues/14362,1,['error'],['error']
Availability,"ped(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^. Java stack trace:; is.hail.utils.HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-45429b1; Error summary: HailException: `)' expected but `e' found; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2655:2567,error,error,2567,https://hail.is,https://github.com/hail-is/hail/issues/2655,2,"['Error', 'error']","['Error', 'error']"
Availability,"pendabot.com/cbeust/testng/pull/2813"">cbeust/testng#2813</a></li>; <li>Streamline dataprovider invoking in abstract classes by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2814"">cbeust/testng#2814</a></li>; <li>Streamline TestResult due to expectedExceptions by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2815"">cbeust/testng#2815</a></li>; <li>Unexpected test runs count with retry analyzer by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2816"">cbeust/testng#2816</a></li>; <li>Make PackageUtils compliant with JPMS by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2817"">cbeust/testng#2817</a></li>; <li>Ability to retry a data provider during failures by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2820"">cbeust/testng#2820</a></li>; <li>Refactoring by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2821"">cbeust/testng#2821</a></li>; <li>Fixing bug with DataProvider retry by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2822"">cbeust/testng#2822</a></li>; <li>Add config key for callback discrepancy behavior by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2823"">cbeust/testng#2823</a></li>; <li>Upgrading versions by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redir",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:3330,failure,failures,3330,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['failure'],['failures']
Availability,"pendabot.com/kubernetes/kubernetes/pull/104873"">kubernetes/kubernetes#104873</a>, <a href=""https://github.com/pohly""><code>@​pohly</code></a>)</li>; <li>JobTrackingWithFinalizers graduates to beta. Feature is enabled by default. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105687"">kubernetes/kubernetes#105687</a>, <a href=""https://github.com/alculquicondor""><code>@​alculquicondor</code></a>)</li>; <li>Kube-apiserver: Fixes handling of CRD schemas containing literal null values in enums. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104969"">kubernetes/kubernetes#104969</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>)</li>; <li>Kube-apiserver: The <code>rbac.authorization.k8s.io/v1alpha1</code> API version is removed; use the <code>rbac.authorization.k8s.io/v1</code> API, available since v1.8. The <code>scheduling.k8s.io/v1alpha1</code> API version is removed; use the <code>scheduling.k8s.io/v1</code> API, available since v1.14. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104248"">kubernetes/kubernetes#104248</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>)</li>; <li>Kube-scheduler: support for configuration file version <code>v1beta1</code> is removed. Update configuration files to v1beta2(xref: <a href=""https://github-redirect.dependabot.com/kubernetes/enhancements/issues/2901"">kubernetes/enhancements#2901</a>) or v1beta3 before upgrading to 1.23. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104782"">kubernetes/kubernetes#104782</a>, <a href=""https://github.com/kerthcet""><code>@​kerthcet</code></a>)</li>; <li>KubeSchedulerConfiguration provides a new field <code>MultiPoint</code> which will register a plugin for all valid extension points (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105611"">kubernetes/kubernetes#105611</a>, <a href=""https://github.com/damemi""><code>@​damemi</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:8139,avail,available,8139,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['avail'],['available']
Availability,perationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:955); 	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1459); 	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1438); 	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1438); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1438); 	at is.hail.utils.richUtils.RichRDD$.writeTable$extension(RichRDD.scala:61); 	at is.hail.keytable.KeyTable.export(KeyTable.scala:537); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```; Edit: I get identical errors if I change export to .to_pandas() or try to write the vds_results to file with .write(). Full script attached (as .txt - it didn't let me attach a .py); [hail_test.txt](https://github.com/hail-is/hail/files/1535717/hail_test.txt). Submitted as; ```; spark-submit hail_test.py; ```; and my spark-defaults.conf only has the spark.local.dir set,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527:6396,error,errors,6396,https://hail.is,https://github.com/hail-is/hail/issues/2527,1,['error'],['errors']
Availability,"perator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: wang-gsa-key; - name: batch-gsa-key; secret:; defaultMode: 420; secretName: batch-gsa-key; - name: batch-12728-job-287-742170; persistentVolumeClaim:; claimName: batch-12728-job-287-742170; - name: batch-output-pod-token-8pkmz; secret:; defaultMode: 420; secretName: batch-output-pod-token-8pkmz; status:; conditions:; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with incomplete status: [setup]'; reason: ContainersNotInitialized; status: ""False""; type: Initialized; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; status: ""True""; type: PodScheduled; containerStatuses:; - image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imageID: """"; lastState: {}; name: cleanup; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; - image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imageID: """"; lastState: {}; name: keep-alive; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; - image: gcr.io/broad-ctsa/benchmark_wang:latest; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; hostIP: 10.128.0.160; initContainerStatuses:; - image: google/cloud-sdk:237.0.0-alpine; imageID: """"; lastState: {}; name: setup; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; phase: Pending; qosClass: Burstable; startTime: 2019-09-05T19:15:42Z; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7016:11218,alive,alive,11218,https://hail.is,https://github.com/hail-is/hail/issues/7016,1,['alive'],['alive']
Availability,"phinx-doc/sphinx/issues/11011"">#11011</a>: Allow configuring a line-length limit for object signatures, via; :confval:<code>maximum_signature_line_length</code> and the domain-specific variants.; If the length of the signature (in characters) is greater than the configured; limit, each parameter in the signature will be split to its own logical line.; This behaviour may also be controlled by options on object description; directives, for example :rst:dir:<code>py:function:single-line-parameter-list</code>.; Patch by Thomas Louf, Adam Turner, and Jean-François B.</li>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/10983"">#10983</a>: Support for multiline copyright statements in the footer block.; Patch by Stefanie Molin</li>; <li><code>sphinx.util.display.status_iterator</code> now clears the current line; with ANSI control codes, rather than overprinting with space characters.</li>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11431"">#11431</a>: linkcheck: Treat SSL failures as broken links.; Patch by Bénédikt Tran</li>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11157"">#11157</a>: Keep the <code>translated</code> attribute on translated nodes.</li>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11451"">#11451</a>: Improve the traceback displayed when using :option:<code>sphinx-build -T</code>; in parallel builds. Patch by Bénédikt Tran</li>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11324"">#11324</a>: linkcheck: Use session-basd HTTP requests.</li>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11438"">#11438</a>: Add support for the :rst:dir:<code>py:class</code> and :rst:dir:<code>py:function</code>; directives for PEP 695 (generic classes and functions declarations) and; PEP 696 (default type parameters). Multi-line support (<a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11011"">#11011</a>) is enabled; for type parameters li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13295:3326,failure,failures,3326,https://hail.is,https://github.com/hail-is/hail/pull/13295,1,['failure'],['failures']
Availability,ping,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2322#issuecomment-339350297:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/2322#issuecomment-339350297,1,['ping'],['ping']
Availability,"ping @catoverdrive , should be addressed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6912#issuecomment-524057804:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/6912#issuecomment-524057804,1,['ping'],['ping']
Availability,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803,3,"['failure', 'ping']","['failure', 'ping']"
Availability,ping @cseed,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4427#issuecomment-424465040:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/4427#issuecomment-424465040,1,['ping'],['ping']
Availability,ping @danking,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5369#issuecomment-466130520:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/5369#issuecomment-466130520,2,['ping'],['ping']
Availability,"ping @danking , should be set",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5523#issuecomment-469831308:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/5523#issuecomment-469831308,1,['ping'],['ping']
Availability,ping @jbloom22,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2505#issuecomment-348619118:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/2505#issuecomment-348619118,1,['ping'],['ping']
Availability,ping @jigold,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4924#issuecomment-446410064:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/4924#issuecomment-446410064,1,['ping'],['ping']
Availability,ping @jigold would be stellar if you can look at it before end of day today :3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3698#issuecomment-393986893:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/3698#issuecomment-393986893,1,['ping'],['ping']
Availability,ping @rcownie,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3546#issuecomment-390226011:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/3546#issuecomment-390226011,1,['ping'],['ping']
Availability,ping https://discuss.hail.is/t/issue-with-hl-experimental-densify/2748/3 when this merges.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12121#issuecomment-1222859271:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/12121#issuecomment-1222859271,1,['ping'],['ping']
Availability,pl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-ebabd77; Error summary: IllegalArgumentException: Size exceeds Integer.MAX_VALUE. ​; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806:6866,Error,Error,6866,https://hail.is,https://github.com/hail-is/hail/issues/1806,1,['Error'],['Error']
Availability,"ple_file='gs://phenotype_31063/ukb31063.imputed_v3.autosomes.sample',; contig_recoding=contigs). # merge sumstats on bgen matrixtable; variants = variants.annotate_rows(ss = sumstats[variants.locus]). # handle strand/allele flips; variants = variants.annotate_rows(beta = hl.case(); .when(((variants.alleles[0] == variants.ss.nt1) &; (variants.alleles[1] == variants.ss.nt2)) | ; ((flip_text(variants.alleles[0]) == variants.ss.nt1) & ; (flip_text(variants.alleles[1]) == variants.ss.nt2)),; (-1*variants.ss.ldpred_inf_beta)); .when(((variants.alleles[0] == variants.ss.nt2) &; (variants.alleles[1] == variants.ss.nt1)) | ; ((flip_text(variants.alleles[0]) == variants.ss.nt2) & ; (flip_text(variants.alleles[1]) == variants.ss.nt1)),; variants.ss.ldpred_inf_beta); .or_missing()). variants = variants.filter_rows(hl.is_defined(variants.beta)); variants.beta.show(); ```. ### What went wrong (all error messages here, including the full java stack trace): When I went to try to show the beta column, Scala ""crashed"" such that I had to type in ""localhost:4040"" to reconnect and go into my application history to see what happened. I didn't get any errors in the Notebook I was using--it just stopped doing any work. . In the Scala tasks console, all of my workers had the following error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508:2552,error,error,2552,https://hail.is,https://github.com/hail-is/hail/issues/3508,1,['error'],['error']
Availability,ply$27.apply(ContextRDD.scala:359); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-15f58831fe57; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4263:12314,Error,Error,12314,https://hail.is,https://github.com/hail-is/hail/issues/4263,1,['Error'],['Error']
Availability,ply$27.apply(ContextRDD.scala:359); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-f80a6f10bc84; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:12320,Error,Error,12320,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719,1,['Error'],['Error']
Availability,"plyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). is.hail.utils.HailException: not a streamable IR: (GetTupleElement 0; (In PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]] 0)); ...; at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:850). ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TContainer]) {; ToStream(node); } else {; node; }; }; }; }; ````. with . ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TContainer]) {; ToStream(node); } else {; node; }; }; }; }. 4 failures in IRSuite, again testArrayAggScan:. Before Lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:6744,failure,failures,6744,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113,1,['failure'],['failures']
Availability,"ponent: bokehjs] [BUG] Hover tooltip breaks with full-circle wedge</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11339"">#11339</a> [BUG] Toggling layout's visibility results with overlapping widgets</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11462"">#11462</a> [component: bokehjs] [BUG] Existing ColorBar tick-digits don't react to theme changes</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11475"">#11475</a> [component: bokehjs] [BUG] SVG export breaks for Wedges</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11623"">#11623</a> [BUG] Placement of toolbar location is broken for gridplots</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11643"">#11643</a> Refs are not resolved in data models' default values</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11673"">#11673</a> [component: bokehjs] [BUG] JavaScript error when setting LabelSet text to None</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11694"">#11694</a> Custom extension breaks with id as key in a dict param</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11750"">#11750</a> [component: bokehjs] [BUG] Hover tool takes long time to render (-&gt; <a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11629"">#11629</a>)</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11770"">#11770</a> [component: bokehjs] [BUG] Linking an axis range can lead to other axis range autoscaling improperly</li>; </ul>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/bokeh/bokeh/commit/17a0b288052afac80ebcf0aa74e3915452fce3ca""><code>17a0b28</code></a> Deployment updates for release 3.0.1</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/3f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12454:6859,error,error,6859,https://hail.is,https://github.com/hail-is/hail/pull/12454,1,['error'],['error']
Availability,"populate the `aggregated_*_resources_by_date` tables.; 5. In 10-way parallelism (maxes out a 4 core database), randomly populate the tables for each chunk.; 6. From the last offset (original first running batch id), we sequentially process attempts in groups of 100. We take note of where we are at with tracking any updates to the attempts table (`attempts_time_msecs_diff`), populate the `aggregated_*_resources_by_date` tables, and then do a final catchup step where we apply any updates from `attempts_time_msecs_diff` for any attempts that we have already processed.; 7. Once we have reached the ""end"" of the attempts table, we lock all tables of interest especially the `attempts` table, and do one last final processing step before we add the new triggers that will auto-populate the `aggregated_*_resources_by_date` tables.; 8. Then we perform an audit and make sure things look correct. (I might need to change or eliminate the billing_project audit query because there are 5 batches with ~20 jobs that aren't perfectly tracked when we did the original switch over to the new billing tables).; 9. If there are any failures, we revert the triggers back to the original state. Also to note, is the new table for billing_projects is keyed by (billing_project, user) which will make queries much faster so they don't have to scan the batches aggregated resources table. I ran the migration successfully on a full test database and the audit was clean for jobs and batches except for the 5 batches that were running right when we started populating the original aggregated billing tables. . I'd like to gather all feedback and then will run the migration one more time to do a final test. Note, that most of the queries in the migration are not tested. The key thing to double check is the triggers will continue to insert data into the old tables and we get the end points correct (start is inclusive and end is exclusive) as unlike the previous migration, this set of updates is NOT idempotent.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11996:2112,failure,failures,2112,https://hail.is,https://github.com/hail-is/hail/pull/11996,1,['failure'],['failures']
Availability,"port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:5054,AVAIL,AVAILABLE,5054,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,"port mode means you are okay with the possibility of a breaking change happening. When the APIs are out of preview, we will remove the <code>@BetaApi</code> annotation to signal they are now considered stable and will not break outside a major version.</p>; <p><strong><em>NOTICE</em></strong>: Using the gRPC transport is exclusive. Any operations which have not yet been implemented for gRPC will result in a runtime error. For those operations which are not yet implemented, please continue to use the existing HTTP transport.</p>; <p>Special thanks (in alphabetical order) to <a href=""https://github.com/BenWhitehead""><code>@​BenWhitehead</code></a>, <a href=""https://github.com/frankyn""><code>@​frankyn</code></a>, <a href=""https://github.com/jesselovelace""><code>@​jesselovelace</code></a> and <a href=""https://github.com/sydney-munro""><code>@​sydney-munro</code></a> for their hard work on this effort.</p>; <h4>Notable Improvements</h4>; <ol>; <li>; <p>For all gRPC media related operations (upload/download) we are now more resource courteous then the corresponding HTTP counterpart. Buffers are fixed to their specified size (can't arbitrarily grow without bounds), are allocated lazily and only if necessary.</p>; <ol>; <li>Investigation into the possibility of backporting these improvements to the HTTP counterparts is ongoing</li>; </ol>; </li>; <li>; <p>Preview support for Accessing GCS via gRPC</p>; <ol>; <li>Set the environment variable <code>GOOGLE_CLOUD_ENABLE_DIRECT_PATH_XDS=true</code>, then run your program.</li>; </ol>; </li>; </ol>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/java-storage/blob/main/CHANGELOG.md"">google-cloud-storage's changelog</a>.</em></p>; <blockquote>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.15.0...v2.15.1"">2.15.1</a> (2022-11-17)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Disable REGAPIC tran",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12529:4873,down,download,4873,https://hail.is,https://github.com/hail-is/hail/pull/12529,1,['down'],['download']
Availability,"port mode means you are okay with the possibility of a breaking change happening. When the APIs are out of preview, we will remove the <code>@BetaApi</code> annotation to signal they are now considered stable and will not break outside a major version.</p>; <p><strong><em>NOTICE</em></strong>: Using the gRPC transport is exclusive. Any operations which have not yet been implemented for gRPC will result in a runtime error. For those operations which are not yet implemented, please continue to use the existing HTTP transport.</p>; <p>Special thanks (in alphabetical order) to <a href=""https://github.com/BenWhitehead""><code>@​BenWhitehead</code></a>, <a href=""https://github.com/frankyn""><code>@​frankyn</code></a>, <a href=""https://github.com/jesselovelace""><code>@​jesselovelace</code></a> and <a href=""https://github.com/sydney-munro""><code>@​sydney-munro</code></a> for their hard work on this effort.</p>; <h4>Notable Improvements</h4>; <ol>; <li>; <p>For all gRPC media related operations (upload/download) we are now more resource courteous then the corresponding HTTP counterpart. Buffers are fixed to their specified size (can't arbitrarily grow without bounds), are allocated lazily and only if necessary.</p>; <ol>; <li>Investigation into the possibility of backporting these improvements to the HTTP counterparts is ongoing</li>; </ol>; </li>; <li>; <p>Preview support for Accessing GCS via gRPC</p>; <ol>; <li>Set the environment variable <code>GOOGLE_CLOUD_ENABLE_DIRECT_PATH_XDS=true</code>, then run your program.</li>; <li>When configuring your <code>StorageOptions</code> mimic the following:; <pre><code> StorageOptions.grpc(); .setAttemptDirectPath(true); .build(); </code></pre>; </li>; <li>Internally the default host endpoint <code>https://storage.googleapis.com:443</code> will be transformed to the applicable <code>google-c2p-experimental:///storage.googleapis.com</code></li>; </ol>; </li>; <li>; <p>Support for <code>java.time</code> types on model classes</p>; <ol>; <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12456:3843,down,download,3843,https://hail.is,https://github.com/hail-is/hail/pull/12456,2,['down'],['download']
Availability,"port mode means you are okay with the possibility of a breaking change happening. When the APIs are out of preview, we will remove the <code>@BetaApi</code> annotation to signal they are now considered stable and will not break outside a major version.</p>; <p><strong><em>NOTICE</em></strong>: Using the gRPC transport is exclusive. Any operations which have not yet been implemented for gRPC will result in a runtime error. For those operations which are not yet implemented, please continue to use the existing HTTP transport.</p>; <p>Special thanks (in alphabetical order) to <a href=""https://github.com/BenWhitehead""><code>@​BenWhitehead</code></a>, <a href=""https://github.com/frankyn""><code>@​frankyn</code></a>, <a href=""https://github.com/jesselovelace""><code>@​jesselovelace</code></a> and <a href=""https://github.com/sydney-munro""><code>@​sydney-munro</code></a> for their hard work on this effort.</p>; <h4>Notable Improvements</h4>; <ol>; <li>; <p>For all gRPC media related operations (upload/download) we are now more resource courteous then the corresponding HTTP counterpart. Buffers are fixed to their specified size (can't arbitrarily grow without bounds), are allocated lazily and only if necessary.</p>; <ol>; <li>Investigation into the possibility of backporting these improvements to the HTTP counterparts is ongoing</li>; </ol>; </li>; <li>; <p>Preview support for Accessing GCS via gRPC</p>; <ol>; <li>Set the environment variable <code>GOOGLE_CLOUD_ENABLE_DIRECT_PATH_XDS=true</code>, then run your program.</li>; <li>When configuring your <code>StorageOptions</code> mimic the following:; <pre><code> StorageOptions.grpc(); </code></pre>; </li>; </ol>; </li>; </ol>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/googleapis/java-storage/commit/bfd48a1b5542ff28ffa337eba883c4ca6c3b0aad""><code>bfd48a1</code></a> chore(main): release 2.15.1 (<a href=""https://github-r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12529:10237,down,download,10237,https://hail.is,https://github.com/hail-is/hail/pull/12529,1,['down'],['download']
Availability,"port pods/status and pods/log, not container level status or logs.; - Pod now writes final status, not containers. Individual containers write their logs.; - I time all the steps of the Pod container (creating, starting, running, uploading log, etc.) with a timing called ""runtime"" which is how long the docker container itself took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""job_id"": 1,; ""user"": ""test"",; ""state"": ""succeeded"",; ""container_statuses"": {; ""setup"": {; ""name"": ""setup"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": 0.038861751556396484,; ""creating"": 0.7245609760284424,; ""starting"": 4.770207166671753,; ""running"": 1.1384251117706299,; ""runtime"": 5.909235715866089,; ""uploading_log"": 0.3659687042236328,; ""deleting"": 0.013197660446166992; },; ""container_status"": {; ""state"": ""exited"",; ""started_at"": ""2019-10-22T09:25:42.477556224Z"",; ""finished_at"": ""2019-10-22T09:25:42.476019599Z"",; ""exit_code"": 0; }; },; ""main"": {; ""name"": ""main"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": 0.031185626983642578,; ""creating"": 0.09947538375854492,; ""starting"": 4.786264657974243,; ""running"": 0.4418",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7354:1749,error,error,1749,https://hail.is,https://github.com/hail-is/hail/pull/7354,1,['error'],['error']
Availability,"port tdt and improve mendel errors, trio matrix",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2768:28,error,errors,28,https://hail.is,https://github.com/hail-is/hail/pull/2768,1,['error'],['errors']
Availability,possible bug + gnarly error message when using aggregator,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3729:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/issues/3729,1,['error'],['error']
Availability,"pp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float32<8>; T = simdpp::arch_avx2::float64<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float32<8>; T = simdpp::arch_avx2::float64<4>]’; libsimdpp-2.0-rc2/simdpp/types/float32x8.h:53:37: required from ‘simdpp::arch_avx2::float32<8>& simdpp::arch_avx2::float32<8>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::float64<4, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:277:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::float32<8>’ with ‘private’ member ‘simdpp::arch_avx2::float32<8>::d_’ from an array of ‘const class simdpp::arch_avx2::float64<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:30,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/float32x8.h:32:7: note: ‘class simdpp::arch_avx2::float32<8>’ declared here; class float32<8, void> : public any_float32<8, float32<8,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::uint8<32>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:49296,error,error,49296,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"pp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float64<4>; T = simdpp::arch_avx2::float32<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float64<4>; T = simdpp::arch_avx2::float32<8>]’; libsimdpp-2.0-rc2/simdpp/types/float64x4.h:55:37: required from ‘simdpp::arch_avx2::float64<4>& simdpp::arch_avx2::float64<4>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::float32<8, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:275:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::float64<4>’ with ‘private’ member ‘simdpp::arch_avx2::float64<4>::d_’ from an array of ‘const class simdpp::arch_avx2::float32<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:33,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/float64x4.h:33:7: note: ‘class simdpp::arch_avx2::float64<4>’ declared here; class float64<4, void> : public any_float64<4, float64<4,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float32<8>; T = simdpp::arch_avx2::float64<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:47440,error,error,47440,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"pp::arch_avx2::int32<8>; T = simdpp::arch_avx2::uint32<8>]’; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:55:35: required from ‘simdpp::arch_avx2::int32<8>& simdpp::arch_avx2::int32<8>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint32<8>]’; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:48:73: required from ‘simdpp::arch_avx2::int32<8>::int32(const simdpp::arch_avx2::uint32<8, E>&) [with E = void]’; libsimdpp-2.0-rc2/simdpp/core/combine.h:73:69: required from ‘simdpp::arch_avx2::int32<(N * 2)> simdpp::arch_avx2::combine(const simdpp::arch_avx2::int32<N, E>&, const simdpp::arch_avx2::int32<N, E2>&) [with unsigned int N = 4; E1 = void; E2 = void]’; libsimdpp-2.0-rc2/simdpp/detail/insn/to_int32.h:188:26: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int32<8>’ with ‘private’ member ‘simdpp::arch_avx2::int32<8>::d_’ from an array of ‘const class simdpp::arch_avx2::uint32<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:24,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:32:7: note: ‘class simdpp::arch_avx2::int32<8>’ declared here; class int32<8, void> : public any_int32<8, int32<8,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::int64<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:131801,error,error,131801,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"pp::arch_avx2::int64<2>; T = simdpp::arch_avx2::int32<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int64<2>; T = simdpp::arch_avx2::int32<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int64<2>; T = simdpp::arch_avx2::int32<4>]’; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:56:35: required from ‘simdpp::arch_avx2::int64<2>::int64(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int32<4>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:265:25: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int64<2>’ with ‘private’ member ‘simdpp::arch_avx2::int64<2>::d_’ from an array of ‘const class simdpp::arch_avx2::int32<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:26,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:33:7: note: ‘class simdpp::arch_avx2::int64<2>’ declared here; class int64<2, void> : public any_int64<2, int64<2,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::int64<2>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_av",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:69513,error,error,69513,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"pp::arch_avx2::int64<2>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::int64<2>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::int64<2>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:62:35: required from ‘simdpp::arch_avx2::int32<4>& simdpp::arch_avx2::int32<4>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int64<2>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:260:13: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int32<4>’ with ‘private’ member ‘simdpp::arch_avx2::int32<4>::d_’ from an array of ‘const class simdpp::arch_avx2::int64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:33:7: note: ‘class simdpp::arch_avx2::int32<4>’ declared here; class int32<4, void> : public any_int32<4, int32<4,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int64<2>; T = simdpp::arch_avx2::int32<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:67759,error,error,67759,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"pp::arch_avx2::int64<4>; T = simdpp::arch_avx2::int32<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int64<4>; T = simdpp::arch_avx2::int32<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int64<4>; T = simdpp::arch_avx2::int32<8>]’; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:51:35: required from ‘simdpp::arch_avx2::int64<4>::int64(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int32<8>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:296:25: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int64<4>’ with ‘private’ member ‘simdpp::arch_avx2::int64<4>::d_’ from an array of ‘const class simdpp::arch_avx2::int32<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:27,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:32:7: note: ‘class simdpp::arch_avx2::int64<4>’ declared here; class int64<4, void> : public any_int64<4, int64<4,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::int32<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_av",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:78444,error,error,78444,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"pp::arch_avx2::int64<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int32<8>; T = simdpp::arch_avx2::int64<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int32<8>; T = simdpp::arch_avx2::int64<4>]’; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:55:35: required from ‘simdpp::arch_avx2::int32<8>& simdpp::arch_avx2::int32<8>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int64<4>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:291:13: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int32<8>’ with ‘private’ member ‘simdpp::arch_avx2::int32<8>::d_’ from an array of ‘const class simdpp::arch_avx2::int64<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:24,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:32:7: note: ‘class simdpp::arch_avx2::int32<8>’ declared here; class int32<8, void> : public any_int32<8, int32<8,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int64<4>; T = simdpp::arch_avx2::int32<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:76690,error,error,76690,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"pper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack trace:. ```; hail: grm: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 45, localhost): java.lang.ArrayIndexOutOfBoundsException: 1048578; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:345); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:47); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:804); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:570); at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:194); at org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:147); at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:185); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:206); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.Spillable$class.m",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/801#issuecomment-247861703:2100,failure,failure,2100,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703,1,['failure'],['failure']
Availability,"ppose, ""applications"" that use Batch. Since we plan to support, maintain, and test the Hail Batch regenie implementation, I think it doesn't belong in a ""contrib"" directory. The hail python package has a `genetics` module for genetics-specific Hail Query functionality, let's mirror that structure. Let's move REGENIE and any non-Python dependencies of it into `hailtop/batch/genetics/regenie`. Sure. > How is the Dockerfile meant to be used? As written it doesn't appear that it would work because there doesn't exist any regenie source code to COPY in. It's meant to create a Regenie docker image that we could use. It's a copy of the regenie c++ repo's dockerfile, with the removal of the ENTRYPOINT /usr/local/bin/regenie, so that I could issue a command that included an executable, which is convenient to give me the ability to check that intermediate files are actually created (wc, ls) by batch, and because that seems more idiomatic for batch. I don't think there is a published regenie image, but docker hub is down so can't double check. . You're right, I shouldn't have deleted the bulk of the repo, kept as is. Didn't want to deal with submodules. > I've made some other in-line comments in the python file. It's not clear to me how all those other files are related to the python files and I'm a bit uncomfortable adding a whole directory with a LICENSE file, especially when not all the files in the directory fall under that license (e.g. the regenie py file) and moreover the license makes claims about things linking to BGEN, which none of our code here does. The license is only contained within the folder with the licensed files. In a previous conversation with Nate/Cotton, if we use any open-source software, best to keep those files segregated, alongside their license (license must be kept alongside the code, easier to see the demarcation if in a separate folder). > We have some BGEN files for testing in the Hail src/test/resources. I already have an example provided. The",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987:1114,down,down,1114,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987,1,['down'],['down']
Availability,"precated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-ebs: Downloading aiohttp_session-2.7.0-py3-none-any.whl (14 kB); 899 | amazon-ebs: Collecting asyncinit<0.3,>=0.2.4; 900 | amazon-ebs: Downloading asyncinit-0.2.4-py3-none-any.whl (2.8 kB); 901 | amazon-ebs: Collecting avro<1.12,>=1.10; 902 | amazon-ebs: Downloading avro-1.11.1.tar.gz (84 kB); 903 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.2/84.2 kB 22.0 MB/s eta 0:00:00; 904 | amazon-ebs: Installing build dependencies: started; 905 | amazon-ebs: Installing build dependencies: finished with status 'done'; 906 | amazon-ebs: Getting requirements to build wheel: started; 907 | amazon-ebs: Getting requirements to build wheel: finished with status 'done'; 908 | amazon-ebs: Preparing metadata (pyproject.toml): started; 909 | amazon-ebs: Preparing metadata (pyproject.toml): finished with status 'done'; 910 | amazon-ebs: Collecting azure-identity==1.6.0; 911 | amazon-ebs: Downloading azure_identity-1.6.0-py2.py3-none-any.whl (108 kB); 912 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.5/108.5 kB 28.5 MB/s eta 0:00:00; 913 | amazon-ebs: Collecting azure-storage-blob==12.11.0; 914 | amazon-ebs: Downloadin",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:2134,Down,Downloading,2134,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,presumably related to: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/mysterious.20behavior/near/182390817 (log posted next message down),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7638#issuecomment-561430746:161,down,down,161,https://hail.is,https://github.com/hail-is/hail/issues/7638#issuecomment-561430746,1,['down'],['down']
Availability,"priate init method. Edit: re-running ./gradlew shadowJar fixes this. Tests are not passing, but seemingly not because of any of these changes. If I checkout last commit in master, they also fail. Furthermore, I can restore all changes from last FS PR manually, and no benefit. ```; HEAD is now at 117c365c3 [ci] also handle batch Ready state (#5909); (hail) alex:~/projects/hail/hail/python:$ pytest test/ -x; ======================================================================================= test session starts ========================================================================================; platform darwin -- Python 3.6.8, pytest-3.8.0, py-1.7.0, pluggy-0.8.1; rootdir: /Users/alex/projects/hail/hail/python, inifile:; plugins: xdist-1.22.2, metadata-1.8.0, html-1.19.0, forked-1.0.2; collected 591 items . test/hail/test_context.py E. ============================================================================================== ERRORS ==============================================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend); hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:1028,ERROR,ERRORS,1028,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928,2,['ERROR'],"['ERROR', 'ERRORS']"
Availability,print compiled byte code if an error arises from it,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1466:31,error,error,31,https://hail.is,https://github.com/hail-is/hail/pull/1466,1,['error'],['error']
Availability,print line of file on error in LoadMatrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3849:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/pull/3849,1,['error'],['error']
Availability,"proc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723e4b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-10-19T03:09:40Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.2' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.124' >> src/main/resources/build-info.properties; creating env/HAIL_DEBUG_MODE which does not exist; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; make -C src/main/c prebuilt; make[1]: Entering directory `/mnt/tmp/hail/hail/src/main/c'; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux testutils/unit-tests.cpp -MG -M -MF build/testutils/unit-tests.d -MT build/testutils/unit-tests.o; g",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:4096,echo,echo,4096,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,"projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 490, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/methods/qc.py"", line 91, in sample_qc; return MatrixTable(Env.hail().methods.SampleQC.apply(require_biallelic(dataset, 'sample_qc')._jvds, name)); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: invalid allele ""<DEL>"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 160, scc-q01.scc.bu.edu, executor 4): is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCombiner$.alleleIndices(SampleQC.scala:44); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413:1744,Error,ErrorHandling,1744,https://hail.is,https://github.com/hail-is/hail/issues/3413,1,['Error'],['ErrorHandling']
Availability,prometheus was on a crash loop in Azure because it was failing to `mmap` a 0-byte file that the previous pod had created before getting shut down. Turns out this bug was fixed in a more recent version of prometheus so a simple upgrade fixed the issue. This is the most recent release.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11607:141,down,down,141,https://hail.is,https://github.com/hail-is/hail/pull/11607,1,['down'],['down']
Availability,"ps://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no files; ```; How can I solve it ?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076:3120,Error,Error,3120,https://hail.is,https://github.com/hail-is/hail/issues/2076,1,['Error'],['Error']
Availability,"ps://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2154"">#2154</a> PR by <a href=""https://github.com/kuviokelluja""><code>@​kuviokelluja</code></a>.</li>; </ul>; </li>; <li>upgrade supported ruby versions.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2205"">#2205</a> PR by <a href=""https://github.com/jalessio""><code>@​jalessio</code></a>.</li>; </ul>; </li>; <li>allow <code>language: conda</code> to use <code>mamba</code> or <code>micromamba</code> via; <code>PRE_COMMIT_USE_MAMBA=1</code> or <code>PRE_COMMIT_USE_MICROMAMBA=1</code> respectively.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2204"">#2204</a> issue by <a href=""https://github.com/janjagusch""><code>@​janjagusch</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2207"">#2207</a> PR by <a href=""https://github.com/xhochy""><code>@​xhochy</code></a>.</li>; </ul>; </li>; <li>display <code>git --version</code> in error report.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2210"">#2210</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; <li>add <code>language: lua</code> as a supported language.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2158"">#2158</a> PR by <a href=""https://github.com/mblayman""><code>@​mblayman</code></a>.</li>; </ul>; </li>; </ul>; <h3>Fixes</h3>; <ul>; <li>temporarily add <code>setuptools</code> to the zipapp.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2122"">#2122</a> issue by <a href=""https://github.com/andreoliwa""><code>@​andreoliwa</code></a>.</li>; <li>a737d5f commit by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; <li>use <code>go install</code> instead of <code>go get</code> for go 1.18+ support.; <ul>; <li><a href=""https://github-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11460:8079,error,error,8079,https://hail.is,https://github.com/hail-is/hail/pull/11460,2,['error'],['error']
Availability,"ps://github.com/michel-kraemer/gradle-download-task/commit/472920e572e4cf45d321868874ced50ad8d1e2d5""><code>472920e</code></a> Add possibility to set request method and body</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/82e70cae2a8d48b4f5165a9b543d4e65bb793d88""><code>82e70ca</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/86a15f1c16eb729dc71b6caf30237d07b8e0bb01""><code>86a15f1</code></a> Fix compiler warnings and deprecations</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/86363072c8239330b28976109a622bdd073507b6""><code>8636307</code></a> Negative timeouts are actually not allowed</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/4ff0ff0e63e0dd45f231990d0dcebffde6e6b709""><code>4ff0ff0</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a1858b494b5f3a51ccef7580c243c6dfdf520731""><code>a1858b4</code></a> Merge pull request <a href=""https://redirect.github.com/michel-kraemer/gradle-download-task/issues/295"">#295</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/c1e212c0fb41b3ea9185a9ea463fb1ea7142f748""><code>c1e212c</code></a> Add integration tests for Gradle 8.0 and 8.0.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/304f68e25f53633a92a4d2d6ce003a4986929503""><code>304f68e</code></a> Fix type inference issue</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/5.3.1...5.4.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=5.3.1&new-version=5.4.0)](https://docs.github.com/en/github/managing-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12893:1992,down,download-task,1992,https://hail.is,https://github.com/hail-is/hail/pull/12893,1,['down'],['download-task']
Availability,"ps://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **711/1000** <br/> **Why?** Mature exploit, Has a fix available, CVSS 6.5 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-SPHINX-570772](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-570772) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Mature ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **701/1000** <br/> **Why?** Mature exploit, Has a fix available, CVSS 6.3 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-SPHINX-570773](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-570773) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Mature ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **586/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SPHINX-5811865](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-5811865) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **586/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SPHINX-5812109](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-5812109) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Proof of C",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:6987,avail,available,6987,https://hail.is,https://github.com/hail-is/hail/pull/13717,3,['avail'],['available']
Availability,"ps://snyk.io/vuln/SNYK-PYTHON-SPHINX-570772) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Mature ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **701/1000** <br/> **Why?** Mature exploit, Has a fix available, CVSS 6.3 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-SPHINX-570773](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-570773) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Mature ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **586/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SPHINX-5811865](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-5811865) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **586/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SPHINX-5812109](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-5812109) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR wa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:7751,avail,available,7751,https://hail.is,https://github.com/hail-is/hail/pull/13717,1,['avail'],['available']
Availability,"ps://snyk.io/vuln/SNYK-PYTHON-SPHINX-570772) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Mature ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **701/1000** <br/> **Why?** Mature exploit, Has a fix available, CVSS 6.3 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-SPHINX-570773](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-570773) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Mature ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **586/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SPHINX-5811865](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-5811865) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **586/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SPHINX-5812109](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-5812109) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/uplo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14108:7786,avail,available,7786,https://hail.is,https://github.com/hail-is/hail/pull/14108,2,['avail'],['available']
Availability,put messages on assertion errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1317:26,error,errors,26,https://hail.is,https://github.com/hail-is/hail/issues/1317,1,['error'],['errors']
Availability,"putCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:4669,AVAIL,AVAILABLE,4669,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,"py as much as possible from the buffer to the destination, then modify `InputBlockBuffer` so that it can read directly into the region while spilling any remainder into a provided buffer. For large reads this saves O(N_BLOCKS) copies and branches.; 8. `ZstdDecompressCtx.decompressByteArray0` (2017, 80). Yikes. Decompression ain't cheap, but, at least on this dataset, it's ~10x decompression ratio. If we had better datatype-aware compressors (like dictionary coding of strings) maybe we could skip Zstd and use something lighter weight. The remaining methods are pretty minor or would be addressed by one of the above mentioned mitigations. <img width=""1551"" alt=""Screenshot 2023-10-11 at 13 27 34"" src=""https://github.com/hail-is/hail/assets/106194/cfd855d6-8762-4b94-8e53-1ef881fde6ae"">. ---. ### Conclusions. LEB128 should not be a buffer spec for two reasons:; 1. It is too slow to be a our default integer encoding.; 2. InputBuffers and OutputBuffers do not expose vectorized `read` and `write` primitives. We could mitigate the slowness of LEB128 with something like Lemire's [MaskedVByte](https://github.com/lemire/MaskedVByte/tree/master) but only if we can communicate to the bufferspec that we want to read multiple integers at once. But buffer specs are the wrong place to do this. ETypes are supposed to be the interface between our in-memory types and a stream of bytes. It is natural to have an `EArrayInt32LEB128` that reads and writes blocks of integers using LEB128. Moreover, removing LEB128 as a buffer spec means that an EType knows if it is ""fixed-width"" or not. In particular, a struct containing only primitive types would know that it has a fixed bytesize. Reading such an EBaseStruct into a PBaseStruct can be a memcpy of the known bytesize because the on-disk and in-memory types are identical. ---. [1] Retrieved from `cat /Users/dking/projects/hail-data/foo.mt/entries/rows/metadata.json.gz|gunzip | jq '.'`. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13792:7022,Mask,MaskedVByte,7022,https://hail.is,https://github.com/hail-is/hail/issues/13792,2,['Mask'],['MaskedVByte']
Availability,"py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 293, in run; await self.get_container_log()); File ""/usr/local/lib/python3.6/site-packages/batch/log_store.py"", line",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8053:1586,error,error,1586,https://hail.is,https://github.com/hail-is/hail/issues/8053,4,['error'],['error']
Availability,"py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-pack",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8054:2022,error,error,2022,https://hail.is,https://github.com/hail-is/hail/pull/8054,2,['error'],['error']
Availability,"pycares seems to have no typing / stubs so lets ignore it via <code>mypy.ini</code></li>; <li>setup: typing exists since Python 3.5</li>; <li>Fix type annotation of gethostbyname()</li>; <li>Updated README</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/saghul/aiodns/blob/master/ChangeLog"">aiodns's changelog</a>.</em></p>; <blockquote>; <h1>3.0.0</h1>; <ul>; <li>Release wheels and source to PyPI with GH actions</li>; <li>Try to make tests more resilient</li>; <li>Don't build universal wheels</li>; <li>Migrate CI to GH Actions</li>; <li>Fix TXT CHAOS test</li>; <li>Add support for CAA queries</li>; <li>Support Python &gt;= 3.6</li>; <li>Bump pycares dependency</li>; <li>Drop tasks.py</li>; <li>Allow specifying dnsclass for queries</li>; <li>Set URL to https</li>; <li>Add license args in setup.py</li>; <li>Converted Type Annotations to Py3 syntax Closes</li>; <li>Only run mypy on cpython versions</li>; <li>Also fix all type errors with latest mypy - pycares seems to have no typing / stubs so lets ignore it via <code>mypy.ini</code></li>; <li>setup: typing exists since Python 3.5</li>; <li>Fix type annotation of gethostbyname()</li>; <li>Updated README</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/saghul/aiodns/commit/cdb33385f46be1e18bc525ccb153c8abc8ac92d4""><code>cdb3338</code></a> Updated changelog</li>; <li><a href=""https://github.com/saghul/aiodns/commit/a57968007a0e6f826e1a8a2160eade23c254bc42""><code>a579680</code></a> Updated README</li>; <li><a href=""https://github.com/saghul/aiodns/commit/efbbcd55493e11ff95cce7845ebe23438c4238aa""><code>efbbcd5</code></a> Release wheels and source to PyPI with GH actions</li>; <li><a href=""https://github.com/saghul/aiodns/commit/0c9ea6f60d5a9306b0c80e6ffea5ccc27c5fa5bd""><code>0c9ea6f</code></a> Try to make tests more resilient</li>; <li><a href=""https://github.com/saghul/aiodns/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11570:1875,error,errors,1875,https://hail.is,https://github.com/hail-is/hail/pull/11570,1,['error'],['errors']
Availability,"pydata.org/pandas-docs/version/1.5.2/whatsnew/v1.5.2.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made this release possible.</p>; <h2>Pandas 1.5.1</h2>; <p>This is a patch release in the 1.5.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.1/whatsnew/v1.5.1.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made this release possible.</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pandas-dev/pandas/commit/2e218d10984e9919f0296931d92ea851c6a6faf5""><code>2e218d1</code></a> RLS: 1.5.3</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/659de5fea920f15e88bed4cf43dc13df8569abad""><code>659de5f</code></a> DOC: Fix whatsnew (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/50821"">#50821</a>)</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/5aff5c9f62893a863d003ee15ba216a71f1bb4f3""><code>5aff5c9</code></a> Manual backport fix github quota (<a href=""https://github-redirect.dependabot.com/pandas-d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12610:2074,avail,available,2074,https://hail.is,https://github.com/hail-is/hail/pull/12610,1,['avail'],['available']
Availability,"pydata.org/pandas-docs/version/1.5.2/whatsnew/v1.5.2.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made this release possible.</p>; <h2>Pandas 1.5.1</h2>; <p>This is a patch release in the 1.5.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.1/whatsnew/v1.5.1.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made this release possible.</p>; <h2>Pandas 1.5.0</h2>; <p>This release includes some new features, bug fixes, and performance improvements. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.0/whatsnew/v1.5.0.html"">full whatsnew</a> for a list of all the changes. pandas 1.5.0 supports Python 3.8 and higher.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <p><code>conda install -c conda-forge pandas</code></p>; <p>Or via PyPI:</p>; <p><code>python3 -m pip install --upgrade pandas</code></p>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12564:1330,avail,available,1330,https://hail.is,https://github.com/hail-is/hail/pull/12564,1,['avail'],['available']
Availability,"pydata.org/pandas-docs/version/1.5.3/whatsnew/v1.5.3.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made this release possible.</p>; <h2>Pandas 1.5.2</h2>; <p>This is a patch release in the 1.5.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.2/whatsnew/v1.5.2.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made this release possible.</p>; <h2>Pandas 1.5.1</h2>; <p>This is a patch release in the 1.5.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.1/whatsnew/v1.5.1.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made thi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12610:1330,avail,available,1330,https://hail.is,https://github.com/hail-is/hail/pull/12610,1,['avail'],['available']
Availability,"python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/hail/python/hail/methods/impex.py"", line 1511, in import_table; t = Table(TableRead(tr)); File ""/hail/python/hail/table.py"", line 334, in __init__; self._type = self._tir.typ; File ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/hail/python/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;. Java stack trace:; java.lang.NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.lambda$getPropsWithPrefix$3(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getLookupKey(HadoopConfigurationProperty.java:120); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getPropsWithPrefix(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8343:2152,Error,Error,2152,https://hail.is,https://github.com/hail-is/hail/issues/8343,1,['Error'],['Error']
Availability,quiet down nose,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3131:6,down,down,6,https://hail.is,https://github.com/hail-is/hail/pull/3131,1,['down'],['down']
Availability,"quot;&quot;)</code>. The new implementation will now throw in that case. You can use <code>Files.createTemporaryDirectory(path, prefix)</code> for those use cases instead.</p>; <p>4a4024a97 Fix temporary directory hijacking or temporary directory information disclosure (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1621"">#1621</a>); 9fd0ecf21 Disable codecov until we can fix the uploader (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1622"">#1622</a>); 347c0ac57 Fix EdgeReadIterator (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1616"">#1616</a>); d15a5bacb Added ULTIMA and ELEMENT as valid value for RG-PL according to SAM spec. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1619"">#1619</a>)</p>; <h2>3.0.0</h2>; <p>Htsjdk 3.0.0: Revenge of the Simple Allele</p>; <p>This is the first htsjdk with a major version increase in a long time. We bumped it to indicate there are some breaking changes that will potentially require downstream code changes. Notably, <code>Allele</code> became an interface instead of a concrete class. <code>SimpleAllele</code> may be used as a replacement if you have classes which previously subclassed allele.</p>; <p>New Plugin Infrastructure:; 6a60de7c2 Move API marker annotations into new annotation package. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1558"">#1558</a>); 7ac95d5f7 Plugin framework and interfaces for versioned file format codecs (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1525"">#1525</a>); d40fe5412 Beta implementation of Bundles. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1546"">#1546</a>)</p>; <p>CRAM; 489c4192d Support CRAM reference regions. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1605"">#1605</a>); 22aec6782 Fix decoding of CRAM Scores read feature during normalization. (<a href=""https://github-redirect.dependabot.c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12310:2466,down,downstream,2466,https://hail.is,https://github.com/hail-is/hail/pull/12310,1,['down'],['downstream']
Availability,"qxswccs180000gq/T/pyscripts_yg_wzlu0.zip \; --properties=; Job [66c1d088108948b2b76bb607f61d7b3f] submitted.; Waiting for job output...; Initializing Spark and Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/pyt",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:1195,error,error,1195,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815,1,['error'],['error']
Availability,"r /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/hailtop/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; msal-extensions 1.0.0 requires portalocker, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiodns 2.0.0 requires pycares, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **451/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.3 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI0Y2E5NmE2ZC02MjMxLTQ1YTctYmQyOS1kYTA0ZmZhN",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14238:1119,avail,available,1119,https://hail.is,https://github.com/hail-is/hail/pull/14238,1,['avail'],['available']
Availability,"r /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; msal-extensions 1.0.0 requires portalocker, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiohttp 3.8.5 requires frozenlist, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **496/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 4.2 | Information Exposure Through Sent Data <br/>[SNYK-PYTHON-URLLIB3-6002459](https://snyk.io/vuln/SNYK-PYTHON-URLLIB3-6002459) | `urllib3:` <br> `1.26.17 -> 1.26.18` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJmNDI4YzVjNi05NmZmLTQ1ZTMtYjY4ZC0zYzU5NjY",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13851:1121,avail,available,1121,https://hail.is,https://github.com/hail-is/hail/pull/13851,1,['avail'],['available']
Availability,"r /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; msal-extensions 1.0.0 requires portalocker, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiohttp 3.8.5 requires frozenlist, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Information Exposure Through Sent Data <br/>[SNYK-PYTHON-URLLIB3-5926907](https://snyk.io/vuln/SNYK-PYTHON-URLLIB3-5926907) | `urllib3:` <br> `1.26.16 -> 1.26.17` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJiODhmOTA3Ny02ZjlmLTRiZjEtYjFiYy0yZjNkNTE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13783:1121,avail,available,1121,https://hail.is,https://github.com/hail-is/hail/pull/13783,1,['avail'],['available']
Availability,"r cancelled requests (<a href=""https://redirect.github.com/Azure/azure-sdk-for-java/issues/36225"">#36225</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/d464be7d9248d773b4aa793ad71c0eca77e8c450""><code>d464be7</code></a> Prepare Core Libraries for August 2023 Release (<a href=""https://redirect.github.com/Azure/azure-sdk-for-java/issues/36239"">#36239</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/530cc4378c650f7fe7c7a528dd119993961088a3""><code>530cc43</code></a> mgmt, upgrade network 2023-04 (<a href=""https://redirect.github.com/Azure/azure-sdk-for-java/issues/36242"">#36242</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/7e3b6a5634be16f65c02523c7d84f9b5b27f4e42""><code>7e3b6a5</code></a> Update TRC API azure core (<a href=""https://redirect.github.com/Azure/azure-sdk-for-java/issues/36237"">#36237</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/4b74642902b11ee73b0982626f88cfff9c6af166""><code>4b74642</code></a> Redact error output (<a href=""https://redirect.github.com/Azure/azure-sdk-for-java/issues/36232"">#36232</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/3255f7fcd235d3eede34c8de5abe7102de7dd140""><code>3255f7f</code></a> Add logback classes that are initialized at build time (<a href=""https://redirect.github.com/Azure/azure-sdk-for-java/issues/36231"">#36231</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/4792bff2b30fb77ca29bd5eaa0502adf68502641""><code>4792bff</code></a> bump the proxy version (<a href=""https://redirect.github.com/Azure/azure-sdk-for-java/issues/36235"">#36235</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/Azure/azure-sdk-for-java/compare/azure-core-http-netty_1.13.3...azure-core-http-netty_1.13.6"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13591:2173,error,error,2173,https://hail.is,https://github.com/hail-is/hail/pull/13591,1,['error'],['error']
Availability,"r getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:4298,AVAIL,AVAILABLE,4298,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,"r node syslogs as well as the Hail log file. For some reason all logs other than the Hail logs are missing from this file. We separately need to determine why all the Spark logs etc. are missing. Based on the syslog, after system start up and just before the Jupyter notebook starts, the system is already using ~8,500MiB:; ```; Nov 22 14:29:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 43808 of 52223 MiB (83.89%), swap free: 0 of 0 MiB ( 0.00%); ```; So, the effective maximum memory that Hail could possibly use is around 43808MiB. After the Notebook and Spark initialize we're down to 42,700 MiB (about ~1000MiB more in use).; ```; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. `hailctl` sets the VM RAM limit to 80% of the instance type's memory, so 80% * 52GiB = 42598MiB. This means the JVM is permitted to effectively use all the remaining memory. At time of sigkill the total memory allocated by the JVM was about 2000MiB below the max heap size. Note that the heap is contained within all memory allocated by the JVM.; ```; Nov 22 15:31:05 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 43 of 52223 MiB ( 0.08%), swap free: 0 of 0 MiB ( 0.00%); Nov 22 15:31:09 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: low memory! at or below SIGTERM limits: mem 0.12%, swap 1.00%; Nov 22 15:31:09 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM to process 8421 uid 0 ""java"": badness 1852, VmRSS 40578 MiB; ```. Indeed, the VmRSS is the memory in use from the kernel's perspective so it includes any off-heap memory created by Hail. The Hail log indicates the region pools are tiny, ~10s of MiB. Not a concern. After the JVM is killed, memory jumps back up to 40683MiB (which checks out, that's roughly what the killed process was using).; ```; Nov 22 15:31:10 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 40683 of 52223 MiB (77.90%), swap free: 0 of 0 MiB ( 0.00%); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1832531419:2001,avail,avail,2001,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832531419,2,['avail'],['avail']
Availability,"r received from the Java gateway into a Python object.; ; For example, string representation of integers are converted to Python; integer, string representation of objects are converted to JavaObject; instances, etc.; ; :param answer: the string returned by the Java gateway; :param gateway_client: the gateway client used to communicate with the Java; Gateway. Only necessary if the answer is a reference (e.g., object,; list, map); :param target_id: the name of the object from which the answer comes from; (e.g., *object1* in `object1.hello()`). Optional.; :param name: the name of the member from which the answer comes from; (e.g., *hello* in `object1.hello()`). Optional.; """"""; if is_error(answer)[0]:; if len(answer) > 1:; type = answer[1]; value = OUTPUT_CONVERTER[type](answer[2:], gateway_client); if answer[1] == REFERENCE_TYPE:; raise Py4JJavaError(; ""An error occurred while calling {0}{1}{2}.\n"".; format(target_id, ""."", name), value); else:; raise Py4JError(; ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; > format(target_id, ""."", name, value)); E py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; E py4j.Py4JException: Method apply([null, class java.lang.String, class scala.Some, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist; E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339); E 	at py4j.Gateway.invoke(Gateway.java:276); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCommand.java:79); E 	at py4j.GatewayConnection.run(GatewayConnection.java:238); E 	at java.lang.Thread.run(Thread.java:748). /miniconda3/envs/hail/lib/python3.6/site-packages/py4j/protocol.py:332: Py4JError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:4109,error,error,4109,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928,1,['error'],['error']
Availability,"r the entries</summary>. ```; {; ""name"": ""LEB128BufferSpec"",; ""child"": {; ""name"": ""BlockingBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""ZstdBlockBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""StreamBlockBufferSpec""; }; }; }; }; ```; </details>. This takes about 35s (after https://github.com/hail-is/hail/pull/13787 merges). ---. The dominant hot spots by wall time (using async periodic sampling via YourKit) are Zstd decompression and inplace decoding of arrays of ints and arrays of a small struct. <img width=""1551"" alt=""Screenshot 2023-10-11 at 13 12 41"" src=""https://github.com/hail-is/hail/assets/106194/09a7722a-3ae9-4793-903d-6201d7e23207"">. ---. Another interesting view to look at is hot spots by samples. If a sample occurs when my stack is `A called B called C called D`, all four functions will increment their samples by one but only D will receive ""own time"" (I believe). Sorting by samples sort of reveals the call stack. You have to be careful to focus on the own time as you scan down the stack though!. My take on the relatively expensive operations:. 1. Struct decoding (the fourth generated code one, with 399 own time and 229 samples) is pretty branchy: it checks a bit for each field. I'm not sure how to speed this up. Consider a struct of 8 optional fields. There are 2^8 possible missingness pattern. Each pattern corresponds to a different sequence of field-decoders. I suppose we could generate 256 different patterns and jump to them? That seems excessive. We could maybe generate 16 patterns but that only saves 3/4 of the branches. Maybe that's enough for a substantial speedup?; 3. `LEB128InputBuffer.readByte` (365, 139). `readByte` just calls its child `InputBuffer`'s `readByte`. My best explanation: we call `readByte` *a lot*. That kinda makes sense: `LEB128InputBuffer` issues a `readByte` for each byte in the variable length encoding of the integer. ; 4. Optional Array of Optional Int32 (450, 138). These arrays will have a bunch of LEB12",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13792:4120,down,down,4120,https://hail.is,https://github.com/hail-is/hail/issues/13792,1,['down'],['down']
Availability,"r(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:18863,error,error,18863,https://hail.is,https://github.com/hail-is/hail/issues/8944,3,['error'],['error']
Availability,"r-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:2308,Error,ErrorHandling,2308,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Error'],['ErrorHandling']
Availability,"r. I think this is currently impossible due to lack of permissions, but we should either explicitly prohibit this or ensure our solution encompasses it. In particular, I am concerned OpenID could be used to grant permission for a GCP identity to write to S3 or ABS. . Pulling an image shouldn’t trigger substantial egress. In the first case, there are three kinds of possible egress:; 1. Egress to the Public Internet.; 2. Egress to a VM in a different Google region.; 3. Egress to a Google Service in a different Google region (e.g. uploading to a bucket in a different region). I believe (2) and (3) are charged equivalently. (1) is simply Internet egress pricing. In (3), I’m not sure who pays the egress from a VM to a bucket in a different region. I assume the VM owner. In all three cases, the destination’s location matters. For public Internet egress, we can use GeoIP to determine the region of the planet. I’m not sure if we can determine the region of (2) and (3). If we can’t, we should either prevent such traffic or we should charge the maximum egress. A final caveat is that we use Premium Networking. As a result, our traffic can use Google’s internal backbone. It’s not clear to me if this means that a packet from us-central to a public IP in Australia incurs just Internet egress or that *and* a region-to-region egress to pay for the use of GCP’s internal global backbone. The priority of various considerations:; 1. Top priority within this issue is to track and recover costs. Even if this means charging a flat fee across all possible kinds of egress. Even if that fee is substantially higher than the real cost to us.; 2. Second priority is to surface this information to the user. Simply providing, in the job page, the usage and cost of each resource for this job.; 3. Fine grained egress so that users can actually intentionally use it at cost or near cost to, for example, move data between clouds or regions. . ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13428:1907,recover,recover,1907,https://hail.is,https://github.com/hail-is/hail/issues/13428,1,['recover'],['recover']
Availability,"r.checks</code> on; an instance of <code>FormatChecker</code> instead.</li>; <li><code>unevaluatedItems</code> has been fixed for draft 2019. It's nonetheless; discouraged to use draft 2019 for any schemas, new or old.</li>; <li>Fix a number of minor annotation issues in <code>protocols.Validator</code></li>; </ul>; <h1>v4.13.0</h1>; <ul>; <li>Add support for creating validator classes whose metaschema uses a different; dialect than its schemas. In other words, they may use draft2020-12 to define; which schemas are valid, but the schemas themselves use draft7 (or a custom; dialect, etc.) to define which <em>instances</em> are valid. Doing this is likely; not something most users, even metaschema authors, may need, but occasionally; will be useful for advanced use cases.</li>; </ul>; <h1>v4.12.1</h1>; <ul>; <li>Fix some stray comments in the README.</li>; </ul>; <h1>v4.12.0</h1>; <ul>; <li>Warn at runtime when subclassing validator classes. Doing so was not; intended to be public API, though it seems some downstream libraries; do so. A future version will make this an error, as it is brittle and; better served by composing validator objects instead. Feel free to reach; out if there are any cases where changing existing code seems difficult; and I can try to provide guidance.</li>; </ul>; <h1>v4.11.0</h1>; <ul>; <li>Make the rendered README in PyPI simpler and fancier. Thanks Hynek (<a href=""https://github-redirect.dependabot.com/python-jsonschema/jsonschema/issues/983"">#983</a>)!</li>; </ul>; <h1>v4.10.3</h1>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python-jsonschema/jsonschema/commit/420fc6bd9a3ecc4cd637ece97cb4b482b4d0d37e""><code>420fc6b</code></a> Minor verbiage tweak for protocols.</li>; <li><a href=""https://github.com/python-jsonschema/jsonschema/commit/8ce8250897e1b2e9b1fea6825965dbc876ec1f4d""><code>8ce8250</code></a> Don't show type checker functio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12163:5700,down,downstream,5700,https://hail.is,https://github.com/hail-is/hail/pull/12163,1,['down'],['downstream']
Availability,"r.ir.CallFunctionsSuite` - run all tests in one or more specified classes. You can use `*` to match anything, e.g. `mill test.testOnly ""*.CallFunctionsSuite""`, or `mill test.testOnly ""is.hail.expr.ir.*""`. You can pass options to the test runner (TestNG currently) after a `--`, e.g. `mill test.testOnly ""is.hail.expr.ir.*"" -- -parallel classes`; * `mill __.testCached` - once the codebase is more modularized, will run tests on only modules whose dependencies have changed since the last test run; * `mill reformat` - runs scalafmt on all sources in the root module (currently that's all scala sources, but hopefully not for long). `mill __.reformat` runs scalaformat on all sources in all modules. `mill __.checkFormat` only checks for rule failures (we run this in ci); * `mill fix` - runs scalafix on all sources in the root module (currently that's all scala sources, but hopefully not for long). `mill __.fix` runs scalafix on all sources in all modules. `mill __.fix --check` only checks for rule failures (we run this in ci). You can pass any options to scalafix, e.g. `mill fix --help`.; * `mill inspect` - see the docstring and dependencies for any target, e.g. `mill inspect compile`; * `mill ivyDepsTree` - show the tree of external dependencies of the root module, highlighting potential incompatibilities in transitive dependencies. `mill ivyDepsTree --withCompile --withRuntime` includes compile-only and runtime-only dependencies. Use `--whatDependsOn` to see an inverted tree showing how a transitive dependency is getting pulled in, e.g. `mill ivyDepsTree --withCompile --withRuntime --whatDependsOn org.slf4j:slf4j-api`; * `mill mill.scalalib.Dependency/showUpdates` - show outdated dependencies (we have a lot!). See [here](https://mill-build.com/mill/Intro_to_Mill.html) and [here](https://mill-build.com/mill/Builtin_Commands.html) for more details. ## IntelliJ setup. To import from scratch:; * delete any `.idea` directories in the hail root, or `hail/` subdirectory (you could ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147:2360,failure,failures,2360,https://hail.is,https://github.com/hail-is/hail/pull/14147,1,['failure'],['failures']
Availability,r.ir.CompileAndEvaluate.evaluateToJSON(CompileAndEvaluate.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.Conte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:10193,Error,ErrorHandling,10193,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Error'],['ErrorHandling']
Availability,"r.scala:488); 	at is.hail.variant.VariantDatasetFunctions$.write$extension(VariantDataset.scala:941); 	at is.hail.variant.VariantDatasetFunctions.write(VariantDataset.scala:911); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 5.0 failed 20 times, most recent failure: Lost task 22.19 in stage 5.0 (TID 133, seqr-pipeline-cluster-grch38-w-1.c.seqr-project.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822:3558,failure,failure,3558,https://hail.is,https://github.com/hail-is/hail/issues/1822,1,['failure'],['failure']
Availability,"r/gradle-download-task/issues/291"">#291</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/53af1049f5514afe58e884d487d7c57dae47759d""><code>53af104</code></a> Bump http-cache-semantics from 4.1.0 to 4.1.1 in /screencast</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/398c14c05c6448b380ac35c6095598299c5e23c5""><code>398c14c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/15cf7eecfbc17d2466143828b9b69494c6cb6f2b""><code>15cf7ee</code></a> Bump up version number to 5.3.1-SNAPSHOT</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/e3c65ffcb49b9c5a33fde5f31fb63043dbf21134""><code>e3c65ff</code></a> Allow extensions to be created from tasks</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/34e2dd41477f18b1ae3d6d5a71dca5449d6cd1e0""><code>34e2dd4</code></a> Downgrade slf4j to fix warning on console about missing slf4j provider</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b3fa29f9ffb4d4544e13ef84601e371fb2778ddf""><code>b3fa29f</code></a> Revert &quot;Update Apache HttpClient to 5.2.1&quot;</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/01f05e046be0dca18f506723c79e88f208336e71""><code>01f05e0</code></a> Add integration tests for Gradle 6.9.3 and 7.6</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a998a544908a8b39f713f4526f717fcb328c06eb""><code>a998a54</code></a> Upgrade Gradle to 7.6</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/5.3.0...5.3.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:2367,down,download-task,2367,https://hail.is,https://github.com/hail-is/hail/pull/12707,2,"['Down', 'down']","['Downgrade', 'download-task']"
Availability,"r/gradle-download-task/releases"">de.undercouch.download's releases</a>.</em></p>; <blockquote>; <h2>5.2.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Use pooling connection manager of Apache HttpClient instead of basic one. The basic one is not meant to be used by multiple threads. This fixes an issue that could cause an <code>IllegalStateException</code> with the message <code>Connection is still allocated</code>. Thanks to <a href=""https://github.com/dmarks2""><code>@​dmarks2</code></a> for spotting this.</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>eachFile</code> method that adds an action to be applied to each source URL before it is downloaded. The action can be used to modify the filename of the target file.</li>; <li>Add <code>runAsync</code> method to download extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:1188,down,download,1188,https://hail.is,https://github.com/hail-is/hail/pull/12332,2,['down'],"['download', 'downloaded']"
Availability,"rGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:2058,error,error,2058,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803,1,['error'],['error']
Availability,rVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-08a1543; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:10289,Error,Error,10289,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['Error'],['Error']
Availability,"r_type != expression.dtype:; 155 raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); --> 156 return Env.backend().execute(expression._ir, True); 157 else:; 158 uid = Env.get_uid(). ~/projects/hail/hail/python/hail/backend/spark_backend.py in execute(self, ir, timed); 294 jir = self._to_java_value_ir(ir); 295 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); --> 296 result = json.loads(self._jhc.backend().executeJSON(jir)); 297 value = ir.typ._from_json(result['value']); 298 timings = result['timings']. ~/miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/projects/hail/hail/python/hail/backend/spark_backend.py in deco(*args, **kwargs); 39 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 40 'Hail version: %s\n'; ---> 41 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 42 except pyspark.sql.utils.CapturedException as e:; 43 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: Index 5 is out of bounds for axis 0 with size 2. Java stack trace:; is.hail.utils.HailException: Index 5 is out of bounds for axis 0 with size 2; 	at __C889Compiled.apply(Unknown Source); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$1.apply$mcJ$sp(CompileAndEvaluate.scala:41); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$1.apply(CompileAndEvaluate.scala:41); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$1.apply(CompileAndEvaluate.scala:41); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:41); 	at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:318); 	at is.hail.backend.spark.SparkBackend$$anonfun$exe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9226:2358,Error,Error,2358,https://hail.is,https://github.com/hail-is/hail/issues/9226,1,['Error'],['Error']
Availability,"rabad) Region (ap-south-2) for latency records, geoproximity records, and private DNS for Amazon VPCs in that region.</li>; </ul>; <h1>1.26.13</h1>; <ul>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow provides a new API called UpdateConnectorRegistration to update a custom connector that customers have previously registered. With this API, customers no longer need to unregister and then register a connector to make an update.</li>; <li>api-change:<code>auditmanager</code>: [<code>botocore</code>] This release introduces a new feature for Audit Manager: Evidence finder. You can now use evidence finder to quickly query your evidence, and add the matching evidence results to an assessment report.</li>; <li>api-change:<code>chime-sdk-voice</code>: [<code>botocore</code>] Amazon Chime Voice Connector, Voice Connector Group and PSTN Audio Service APIs are now available in the Amazon Chime SDK Voice namespace. See <a href=""https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html"">https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html</a> for more details.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/d872f34494e33473126a887499262c6d3139d0f3""><code>d872f34</code></a> Merge branch 'release-1.26.17'</li>; <li><a href=""https://github.com/boto/boto3/commit/c547ba545c4aeb40bc1848e50d9b89f54df8937c""><code>c547ba5</code></a> Bumping version to 1.26.17</li>; <li><a href=""https://github.com/boto/boto3/commit/083655fd0ece1370b4c5100fdb11f1cf11ac3f9a""><code>083655f</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/865ba3450a2408cf04413a2209e8fe4959f162e6""><code>865ba34</code></a> Avoid duplicate serialization in DynamoDB BatchWriter (<a href=""https://github-redirect.dependabot.com/boto/boto3/issues/3504"">#3504</a>)</li>; <li><a href=""h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12507:6437,avail,available-regions,6437,https://hail.is,https://github.com/hail-is/hail/pull/12507,1,['avail'],['available-regions']
Availability,"rabad) Region (ap-south-2) for latency records, geoproximity records, and private DNS for Amazon VPCs in that region.</li>; </ul>; <h1>1.26.13</h1>; <ul>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow provides a new API called UpdateConnectorRegistration to update a custom connector that customers have previously registered. With this API, customers no longer need to unregister and then register a connector to make an update.</li>; <li>api-change:<code>auditmanager</code>: [<code>botocore</code>] This release introduces a new feature for Audit Manager: Evidence finder. You can now use evidence finder to quickly query your evidence, and add the matching evidence results to an assessment report.</li>; <li>api-change:<code>chime-sdk-voice</code>: [<code>botocore</code>] Amazon Chime Voice Connector, Voice Connector Group and PSTN Audio Service APIs are now available in the Amazon Chime SDK Voice namespace. See <a href=""https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html"">https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html</a> for more details.</li>; <li>api-change:<code>cloudfront</code>: [<code>botocore</code>] CloudFront API support for staging distributions and associated traffic management policies.</li>; <li>api-change:<code>connect</code>: [<code>botocore</code>] Added AllowedAccessControlTags and TagRestrictedResource for Tag Based Access Control on Amazon Connect Webpage</li>; <li>api-change:<code>dynamodb</code>: [<code>botocore</code>] Updated minor fixes for DynamoDB documentation.</li>; <li>api-change:<code>dynamodbstreams</code>: [<code>botocore</code>] Update dynamodbstreams client to latest version</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds support for copying an Amazon Machine Image's tags when copying an AMI.</li>; <li>api-change:<code>glue</code>: [<code>botocore</code>] AWSGlue Crawler - Adding support for Table and Column level Comments with da",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12498:2123,avail,available-regions,2123,https://hail.is,https://github.com/hail-is/hail/pull/12498,2,['avail'],['available-regions']
Availability,"race):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-143-c80e74dd23d2> in <module>(); ----> 1 conc.aggregate(hl.agg.count_where(hl.sum(conc.concordance[0].concordance_matrix.concordance[:2].map(lambda x: hl.sum(x[:2])))>0)). /home/hail/hail.zip/hail/table.py in aggregate(self, expr); 1107 analyze('Table.aggregate', expr, self._global_indices, {self._row_axis}); 1108 ; -> 1109 result_json = base._jt.aggregateJSON(expr._ast.to_hql()); 1110 return expr.dtype._from_json(result_json); 1111 . /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: VerifyError: Bad local variable type; Exception Details:; Location:; is/hail/codegen/generated/C423.apply(Lis/hail/annotations/Region;[Lis/hail/annotations/aggregators/RegionValueAggregator;JZJZ)V @2710: iload; Reason:; Type top (current frame, locals[130]) is not assignable to integer; Current Frame:; bci: @2710; flags: { }; locals: { 'is/hail/codegen/generated/C423', 'is/hail/annotations/Region', '[Lis/hail/annotations/aggregators/RegionValueAggregator;', long, long_2nd, integer, long, long_2nd, integer, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, integer, long, long_2nd, top, integer, integer, top, top, t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3729:2168,Error,Error,2168,https://hail.is,https://github.com/hail-is/hail/issues/3729,1,['Error'],['Error']
Availability,"raemer/gradle-download-task/commit/05a4433770f7020ff845add9348bdc12c82793dd""><code>05a4433</code></a> Add eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/09d1eca91afbf21ace3672be24c68d9028ee1e33""><code>09d1eca</code></a> Document runAsync method</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/800e3df1647c5ce65bffdd25c3240dfa5244e6c5""><code>800e3df</code></a> Add runAsync method to download extension</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/80f04c6a46fe7df053ac55bcfc6f90ff74c4b873""><code>80f04c6</code></a> Bump up version number to 5.1.3</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/3.2.0...5.2.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=3.2.0&new-version=5.2.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:5070,down,download,5070,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['down'],['download']
Availability,"raemer/gradle-download-task/commit/4c983ed5cd229fa64912294737c858c2ba8486d6""><code>4c983ed</code></a> Bump up version number to 5.4.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/cc20442ab67bf37687c08e67af7e7de3a21c8fbe""><code>cc20442</code></a> Add integration tests for Gradle 8.0.2</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/472920e572e4cf45d321868874ced50ad8d1e2d5""><code>472920e</code></a> Add possibility to set request method and body</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/82e70cae2a8d48b4f5165a9b543d4e65bb793d88""><code>82e70ca</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/86a15f1c16eb729dc71b6caf30237d07b8e0bb01""><code>86a15f1</code></a> Fix compiler warnings and deprecations</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/86363072c8239330b28976109a622bdd073507b6""><code>8636307</code></a> Negative timeouts are actually not allowed</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/4ff0ff0e63e0dd45f231990d0dcebffde6e6b709""><code>4ff0ff0</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a1858b494b5f3a51ccef7580c243c6dfdf520731""><code>a1858b4</code></a> Merge pull request <a href=""https://redirect.github.com/michel-kraemer/gradle-download-task/issues/295"">#295</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/c1e212c0fb41b3ea9185a9ea463fb1ea7142f748""><code>c1e212c</code></a> Add integration tests for Gradle 8.0 and 8.0.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/304f68e25f53633a92a4d2d6ce003a4986929503""><code>304f68e</code></a> Fix type inference issue</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12893:1633,down,download-task,1633,https://hail.is,https://github.com/hail-is/hail/pull/12893,1,['down'],['download-task']
Availability,"raemer/gradle-download-task/commit/cc20442ab67bf37687c08e67af7e7de3a21c8fbe""><code>cc20442</code></a> Add integration tests for Gradle 8.0.2</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/472920e572e4cf45d321868874ced50ad8d1e2d5""><code>472920e</code></a> Add possibility to set request method and body</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/82e70cae2a8d48b4f5165a9b543d4e65bb793d88""><code>82e70ca</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/86a15f1c16eb729dc71b6caf30237d07b8e0bb01""><code>86a15f1</code></a> Fix compiler warnings and deprecations</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/86363072c8239330b28976109a622bdd073507b6""><code>8636307</code></a> Negative timeouts are actually not allowed</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/4ff0ff0e63e0dd45f231990d0dcebffde6e6b709""><code>4ff0ff0</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a1858b494b5f3a51ccef7580c243c6dfdf520731""><code>a1858b4</code></a> Merge pull request <a href=""https://redirect.github.com/michel-kraemer/gradle-download-task/issues/295"">#295</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/c1e212c0fb41b3ea9185a9ea463fb1ea7142f748""><code>c1e212c</code></a> Add integration tests for Gradle 8.0 and 8.0.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/304f68e25f53633a92a4d2d6ce003a4986929503""><code>304f68e</code></a> Fix type inference issue</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/5.3.1...5.4.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/comp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12893:1824,down,download-task,1824,https://hail.is,https://github.com/hail-is/hail/pull/12893,1,['down'],['download-task']
Availability,random test failure...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6349#issuecomment-504085816:12,failure,failure,12,https://hail.is,https://github.com/hail-is/hail/pull/6349#issuecomment-504085816,1,['failure'],['failure']
Availability,"randomly assigned Patrick. I expected to get an error like:; ```; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: <<empty key>>; Index Expressions: int32; ```; But instead got:; ```; # ipython; import hail Python 3.7.3 (default, Mar 27 2019, 09:23:15) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl ; hl.utils.ra; In [2]: t = hl.utils.range_table(10) . In [3]: t.describe() ; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'idx': int32 ; ----------------------------------------; Key: ['idx']; ----------------------------------------. In [4]: t = t.key_by() . In [5]: t[t.idx] ; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-5-a44cae835751> in <module>; ----> 1 t[t.idx]. /usr/local/lib/python3.7/site-packages/hail/table.py in __getitem__(self, item); 366 else:; 367 try:; --> 368 return self.index(*wrap_to_tuple(item)); 369 except TypeError as e:; 370 raise TypeError(f""Table.__getitem__: invalid index argument(s)\n"". /usr/local/lib/python3.7/site-packages/hail/table.py in index(self, all_matches, *exprs); 1530 """"""; 1531 try:; -> 1532 return self._index(*exprs, all_matches=all_matches); 1533 except TableIndexKeyError as err:; 1534 key_type, exprs = err.args. /usr/local/lib/python3.7/site-packages/hail/table.py in _index(self, all_matches, *exprs); 1554 ; 1555 is_interval = (len(exprs) == 1; -> 1556 and isinstance(self.key[0].dtype, hl.tinterval); 1557 and exprs[0].dtype == self.key[0].dtype.point_type); 1558 . </usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-570> in __getitem__(self, item). /usr/local/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kw",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6663:48,error,error,48,https://hail.is,https://github.com/hail-is/hail/issues/6663,1,['error'],['error']
Availability,raversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$ano,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:12823,Error,ErrorHandling,12823,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Error'],['ErrorHandling']
Availability,"rch_avx2::int32<4>; T = simdpp::arch_avx2::uint64<2>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::uint64<2>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::uint64<2>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:58:35: required from ‘simdpp::arch_avx2::int32<4>::int32(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint64<2>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:272:34: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int32<4>’ with ‘private’ member ‘simdpp::arch_avx2::int32<4>::d_’ from an array of ‘const class simdpp::arch_avx2::uint64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:33:7: note: ‘class simdpp::arch_avx2::int32<4>’ declared here; class int32<4, void> : public any_int32<4, int32<4,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::int32<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:73071,error,error,73071,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"rch_avx2::int8<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::int8<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::int8<16>]’; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:133:36: required from ‘simdpp::arch_avx2::uint16<8>& simdpp::arch_avx2::uint16<8>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int8<16>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:39:15: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint16<8>’ with ‘private’ member ‘simdpp::arch_avx2::uint16<8>::d_’ from an array of ‘const class simdpp::arch_avx2::int8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:104:7: note: ‘class simdpp::arch_avx2::uint16<8>’ declared here; class uint16<8, void> : public any_int16<8, uint16<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::int16<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:56640,error,error,56640,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"rch_avx2::shuffle1(const simdpp::arch_avx2::any_vec64<N, V1>&, const simdpp::arch_avx2::any_vec64<N, V2>&) [with unsigned int s0 = 1; unsigned int s1 = 0; unsigned int N = 2; V1 = simdpp::arch_avx2::uint64<2>; V2 = simdpp::arch_avx2::uint64<2>; typename simdpp::arch_avx2::detail::get_expr2_nomask<V1, V2>::empty = simdpp::arch_avx2::uint64<2, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/mem_unpack.h:216:23: required from ‘void simdpp::arch_avx2::detail::insn::v_mem_unpack3_impl16_128(T&, T&, T&) [with T = simdpp::arch_avx2::uint16<8>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/mem_unpack.h:367:29: required from ‘void simdpp::arch_avx2::detail::insn::mem_unpack3(simdpp::arch_avx2::uint16<N>&, simdpp::arch_avx2::uint16<N>&, simdpp::arch_avx2::uint16<N>&) [with unsigned int N = 8]’; libsimdpp-2.0-rc2/simdpp/detail/insn/load_packed3.h:239:16: required from ‘void simdpp::arch_avx2::detail::insn::v128_load_packed3(V&, V&, V&, const char*) [with V = simdpp::arch_avx2::uint16<8>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/load_packed3.h:69:33: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::float64<2>’ with ‘private’ member ‘simdpp::arch_avx2::float64<2>::d_’ from an array of ‘const class simdpp::arch_avx2::uint64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:32,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/float64x2.h:32:7: note: ‘class simdpp::arch_avx2::float64<2>’ declared here; class float64<2, void> : public any_float64<2, float64<2,void>> {; ^~~~~~~~~~~~~~~~; cc1plus: all warnings being treated as errors; <builtin>: recipe for target 'ibs.o' failed; make: *** [ibs.o] Error 1; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:146748,error,error,146748,https://hail.is,https://github.com/hail-is/hail/issues/3955,3,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions: Int, shuffle: Boolean)(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; match argument types (Int,shuffle: Boolean); Error occurred in an application involving default arguments.; start.copy(rdd = start.rdd.coalesce(k, shuffle = shuffle)(null).asOrderedRDD); ^; 5 errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 42.509 secs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:3027,Error,Error,3027,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,3,"['Error', 'FAILURE', 'error']","['Error', 'FAILURE', 'errors']"
Availability,"rderedRVD.scala:285); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:21); is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:243); is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); is.hail.rvd.RVD$class.take(RVD.scala:247); is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); is.hail.table.Table.take(Table.scala:990); is.hail.table.Table.showString(Table.scala:1031); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); java.lang.reflect.Method.invoke(Method.java:498); py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); py4j.Gateway.invoke(Gateway.java:280); py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); py4j.commands.CallCommand.execute(CallCommand.java:79); ```. Also under Failed Stages, the Failure Reason was given as:; ```; Job aborted due to stage failure: Task 0 in stage 10.0 failed 20 times, most recent failure: Lost task 0.19 in stage 10.0 (TID 526, ccarey-sw-svrp.c.ukbb-robinson.internal, executor 43): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:975); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508:7507,Failure,Failure,7507,https://hail.is,https://github.com/hail-is/hail/issues/3508,2,"['Failure', 'failure']","['Failure', 'failure']"
Availability,"re are two ordering operations in hail: `Table.order_by`, and `MatrixTable.choose_cols`. Of the three keying operations, only `Table.key_by` and `MatrixTable.key_rows_by` enforce ordering. The ordering of the columns of a matrix table has no relationship to the keys of the columns of a MatrixTable. **NB**: when a table is created from the columns of a matrix table (`MatrixTable.cols`), the table is sorted by the keys (there exists an implicit `Table.key_by`). Moreover, we guarantee and document (in `MatrixTable.cols`) that when the matrix table has a zero-length column key, the table's ordering is given by matrix table columns' ordering. According to ""Ordering and keys in relational objects"", all sorts (whether triggered by an order_by or a key_by) are unstable. A common user operation is to export or collect a field of a relational object. Sometimes users do not want the keys of an expression exported or collected. In this situation, the user requires that the data is sorted in a sensible way (otherwise they cannot recover which item came from which key). Hail internally guarantees (but does not guarantee to our users or document) that localizing operations (take, collect, and show) and `export` produce data in the ordering of the relational object. For example:. ```; In [38]: t = hl.utils.range_table(3) ; ...: t = t.order_by(-t.idx).show() ; +-------+; | idx |; +-------+; | int32 |; +-------+; | 2 |; | 1 |; | 0 |; +-------+; ```. # Ordering and Library Developers. On occasion, a user may have a table of unknown ordering and keying. For example, the implementor of `Expression.collect` (e.g. `mt.GT.collect()`). In this situation, it is desirable to be able to remove keys without modifying the order. In particular, the values should appear in the same order that they appear in the relational object (for a table, in the order of the rows; for a matrix table, ordered first by the row and then by the column). For example, the multiplication table for 0 to 2:. ```; In [39",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6929:1312,recover,recover,1312,https://hail.is,https://github.com/hail-is/hail/issues/6929,1,['recover'],['recover']
Availability,"re: test failures, that policy makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3322#issuecomment-379580976:9,failure,failures,9,https://hail.is,https://github.com/hail-is/hail/pull/3322#issuecomment-379580976,1,['failure'],['failures']
Availability,read -i foo.vcf gives bad error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/90:26,error,error,26,https://hail.is,https://github.com/hail-is/hail/issues/90,1,['error'],['error']
Availability,"read-2; 2023-09-22 19:11:13.127 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-09-22 19:11:13.127 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: RegionPool: FREE: 128.0K allocated (128.0K blocks / 0 chunks), regions.size = 2, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.138 : ERROR: GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=/result.0; From is.hail.relocated.com.google.cloud.storage.StorageException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=/result.0; 	at is.hail.relocated.com.google.cloud.storage.StorageException.translate(StorageException.java:165); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:298); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.load(HttpStorageRpc.java:729); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.lambda$readAllBytes$20(StorageImpl.java:610); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(Stor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:4387,down,download,4387,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['down'],['download']
Availability,ready to approve once tests passing (test_hail_java has current failures),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8243#issuecomment-594854003:64,failure,failures,64,https://hail.is,https://github.com/hail-is/hail/pull/8243#issuecomment-594854003,1,['failure'],['failures']
Availability,"real_timeout\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 535, in connect\n proto = await self._create_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 892, in _create_connection\n _, proto = await self._create_direct_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 975, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.128.0.11:5000 ssl:default [Connection refused]; INFO	2022-03-02 19:06:38,141	resource_manager.py	create_vm:191	created machine batch-worker-pr-11438-default-g6cibyji6520-standard-4d9n8; ERROR	2022-03-02 19:06:39,183	job.py	schedule_job:473	error while scheduling job (98, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:20016,ERROR,ERROR,20016,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['ERROR'],['ERROR']
Availability,"rebased, and yet another transient error added (EAI_EAGAIN, temporary failure in name resolution).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10390#issuecomment-832198390:35,error,error,35,https://hail.is,https://github.com/hail-is/hail/pull/10390#issuecomment-832198390,2,"['error', 'failure']","['error', 'failure']"
Availability,"redirect.dependabot.com/jmoiron/humanize/issues/243"">#243</a>) <a href=""https://github.com/hugovk""><code>@​hugovk</code></a></li>; </ul>; <h2>3.13.0</h2>; <h2>Added</h2>; <ul>; <li>Add da_DK language (<a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/238"">#238</a>) <a href=""https://github.com/dejurin""><code>@​dejurin</code></a></li>; <li>Fix and add Russian and Ukrainian words (<a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/235"">#235</a>) <a href=""https://github.com/dejurin""><code>@​dejurin</code></a></li>; <li>Add missing strings for Polish translation (<a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/182"">#182</a>) <a href=""https://github.com/kpostekk""><code>@​kpostekk</code></a></li>; <li>Add Traditional Chinese (zh-HK) (<a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/233"">#233</a>) <a href=""https://github.com/edwardmfho""><code>@​edwardmfho</code></a></li>; </ul>; <h2>Changed</h2>; <ul>; <li>Remove redundant setuptools from install_requires (<a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/232"">#232</a>) <a href=""https://github.com/arthurzam""><code>@​arthurzam</code></a></li>; </ul>; <h2>Deprecated</h2>; <ul>; <li>This is the last release to support Python 3.6</li>; <li>Deprecate private functions (<a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/234"">#234</a>) <a href=""https://github.com/samueljsb""><code>@​samueljsb</code></a></li>; <li>Reinstate <code>VERSION</code> and deprecate (<a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/240"">#240</a>) <a href=""https://github.com/hugovk""><code>@​hugovk</code></a></li>; </ul>; <h2>3.12.0</h2>; <h2>Added</h2>; <ul>; <li>Add support for Python 3.10 (<a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/223"">#223</a>) <a href=""https://github.com/hugovk""><code>@​hugovk</code></a></li>; </ul>; <h2>Changed</h2>; <ul>; <li>Use importlib.metadata to ge",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11517:3103,redundant,redundant,3103,https://hail.is,https://github.com/hail-is/hail/pull/11517,2,['redundant'],['redundant']
Availability,"ref: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Error.20importing.20dbsnp.20VCF/near/177209550. I have confirmed that the test I added fails with the same error laurent saw in the linked conversation, and passes with the change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7182#issuecomment-537763919:81,Error,Error,81,https://hail.is,https://github.com/hail-is/hail/pull/7182#issuecomment-537763919,2,"['Error', 'error']","['Error', 'error']"
Availability,"ref=""https://github-redirect.dependabot.com/cbeust/testng/pull/2839"">cbeust/testng#2839</a></li>; <li>Honour regex in dependsOnMethods by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2838"">cbeust/testng#2838</a></li>; <li>Ensure All tests run all the time by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2842"">cbeust/testng#2842</a></li>; <li>Deprecate support for running Spock Tests by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2846"">cbeust/testng#2846</a></li>; <li>Streamline dependsOnMethods for configurations by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2845"">cbeust/testng#2845</a></li>; <li>Ensure ITestContext available for JUnit4 tests by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2848"">cbeust/testng#2848</a></li>; <li>Deprecate support for running JUnit tests by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2849"">cbeust/testng#2849</a></li>; </ul>; <h2>New Contributors</h2>; <ul>; <li><a href=""https://github.com/gruenich""><code>@​gruenich</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2781"">cbeust/testng#2781</a></li>; <li><a href=""https://github.com/anatolyuzhakov""><code>@​anatolyuzhakov</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2793"">cbeust/testng#2793</a></li>; <li><a href=""https://github.com/spkrka""><code>@​spkrka</code></a> made their first contribu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:7595,avail,available,7595,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['avail'],['available']
Availability,"regarding doc test failure, you need `g.DP`, not `g.dp`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2497#issuecomment-348701713:19,failure,failure,19,https://hail.is,https://github.com/hail-is/hail/pull/2497#issuecomment-348701713,1,['failure'],['failure']
Availability,release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/p,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:5560,echo,echo,5560,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,remove redundant fixmes (obviated by Literal IR),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4381:7,redundant,redundant,7,https://hail.is,https://github.com/hail-is/hail/pull/4381,1,['redundant'],['redundant']
Availability,removed AnnotationImpex; removed some dead code from SparkAnnotationImpex; Table.toDF now requires expandTypes to have been run (will fail with match error in SparkAnnotationImpex.exportType); Handle tuple in Spark conversion (convert to struct),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3318:150,error,error,150,https://hail.is,https://github.com/hail-is/hail/pull/3318,1,['error'],['error']
Availability,repartition(shuffle=False) should error out if increasing number of partitions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3756:34,error,error,34,https://hail.is,https://github.com/hail-is/hail/issues/3756,1,['error'],['error']
Availability,report/accumulators have been removed: https://github.com/hail-is/hail/pull/2024. No longer relevant. .... although better input integrity checking and error reporting would still be nice.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/371#issuecomment-319497230:152,error,error,152,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-319497230,1,['error'],['error']
Availability,"reproduces with this command:. ``` text; hail importvcf src/test/resources/sample.vcf annotatevariants expr -c 'va.filters = ""HELLO""' exportvcf -o /tmp/out.vcf; ```. ``` text; hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.ClassCastException: java.lang.String cannot be cast to scala.collection.immutable.Set; at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at scala.Option.map(Option.scala:145); at org.broadinstitute.hail.driver.ExportVCF$.org$broadinstitute$hail$driver$ExportVCF$$appendRow$1(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:278); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:276); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1109); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/785:269,failure,failure,269,https://hail.is,https://github.com/hail-is/hail/issues/785,2,['failure'],['failure']
Availability,resiliency improvements,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6201:0,resilien,resiliency,0,https://hail.is,https://github.com/hail-is/hail/pull/6201,1,['resilien'],['resiliency']
Availability,resolves #791 . This should cause type errors much sooner if a signature disagrees with the actual types. Much of the sample annotating code uses multiple inserters and therefore cannot be rewritten to use `annotateSamples`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/809:39,error,errors,39,https://hail.is,https://github.com/hail-is/hail/pull/809,1,['error'],['errors']
Availability,"response. During handling of the above exception, another exception occurred:. ProtocolError Traceback (most recent call last); File /opt/conda/lib/python3.10/site-packages/requests/adapters.py:487, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies); 486 try:; --> 487 resp = conn.urlopen(; 488 method=request.method,; 489 url=url,; 490 body=request.body,; 491 headers=request.headers,; 492 redirect=False,; 493 assert_same_host=False,; 494 preload_content=False,; 495 decode_content=False,; 496 retries=self.max_retries,; 497 timeout=timeout,; 498 chunked=chunked,; 499 ); 501 except (ProtocolError, OSError) as err:. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:787, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw); 785 e = ProtocolError(""Connection aborted."", e); --> 787 retries = retries.increment(; 788 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]; 789 ); 790 retries.sleep(). File /opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py:550, in Retry.increment(self, method, url, response, error, _pool, _stacktrace); 549 if read is False or not self._is_method_retryable(method):; --> 550 raise six.reraise(type(error), error, _stacktrace); 551 elif read is not None:. File /opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py:769, in reraise(tp, value, tb); 768 if value.__traceback__ is not tb:; --> 769 raise value.with_traceback(tb); 770 raise value. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:703, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw); 702 # Make the request on the httplib connection object.; --> 703 httplib_response = self._make_request(; 704 conn,; 705 method,; 706 url,; 707 timeout=timeout_obj,; 708 body=bod",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:7087,error,error,7087,https://hail.is,https://github.com/hail-is/hail/issues/13960,1,['error'],['error']
Availability,"ress=progress); /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:451: in _rpc; result_bytes = await retry_transient_errors(self._read_output, ir, iodir + '/out'); /usr/local/lib/python3.8/dist-packages/hailtop/utils/utils.py:781: in retry_transient_errors; return await retry_transient_errors_with_debug_string('', 0, f, *args, **kwargs); /usr/local/lib/python3.8/dist-packages/hailtop/utils/utils.py:794: in retry_transient_errors_with_debug_string; return await f(*args, **kwargs); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <hail.backend.service_backend.ServiceBackend object at 0x7ff6aa3b15b0>; ir = <hail.ir.ir.MakeTuple object at 0x7ff6a73b8d30>; output_uri = 'hail-az://haildevtest/test/tmp/hail/mGZ0y8JSUQM6CVs20XzdIu/eLwQUTyLRv/out'. async def _read_output(self, ir: Optional[BaseIR], output_uri: str) -> bytes:; assert self._batch; ; try:; driver_output = await self._async_fs.open(output_uri); except FileNotFoundError as exc:; raise FatalError('Hail internal error. Please contact the Hail team and provide the following information.\n\n' + yamlx.dump({; 'service_backend_debug_info': self.debug_info(),; 'batch_debug_info': await self._batch.debug_info(); })) from exc; ; async with driver_output as outfile:; success = await read_bool(outfile); if success:; return await read_bytes(outfile); ; short_message = await read_str(outfile); expanded_message = await read_str(outfile); error_id = await read_int(outfile); ; reconstructed_error = fatal_error_from_java_error_triplet(short_message, expanded_message, error_id); if ir is None:; raise reconstructed_error; > raise reconstructed_error.maybe_user_error(ir); E hail.utils.java.FatalError: RuntimeException: Stream is already closed.; E ; E Java stack trace:; E java.util.concurrent.ExecutionException: java.lang.RuntimeException: Stream is already closed.; E 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); E 	at java.util.concurrent.FutureTask.get",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:12938,error,error,12938,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['error'],['error']
Availability,"retry in get_userinfo; retry on ClientOSError (client connection error) on timeout; log errno to diagnose future failures. Some of the current errors are ClientOSErrors (ClientConnectionErrors, actually) with strerror ""Connect call failed"" but that doesn't correspond to a standard errno message returned by perror/os.strerror as far as I can tell. So I'm retrying on timeout (good) and logging the errno so we can diagnose future transient failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7279:65,error,error,65,https://hail.is,https://github.com/hail-is/hail/pull/7279,4,"['error', 'failure']","['error', 'errors', 'failures']"
Availability,"return CreateDatabaseStep(; + return CreateDatabase2Step(; params,; json['databaseName'],; json['namespace'],; @@ -1111,12 +1113,12 @@ EOF; attributes={'name': self.name},; secrets=[; {; - 'namespace': self.database_server_config_namespace,; + 'namespace': self.namespace,; 'name': 'database-server-config',; 'mount_path': '/sql-config',; }; ],; - service_account={'namespace': DEFAULT_NAMESPACE, 'name': 'ci-agent'},; + service_account={'namespace': self.namespace, 'name': 'admin'},; input_files=input_files,; parents=[self.create_passwords_job] if self.create_passwords_job else self.deps_parents(),; network='private',; @@ -1125,42 +1127,4 @@ EOF; ); ; def cleanup(self, batch, scope, parents):; - if scope in ['deploy', 'dev'] or self.cant_create_database:; - return; -; - cleanup_script = f'''; -set -ex; -; -commands=$(mktemp); -; -cat >$commands <<EOF; -DROP DATABASE IF EXISTS \\`{self._name}\\`;; -DROP USER IF EXISTS '{self.admin_username}';; -DROP USER IF EXISTS '{self.user_username}';; -EOF; -; -until mysql --defaults-extra-file=/sql-config/sql-config.cnf <$commands; -do; - echo 'failed, will sleep 2 and retry'; - sleep 2; -done; -; -'''; -; - self.cleanup_job = batch.create_job(; - CI_UTILS_IMAGE,; - command=['bash', '-c', cleanup_script],; - attributes={'name': f'cleanup_{self.name}'},; - secrets=[; - {; - 'namespace': self.database_server_config_namespace,; - 'name': 'database-server-config',; - 'mount_path': '/sql-config',; - }; - ],; - service_account={'namespace': DEFAULT_NAMESPACE, 'name': 'ci-agent'},; - parents=parents,; - always_run=True,; - network='private',; - regions=[REGION],; - ); + pass; diff --git a/ci/test/resources/build.yaml b/ci/test/resources/build.yaml; index e6f67bb486..662c873590 100644; --- a/ci/test/resources/build.yaml; +++ b/ci/test/resources/build.yaml; @@ -190,7 +190,7 @@ steps:; to: /io/pyproject.toml; dependsOn:; - hello_image; - - kind: createDatabase; + - kind: createDatabase2; name: hello_database; databaseName: hello; image:; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600:3199,echo,echo,3199,https://hail.is,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600,1,['echo'],['echo']
Availability,"return NA for median of empty arrays, instead of inscrutable error",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1287:61,error,error,61,https://hail.is,https://github.com/hail-is/hail/issues/1287,1,['error'],['error']
Availability,"rf_kt.schema()). rf_df = rf_kt.to_dataframe(); rf_df.printSchema(); rf_df.show(); ```. And here is the output and stacktrace as it crashes when running `show()`:. ```; Struct {; `va.info.MQRankSum`: Double; }; root; |-- va.info.MQRankSum: double (nullable = true); Traceback (most recent call last):; File ""/tmp/db4d8e04-85c9-4eba-b0b3-4ade69200fd3/test.py"", line 60, in <module>; rf_df.show(); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o73.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 2520, gnomad-prod-sw-s89f.c.broad-mpg-gnomad.internal): java.lang.IndexOutOfBoundsException: 21; at scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43); at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:48); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.keytable.KeyTable$$anonfun$8.apply(KeyTable.scala:68); at is.hail.keytabl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1275:1169,failure,failure,1169,https://hail.is,https://github.com/hail-is/hail/issues/1275,1,['failure'],['failure']
Availability,"rface wlp0s20f3); 2022-10-06 15:56:03 WARN Utils:69 - Set SPARK_LOCAL_IP if you need to bind to another address; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/med/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 2022-10-06 15:56:03 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.1.3; SparkUI available at http://192.168.248.80:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.100-2ea2615a797a; LOGGING: writing to /; --------------------------------------------------------------------------; mt.filter_rows(mt.locus.position==2867101).count_rows(); ```; ### Expected ; Return a count of rows with that condition. ### Error ; ```; FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:208); at is.hail.expr.ir.LoweredTableReader$.makeCoercer(TableIR.scala:135); at is.hail.expr.ir.GenericTableValue.getLTVCoercer(GenericTableValue.scala:137); at is.hail.expr.ir.GenericTableValue.toTableStage(GenericTableValue.scala:162); at is.hail.io.vcf.MatrixVCFReader.lower(LoadVCF.scala:1798); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:717); at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:697); at is.hail.expr.ir.lowerin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12280:1598,avail,available,1598,https://hail.is,https://github.com/hail-is/hail/issues/12280,1,['avail'],['available']
Availability,"rg.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.lau",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:3201,failure,failure,3201,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705,1,['failure'],['failure']
Availability,"rg.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: ERROR: Task 35 in stage 0.0 failed 4 times; aborting job; 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.serv",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:194331,ERROR,ERROR,194331,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,"rgs); File ""/usr/local/lib/python3.10/site-packages/hailtop/utils/utils.py"", line 834, in retry_transient_errors_with_debug_string; st = ''.join(traceback.format_stack()); . The most recent error was <class 'hailtop.httpx.ClientResponseError'> 500, message='Internal Server Error', url=URL('http://batch.hail/api/v1alpha/batches/485962/updates/1/jobs/create') body='500 Internal Server Error\n\nServer got itself in trouble'. ; Traceback (most recent call last):; File ""/usr/local/lib/python3.10/site-packages/hailtop/utils/utils.py"", line 809, in retry_transient_errors_with_debug_string; return await f(*args, **kwargs); File ""/usr/local/lib/python3.10/site-packages/hailtop/aiocloud/common/session.py"", line 117, in _request_with_valid_authn; return await self._http_session.request(method, url, **kwargs); File ""/usr/local/lib/python3.10/site-packages/hailtop/httpx.py"", line 148, in request_and_raise_for_status; raise ClientResponseError(; hailtop.httpx.ClientResponseError: 500, message='Internal Server Error', url=URL('http://batch.hail/api/v1alpha/batches/485962/updates/1/jobs/create') body='500 Internal Server Error\n\nServer got itself in trouble'; 2024-09-25 01:54:55,288 - hailtop.utils 835 - WARNING - A transient error occured. We will automatically retry. We have thus far seen 50 transient errors (next delay: 60.0s).; ```. The corresponding server-side error was. ```; pymysql.err.DataError: (1406, \""Data too long for column 'value' at row 106\""); ```. coming from the `INSERT INTO job_attributes …` query in `insert_jobs_into_db()`. We write a list of the samples being processed as a job attribute, and it turned out that for at least some of the jobs of this batch this list had grown to longer than 64K of text. The `job_attributes.value` database field is of type TEXT, which limits each individual attribute to 64KiB bytes. While writing a long list of sample ids as an attribute may or may not be a great idea :smile: it is fair to say that 64K is not a large maximum for ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14702:1369,Error,Error,1369,https://hail.is,https://github.com/hail-is/hail/issues/14702,1,['Error'],['Error']
Availability,"rgs_); File ""/opt/conda/default/lib/python3.8/site-packages/hail/table.py"", line 1335, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 105, in execute; raise e.maybe_user_error(ir) from None; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 99, in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); File ""/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1304, in __call__; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 31, in deco; raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; hail.utils.java.FatalError: ClassFormatError: Too many arguments in method signature in class file __C2866stream. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 8.0 failed 20 times, most recent failure: Lost task 3.19 in stage 8.0 (TID 54368) (leo-test-w-8.australia-southeast1-a.c.ourdna-browser.internal executor 14): java.lang.ClassFormatError: Too many arguments in method signature in class file __C2866stream; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:635); 	at is.hail.asm4s.HailClassLoader.liftedTree1$1(HailClassLoader.scala:10); 	at is.hail.asm4s.HailClassLoader.loadOrDefineClass(HailClassLoader.scala:6); 	at is.hail.asm4s.ClassesBytes.$anonfun$load$1(ClassBuilder.scala:64); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:3147,failure,failure,3147,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['failure'],['failure']
Availability,"rgs_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in show(self, n, width, truncate, types); 1215 Print an extra header line with the type of each field.; 1216 """"""; -> 1217 print(self._show(n,width, truncate, types)); 1218 ; 1219 def _show(self, n=10, width=90, truncate=None, types=True):. /home/hail/hail.zip/hail/table.py in _show(self, n, width, truncate, types); 1218 ; 1219 def _show(self, n=10, width=90, truncate=None, types=True):; -> 1220 return self._jt.showString(n, joption(truncate), types, width); 1221 ; 1222 def index(self, *exprs):. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 199 'Hail version: %s\n'; --> 200 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 201 except pyspark.sql.utils.CapturedException as e:; 202 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: RuntimeException: Cannot find row in Map(). Java stack trace:; is.hail.utils.HailException: Error while typechecking IR:; (MakeStruct; (titv; (ApplyBinaryPrimOp FloatingPointDivide; (GetField n_ti; (Ref Struct{rank_id:String,snv:Boolean,bi_allelic:Boolean,singleton:Boolean,bin:Int32,min_score:Float64,max_score:Float64,n_ti:Int64,n_tv:Int64,model:String} row)); (GetField n_tv; (Ref Struct{rank_id:String,snv:Boolean,bi_allelic:Boolean,singleton:Boolean,bin:Int32,min_score:Float64,max_score:Float64,n_ti:Int64,n_tv:Int64,model:String} row)))); (min_score; (GetField `0`; (In Struct{`0`:Float64,`1`:Float64} 0))); (max_score; (GetField `1`; (In Struct{`0`:Float64,`1`:Float64} 0)))); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:11); 	at is.hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4110:2369,Error,Error,2369,https://hail.is,https://github.com/hail-is/hail/issues/4110,1,['Error'],['Error']
Availability,"ricted/projectnb/genpro/github/hail/test.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test. vdf'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 4 times, most recent failure: Lost task 6.3 in stage 2.0 (TID 2 53, scc-q15.scc.bu.edu, executor 1): java.io.IOException: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:2395,failure,failure,2395,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['failure'],['failure']
Availability,"rk-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583:2399,Error,ErrorHandling,2399,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583,1,['Error'],['ErrorHandling']
Availability,rkload-dependent and is based on the number of jobs / number of files. The GCS best practices states the initial capacity is 5000 read requests / second per bucket including list operations until the bucket has time to scale up its capacity. https://cloud.google.com/storage/docs/request-rate#best-practices. ```. ==============================================================================; DIAGNOSTIC RESULTS ; ==============================================================================. ------------------------------------------------------------------------------; Latency ; ------------------------------------------------------------------------------; Operation Size Trials Mean (ms) Std Dev (ms) Median (ms) 90th % (ms); ========= ========= ====== ========= ============ =========== ===========; Delete 0 B 5 43.1 6.4 40.9 50.9 ; Delete 1 KiB 5 44.2 12.7 42.5 58.1 ; Delete 100 KiB 5 44.7 10.4 42.8 56.3 ; Delete 1 MiB 5 41.5 3.7 40.2 45.7 ; Download 0 B 5 74.6 7.9 73.2 84.0 ; Download 1 KiB 5 84.3 15.9 80.6 103.4 ; Download 100 KiB 5 81.9 16.0 82.7 99.6 ; Download 1 MiB 5 90.6 6.5 94.5 96.8 ; Metadata 0 B 5 23.6 2.7 23.6 26.3 ; Metadata 1 KiB 5 25.5 2.1 26.9 27.4 ; Metadata 100 KiB 5 26.2 3.6 27.3 29.9 ; Metadata 1 MiB 5 24.0 3.7 23.3 28.4 ; Upload 0 B 5 98.1 16.6 95.5 117.9 ; Upload 1 KiB 5 116.7 21.8 115.5 142.1 ; Upload 100 KiB 5 116.5 17.8 115.1 135.1 ; Upload 1 MiB 5 168.2 18.5 179.6 185.6 . ------------------------------------------------------------------------------; Write Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Write throughput: 977.7 Mibit/s.; Parallelism strategy: both. ------------------------------------------------------------------------------; Read Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Read throughput: 1.11 Gibit/s.; ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:1320,Down,Download,1320,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['Down'],['Download']
Availability,"roblem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.908 [ERROR] [system.err] Ran 7 tests in 83",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419:1121,ERROR,ERROR,1121,https://hail.is,https://github.com/hail-is/hail/issues/1419,1,['ERROR'],['ERROR']
Availability,"rojects/hail/hail/python/hail/matrixtable.py"", line 2528, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 114, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 379, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 44, in async_request; raise FatalError(f'Error from server: {result[""value""]}'); FatalError: Error from server: java.util.NoSuchElementException: key not found: RefEquality(WriteMetadata(WriteMetadata(WriteMetadata(WriteMetadata(WriteMetadata(Let(__iruid_368,WritePartition(ToStream(Literal(array<struct{s: str}>,WrappedArray([HG00096], [HG00099], [HG00105], [HG00118], [HG00129], [HG00148], [HG00177], [HG00182], [HG00242], [HG00254], [HG00265], [HG00271], [HG00274], [HG00332], [HG00335], [HG00369], [HG00421], [HG00436], [HG00452], [HG00472], [HG00530], [HG00534], [HG00583], [HG00590], [HG00598], [HG00607], [HG00619], [HG00623], [HG00657], [HG00663], [HG00704], [HG00705], [HG00733], [HG00864], [HG00881], [HG01052], [HG01070], [HG01075], [HG01164], [HG01174], [HG01241], [HG01248], [HG01256], [HG01275], [HG01284], [HG01334], [HG01348], [HG01396], [HG01443], [HG01491], [HG01498], [HG01537], [HG01572], [HG01606], [HG01623], [HG01630], [HG01783], [HG01784], [HG01790], [HG01799], [HG01801], [HG01806], [HG01812], [HG01813], [HG01817], [HG01848], [H",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856:1849,Error,Error,1849,https://hail.is,https://github.com/hail-is/hail/issues/9856,2,['Error'],['Error']
Availability,"rom 1.3.1 to 2.0.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aioredis-py/releases"">aioredis's releases</a>.</em></p>; <blockquote>; <h2>v2.0.1</h2>; <p>Version v2.0.1</p>; <h2>Features</h2>; <ul>; <li>Added Python 3.10 to CI &amp; Updated the Docs (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1160"">#1160</a>)</li>; <li>Enable mypy in CI (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1101"">#1101</a>)</li>; <li>Synchronized reading the responses from a connection (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1106"">#1106</a>)</li>; </ul>; <h2>Fixes</h2>; <ul>; <li>Remove del from Redis (Fixes <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1115"">#1115</a>) (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1227"">#1227</a>)</li>; <li>fix socket.error raises (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1129"">#1129</a>)</li>; <li>Fix buffer is closed error when using PythonParser class (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1213"">#1213</a>)</li>; </ul>; <h2>Version v2.0.0</h2>; <p>Version 2.0 is a complete rewrite of aioredis. Starting with this version, aioredis now follows the API of <a href=""https://github.com/andymccurdy/redis-py"">redis-py</a>, so you can easily adapt synchronous code that uses redis-py for async applications with aioredis-py.</p>; <p><strong>NOTE:</strong> This version is <em>not</em> compatible with earlier versions of aioredis. If you upgrade, you will need to make code changes.</p>; <p>For more details, read our <a href=""https://aioredis.readthedocs.io/en/latest/migration/"">documentation on migrating to version 2.0</a>.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aior",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11569:1034,error,error,1034,https://hail.is,https://github.com/hail-is/hail/pull/11569,1,['error'],['error']
Availability,"rom an array of ‘const class simdpp::arch_avx2::uint16<32>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:36,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16.h:31:7: note: ‘class simdpp::arch_avx2::int16<32>’ declared here; class int16<N, void> : public any_int16<N, int16<N,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint16<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint16<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint16<8>]’; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:129:36: required from ‘simdpp::arch_avx2::uint64<2>& simdpp::arch_avx2::uint64<2>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint16<8, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/mem_unpack.h:471:8: required from ‘void simdpp::arch_avx2::detail::insn::v_mem_unpack4_impl16_128(T&, T&, T&, T&) [with T = simdpp::arch_avx2::uint16<8>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/mem_unpack.h:585:29: required from ‘void simdpp::arch_avx2::detail::insn::mem_unpack4(simdpp::arch_avx2::uint16<N>&, simdpp::arch_avx2::uint16<N>&, simdpp::arch_avx2::uint16<N>&, simdpp::arch_avx2::uint16<N>&) [with unsigned int N = 8]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:139633,Mask,MaskCastOverride,139633,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"rom an array of ‘const class simdpp::arch_avx2::uint16<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:33:7: note: ‘class simdpp::arch_avx2::int8<16>’ declared here; class int8<16, void> : public any_int8<16, int8<16,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::int8<32>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::int8<32>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::int8<32>]’; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:115:37: required from ‘simdpp::arch_avx2::uint16<16>& simdpp::arch_avx2::uint16<16>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int8<32>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:61:15: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint16<16>’ with ‘private’ member ‘simdpp::arch_avx2::uint16<16>::d_’ from an array of ‘const class simdpp::arch_avx2::int8<32>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:61525,Mask,MaskCastOverride,61525,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"ror. -- by :user:<code>Dreamsorcerer</code></p>; <p><code>[#7366](https://github.com/aio-libs/aiohttp/issues/7366) &lt;https://github.com/aio-libs/aiohttp/issues/7366&gt;</code>_</p>; </li>; </ul>; <h2>Bugfixes</h2>; <ul>; <li>; <p>Fixed a transport is :data:<code>None</code> error -- by :user:<code>Dreamsorcerer</code>.</p>; <p><code>[#3355](https://github.com/aio-libs/aiohttp/issues/3355) &lt;https://github.com/aio-libs/aiohttp/issues/3355&gt;</code>_</p>; </li>; </ul>; <hr />; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/9c13a52c21c23dfdb49ed89418d28a5b116d0681""><code>9c13a52</code></a> Bump aiohttp to v3.8.5 a security release</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/7c02129567bc4ec59be467b70fc937c82920948c""><code>7c02129</code></a>  Bump pypa/cibuildwheel to v2.14.1</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/135a45e9d655d56e4ebad78abe84f1cb7b5c62dc""><code>135a45e</code></a> Improve error messages from C parser (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7366"">#7366</a>) (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7380"">#7380</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/9337fb3f2ab2b5f38d7e98a194bde6f7e3d16c40""><code>9337fb3</code></a> Fix bump llhttp to v8.1.1 (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7367"">#7367</a>) (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7377"">#7377</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/f07e9b44b5cb909054a697c8dd447b30dbf8073e""><code>f07e9b4</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7373"">#7373</a>/66e261a5 backport][3.8] Drop azure mention (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7374"">#7374</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/01d9b70e5477cd746561b52225992d8a2ebde953""><code>01d9b70</code></a> [PR <a href=""htt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13270:3552,error,error,3552,https://hail.is,https://github.com/hail-is/hail/pull/13270,5,['error'],['error']
Availability,roundoff-error-tolerant comparison operators for doubles,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/51:9,error,error-tolerant,9,https://hail.is,https://github.com/hail-is/hail/issues/51,1,['error'],['error-tolerant']
Availability,"rray of ‘const class simdpp::arch_avx2::uint64<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:22,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:92:7: note: ‘class simdpp::arch_avx2::uint16<16>’ declared here; class uint16<16, void> : public any_int16<16, uint16<16,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::int8<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::int8<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::int8<16>]’; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:133:36: required from ‘simdpp::arch_avx2::uint16<8>& simdpp::arch_avx2::uint16<8>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int8<16>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:39:15: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint16<8>’ with ‘private’ member ‘simdpp::arch_avx2::uint16<8>::d_’ from an array of ‘const class simdpp::arch_avx2::int8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file inclu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:55982,Mask,MaskCastOverride,55982,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"rray; ```. rather than. ```; FatalError Traceback (most recent call last); <ipython-input-18-89d0abef0d0a> in <module>(); ----> 1 vds1 = vds.linreg('sa.pheno', 'sa.cov1').count(). <decorator-gen-240> in linreg(self, y, covariates, root, use_dosages, min_ac, min_af). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^. Java stack trace:; is.hail.utils.HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:20); at is.hail.expr.ParserUtils$.error(Parser.scala:24); at is.hail.expr.AST.parseError(AST.scala:222); at is.hail.expr.SymRef.typecheckThis(AST.scala:648); at is.hail.expr.AST.typecheck(AST.scala:219); at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); at is.hail.expr.Parser$.parseExpr(Parser.scala:67); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at is.hail.stats.RegressionUtils$.getPhenoCovCompleteSamples(RegressionUtils.scala:36); at is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1654:1077,error,error,1077,https://hail.is,https://github.com/hail-is/hail/pull/1654,1,['error'],['error']
Availability,"rs $HAIL_HOME/build/libs/hail-all-spark.jar --conf=spark.driver.extraClassPath=$HAIL_HOME/build/libs/hail-all-spark.jar --conf=spark.executor.extraClassPath=./hail-all-spark.jar; WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).; WARNING: Running pyspark from user-defined location.; Python 2.7.5 (default, Aug 4 2017, 00:39:18) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 17/10/19 08:45:43 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.2.0.cloudera1; /_/. Using Python version 2.7.5 (default, Aug 4 2017 00:39:18); SparkSession available as 'spark'.; >>> import hail; >>> hc = hail.HailContext(); log4j:ERROR setFile(null,false) call failed.; java.io.FileNotFoundException: hail.log (Permission denied); 	at java.io.FileOutputStream.open0(Native Method); 	at java.io.FileOutputStream.open(FileOutputStream.java:270); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:213); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:133); 	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294); 	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165); 	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104); 	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842); 	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768); 	at org.ap",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198:1101,avail,available,1101,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198,1,['avail'],['available']
Availability,"rs or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there either is, or should be a way of querying one selector across all resources). . * Performance, even for very basic queries. An initial assumption, may prove to be incorrect, but I think we will find a fast db to be much faster than K8, even if we could directly query etcd. This is based on my experience with Amazon data stores, and general impression/experience with google cs products.; * Open issue on this: even directly accessed, etcd is slow, doesn't index selectors https://github.com/kubernetes/kubernetes/issues/4817. * We may want to differentiate between k8 outages and lack of user requests. I also want to be able to quickly present a potential investor aggregate data, for notebook and all other manner of hail services: for notebook case: # notebooks created, length a notebook was used, how many notebooks were shared with others (I think this could be a useful feature, even if ""sharing"" meant taking a user-opt-in text dump that could be used to re-create a notebook, rather than connecting to that user's notebook), how many times a user re-visited a notebook, what our fullfillment rate was, what our error rate was, what the conversion rate is ( users that visited and created / users that visited our service). Cons against using a separate data store: non-atomic operations at the boundary between sql and k8. This is isn't a problem if we choose one master view (our db), and frankly seems unavoidable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:2567,outage,outages,2567,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290,2,"['error', 'outage']","['error', 'outages']"
Availability,"rs the issue:. Setup:. $ $SPARK_HOME/sbin/start-master.sh --host localhost --port 7077; $ $SPARK_HOME/sbin/start-shuffle-service.sh; $ $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077 --work-dir /scratch/local/. Test:. import hail; hail.init(master=""spark://localhost:7077""); P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = hail.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.n_hom_var > V*0.32); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception:; print(""\n[FAIL] with "", N, ""partitions""); break. Test Output (SIGSEGV is reported in Spark worker logs, see end):. 2020-06-10 10:29:56 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 	Setting default log level to ""WARN"".; 	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 	Running on Apache Spark version 2.4.5; 	SparkUI available at http://US0HPN0036.cm.cluster:4047; 	Welcome to; 		 __ __ <>__; 		/ /_/ /__ __/ /; 	 / __ / _ `/ / /; 	 /_/ /_/\_,_/_/_/ version 0.2.44-6cfa355a1954; 	LOGGING: writing to /bmrn/apps/bmrn-hugelib/0.3.0/test/hail-20200610-1029-0.2.44-6cfa355a1954.log; 	2020-06-10 10:29:59 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 1:==========================> (171 + 80) / 350]; 	[PASS] with 350 partitions: (50000, 984); 	2020-06-10 10:30:08 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 3:==========================> (169 + 80) / 351]; 	[PASS] with 351 partitions: (50000, 998); 	2020-06-10 10:30:10 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 5:=====================================================> (344 + 8) / 352]; 	[PASS] with 352 partitions: (50000, 1000); 	2020-06-10 10:30:13 H",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:1495,avail,available,1495,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['avail'],['available']
Availability,"rst"">boto3's changelog</a>.</em></p>; <blockquote>; <h1>1.21.12</h1>; <ul>; <li>api-change:<code>greengrassv2</code>: [<code>botocore</code>] Doc only update that clarifies Create Deployment section.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for data repository associations to use root (&quot;/&quot;) as the file system path</li>; <li>api-change:<code>kendra</code>: [<code>botocore</code>] Amazon Kendra now suggests spell corrections for a query. For more information, see <a href=""https://docs.aws.amazon.com/kendra/latest/dg/query-spell-check.html"">https://docs.aws.amazon.com/kendra/latest/dg/query-spell-check.html</a></li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] Launching Amazon AppFlow Marketo as a destination connector SDK.</li>; <li>api-change:<code>timestream-query</code>: [<code>botocore</code>] Documentation only update for SDK and CLI</li>; </ul>; <h1>1.21.11</h1>; <ul>; <li>api-change:<code>gamelift</code>: [<code>botocore</code>] Minor updates to address errors.</li>; <li>api-change:<code>cloudtrail</code>: [<code>botocore</code>] Add bytesScanned field into responses of DescribeQuery and GetQueryResults.</li>; <li>api-change:<code>athena</code>: [<code>botocore</code>] This release adds support for S3 Object Ownership by allowing the S3 bucket owner full control canned ACL to be set when Athena writes query results to S3 buckets.</li>; <li>api-change:<code>keyspaces</code>: [<code>botocore</code>] This release adds support for data definition language (DDL) operations</li>; <li>api-change:<code>ecr</code>: [<code>botocore</code>] This release adds support for tracking images lastRecordedPullTime.</li>; </ul>; <h1>1.21.10</h1>; <ul>; <li>api-change:<code>mediapackage</code>: [<code>botocore</code>] This release adds Hybridcast as an available profile option for Dash Origin Endpoints.</li>; <li>api-change:<code>rds</code>: [<code>botocore</code>] Documentation updates for Multi-AZ DB cluste",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11486:1242,error,errors,1242,https://hail.is,https://github.com/hail-is/hail/pull/11486,1,['error'],['errors']
Availability,"rted Type Annotations to Py3 syntax Closes</li>; <li>Only run mypy on cpython versions</li>; <li>Also fix all type errors with latest mypy - pycares seems to have no typing / stubs so lets ignore it via <code>mypy.ini</code></li>; <li>setup: typing exists since Python 3.5</li>; <li>Fix type annotation of gethostbyname()</li>; <li>Updated README</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/saghul/aiodns/commit/cdb33385f46be1e18bc525ccb153c8abc8ac92d4""><code>cdb3338</code></a> Updated changelog</li>; <li><a href=""https://github.com/saghul/aiodns/commit/a57968007a0e6f826e1a8a2160eade23c254bc42""><code>a579680</code></a> Updated README</li>; <li><a href=""https://github.com/saghul/aiodns/commit/efbbcd55493e11ff95cce7845ebe23438c4238aa""><code>efbbcd5</code></a> Release wheels and source to PyPI with GH actions</li>; <li><a href=""https://github.com/saghul/aiodns/commit/0c9ea6f60d5a9306b0c80e6ffea5ccc27c5fa5bd""><code>0c9ea6f</code></a> Try to make tests more resilient</li>; <li><a href=""https://github.com/saghul/aiodns/commit/a1d4d550acc573f563196af73568c5227a1cfe20""><code>a1d4d55</code></a> Don't build universal wheels</li>; <li><a href=""https://github.com/saghul/aiodns/commit/1f84d1a0267e5629ead8355816d47d8ae892ef24""><code>1f84d1a</code></a> Migrate CI to GH Actions</li>; <li><a href=""https://github.com/saghul/aiodns/commit/7b58bcb798d32e07bba7391dd98c7405e2523095""><code>7b58bcb</code></a> Bump version to 3.0.0</li>; <li><a href=""https://github.com/saghul/aiodns/commit/5d94ab0b9f81ee1feabee6a222d9352d244d76f2""><code>5d94ab0</code></a> Fix TXT CHAOS test</li>; <li><a href=""https://github.com/saghul/aiodns/commit/7bb002ea8eee1a8c1651f21d5ae0350857f00233""><code>7bb002e</code></a> Add support for CAA queries</li>; <li><a href=""https://github.com/saghul/aiodns/commit/c81387fd26d2e1c9bdcb85f7d12a13cda1c389d4""><code>c81387f</code></a> Support Python &gt;= 3.6</li>; <li>Additional commits viewable in <a href=""h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11570:2796,resilien,resilient,2796,https://hail.is,https://github.com/hail-is/hail/pull/11570,1,['resilien'],['resilient']
Availability,"rties"": [],; ""url"": ""https://www.someurlhere.com"",; ""versions"": [{""url"": {""eu"": ""gs://hail-datasets-eu/dataset"",; ""us"": ""gs://hail-datasets-us/dataset""},; ""version"": ""one_version""},; {""url"": {""eu"": ""gs://hail-datasets-eu/dataset"",; ""us"": ""gs://hail-datasets-us/dataset""},; ""version"": ""another_version""}]}; ```. The `annotation_db.json` file is now used by the `load_dataset()` function in `datasets.py` as well, any dataset in the JSON file should now be able to be loaded this way. Made changes to the following:; - `DB` class now requires a `region` parameter.; - `Dataset.from_name_and_json()` has had a `custom_config` parameter added that indicates whether or not the user has supplied their own `config` or `url`. `Dataset.from_name_and_json()` now calls `DatasetVersion.get_region()` method to retrieve the dataset from the bucket in the selected region if `custom_config` is `False`. ; - The `DatasetVersion.get_region()` method takes the dataset `name`, a list of `DatasetVersion` objects, and a `region`, and returns a list of the versions that are available for that region. This method calls the instance method `in_region()` to check if the dataset is available in the requested region.; - If `in_region()` determines the desired region is not available for some dataset that otherwise is available in another region, it will raise a warning. If user still tries to call `db.annotate_rows_db()` using a dataset unavailable in their region, then it will get caught by the `_check_availability` instance method in the `DB` class and raise a `ValueError`.; - Started to add documentation to the classes and methods, still a work in progress. Changes to datasets and datasets API site:; - Added the `ldsc_baselineLD_annotations`, `ldsc_baselineLD_ldscores`, and `ldsc_baseline_ldscores` datasets to the `annotation_db.json` configuration file. Now accessible via `load_dataset()` and `db.annotate_rows_db()` (for the annotations at least).; - New `.rst` files in `hail/python/hail/docs/datase",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9496:1513,avail,available,1513,https://hail.is,https://github.com/hail-is/hail/pull/9496,1,['avail'],['available']
Availability,"rue, -1)); 98 try:; ---> 99 result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); 100 (result, timings) = (result_tuple._1(), result_tuple._2()); 101 value = ir.typ._from_encoding(result). /opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1321 answer = self.gateway_client.send_command(command); 1322 return_value = get_return_value(; -> 1323 answer, self.gateway_client, self.target_id, self.name); 1324 ; 1325 for temp_arg in temp_args:. /opt/conda/lib/python3.7/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 29 tpl = Env.jutils().handleForPython(e.java_exception); 30 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 31 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 32 except pyspark.sql.utils.CapturedException as e:; 33 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal. Exit status: 137. Diagnostics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287:5084,failure,failure,5084,https://hail.is,https://github.com/hail-is/hail/issues/13287,1,['failure'],['failure']
Availability,"rver will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, in init_server_extensions; Mar 01 19:59:04 dk-m python[5149]: func(self); Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/__init__.py"", line 30, in load_jupyter_server_extension; Mar 01 19:59:04 dk-m python[5149]: from .handlers import SparkHandler; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 8, in <module>; Mar 01 19:59:04 dk-m python[5149]: class SparkHandler(IPythonHandler):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 13, in SparkHandler; Mar 01 19:59:04 dk-m python[5149]: @tornado.web.asynchronous; Mar 01 19:59:04 dk-m python[5149]: AttributeError: module 'tornado.web' has no attribute 'asynchronous'; ```. It appears that Jupyter starts even though on",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5505:1782,Error,Error,1782,https://hail.is,https://github.com/hail-is/hail/issues/5505,1,['Error'],['Error']
Availability,"ry': '500M'}},\n 'security_context': None,\n 'stdin': None,\n 'stdin_once': None,\n 'termination_message_path': '/dev/termination-log',\n 'termination_message_policy': 'File',\n 'tty': None,\n 'volume_devices': None,\n 'volume_mounts': [{'mount_path': '/gsa-key',\n 'mount_propagation': None,\n 'name': 'gsa-key',\n 'read_only': None,\n 'sub_path': None},\n {'mount_path': '/var/run/secrets/kubernetes.io/serviceaccount',\n 'mount_propagation': None,\n 'name': 'default-token-brr2f',\n 'read_only': True,\n 'sub_path': None}],\n 'working_dir': None}],\n 'dns_config': None,\n 'dns_policy': 'ClusterFirst',\n 'enable_service_links': True,\n 'host_aliases': None,\n 'host_ipc': None,\n 'host_network': None,\n 'host_pid': None,\n 'hostname': None,\n 'image_pull_secrets': None,\n 'init_containers': None,\n 'node_name': 'gke-vdc-preemptible-pool-9c7148b2-8hq5',\n 'node_selector': None,\n 'priority': 500000,\n 'priority_class_name': 'user',\n 'readiness_gates': None,\n 'restart_policy': 'Never',\n 'runtime_class_name': None,\n 'scheduler_name': 'default-scheduler',\n 'security_context': {'fs_group': None,\n 'run_as_group': None,\n 'run_as_non_root': None,\n 'run_as_user': None,\n 'se_linux_options': None,\n 'supplemental_groups': None,\n 'sysctls': None},\n 'service_account': 'default',\n 'service_account_name': 'default',\n 'share_process_namespace': None,\n 'subdomain': None,\n 'termination_grace_period_seconds': 30,\n 'tolerations': [{'effect': None,\n 'key': 'preemptible',\n 'operator': None,\n 'toleration_seconds': None,\n 'value': 'true'},\n {'effect': 'NoExecute',\n 'key': 'node.kubernetes.io/not-ready',\n 'operator': 'Exists',\n 'toleration_seconds': 300,\n 'value': None},\n {'effect': 'NoExecute',\n 'key': 'node.kubernetes.io/unreachable',\n 'operator': 'Exists',\n 'toleration_seconds': 300,\n 'value': None}],\n 'volumes': [{'aws_elastic_block_store': None,\n 'azure_disk': None,\n 'azure_file': None,\n 'cephfs': None,\n 'cinder': None,\n 'config_map': None,\n 'downward_ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6616:4229,toler,tolerations,4229,https://hail.is,https://github.com/hail-is/hail/issues/6616,2,['toler'],['tolerations']
Availability,"ry>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pytest-dev/pytest-asyncio/releases"">pytest-asyncio's releases</a>.</em></p>; <blockquote>; <h2>pytest-asyncio 0.23.6</h2>; <h1>0.23.6 (2024-03-19)</h1>; <ul>; <li>Fix compatibility with pytest 8.2 <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/pull/800"">#800</a></li>; </ul>; <h2>Known issues</h2>; <p>As of v0.23, pytest-asyncio attaches an asyncio event loop to each item of the test suite (i.e. session, packages, modules, classes, functions) and allows tests to be run in those loops when marked accordingly. Pytest-asyncio currently assumes that async fixture scope is correlated with the new event loop scope. This prevents fixtures from being evaluated independently from the event loop scope and breaks some existing test suites (see <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/706"">#706</a>). For example, a test suite may require all fixtures and tests to run in the same event loop, but have async fixtures that are set up and torn down for each module. If you're affected by this issue, please continue using the v0.21 release, until it is resolved.</p>; <h2>pytest-asyncio 0.23.5.post1</h2>; <h1>0.23.5 (2024-02-09)</h1>; <ul>; <li>Declare compatibility with pytest 8 <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/737"">#737</a></li>; <li>Fix typing errors with recent versions of mypy <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/769"">#769</a></li>; <li>Prevent DeprecationWarning about internal use of <code>asyncio.get_event_loop()</code> from affecting test cases <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/757"">#757</a></li>; </ul>; <h2>Known issues</h2>; <p>As of v0.23, pytest-asyncio attaches an asyncio event loop to each item of the test suite (i.e. session, packages, modules, classes, functions) and allows tests to be run in those loops when marked accordingly. Pytest-asyncio curre",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14507:1171,down,down,1171,https://hail.is,https://github.com/hail-is/hail/pull/14507,1,['down'],['down']
Availability,"ryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a8b81aa0,0x00007fe4a8b82100] = 1632; stub code [0x00007fe4a8b82100,0x00007fe4a8b822b8] = 440; oops [0x00007fe4a8b822b8,0x00007fe4a8b822c0] = 8; metadata [0x00007fe4a8b822c0,0x00007fe4a8b82338] = 120; scopes data [0x00007fe4a8b82338,0x00007fe4a8b82f30] = 3064; scopes pcs [0x00007fe4a8b82f30,0x00007fe4a8b83340] = 1040; dependencies [0x00007fe4a8b83340,0x00007fe4a8b83348] = 8; nul chk table [0x00007fe4a8b83348,0x00007fe4a8b83430] = 232; #; FATAL: caught signal 6 SIGABRT; # If you would like to submit a bug report, please visit:; # http://bugreport.sun.com/bugreport/; #; /tmp/libhail8122447512081932366.so(+0x18f5f)[0x7fe3a7bf0f5f]; /lib/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4718:10380,error,error,10380,https://hail.is,https://github.com/hail-is/hail/issues/4718,1,['error'],['error']
Availability,"ryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Use After Free <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315324](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315324) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **584/1000** <br/> **Why?** Has a fix available, CVSS 7.4 | Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315328](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315328) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Timing Attack <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315331](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315331) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315452](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315452) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315972](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315972) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:3429,avail,available,3429,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"ryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Use After Free <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315324](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315324) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **584/1000** <br/> **Why?** Has a fix available, CVSS 7.4 | Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315328](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315328) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Timing Attack <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315331](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315331) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315452](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315452) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315972](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315972) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:3421,avail,available,3421,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,"s Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/subscriptions ws://localhost:8111/app/subscriptions. ProxyPass / http://localhost:8111/ connectiontimeout=240 timeout=1200; ProxyPassReverse / http://localhost:8111/; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1873,avail,available,1873,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,1,['avail'],['available']
Availability,"s build directory)</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Use pooling connection manager of Apache HttpClient instead of basic one. The basic one is not meant to be used by multiple threads. This fixes an issue that could cause an <code>IllegalStateException</code> with the message <code>Connection is still allocated</code>. Thanks to <a href=""https://github.com/dmarks2""><code>@​dmarks2</code></a> for spotting this.</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>eachFile</code> method that adds an action to be applied to each source URL before it is downloaded. The action can be used to modify the filename of the target file.</li>; <li>Add <code>runAsync</code> method to download extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:1907,down,download,1907,https://hail.is,https://github.com/hail-is/hail/pull/12345,2,['down'],"['download', 'downloaded']"
Availability,"s changelog</a>.</em></p>; <blockquote>; <h2>2.27.1 (2022-01-05)</h2>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Fixed parsing issue that resulted in the <code>auth</code> component being; dropped from proxy URLs. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/6028"">#6028</a>)</li>; </ul>; <h2>2.27.0 (2022-01-03)</h2>; <p><strong>Improvements</strong></p>; <ul>; <li>; <p>Officially added support for Python 3.10. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5928"">#5928</a>)</p>; </li>; <li>; <p>Added a <code>requests.exceptions.JSONDecodeError</code> to unify JSON exceptions between; Python 2 and 3. This gets raised in the <code>response.json()</code> method, and is; backwards compatible as it inherits from previously thrown exceptions.; Can be caught from <code>requests.exceptions.RequestException</code> as well. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5856"">#5856</a>)</p>; </li>; <li>; <p>Improved error text for misnamed <code>InvalidSchema</code> and <code>MissingSchema</code>; exceptions. This is a temporary fix until exceptions can be renamed; (Schema-&gt;Scheme). (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/6017"">#6017</a>)</p>; </li>; <li>; <p>Improved proxy parsing for proxy URLs missing a scheme. This will address; recent changes to <code>urlparse</code> in Python 3.9+. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5917"">#5917</a>)</p>; </li>; </ul>; <p><strong>Bugfixes</strong></p>; <ul>; <li>; <p>Fixed defect in <code>extract_zipped_paths</code> which could result in an infinite loop; for some paths. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5851"">#5851</a>)</p>; </li>; <li>; <p>Fixed handling for <code>AttributeError</code> when calculating length of files obtained; by <code>Tarfile.extractfile()</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5239"">#5239</a>)</p>; </li>; <li>; <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11528:4533,error,error,4533,https://hail.is,https://github.com/hail-is/hail/pull/11528,2,['error'],['error']
Availability,"s of v0.23, pytest-asyncio attaches an asyncio event loop to each item of the test suite (i.e. session, packages, modules, classes, functions) and allows tests to be run in those loops when marked accordingly. Pytest-asyncio currently assumes that async fixture scope is correlated with the new event loop scope. This prevents fixtures from being evaluated independently from the event loop scope and breaks some existing test suites (see <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/706"">#706</a>). For example, a test suite may require all fixtures and tests to run in the same event loop, but have async fixtures that are set up and torn down for each module. If you're affected by this issue, please continue using the v0.21 release, until it is resolved.</p>; <h2>pytest-asyncio 0.23.5a0</h2>; <h1>0.23.5 (UNRELEASED)</h1>; <ul>; <li>Declare compatibility with pytest 8 <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/737"">#737</a></li>; <li>Fix typing errors with recent versions of mypy <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/769"">#769</a></li>; </ul>; <h2>Known issues</h2>; <p>As of v0.23, pytest-asyncio attaches an asyncio event loop to each item of the test suite (i.e. session, packages, modules, classes, functions) and allows tests to be run in those loops when marked accordingly. Pytest-asyncio currently assumes that async fixture scope is correlated with the new event loop scope. This prevents fixtures from being evaluated independently from the event loop scope and breaks some existing test suites (see <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/706"">#706</a>). For example, a test suite may require all fixtures and tests to run in the same event loop, but have async fixtures that are set up and torn down for each module. If you're affected by this issue, please continue using the v0.21 release, until it is resolved.</p>; <h2>pytest-asyncio 0.23.4</h2>; <h1>0.23.4 (2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14507:4256,error,errors,4256,https://hail.is,https://github.com/hail-is/hail/pull/14507,1,['error'],['errors']
Availability,"s with pyspark:. Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1.tar.gz (215.7 MB); ERROR: Command errored out with exit status 1:; command: /Users/spascal/.pyenv/versions/3.7.9/bin/python3.7 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/setup.py'""'""'; __file__='""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-pip-egg-info-vlaj8k6d; cwd: /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/; Complete output (47 lines):; Could not import pypandoc - required to package PySpark; WARNING: The wheel package is not available.; ERROR: Command errored out with exit status 1:; command: /Users/spascal/.pyenv/versions/3.7.9/bin/python3.7 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""'; __file__='""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-ggmq8ipk; cwd: /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/; Complete output (8 lines):; no pandoc found, building platform unspecific wheel...; use 'python setup.py download_pandoc' to download pandoc.; usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]; or: setup.py --help [cmd1 cmd2 ...]; or: setup.py --help-commands; or: setup.py cmd --help; ; er",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9742:1195,ERROR,ERROR,1195,https://hail.is,https://github.com/hail-is/hail/issues/9742,2,"['ERROR', 'error']","['ERROR', 'errored']"
Availability,"s']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37; ```. ### Traces No.2:; ```java; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 220, ip-172-31-2-255.ec2.internal, executor 2): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:879); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:877); 	at scala.Option.map(Option.scala:146); 	at is.hail.HailContext$$anon$3.compute(HailContext.scala:877); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:30352,error,error,30352,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['error'],['error']
Availability,"s) = (result_tuple._1(), result_tuple._2()); 100 value = ir.typ._from_encoding(result); 102 return (value, timings) if timed else value. File ~/mambaforge/lib/python3.9/site-packages/py4j/java_gateway.py:1304, in JavaMember.__call__(self, *args); 1298 command = proto.CALL_COMMAND_NAME +\; 1299 self.command_header +\; 1300 args_command +\; 1301 proto.END_COMMAND_PART; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1307 for temp_arg in temp_args:; 1308 temp_arg._detach(). File ~/mambaforge/lib/python3.9/site-packages/hail/backend/py4j_backend.py:21, in handle_java_exception.<locals>.deco(*args, **kwargs); 19 import pyspark; 20 try:; ---> 21 return f(*args, **kwargs); 22 except py4j.protocol.Py4JJavaError as e:; 23 s = e.java_exception.toString(). File ~/mambaforge/lib/python3.9/site-packages/py4j/protocol.py:330, in get_return_value(answer, gateway_client, target_id, name); 326 raise Py4JJavaError(; 327 ""An error occurred while calling {0}{1}{2}.\n"".; 328 format(target_id, ""."", name), value); 329 else:; --> 330 raise Py4JError(; 331 ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; 332 format(target_id, ""."", name, value)); 333 else:; 334 raise Py4JError(; 335 ""An error occurred while calling {0}{1}{2}"".; 336 format(target_id, ""."", name)). Py4JError: An error occurred while calling o83._1. Trace:; java.lang.NegativeArraySizeException: -1966455376; 	at py4j.Base64.encodeToChar(Base64.java:681); 	at py4j.Base64.encodeToString(Base64.java:734); 	at py4j.Protocol.encodeBytes(Protocol.java:154); 	at py4j.ReturnObject.getPrimitiveReturnObject(ReturnObject.java:150); 	at py4j.Gateway.getReturnObject(Gateway.java:188); 	at py4j.Gateway.invoke(Gateway.java:283); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12035#issuecomment-1186014691:3059,error,error,3059,https://hail.is,https://github.com/hail-is/hail/issues/12035#issuecomment-1186014691,1,['error'],['error']
Availability,"s). [//]: # (snyk:metadata:{""prId"":""4c874ad7-563f-49cd-9773-8b9f1095e36c"",""prPublicId"":""4c874ad7-563f-49cd-9773-8b9f1095e36c"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""41.0.6""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""SNYK-PYTHON-CRYPTOGRAPHY-5663682"",""SNYK-PYTHON-CRYPTOGRAPHY-5777683"",""SNYK-PYTHON-CRYPTOGRAPHY-5813745"",""SNYK-PYTHON-CRYPTOGRAPHY-5813746"",""SNYK-PYTHON-CRYPTOGRAPHY-5813750"",""SNYK-PYTHON-CRYPTOGRAPHY-5914629"",""SNYK-PYTHON-CRYPTOGRAPHY-6036192"",""SNYK-PYTHON-CRYPTOGRAPHY-6092044"",""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[554,704,509,454,616,584,479,509,509,509,509,589,509,691,399,479,399,539,479,616,519],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Use After Free](https://learn.snyk.io/lesson/use-after-free/?loc&#x3D;fix-pr); 🦉 [Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;)](https://learn.snyk.io/lesson/type-confusion/?loc&#x3D;fix-pr); 🦉 [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:12198,avail,available,12198,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"s. The Python versions supported for this release; are 3.8-3.11.</p>; <p>Note that the mypy version needs to be 0.981+ if you test using Python; 3.10.7, otherwise the typing tests will fail.</p>; <h2>Contributors</h2>; <p>A total of 8 people contributed to this release. People with a &quot;+&quot; by; their names contributed a patch for the first time.</p>; <ul>; <li>Bas van Beek</li>; <li>Charles Harris</li>; <li>Matthew Barber</li>; <li>Matti Picus</li>; <li>Ralf Gommers</li>; <li>Ross Barnowski</li>; <li>Sebastian Berg</li>; <li>Sicheng Zeng +</li>; </ul>; <h2>Pull requests merged</h2>; <p>A total of 13 pull requests were merged for this release.</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22368"">#22368</a>: BUG: Add <code>__array_api_version__</code> to <code>numpy.array_api</code> namespace</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22370"">#22370</a>: MAINT: update sde toolkit to 9.0, fix download link</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22382"">#22382</a>: BLD: use macos-11 image on azure, macos-1015 is deprecated</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22383"">#22383</a>: MAINT: random: remove <code>get_info</code> from &quot;extending with Cython&quot;...</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22384"">#22384</a>: BUG: Fix complex vector dot with more than NPY_CBLAS_CHUNK elements</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22387"">#22387</a>: REV: Loosen <code>lookfor</code>'s import try/except again</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22388"">#22388</a>: TYP,ENH: Mark <code>numpy.typing</code> protocols as runtime checkable</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22389"">#22389</a>: TYP,MAINT: Change more overloads to play nice with pyright</li>; <li><a href=""https://github-redirect.de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12441:1581,down,download,1581,https://hail.is,https://github.com/hail-is/hail/pull/12441,1,['down'],['download']
Availability,s.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(LocalBackend.scala:309); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1$adapted(LocalBackend.scala:308); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65); 	at is.hail.backend.local.LocalBackend.$anonfun$withExecuteContext$2(LocalBackend.scala:144); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); 	at is.hail.backend.local.LocalBackend.withExecuteContext(LocalBackend.scala:130); 	at is.hail.backend.local.LocalBackend.execute(LocalBackend.scala:308); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:692); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:664); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:159); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:442); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:408); 	at java.base/java.lang.Thread.run(Thread.java:834). Hail version: 0.2.128-ce3ca9c77507; Error summary: SocketTimeoutException: connect timed out; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:22240,Error,Error,22240,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,1,['Error'],['Error']
Availability,"s.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:77); 	at is.hail.backend.Backend.executeJSON(Backend.scala:96); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). org.apache.spark.SparkException: Job aborted due to stage failure: Task 40 in stage 7.0 failed 20 times, most recent failure: Lost task 40.19 in stage 7.0 (TID 3171, seqr-loading-cluster-sw-z91p.c.seqr-project.internal, executor 14): is.hail.utils.HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:210); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:974); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:967); 	at is.hail.utils.FlipbookIterator$$anon$5.<init>(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:174); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:145); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:967); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:963); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:147); 	at i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:51172,failure,failure,51172,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['failure'],['failure']
Availability,s.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-c37301a; Error summary: IllegalArgumentException: requirement failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465:18510,Error,Error,18510,https://hail.is,https://github.com/hail-is/hail/issues/3465,1,['Error'],['Error']
Availability,"s.hail.variant.MatrixTable.write(MatrixTable.scala:2428); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz: caught java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:9404,Error,ErrorHandling,9404,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Error'],['ErrorHandling']
Availability,"s.kinesis.client.version=1.12.0 -Daws.kinesis.producer.version=0.12.9 -Dscala.version=2.12.15 -DrecompileMode=all -Dmaven.deploy.plugin.version=2.8.2 -Dmaven.scaladoc.skip -Pyarn -Phadoop-3.2 -Phive -Phive-thriftserver -Psparkr -Pspark-ganglia-lgpl -Pnetlib-lgpl -Pscala-2.12 -Pkubernetes -Pvolcano -Pkinesis-asl -DskipTests; ```; I still did not found why scala is downgraded to 2.12.13. <details><summary>Hail logs</summary>; <p>; # Build Hail #; WARNING: Package(s) not found: hail; REVISION is set to ""13536b531342a263b24a7165bfeec7bd02723e4b"" which is different from old value """"; printf ""13536b531342a263b24a7165bfeec7bd02723e4b"" > env/REVISION; echo 13536b531342a263b24a7165bfeec7bd02723e4b > python/hail/hail_revision; SHORT_REVISION is set to ""13536b531342"" which is different from old value """"; printf ""13536b531342"" > env/SHORT_REVISION; HAIL_PIP_VERSION is set to ""0.2.124"" which is different from old value """"; printf ""0.2.124"" > env/HAIL_PIP_VERSION; echo 0.2.124-13536b531342 > python/hail/hail_version; echo 0.2.124 > python/hail/hail_pip_version; cp -f python/hail/hail_version python/hailtop/hail_version; printf 'hail_version=""0.2.124-13536b531342"";' > python/hail/docs/_static/hail_version.js; printf 'hail_pip_version=""0.2.124""' >> python/hail/docs/_static/hail_version.js; cloud_base is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:2130,echo,echo,2130,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,s.package$.using(package.scala:664); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); E 	at is.hail.utils.package$.using(package.scala:664); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65); E 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407); E 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); E 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); E 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393); E 	at is.hail.backend.Backend.$anonfun$matrixTableType$1(Backend.scala:185); E 	at is.hail.backend.Backend.jsonToBytes(Backend.scala:175); E 	at is.hail.backend.Backend.matrixTableType(Backend.scala:185); E 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:104); E 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); E 	at jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82); E 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:848); E 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:817); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); E 	at java.base/java.lang.Thread.run(Thread.java:829); E; E; E; E Hail version: 0.2.127-e81ad92151ab; E Error summary: ChecksumException: Checksum error: file:/Users/willtyler/Desktop/hail/hail/python/hail/docs/data/example.8bits.bgen.idx2/metadata.json.gz at 0 exp: 982431825 got: -2031629660; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001:5953,Error,Error,5953,https://hail.is,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001,2,"['Error', 'error']","['Error', 'error']"
Availability,"s/1330"">#1330</a></p>; </li>; <li>; <p>Use the <code>end_lineno</code> attribute for the <code>NodeNG.tolineno</code> property; when it is available.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1350"">#1350</a></p>; </li>; <li>; <p>Add <code>is_dataclass</code> attribute to <code>ClassDef</code> nodes.</p>; </li>; <li>; <p>Use <code>sysconfig</code> instead of <code>distutils</code> to determine the location of; python stdlib files and packages.</p>; <p>Related pull requests: <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1322"">#1322</a>, <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1323"">#1323</a>, <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1324"">#1324</a>; Closes <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1282"">#1282</a>; Ref <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1103"">#1103</a></p>; </li>; <li>; <p>Fixed crash with recursion error for inference of class attributes that referenced; the class itself.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5408"">PyCQA/pylint#5408</a></p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/PyCQA/astroid/commit/07c0f60ffc1017d0a9a2bb605a5c645781a8c088""><code>07c0f60</code></a> Bump astroid to 2.10.0, update changelog</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/e6dc5ef0f8c2d28bc9d2ffa226fbb5e4e58d88f3""><code>e6dc5ef</code></a> Fix some typoes in the Changelog</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/b6d17107f2e02df4ce5080536bb783a25273b33f""><code>b6d1710</code></a> Changed NodeNG.tolineno to use end_lineno when it is available (<a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1351"">#1351</a>)</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/0acb961d73",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11463:2897,error,error,2897,https://hail.is,https://github.com/hail-is/hail/pull/11463,2,['error'],['error']
Availability,"s: array<str>,; impact: str,; minimised: int32,; regulatory_feature_id: str,; variant_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; appris: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; tsl: int32,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'xpos': int64; 'xstart': int64; 'xstop': int64; ----------------------------------------; Entry fields:; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'GT': call; 'PL': array<int32>; 'BX': array<str>; 'PS': int32; 'PQ': int32; 'JQ': int32; 'MIN_DP': int32; 'PGT': call; 'PID': str; 'RGQ': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 4:===================================================> (480 + 20) / 500]2020-04-05 14:09:48 Hail: INFO: Coerced almost-sorted dataset; [Stage 5:======================================================>(498 + 2) / 500]2020-04-05 14:09:50 Hail: INFO: Coerced almost-sorted dataset; [Stage 7:> (0 + 108) / 500]ERROR: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) failed SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:46860,ERROR,ERROR,46860,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['ERROR'],['ERROR']
Availability,"s://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""12691d21-0395-41b3-80d2-f212830f66ea"",""prPublicId"":""12691d21-0395-41b3-80d2-f212830f66ea"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""},{""name"":""urllib3"",""from"":""1.26.17"",""to"":""1.26.18""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-URLLIB3-6002459""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494,496],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13873:6336,avail,available,6336,https://hail.is,https://github.com/hail-is/hail/pull/13873,1,['avail'],['available']
Availability,"s://ci.azure.hail.is/batches/3778899/jobs/47; ```; E hail.utils.java.FatalError: NativeIoException: readAddress(..) failed: Connection reset by peer; E ; E Java stack trace:; E io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer; E 	at ; E ; E ; E ; E Hail version: 0.2.115-330031a5d973; E Error summary: NativeIoException: readAddress(..) failed: Connection reset by peer; ```. I'm not sure why we lost the stack trace. ### Version. 330031a5d9734fd33a50e5651e7a2505f352b239. ### Relevant log output. ```shell; ________________________ test_pc_relate_against_R_truth ________________________; [gw2] linux -- Python 3.8.10 /usr/bin/python3. def test_pc_relate_against_R_truth():; mt = hl.import_vcf(resource('pc_relate_bn_input.vcf.bgz')); > hail_kin = hl.pc_relate(mt.GT, 0.00, k=2).checkpoint(utils.new_temp_file(extension='ht')). test/hail/methods/relatedness/test_pc_relate.py:9: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; <decorator-gen-1104>:2: in checkpoint; ???; /usr/local/lib/python3.8/dist-packages/hail/typecheck/check.py:584: in wrapper; return __original_func(*args_, **kwargs_); /usr/local/lib/python3.8/dist-packages/hail/table.py:1347: in checkpoint; self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); <decorator-gen-1106>:2: in write; ???; /usr/local/lib/python3.8/dist-packages/hail/typecheck/check.py:584: in wrapper; return __original_func(*args_, **kwargs_); /usr/local/lib/python3.8/dist-packages/hail/table.py:1393: in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:490: in execute; return self._cancel_on_ctrl_c(self._async_execute(ir, timed=timed)); /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:481: in _cancel_on_ctrl_c; return async_to_blocking(coro); /usr/local",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12980:1063,checkpoint,checkpoint,1063,https://hail.is,https://github.com/hail-is/hail/issues/12980,1,['checkpoint'],['checkpoint']
Availability,"s://github.com/michel-kraemer/gradle-download-task/commit/9703f764df56c52626f7d6f44bca8b1d51312389""><code>9703f76</code></a> Use pooling connection manager instead of basic one</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/306172e4c6532e185c8a6a9998bca7d22d2d0c63""><code>306172e</code></a> Bump up version number to 5.2.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b9df0c0daa080450772c365f16a9406fe0ca607a""><code>b9df0c0</code></a> Document eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/05a4433770f7020ff845add9348bdc12c82793dd""><code>05a4433</code></a> Add eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/09d1eca91afbf21ace3672be24c68d9028ee1e33""><code>09d1eca</code></a> Document runAsync method</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/800e3df1647c5ce65bffdd25c3240dfa5244e6c5""><code>800e3df</code></a> Add runAsync method to download extension</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/80f04c6a46fe7df053ac55bcfc6f90ff74c4b873""><code>80f04c6</code></a> Bump up version number to 5.1.3</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/3.2.0...5.2.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=3.2.0&new-version=5.2.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:4450,down,download-task,4450,https://hail.is,https://github.com/hail-is/hail/pull/12332,2,['down'],"['download', 'download-task']"
Availability,"s://redirect.github.com/Textualize/rich/issues/2875"">Textualize/rich#2875</a></li>; <li>Fix rich.pretty.install breakage in iPython <a href=""https://redirect.github.com/Textualize/rich/issues/3013"">Textualize/rich#3013</a></li>; </ul>; <h3>Added</h3>; <ul>; <li>Added Text.extend_style method.</li>; <li>Added Span.extend method.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Text.tab_size now defaults to <code>None</code> to indicate that Console.tab_size should be used.</li>; </ul>; <h2>v13.4.2</h2>; <h2>[13.4.2] - 2023-06-12</h2>; <h3>Changed</h3>; <ul>; <li>Relaxed markdown-it-py dependency</li>; </ul>; <h2>Hot fix for typing extension issue</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/Textualize/rich/blob/master/CHANGELOG.md"">rich's changelog</a>.</em></p>; <blockquote>; <h2>[13.5.2] - 2023-08-01</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tab assertion error</li>; </ul>; <h2>[13.5.1] - 2023-07-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fix tilde character (<code>~</code>) not included in link regex when printing to console <a href=""https://redirect.github.com/Textualize/rich/issues/3057"">Textualize/rich#3057</a></li>; </ul>; <h2>[13.5.0] - 2023-07-29</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs not expanding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13380:2685,error,error,2685,https://hail.is,https://github.com/hail-is/hail/pull/13380,2,['error'],['error']
Availability,"s://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **624/1000** <br/> **Why?** Has a fix available, CVSS 8.2 | Arbitrary Code Execution <br/>[SNYK-PYTHON-IPYTHON-2348630](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-2348630) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **556/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.4 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-JINJA2-6150717](https://snyk.io/vuln/SNYK-PYTHON-JINJA2-6150717) | `jinja2:` <br> `2.11.3 -> 3.1.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **604/1000** <br/> **Why?** Has a fix available, CVSS 7.8 | Improper Privilege Management <br/>[SNYK-PYTHON-JUPYTERCORE-3063766](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERCORE-3063766) | `jupyter-core:` <br> `4.6.3 -> 4.11.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-MISTUNE-2940625](https://snyk.io/vuln/SNYK-PYTHON-MISTUNE-2940625) | `mistune:` <br> `0.8.4 -> 2.0.3` <br> | No | No Known Exploit ; ![high severity](https://res.cl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:2897,avail,available,2897,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['avail'],['available']
Availability,"s://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3316038) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3316211](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3316211) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5663682](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5663682) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **691/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.4 | Improper Certificate Validation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5777683](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5777683) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:6020,avail,available,6020,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"s://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3316038) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3316211](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3316211) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5663682](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5663682) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **691/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.4 | Improper Certificate Validation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5777683](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5777683) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:6012,avail,available,6012,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,"s>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/1b1b22e999541e8ce7ac3147eb468e0cde6c157a""><code>1b1b22e</code></a> Patch Release 11/15/2022 (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32171"">#32171</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/68d7b8992d77ad00cdd985bfd764b81f42085fe3""><code>68d7b89</code></a> Eagerly Convert Headers Always in Download (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32173"">#32173</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/c10e612d913b03f044ddd58aa591850615b61ecd""><code>c10e612</code></a> Sync eng/common directory with azure-sdk-tools for PR 4701 (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32168"">#32168</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/44682b71c0216aae1530af287e745803feeec2fc""><code>44682b7</code></a> Regenerate Storage Blobs with Fix for Download to File (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32163"">#32163</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/11f065d4d592d14977d178ddd58f6a6ec6b16276""><code>11f065d</code></a> Increment package versions for keyvault releases (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32151"">#32151</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/44d679faee209611bee14fcea08207f9753bb466""><code>44d679f</code></a> Increment versions for appcomplianceautomation releases (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32155"">#32155</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/020145e20dff874555f4e610bc9b5b39213b1740""><code>020145e</code></a> move processor-lifecycle-manager from messaging to service module (<a href=""https://github-redirect.dependabot.com/Azure/a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12477:3739,Down,Download,3739,https://hail.is,https://github.com/hail-is/hail/pull/12477,1,['Down'],['Download']
Availability,"s_method=is_method); --> 585 return __original_func(*args_, **kwargs_). File ~/hail/hail/python/hail/methods/statgen.py:717, in _linear_regression_rows_nd(y, x, covariates, block_size, weights, pass_through); 714 res = res.select_globals(); 716 temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); --> 717 res = res.checkpoint(temp_file_name); 719 return res. File <decorator-gen-1234>:2, in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals). File ~/hail/hail/python/hail/typecheck/check.py:585, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 582 @decorator; 583 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_). File ~/hail/hail/python/hail/table.py:1963, in Table.checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals); 1960 hl.current_backend().validate_file(output); 1962 if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; -> 1963 self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); 1964 _assert_type = self._type; 1965 _load_refs = False. File <decorator-gen-1236>:2, in write(self, output, overwrite, stage_locally, _codec_spec). File ~/hail/hail/python/hail/typecheck/check.py:585, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 582 @decorator; 583 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_). File ~/hail/hail/python/hail/table.py:2005, in Table.write(self, output, overwrite, stage_locally, _codec_spec); 1979 """"""Write to disk.; 1980; 1981 Examples; (...); 2000 If ``True``, overwrite an existing file ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14594:2146,checkpoint,checkpoint,2146,https://hail.is,https://github.com/hail-is/hail/issues/14594,1,['checkpoint'],['checkpoint']
Availability,"s_model(1, 10, 10). # Aggregate concatenated alleles (works fine); > mt.aggregate_rows(hl.agg.counter(hl.delimit(mt.alleles, '|'))); {'A|C': 10}. # Group by the array directly (gives an expected error); > mt.aggregate_rows(hl.agg.counter(mt.alleles)); TypeError: unhashable type: 'list'. # Aggregate sorted arrays (works but gives wrong result); > mt.aggregate_rows(hl.agg.counter(hl.delimit(hl.sorted(mt.alleles), '|'))); {'A|A|A|C|\x0b\x00\x00': 2, 'A|A|A|C|C|C': 8}. # Aggregate the sorted arrays directly (segfault); # *This should probably throw ""unhashable type list"" like it does without the sort*; mt.aggregate_rows(hl.agg.counter(hl.sorted(mt.alleles))); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/opt/conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty; ...; Py4JError: An error occurred while calling o59.executeJSON; ```. Here is the full [stack trace](https://github.com/hail-is/hail/files/4187400/stacktrace.txt) and [core dump](https://github.com/hail-is/hail/files/4187399/coredump.txt). I think some related questions that arise from this are:. 1. What's the best way to group by an array to avoid the conversion to a delimited string? In this case I could do something like ```mt.aggregate_rows(hl.agg.counter(hl.tuple([mt.alleles[0], mt.alleles[1]])))``` but I can't find a solution for getting a tuple from an array without knowing the length of it beforehand for every row. Is there a more fundamental reason why the API doesn't allow aggregation by arrays even if Spark does?; 2. When the Py4J server crashes, it's no longer reachable from the python clients so I have to restart my process and re-initialize Hail. Is there already functionality implemented for bringing that server up if it's down? I'd imagine segfaults aren't the only reason it could down, so it woul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8076:1209,error,error,1209,https://hail.is,https://github.com/hail-is/hail/issues/8076,1,['error'],['error']
Availability,"s_v2_after_update AFTER UPDATE ON aggregated_job_resources_v2; FOR EACH ROW; BEGIN; DECLARE new_deduped_resource_id INT;. IF OLD.migrated = 0 AND NEW.migrated = 1 THEN; SELECT deduped_resource_id INTO new_deduped_resource_id FROM resources WHERE resource_id = OLD.resource_id;. INSERT INTO aggregated_job_resources_v3 (batch_id, job_id, resource_id, `usage`); VALUES (NEW.batch_id, NEW.job_id, new_deduped_resource_id, NEW.usage); ON DUPLICATE KEY UPDATE; `usage` = `usage` + NEW.usage;; END IF;; END $$; ```. What this PR does is find the keys of all rows in the `aggregated_jobs_resources_v2` table in intervals of 100 rows. This is a ""chunk"". The reason is because we want to keep the transactions small and fast. I optimized this and found 100 rows worked best for performance. We then want to set `migrated=1` for all rows in the given chunk which activates the trigger and also maintains idempotency so we only run the update for each chunk once. . Most of the code in this PR is identifying the bounds of each chunk and then doing the update. We have a burn-in period at the beginning where we migrate chunks serially. Then we migrate the chunks in 10-way parallel. This is to get rid of deadlock errors due to row locks with the ""birthday problem"". Lastly, once all of the updates are complete, we run an audit that makes sure the ""v2"" and ""v3"" tables are equivalent and have the same total aggregate resource usage. I believe I also run this audit in chunks here as these tables are massive and a single audit query would take hours. The bounds of the audit for these chunks are on the order of `(batch_id, job_id)` rather than `(batch_id, job_id, resource_id)` which was used for the actual updates. This is because the resource_ids can differ between ""v2"" and ""v3"", so we just check the overall job adds up to the same usage after deduplicating the resource IDs on both tables. I recommend looking at the main function towards the bottom of the script and then working your way through it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782:2979,error,errors,2979,https://hail.is,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782,1,['error'],['errors']
Availability,sample_qc errors on haploid genotypes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3900:10,error,errors,10,https://hail.is,https://github.com/hail-is/hail/issues/3900,1,['error'],['errors']
Availability,"sample_qc on mt table imported from Delly vcf. ; hl.sample_qc(ds). ### What went wrong (all error messages here, including the full java stack trace):. The sample_qc had a problem when alt ref is <DEL>. ```; [Stage 3:> (0 + 140) / 415]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail.qc/delly-qc.py"", line 35, in <module>; ds = hl.sample_qc(ds); File ""<decorator-gen-902>"", line 2, in sample_qc; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 490, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/methods/qc.py"", line 91, in sample_qc; return MatrixTable(Env.hail().methods.SampleQC.apply(require_biallelic(dataset, 'sample_qc')._jvds, name)); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: invalid allele ""<DEL>"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 160, scc-q01.scc.bu.edu, executor 4): is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCombiner$.alleleIndices(SampleQC.scala:44); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.col",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413:1292,Error,Error,1292,https://hail.is,https://github.com/hail-is/hail/issues/3413,1,['Error'],['Error']
Availability,scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5527,Error,ErrorHandling,5527,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Error'],['ErrorHandling']
Availability,"scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:11513,Error,Error,11513,https://hail.is,https://github.com/hail-is/hail/issues/3507,2,"['Error', 'error']","['Error', 'error']"
Availability,scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:202830,ERROR,ERROR,202830,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_value_ir$1(Parser.scala:2157); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:2153); 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:2157); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_value_ir$2(SparkBackend.scala:691); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$1(SparkBackend.scala:345); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_value_ir$1(SparkBackend.scala:690); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); 	at is.hail.backend.spark.SparkBackend.parse_value_ir(SparkBackend.scala:689); 	at sun.reflect.GeneratedMethodAccessor193.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182); 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106); 	at java.lang.Thread.run(Thread.java:750). java.lang.ClassCastException: null; 	at . Hail version: 0.2.123-a21b8ad2f534; Error summary: ClassCastException: null; ```. ### Version. 0.2.123. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13693:2614,Error,Error,2614,https://hail.is,https://github.com/hail-is/hail/issues/13693,1,['Error'],['Error']
Availability,scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:77); 	at is.hail.backend.Backend.executeJSON(Backend.scala:96); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). is.hail.utils.HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:210); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:974); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:967); 	at is.hail.utils.FlipbookIterator$$anon$5.<init>(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:174); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:145); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:967); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:963); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:147); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:146); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$appl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:87675,Error,ErrorHandling,87675,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['Error'],['ErrorHandling']
Availability,scorecard was down briefly during an upgrade,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4464:14,down,down,14,https://hail.is,https://github.com/hail-is/hail/issues/4464,1,['down'],['down']
Availability,"script I am running:. ```python; rf_kt =(; hc.read(rf_path,sites_only=True); .variants_keytable(); .flatten(); .select(['va.info.MQRankSum']); ); print(rf_kt.schema()). rf_df = rf_kt.to_dataframe(); rf_df.printSchema(); rf_df.show(); ```. And here is the output and stacktrace as it crashes when running `show()`:. ```; Struct {; `va.info.MQRankSum`: Double; }; root; |-- va.info.MQRankSum: double (nullable = true); Traceback (most recent call last):; File ""/tmp/db4d8e04-85c9-4eba-b0b3-4ade69200fd3/test.py"", line 60, in <module>; rf_df.show(); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o73.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 2520, gnomad-prod-sw-s89f.c.broad-mpg-gnomad.internal): java.lang.IndexOutOfBoundsException: 21; at scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43); at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:48); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1275:1005,error,error,1005,https://hail.is,https://github.com/hail-is/hail/issues/1275,1,['error'],['error']
Availability,"se assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:32,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/float64x2.h:32:7: note: ‘class simdpp::arch_avx2::float64<2>’ declared here; class float64<2, void> : public any_float64<2, float64<2,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:255:45: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::float32<4>’ with ‘private’ member ‘simdpp::arch_avx2::float32<4>::d_’ from an array of ‘const class simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:29,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:3290,Mask,MaskCastOverride,3290,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,se$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217459,recover,recover,217459,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['recover'],['recover']
Availability,see build failures at: https://github.com/hail-is/hail/pull/4520 and https://github.com/hail-is/hail/pull/4529.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4530:10,failure,failures,10,https://hail.is,https://github.com/hail-is/hail/issues/4530,1,['failure'],['failures']
Availability,see gitter. Possible different Python semantics on windows throw a name error at `__all__.extend(genetics.__all__)`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3286#issuecomment-378015179:72,error,error,72,https://hail.is,https://github.com/hail-is/hail/pull/3286#issuecomment-378015179,1,['error'],['error']
Availability,"seems like this is due to Hail's dependency of bokeh using the latest version of jinja2. Downgrading jinja2 to 3.0.0 solves the problem, and it seems like other people have seen this too with the latest release of jinja2:. https://github.com/holoviz/panel/issues/3260. This may be transient and may be solved by bokeh / jinja2 folks but thought I'd let you know in case you hit this issue. ```; ../conda/envs/glow/lib/python3.7/site-packages/bokeh/core/templates.py:43: in <module>; from jinja2 import Environment, Markup, FileSystemLoader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); [error] 	at java.util.concurre",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:1130,error,error,1130,https://hail.is,https://github.com/hail-is/hail/issues/11705,1,['error'],['error']
Availability,"self, exprs, broadcast_f); 1462 ; 1463 def cache(self):. /home/hail/hail.zip/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 354 for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; 355 if j not in used_joins:; --> 356 left = j.join_func(left); 357 all_uids.extend(j.temp_vars); 358 used_joins.add(j). /home/hail/hail.zip/hail/table.py in joiner(obj); 1448 else:; 1449 assert isinstance(obj, Table); -> 1450 return Table(Env.jutils().joinGlobals(obj._jt, self._jt, uid)); 1451 ; 1452 ast = Join(Select(TopLevelReference('global', Indices()), uid),. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.TableMapGlobals.execute(Relational.scala:2158); 	at is.hail.table.Table.value$lzycompute(Table.scala:243); 	at is.hail.table.Table.value(Table.scala:238); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:246); 	at is.hail.table.Table.x$5(Table.scala:246); 	at is.hail.table.Table.globals$lzycompute(Table.scala:246); 	at is.hail.table.Table.globals(Table.scala:246); 	at is.hail.utils.Py4jUtils$class.joinGlobals(Py4jUtils.scala:137); 	at is.hail.utils.package$.joinGlobals(package.scala:26); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3728:4549,Error,Error,4549,https://hail.is,https://github.com/hail-is/hail/issues/3728,1,['Error'],['Error']
Availability,"self.assertEqual is unittest style. We use pytest now, which rewrites Python assert statements to provide good failure messages. New code should use `assert`, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7611#issuecomment-559186521:111,failure,failure,111,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-559186521,1,['failure'],['failure']
Availability,"send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 168, in _new_conn\n self, \""Failed to establish a new connection: %s\"" % e)\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 449, in send\n timeout=timeout\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 638, in urlopen\n _stacktrace=sys.exc_info()[2])\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\"", line 399, in increment\n raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1296, in refresh_k8s_pods\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1229, in update_job_with_pod\n await job.mark_complete(success=True, pod=pod)\n File \""/usr/local/lib/python",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6754:2800,error,error,2800,https://hail.is,https://github.com/hail-is/hail/issues/6754,1,['error'],['error']
Availability,"seqr hail search code is here: https://github.com/broadinstitute/seqr/tree/master/hail_search. Running the aiohttp service is just `python -m hail_search`. Runs on hail 0.2.126. The data you need is `gs://seqr-datasets/v03/GRCh38/SNV_INDEL/runs/manual__2023-11-07T23-31-23.149902+00-00`; For running the service, you need a `DATASETS_DIR` env variable defined, and that data should be available at `$DATASETS_DIR/GRCh38/SNV_INDEL`. Post body for a relatively quick search:; ```; {; ""genome_version"": ""GRCh38"",; ""num_results"": 100,; ""annotations"": {; ""in_frame"": [; ""inframe_insertion"",; ""inframe_deletion""; ],; ""missense"": [; ""stop_lost"",; ""initiator_codon_variant"",; ""start_lost"",; ""protein_altering_variant"",; ""missense_variant""; ],; ""nonsense"": [; ""stop_gained""; ],; ""splice_ai"": ""0.2"",; ""frameshift"": [; ""frameshift_variant""; ],; ""structural"": [],; ""extended_splice_site"": [],; ""essential_splice_site"": [; ""splice_donor_variant"",; ""splice_acceptor_variant""; ],; ""structural_consequence"": [; ""LOF"",; ""DUP_LOF"",; ""INV_SPAN"",; ""COPY_GAIN""; ]; },; ""datasetType"": ""VARIANTS"",; ""pathogenicity"": {; ""hgmd"": [; ""disease_causing""; ],; ""clinvar"": [; ""pathogenic"",; ""likely_pathogenic""; ]; },; ""dataset_type"": ""ALL"",; ""secondary_dataset_type"": null,; ""inheritance_mode"": ""de_novo"",; ""inheritance_filter"": {; ""A"": ""has_alt"",; ""N"": ""ref_ref""; },; ""sample_data"": {; ""SNV_INDEL"": [; {; ""sample_id"": ""RGP_2436_2_D1"",; ""individual_guid"": ""I0097169_rgp_2436_2"",; ""family_guid"": ""F041731_rgp_2436"",; ""project_guid"": ""R0594_rare_genomes_project_gen"",; ""affected"": ""N""; },; {; ""sample_id"": ""RGP_2436_3_D1"",; ""individual_guid"": ""I0097170_rgp_2436_3"",; ""family_guid"": ""F041731_rgp_2436"",; ""project_guid"": ""R0594_rare_genomes_project_gen"",; ""affected"": ""A""; },; {; ""sample_id"": ""RGP_2436_1_D1"",; ""individual_guid"": ""I0097168_rgp_2436_1"",; ""family_guid"": ""F041731_rgp_2436"",; ""project_guid"": ""R0594_rare_genomes_project_gen"",; ""affected"": ""N""; }; ]; },; ""sort"": ""xpos"",; ""sort_metadata"": null,; ""frequencies"": {; ""g1k"": {;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1810939501:385,avail,available,385,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1810939501,1,['avail'],['available']
Availability,"sequences"", ),; File ""/tmp/ffc9fb0b99f64080b674ab7a07962df9/load_dataset_to_es.py"", line 358, in export_to_elasticsearch; verbose=True,; File ""/tmp/ffc9fb0b99f64080b674ab7a07962df9/hail_scripts.zip/hail_scripts/v01/utils/elasticsearch_client.py"", line 140, in export_vds_to_elasticsearch; File ""/tmp/ffc9fb0b99f64080b674ab7a07962df9/hail_scripts.zip/hail_scripts/v01/utils/elasticsearch_client.py"", line 287, in export_kt_to_elasticsearch; File ""<decorator-gen-143>"", line 2, in export_elasticsearch; File ""/tmp/ffc9fb0b99f64080b674ab7a07962df9/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: EsHadoopIllegalArgumentException: Spark SQL types are not handled through basic RDD saveToEs() calls; typically this is a mistake(as the SQL schema will be ignored). Use 'org.elasticsearch.spark.sql' package instead. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 68 in stage 3.0 failed 20 times, most recent failure: Lost task 68.19 in stage 3.0 (TID 3771, vep-grch37-sw-9767.c.seqr-project.internal): org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Spark SQL types are not handled through basic RDD saveToEs() calls; typically this is a mistake(as the SQL schema will be ignored). Use 'org.elasticsearch.spark.sql' package instead; 	at org.elasticsearch.spark.serialization.ScalaValueWriter.doWriteScala(ScalaValueWriter.scala:124); 	at org.elasticsearch.spark.serialization.ScalaValueWriter.doWrite(ScalaValueWriter.scala:50); 	at org.elasticsearch.spark.serialization.ScalaValueWriter$$anonfun$doWriteScala$3.apply(ScalaValueWriter.scala:78); 	at org.elasticsearch.spark.serialization.ScalaValueWriter$$anonfun$doWriteScala$3.apply(ScalaValueWriter.scala:77); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.elasticsearch.spark.serialization.ScalaValueWriter.doWriteScala(Sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4250:2523,failure,failure,2523,https://hail.is,https://github.com/hail-is/hail/issues/4250,1,['failure'],['failure']
Availability,"ser=ci; uuid=f53f127847864f1cbf7d4bdc911a6646; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; Image ID: ; Port: <none>; Host Port: <none>; Command:; bash; -c; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-3-job-41-39d17b (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /test-gsa-key from test-gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6625:1844,Toler,Tolerations,1844,https://hail.is,https://github.com/hail-is/hail/issues/6625,1,['Toler'],['Tolerations']
Availability,"set size unnecessarily if image fails to open <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7056"">#7056</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Removed unused code <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7210"">#7210</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Removed unused variables <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7205"">#7205</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed signedness comparison warning <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7203"">#7203</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed combining single duration across duplicate APNG frames <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7146"">#7146</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Remove temporary file when error is raised <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7148"">#7148</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Do not use temporary file when grabbing clipboard on Linux <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7200"">#7200</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>If the clipboard fails to open on Windows, wait and try again <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7141"">#7141</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed saving multiple 1 mode frames to GIF <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7181"">#7181</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Replaced absolute PIL import with relative import <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7173"">#7173</a> [<a href=""https://github.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:6511,error,error,6511,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['error'],['error']
Availability,shift headings down a level when creating single-page documentation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/635:15,down,down,15,https://hail.is,https://github.com/hail-is/hail/issues/635,1,['down'],['down']
Availability,shifted two lines down and fixed links,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1931:18,down,down,18,https://hail.is,https://github.com/hail-is/hail/pull/1931,1,['down'],['down']
Availability,"should fix your error from friday, Dan.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8896:16,error,error,16,https://hail.is,https://github.com/hail-is/hail/pull/8896,1,['error'],['error']
Availability,should we error or return NaN here? I'm not sure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4819#issuecomment-440772136:10,error,error,10,https://hail.is,https://github.com/hail-is/hail/pull/4819#issuecomment-440772136,1,['error'],['error']
Availability,shutdowns in createDatabase2 is a list of deployments to shut down before applying any migrations.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7855:62,down,down,62,https://hail.is,https://github.com/hail-is/hail/pull/7855,1,['down'],['down']
Availability,"simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::int16<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::int16<16>]’; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:115:37: required from ‘simdpp::arch_avx2::uint16<16>& simdpp::arch_avx2::uint16<16>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int16<16, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:64:37: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint16<16>’ with ‘private’ member ‘simdpp::arch_avx2::uint16<16>::d_’ from an array of ‘const class simdpp::arch_avx2::int16<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:22,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:92:7: note: ‘class simdpp::arch_avx2::uint16<16>’ declared here; class uint16<16, void> : public any_int16<16, uint16<16,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int8<32>; T = simdpp::arch_avx2::uint16<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:64035,error,error,64035,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"simpler:; ```; In [16]: import hail as hl; ...: ; ...: t1kg = hl.utils.range_matrix_table(1,1); ...: t1kg = t1kg.key_rows_by(locus=hl.locus(hl.str(t1kg.row_idx+1), t1kg.row_idx+1), alleles=['A','T']); ...: t1kg.write('/tmp/foo.mt', overwrite=True); 2018-10-11 13:42:55 Hail: INFO: Coerced sorted dataset; 2018-10-11 13:42:55 Hail: INFO: wrote 1 items in 1 partitions to /tmp/foo.mt; ^[[A; In [17]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt'); ...: t1kg = hl.split_multi(t1kg); ...: t1kg._force_count_rows(); ```; error:; ```; FatalError: HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; after: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; ```; describe:; ```; In [18]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt').describe(); ...: ; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'col_idx': int32 ; ----------------------------------------; Row fields:; 'row_idx': int32 ; 'locus': locus<GRCh37> ; 'alleles': array<str> ; ----------------------------------------; Entry fields:; None; ----------------------------------------; Column key: ['col_idx']; Row key: ['locus', 'alleles']; ----------------------------------------; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4527#issuecomment-429051397:543,error,error,543,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429051397,1,['error'],['error']
Availability,simplify error handling logic,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1888:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/pull/1888,1,['error'],['error']
Availability,"sion"": ""another_version""}]}; ```. The `annotation_db.json` file is now used by the `load_dataset()` function in `datasets.py` as well, any dataset in the JSON file should now be able to be loaded this way. Made changes to the following:; - `DB` class now requires a `region` parameter.; - `Dataset.from_name_and_json()` has had a `custom_config` parameter added that indicates whether or not the user has supplied their own `config` or `url`. `Dataset.from_name_and_json()` now calls `DatasetVersion.get_region()` method to retrieve the dataset from the bucket in the selected region if `custom_config` is `False`. ; - The `DatasetVersion.get_region()` method takes the dataset `name`, a list of `DatasetVersion` objects, and a `region`, and returns a list of the versions that are available for that region. This method calls the instance method `in_region()` to check if the dataset is available in the requested region.; - If `in_region()` determines the desired region is not available for some dataset that otherwise is available in another region, it will raise a warning. If user still tries to call `db.annotate_rows_db()` using a dataset unavailable in their region, then it will get caught by the `_check_availability` instance method in the `DB` class and raise a `ValueError`.; - Started to add documentation to the classes and methods, still a work in progress. Changes to datasets and datasets API site:; - Added the `ldsc_baselineLD_annotations`, `ldsc_baselineLD_ldscores`, and `ldsc_baseline_ldscores` datasets to the `annotation_db.json` configuration file. Now accessible via `load_dataset()` and `db.annotate_rows_db()` (for the annotations at least).; - New `.rst` files in `hail/python/hail/docs/datasets` have been generated to reflect the available datasets in the config file, and `hail/python/hail/docs/datasets.rst` has been updated with the new files as well. Future updates:; - In next PR can add the functionality to automatically determine the users region.; - Also con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9496:1711,avail,available,1711,https://hail.is,https://github.com/hail-is/hail/pull/9496,2,['avail'],['available']
Availability,"skopeo avoids downloading images that are already up-to-date, so it; is often a lot faster.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11190:14,down,downloading,14,https://hail.is,https://github.com/hail-is/hail/pull/11190,1,['down'],['downloading']
Availability,"slack_utils.py"", line 97, in try_slack; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/pyscripts_F47nn5.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/generate_qc_annotations.py"", line 203, in main; vds.write(annotations_vds_path(data_type, 'truth_data'), args.overwrite); File ""<decorator-gen-528>"", line 2, in write; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/typecheck/check.py"", line 479, in _typecheck; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/matrixtable.py"", line 1807, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/utils/java.py"", line 238, in deco; hail.utils.java.FatalError: HailException: found non-left aligned variant: 18:76051965:C:G. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 56 in stage 3.0 failed 20 times, most recent failure: Lost task 56.19 in stage 3.0 (TID 685, exomes2-sw-8mf1.c.broad-mpg-gnomad.internal, executor 55): is.hail.utils.HailException: found non-left aligned variant: 18:76051965:C:G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.methods.SplitMultiPartitionContext.splitRow(SplitMulti.scala:98); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:226); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:225); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3040:1366,failure,failure,1366,https://hail.is,https://github.com/hail-is/hail/issues/3040,1,['failure'],['failure']
Availability,"so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tells `clang` to not error if an included file does not exist. This can be helpful if the project's `Makefile` knows how to generate these header files. In our example, `catch.hpp` can be generated by `hail/src/main/c/Makefile` by downloading it from GitHub.; ```; # rm -rf bar.h; # clang -MM foo.cpp ; foo.cpp:1:10: fatal error: 'bar.h' file not found; #include ""bar.h""; ^~~~~~~; 1 error generated.; # clang -MM foo.cpp -MG; foo.o: foo.cpp bar.h; ```. The `-MF file` argument tells `clang` to write to `file` instead of stdout. The `-MMD` argument is used if we *also want to compile the file*. The `-MM` argument defaults to adding `-E` meaning ""only run the preprocessor"".",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5331:2019,error,error,2019,https://hail.is,https://github.com/hail-is/hail/pull/5331,4,"['down', 'error']","['downloading', 'error']"
Availability,so this fixes a match error in that case?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4000#issuecomment-408235922:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/pull/4000#issuecomment-408235922,1,['error'],['error']
Availability,some inference assertion failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8013#issuecomment-580526563:25,failure,failures,25,https://hail.is,https://github.com/hail-is/hail/pull/8013#issuecomment-580526563,1,['failure'],['failures']
Availability,"sonschema/compare/v4.10.3...v4.11.0</a></p>; <h2>v4.10.3</h2>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.2...v4.10.3"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.2...v4.10.3</a></p>; <h2>v4.10.2</h2>; <ul>; <li>Fix a second place where subclasses may have added attrs attributes (<a href=""https://github-redirect.dependabot.com/python-jsonschema/jsonschema/issues/982"">#982</a>).</li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.1...v4.10.2"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.1...v4.10.2</a></p>; <h2>v4.10.1</h2>; <ul>; <li>Fix Validator.evolve (and APIs like <code>iter_errors</code> which call it) for cases; where the validator class has been subclassed. Doing so wasn't intended to be; public API, but given it didn't warn or raise an error it's of course; understandable. The next release however will make it warn (and a future one; will make it error). If you need help migrating usage of inheriting from a; validator class feel free to open a discussion and I'll try to give some; guidance (<a href=""https://github-redirect.dependabot.com/python-jsonschema/jsonschema/issues/982"">#982</a>).</li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.0...v4.10.1"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.0...v4.10.1</a></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/python-jsonschema/jsonschema/blob/main/CHANGELOG.rst"">jsonschema's changelog</a>.</em></p>; <blockquote>; <h1>v4.15.0</h1>; <ul>; <li>A specific API Reference page is now present in the documentation.</li>; <li><code>$ref</code> on earlier drafts (specifically draft 7 and 6) has been &quot;fixed&quot; to; follow the specified behavior",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12163:3412,error,error,3412,https://hail.is,https://github.com/hail-is/hail/pull/12163,1,['error'],['error']
Availability,"sorry, wasn't clear. I don't think it's trivial to figure out what a no-args checkpoint should do, but it IS trivial to make that change back-compatibly when we do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5528#issuecomment-469468966:77,checkpoint,checkpoint,77,https://hail.is,https://github.com/hail-is/hail/pull/5528#issuecomment-469468966,1,['checkpoint'],['checkpoint']
Availability,"sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206386,down,down,206386,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,3,"['ERROR', 'down']","['ERROR', 'down']"
Availability,sped up mendel errors with sampleKidRole,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/68:15,error,errors,15,https://hail.is,https://github.com/hail-is/hail/pull/68,1,['error'],['errors']
Availability,split_multi_hts fails with bad error message on string PGT field,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4415:31,error,error,31,https://hail.is,https://github.com/hail-is/hail/issues/4415,1,['error'],['error']
Availability,split_multi_hts should verify its assumptions and throw reasonable error messages,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5157:67,error,error,67,https://hail.is,https://github.com/hail-is/hail/issues/5157,1,['error'],['error']
Availability,ssLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collecti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:2789,Heartbeat,HeartbeatReceiver,2789,https://hail.is,https://github.com/hail-is/hail/issues/4780,1,['Heartbeat'],['HeartbeatReceiver']
Availability,"ssl_context`. The client context is configured to seek certificates from its peers. Both; internal contexts load the Hail certificate chain specified in the Hail SSL; Config. The external client context does not load the hail certificate chain. I intend all Hail's HTTP(S) requests to use `httpx.py` (so named to not conflict; with modules named `http`). Again, I have simplified the landscape. We now have; two functions:. - `httpx.client_session`: The constructor for all asynchronous, HTTPS client; sessions.; - `httpx.blocking_client_session`: The constructor for all synchronous, HTTPS; client sessions. Both sessions have the exact same configuration parameters. The API is exactly; the same except the blocking client session replaces asynchronous methods with; synchronous ones. Both sessions accept the `aiohttp.ClientSession` constructor parameters. They; support one new parameter and modify the behavior of one old parameter.; - `retry_transient`: when set to `True` this parameter will retry all transient; errors in all requests made by this session. This defaults to `True`.; - `raise_for_status`: this parameter now defaults to `True` and includes the; response body text in the error message. Both; Both parameters may be overridden on a per-request basis. - `httpx.ResponseManager` and `httpx.ClientSession` work together to enable; `retry_transient` and `raise_for_status`. Aiohttp has this unusual structure where; all the request methods are synchronous but they return an object that is both; awaitable and an async context manager. I mirror their structure exactly. The; `httpx.ResponseManager` is both awaitable and an async context manager. Its; `response_coroutine` field is a coroutine that includes the retry and; raise-for-status logic. - The `HailResolver` overrides domain name resolution to first consult the Hail; `address` service. `address` is effectively a domain name server. It watches; kubernetes services and publishes the pod IPs. It supports two name styles:;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9554:1356,error,errors,1356,https://hail.is,https://github.com/hail-is/hail/pull/9554,1,['error'],['errors']
Availability,"ssoc_vds.filter_variants_expr('va.useInKinship'); lmm_vds = assoc_vds.lmmreg(kinship_vds, 'sa.pheno', ['sa.cov1', 'sa.cov2']). lmm_vds.globals; ```. Error message:; ```; Failed example:; lmm_vds.globals; Exception raised:; Traceback (most recent call last):; File ""//anaconda/lib/python2.7/doctest.py"", line 1315, in __run; compileflags, 1) in test.globs; File ""<doctest default[1]>"", line 1, in <module>; lmm_vds.globals; File ""/Users/jigold/hail/python/hail/dataset.py"", line 1958, in globals; self._globals = self.global_schema._convert_to_py(self._jvds.globalAnnotation()); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 243, in _convert_to_py; lst = env.jutils.iterableToArrayList(annotation); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py"", line 63, in deco; return f(*a, **kw); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 323, in get_return_value; format(target_id, ""."", name, value)); Py4JError: An error occurred while calling o155.iterableToArrayList. Trace:; py4j.Py4JException: Method iterableToArrayList([class [D]) does not exist; at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); at py4j.Gateway.invoke(Gateway.java:272); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1368:1989,error,error,1989,https://hail.is,https://github.com/hail-is/hail/issues/1368,1,['error'],['error']
Availability,"sspath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sessionInfo(); sparklyr::invoke(ht, ""count""); ```. it generates this output:; ```; (foo) # Rscript /tmp/failure.R; R version 3.5.1 (2018-07-02); Platform: x86_64-apple-darwin17.6.0 (64-bit); Running under: macOS High Sierra 10.13.6. Matrix products: default; BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib; LAPACK: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib. locale:; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8. attached base packages:; [1] stats graphics grDevices utils datasets methods base . loaded via a namespace (and not attached):; [1] Rcpp_0.12.19 dbplyr_1.2.2 compiler_3.5.1 pillar_1.3.0 ; [5] later_0.7.5 bindr_0.1.1 r2d3_0.2.2 base64enc_0.1-3 ; [9] tools_3.5.1 digest_0.6.18 jsonlite_1.5 tibble_1.4.2 ; [13] nlme_3.1-137 lattice_0.20-35 pkgconfig_2.0.2 rlang_0.2.2 ; [17] shiny_1.1.0 DBI_1.0.0 rstudioapi_0.8 bindrcpp_0.2.2 ; [21] withr_2.1.2 dplyr_0.7.7 httr_1.3.1 sparklyr_0.9.2 ; [25] rappdirs_0.3.1 h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977:1281,failure,failure,1281,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977,1,['failure'],['failure']
Availability,"ssues/3215"">#3215</a>)</li>; <li>Fix misdetection of project root and verbose logging of sources in cases involving <code>--stdin-filename</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3216"">#3216</a>)</li>; <li>Immediate <code>.gitignore</code> files in source directories given on the command line are now also respected, previously only <code>.gitignore</code> files in the project root and automatically discovered directories were respected (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3237"">#3237</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Recommend using BlackConnect in IntelliJ IDEs (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3150"">#3150</a>)</li>; </ul>; <h3>Integrations</h3>; <ul>; <li>Vim plugin: prefix messages with <code>Black: </code> so it's clear they come from Black (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3194"">#3194</a>)</li>; <li>Docker: changed to a /opt/venv installation + added to PATH to be available to non-root users (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3202"">#3202</a>)</li>; </ul>; <h3>Output</h3>; <ul>; <li>Change from deprecated <code>asyncio.get_event_loop()</code> to create our event loop which removes DeprecationWarning (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3164"">#3164</a>)</li>; <li>Remove logging from internal <code>blib2to3</code> library since it regularly emits error logs about failed caching that can and should be ignored (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3193"">#3193</a>)</li>; </ul>; <h3>Parser</h3>; <ul>; <li>Type comments are now included in the AST equivalence check consistently so accidental deletion raises an error. Though type comments can't be tracked when running on PyPy 3.7 due to standard library limitations. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2874"">#2874</a>)</li>; </ul>; <h3>Performance</h3>; <!-- raw H",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12174:4029,avail,available,4029,https://hail.is,https://github.com/hail-is/hail/pull/12174,1,['avail'],['available']
Availability,st practices states the initial capacity is 5000 read requests / second per bucket including list operations until the bucket has time to scale up its capacity. https://cloud.google.com/storage/docs/request-rate#best-practices. ```. ==============================================================================; DIAGNOSTIC RESULTS ; ==============================================================================. ------------------------------------------------------------------------------; Latency ; ------------------------------------------------------------------------------; Operation Size Trials Mean (ms) Std Dev (ms) Median (ms) 90th % (ms); ========= ========= ====== ========= ============ =========== ===========; Delete 0 B 5 43.1 6.4 40.9 50.9 ; Delete 1 KiB 5 44.2 12.7 42.5 58.1 ; Delete 100 KiB 5 44.7 10.4 42.8 56.3 ; Delete 1 MiB 5 41.5 3.7 40.2 45.7 ; Download 0 B 5 74.6 7.9 73.2 84.0 ; Download 1 KiB 5 84.3 15.9 80.6 103.4 ; Download 100 KiB 5 81.9 16.0 82.7 99.6 ; Download 1 MiB 5 90.6 6.5 94.5 96.8 ; Metadata 0 B 5 23.6 2.7 23.6 26.3 ; Metadata 1 KiB 5 25.5 2.1 26.9 27.4 ; Metadata 100 KiB 5 26.2 3.6 27.3 29.9 ; Metadata 1 MiB 5 24.0 3.7 23.3 28.4 ; Upload 0 B 5 98.1 16.6 95.5 117.9 ; Upload 1 KiB 5 116.7 21.8 115.5 142.1 ; Upload 100 KiB 5 116.5 17.8 115.1 135.1 ; Upload 1 MiB 5 168.2 18.5 179.6 185.6 . ------------------------------------------------------------------------------; Write Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Write throughput: 977.7 Mibit/s.; Parallelism strategy: both. ------------------------------------------------------------------------------; Read Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Read throughput: 1.11 Gibit/s.; Parallelism strategy: both. -----------------------------------------------------,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:1401,Down,Download,1401,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['Down'],['Download']
Availability,"started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44817.; 2019-01-22 13:11:37 NettyBlockTransferService: INFO: Server created on 10.48.225.55:44817; 2019-01-22 13:11:37 BlockManager: INFO: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 2019-01-22 13:11:37 BlockManagerMaster: INFO: Registering BlockManager BlockManagerId(driver, 10.48.225.55, 44817, None); 2019-01-22 13:11:37 BlockManagerMasterEndpoint: INFO: Registering block manager 10.48.225.55:44817 with 2.5 GB RAM, BlockManagerId(driver, 10.48.225.55, 44817, None); 2019-01-22 13:11:37 BlockManagerMaster: INFO: Registered BlockManager BlockManagerId(driver, 10.48.225.55, 44817, None); 2019-01-22 13:11:37 BlockManager: INFO: Initialized BlockManager: BlockManagerId(driver, 10.48.225.55, 44817, None); 2019-01-22 13:11:37 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@77390398{/metrics/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:38 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.203:44870) with ID 8; 2019-01-22 13:11:38 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q19.scc.bu.edu:44319 with 21.2 GB RAM, BlockManagerId(8, scc-q19.scc.bu.edu, 44319, None); 2019-01-22 13:11:40 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.229:36354) with ID 7; 2019-01-22 13:11:40 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.202:50198) with ID 6; 2019-01-22 13:11:40 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q21.scc.bu.edu:33025 with 21.2 GB RAM, BlockManagerId(7, scc-q21.scc.bu.edu, 33025, None); 2019-01-22 13:11:40 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q18.scc.bu.edu:39123 with 21.2 GB RAM, BlockManagerId(6, scc-q18.scc.bu.edu, 39123, None); 2019-01-22 ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:20172,AVAIL,AVAILABLE,20172,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"ster/CHANGELOG.md"">aioredis's changelog</a>.</em></p>; <blockquote>; <h2>2.0.1 - (2021-12-20)</h2>; <h3>Features</h3>; <ul>; <li>Added Python 3.10 to CI &amp; Updated the Docs; (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1160"">#1160</a>)</li>; <li>Enable mypy in CI (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1101"">#1101</a>)</li>; <li>Synchronized reading the responses from a connection; (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1106"">#1106</a>)</li>; </ul>; <h3>Fixes</h3>; <ul>; <li>Remove <strong>del</strong> from Redis (Fixes <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1115"">#1115</a>); (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1227"">#1227</a>)</li>; <li>fix socket.error raises (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1129"">#1129</a>)</li>; <li>Fix buffer is closed error when using PythonParser class; (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1213"">#1213</a>)</li>; </ul>; <h2>2.0.0 - (2021-03-18)</h2>; <h3>Features</h3>; <ul>; <li>; <p>Port redis-py's client implementation to aioredis.<br />; (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/891"">#891</a>)</p>; </li>; <li>; <p>Make hiredis an optional dependency.<br />; (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/917"">#917</a>)</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/224f843bd4b33d657770bded6f86ce33b881257c""><code>224f843</code></a> Release version 2.0.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1247"">#1247</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/a9825c2ac35939b9ad8928e9468335d8efab963f""><code>a9825c2</code></a> Bum",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11569:3078,error,error,3078,https://hail.is,https://github.com/hail-is/hail/pull/11569,1,['error'],['error']
Availability,still a bad error message. almost done with a fix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4078#issuecomment-410711084:12,error,error,12,https://hail.is,https://github.com/hail-is/hail/issues/4078#issuecomment-410711084,1,['error'],['error']
Availability,still chasing down ndarray regressions...; ```; linear_regression_rows_nd 246.9% 50.654 125.085 100.0% 8 8; ndarray_matmul_float64_benchmark 165.8% 4.999 8.290 100.0% 1 1; hwe_normalized_pca_blanczos_small_data_10_iterations 128.7% 54.708 70.395 100.0% 8 8; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10539#issuecomment-869629782:14,down,down,14,https://hail.is,https://github.com/hail-is/hail/pull/10539#issuecomment-869629782,1,['down'],['down']
Availability,still failing:; ```gsutil cat gs://hail-ci-0-1/deploy/c28a3f9863a0\*/job-log ```; ```; + gcloud -q auth activate-service-account --key-file=/secrets/gcr-push-service-account-key.json; Activated service account credentials for: [gcr-push@broad-ctsa.iam.gserviceaccount.com]; + gcloud -q auth configure-docker; Docker configuration file updated.; + make push-batch; docker build -t batch .; Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.38/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&session=nt5ube8nzit2kbdia2afrfify&shmsize=0&t=batch&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied; make: *** [build-batch] Error 1; Makefile:14: recipe for target 'build-batch' failed. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708:901,Error,Error,901,https://hail.is,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708,1,['Error'],['Error']
Availability,still got test failures: https://ci.hail.is/batches/48405/jobs/45,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8778#issuecomment-631468022:15,failure,failures,15,https://hail.is,https://github.com/hail-is/hail/pull/8778#issuecomment-631468022,1,['failure'],['failures']
Availability,"stream_codec, timed); File ""/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1304, in __call__; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 31, in deco; raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.project-.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.gbsc-project.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFaile",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:2060,heartbeat,heartbeat,2060,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['heartbeat'],['heartbeat']
Availability,sum array null pointer error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1107:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/issues/1107,1,['error'],['error']
Availability,"summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.8 (2022-01-07)</h2>; <ul>; <li>Added extra message to <code>urllib3.exceptions.ProxyError</code> when urllib3 detects that; a proxy is configured to use HTTPS but the proxy itself appears to only use HTTP.</li>; <li>Added a mention of the size of the connection pool when discarding a connection due to the pool being full.</li>; <li>Added explicit support for Python 3.11.</li>; <li>Deprecated the <code>Retry.MAX_BACKOFF</code> class property in favor of <code>Retry.DEFAULT_MAX_BACKOFF</code>; to better match the rest of the default parameter names. <code>Retry.MAX_BACKOFF</code> is removed in v2.0.</li>; <li>Changed location of the vendored <code>ssl.match_hostname</code> function from <code>urllib3.packages.ssl_match_hostname</code>; to <code>urllib3.util.ssl_match_hostname</code> to ensure Python 3.10+ compatibility after being repackaged; by downstream distributors.</li>; <li>Fixed absolute imports, all imports are now relative.</li>; </ul>; <h1>1.26.7 (2021-09-22)</h1>; <ul>; <li>Fixed a bug with HTTPS hostname verification involving IP addresses and lack; of SNI. (Issue <a href=""https://github-redirect.dependabot.com/urllib3/urllib3/issues/2400"">#2400</a>)</li>; <li>Fixed a bug where IPv6 braces weren't stripped during certificate hostname; matching. (Issue <a href=""https://github-redirect.dependabot.com/urllib3/urllib3/issues/2240"">#2240</a>)</li>; </ul>; <h1>1.26.6 (2021-06-25)</h1>; <ul>; <li>Deprecated the <code>urllib3.contrib.ntlmpool</code> module. urllib3 is not able to support; it properly due to <code>reasons listed in this issue &lt;https://github.com/urllib3/urllib3/issues/2282&gt;</code>_.; If you are a user of this module please leave a comment.</li>; <li>Changed <code>HTTPConnection.request_chunked()</code> to not erroneously emit multiple; <code>Transfer-Encoding</code> headers in the case that ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11532:4270,down,downstream,4270,https://hail.is,https://github.com/hail-is/hail/pull/11532,1,['down'],['downstream']
Availability,"sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:768); at is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:11576,Error,ErrorHandling,11576,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['Error'],['ErrorHandling']
Availability,"t :; WARNING: Hail primarily tested with Spark 3.3.0, use other versions at your own risk. > Task :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; WARNING: The wheel package is not available.; WARNING: The wheel package is not available.; installing to build/bdist.linux-x86_64/wheel; creating build/bdist.linux-x86_64/wheel/hail-0.2.124.dist-info/WHEEL; creating 'dist/hail-0.2.124-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it; adding 'hail/__init__.py'; adding 'hail/builtin_references.py'; adding 'hail/conftest.py'; adding 'hail/context.py'; adding 'hail/hail_logging.py'; adding 'hail/hail_pip_version'; adding 'hail/hail_revision'; adding 'hail/hail_version'; adding 'hail/matrixtable.py'; adding 'hail/table.py'; adding 'hail/backend/__init__.py'; adding 'hail/backend/backend.py'; adding 'hail/backend/hail-all-spark.jar'; adding 'hail/backend/local_backend.py'; adding 'hail/backend/py4j_backend.py'; adding 'hail/backend/service_backend.py'; adding 'hail/backend/spark_backend.py'; adding 'hail/experimental/__init__.py'; adding 'hail/experimental/codec.py'; adding 'hail/experimental/compile.py'; adding 'hail/experimental/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:16960,avail,available,16960,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['avail'],['available']
Availability,"t = meta_ht.repartition(1000); - meta_ht = meta_ht.checkpoint(; - re.sub("".tsv(.b?gz)?"", """", args.sample_metadata_tsv) + "".ht"", overwrite=True, _read_if_exists=True); -; + hl.init(log=""/tmp/select_samples"", default_reference=""GRCh38"", idempotent=True, tmp_dir=args.temp_bucket); vds = gnomad_v4_genotypes.vds(); ; # see https://github.com/broadinstitute/ukbb_qc/pull/227/files; @@ -55,19 +48,8 @@ def main(args):; ; v4_qc_meta_ht = meta.ht(); ; - mt = vds.variant_data; - #mt = vds.variant_data._filter_partitions([41229]); -; - mt = mt.filter_cols(v4_qc_meta_ht[mt.s].release); -; - meta_join = meta_ht[mt.s]; - mt = mt.annotate_cols(; - meta=hl.struct(; - sex_karyotype=meta_join.sex_karyotype,; - cram=meta_join.cram_path,; - crai=meta_join.crai_path,; - ); - ); + #mt = vds.variant_data; + mt = vds.variant_data._filter_partitions([41229]); ; logger.info(""Adjusting samples' sex ploidy""); lgt_expr = hl.if_else(; @@ -88,9 +70,9 @@ def main(args):; logger.info(""Filter variants with at least one non-ref GT""); mt = mt.filter_rows(hl.agg.any(mt.GT.is_non_ref())); ; - #logger.info(f""Saving checkpoint""); - #mt = mt.checkpoint(os.path.join(args.temp_bucket, ""readviz_select_samples_checkpoint1.vds""),; - # overwrite=True, _read_if_exists=True); + logger.info(f""Saving checkpoint""); + mt = mt.checkpoint(""readviz_select_samples_checkpoint1.vds"",; + overwrite=True, _read_if_exists=True); ; def sample_ordering_expr(mt):; """"""For variants that are present in more than 10 samples (or whatever number args.num_samples is set to),; ```. And tried running the bad step:. ```bash; python3 step1__select_samples.py; ```. I was able to get past the checkpoint: ; ```; INFO (Readviz_prep 73): Saving checkpoint; [Stage 0:> (0 + 1) / 1]. 2023-09-01 18:10:29.262 Hail: INFO: wrote matrix table with 11450 rows and 955359 columns in 1 partition to readviz_select_samples_checkpoint1.vds; ```. @bw2 are you still encountering this issue? Did my diff oversimplify it? Do you suspect the issue after the checkpoint?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13248#issuecomment-1703383664:2110,checkpoint,checkpoint,2110,https://hail.is,https://github.com/hail-is/hail/issues/13248#issuecomment-1703383664,7,['checkpoint'],['checkpoint']
Availability,"t analysis segments will be published.</li>; </ul>; <h1>1.21.12</h1>; <ul>; <li>api-change:<code>greengrassv2</code>: [<code>botocore</code>] Doc only update that clarifies Create Deployment section.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for data repository associations to use root (&quot;/&quot;) as the file system path</li>; <li>api-change:<code>kendra</code>: [<code>botocore</code>] Amazon Kendra now suggests spell corrections for a query. For more information, see <a href=""https://docs.aws.amazon.com/kendra/latest/dg/query-spell-check.html"">https://docs.aws.amazon.com/kendra/latest/dg/query-spell-check.html</a></li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] Launching Amazon AppFlow Marketo as a destination connector SDK.</li>; <li>api-change:<code>timestream-query</code>: [<code>botocore</code>] Documentation only update for SDK and CLI</li>; </ul>; <h1>1.21.11</h1>; <ul>; <li>api-change:<code>gamelift</code>: [<code>botocore</code>] Minor updates to address errors.</li>; <li>api-change:<code>cloudtrail</code>: [<code>botocore</code>] Add bytesScanned field into responses of DescribeQuery and GetQueryResults.</li>; <li>api-change:<code>athena</code>: [<code>botocore</code>] This release adds support for S3 Object Ownership by allowing the S3 bucket owner full control canned ACL to be set when Athena writes query results to S3 buckets.</li>; <li>api-change:<code>keyspaces</code>: [<code>botocore</code>] This release adds support for data definition language (DDL) operations</li>; <li>api-change:<code>ecr</code>: [<code>botocore</code>] This release adds support for tracking images lastRecordedPullTime.</li>; </ul>; <h1>1.21.10</h1>; <ul>; <li>api-change:<code>mediapackage</code>: [<code>botocore</code>] This release adds Hybridcast as an available profile option for Dash Origin Endpoints.</li>; <li>api-change:<code>rds</code>: [<code>botocore</code>] Documentation updates for Multi-AZ DB cluste",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11504:2689,error,errors,2689,https://hail.is,https://github.com/hail-is/hail/pull/11504,1,['error'],['errors']
Availability,"t is resolved.</p>; <h2>pytest-asyncio 0.23.5a0</h2>; <h1>0.23.5 (UNRELEASED)</h1>; <ul>; <li>Declare compatibility with pytest 8 <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/737"">#737</a></li>; <li>Fix typing errors with recent versions of mypy <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/769"">#769</a></li>; </ul>; <h2>Known issues</h2>; <p>As of v0.23, pytest-asyncio attaches an asyncio event loop to each item of the test suite (i.e. session, packages, modules, classes, functions) and allows tests to be run in those loops when marked accordingly. Pytest-asyncio currently assumes that async fixture scope is correlated with the new event loop scope. This prevents fixtures from being evaluated independently from the event loop scope and breaks some existing test suites (see <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/706"">#706</a>). For example, a test suite may require all fixtures and tests to run in the same event loop, but have async fixtures that are set up and torn down for each module. If you're affected by this issue, please continue using the v0.21 release, until it is resolved.</p>; <h2>pytest-asyncio 0.23.4</h2>; <h1>0.23.4 (2024-01-28)</h1>; <ul>; <li>pytest-asyncio no longer imports additional, unrelated packages during test collection <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/729"">#729</a></li>; <li>Addresses further issues that caused an internal pytest error during test collection</li>; <li>Declares incompatibility with pytest 8 <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/737"">#737</a></li>; </ul>; <h2>pytest-asyncio 0.23.4a2</h2>; <h1>0.23.4 (UNRELEASED)</h1>; <ul>; <li>pytest-asyncio no longer imports additional, unrelated packages during test collection <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/729"">#729</a></li>; <li>Addresses further issues that caused an internal pytest error during test col",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14507:5082,down,down,5082,https://hail.is,https://github.com/hail-is/hail/pull/14507,1,['down'],['down']
Availability,"t is.hail.methods.MendelErrors.nErrorPerNuclearFamily(MendelErrors.scala:144); 	at is.hail.methods.MendelErrors.fMendelKT(MendelErrors.scala:183); 	at is.hail.variant.MatrixTable.mendelErrors(MatrixTable.scala:1915); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-42db165; Error summary: AssertionError: assertion failed; ```; Pipeline (happens on the 2nd mendel_errrors):; ```; vds = vds.select_rows(vds.locus, vds.alleles); fam_kt = hl.import_fam(fam_file); vds = vds.annotate_cols(fam=fam_kt[vds.s]). # Unphased for now, since mendel_errors does not support phased alleles; vds = vds.annotate_entries(GT=hl.call(vds.GT[0], vds.GT[1], phased=False)); ped = hl.Pedigree.read(fam_file); tdt_table = hl.tdt(vds, ped); _, _, per_sample, per_variant = hl.mendel_errors(vds, ped); family_stats = Struct(mendel=per_variant[(vds.locus, vds.alleles)],; tdt=tdt_table[(vds.locus, vds.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(vds.fam.pat_id),; vds.GT.num_alt_alleles())),; meta={'group': 'raw'}); vds = vds.annotate_rows(family_stats=[family_stats]). vds = filter_to_adj(vds); adj_tdt_table = hl.tdt(vds, ped); _, _, adj_per_sample, adj_per_variant = hl.mendel_errors(vds, ped); family_stats_adj = Struct(mendel=adj_per_variant[(vds.locus, vds.alleles)],; tdt=adj_tdt_table[(vds.locus, vds.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3039:11621,Error,Error,11621,https://hail.is,https://github.com/hail-is/hail/issues/3039,1,['Error'],['Error']
Availability,"t is.hail.methods.SplitMulti$.unionMovedVariants(SplitMulti.scala:237); 	at is.hail.methods.SplitMulti.split(SplitMulti.scala:332); 	at is.hail.methods.SplitMulti$.apply(SplitMulti.scala:232); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: invalid allele ""GN""; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); 	at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:28); 	at is.hail.variant.AltAlleleMethods$.isStar(AltAlleleMethods.scala:73); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.mutable.WrappedArray.forall(WrappedArray.scala:35); 	at is.hail.variant.VariantMethods$.minRep(VariantMethods.scala:43); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:196); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:192); 	at scala.collection.Iterator$$ano",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3480:8772,Error,ErrorHandling,8772,https://hail.is,https://github.com/hail-is/hail/issues/3480,1,['Error'],['ErrorHandling']
Availability,"t java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 		at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; 	Caused by: com.google.api.client.http.HttpResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""domain"": ""global"",; ""reason"": ""forbidden""; }; ]; }; }. 		at com.google.api.client.http.HttpResponseException$Builder.build(HttpResponseException.java:293) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1118) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.open(HttpStorageRpc.java:1022) ~[gs:__hail-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:25917,error,error,25917,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['error'],['error']
Availability,"t look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, ti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4083,error,errors,4083,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,1,['error'],['errors']
Availability,t org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199159,ERROR,ERROR,199159,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,"['ERROR', 'down']","['ERROR', 'down']"
Availability,"t org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:488); 	at is.hail.variant.VariantDatasetFunctions$.write$extension(VariantDataset.scala:941); 	at is.hail.variant.VariantDatasetFunctions.write(VariantDataset.scala:911); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 5.0 failed 20 times, most recent failure: Lost task 22.19 in stage 5.0 (TID 133, seqr-pipeline-cluster-grch38-w-1.c.seqr-project.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurren",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822:3499,failure,failure,3499,https://hail.is,https://github.com/hail-is/hail/issues/1822,1,['failure'],['failure']
Availability,"t py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.ClassNotFoundException: com.amazonaws.AmazonClientException; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:419); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:352); 	... 27 more. During handling of the above exception, another exception occurred:. Py4JJavaError Traceback (most recent call last); /bmrn/apps/hail/0.2.72-spark-3.1.2/python/hail-0.2.72-py3-none-any.egg/hail/backend/py4j_backend.py in deco(*args, **kwargs); 15 try:; ---> 16 return f(*args, **kwargs); 17 except py4j.protocol.Py4JJavaError as e:. /bmrn/apps/python/miniconda/64/3.7/envs/piranha_0.2.0_20210812/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 327 ""An error occurred while calling {0}{1}{2}.\n"".; --> 328 format(target_id, ""."", name), value); 329 else:. Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.backend.spark.SparkBackend$.apply(SparkBackend.scala:218); 	at is.hail.backend.spark.SparkBackend.apply(SparkBackend.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:3000,error,error,3000,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,1,['error'],['error']
Availability,"t status 1.; ```. Then looking at the error log; ```; $ gsutil cat gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output; pip packages are ['setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclient==2.0.0', 'websocket-client', 'sklearn', 'tabulate', 'statsmodels', 'scikit-learn', 'hdbscan', 'matplotlib']; b""Double requirement given: tabulate (already in tabulate==0.8.3, name='tabulate')\nYou are using pip version 10.0.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n""; Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 16, in safe_call; sp.check_output(args, stderr=sp.STDOUT); File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 336, in check_output; **kwargs).stdout; File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 418, in run; output=stdout, stderr=stderr); subprocess.CalledProcessError: Command '('pip', 'install', 'setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6634:5800,avail,available,5800,https://hail.is,https://github.com/hail-is/hail/issues/6634,1,['avail'],['available']
Availability,"t to ""0.2.124"" which is different from old value """"; printf ""0.2.124"" > env/HAIL_PIP_VERSION; echo 0.2.124-13536b531342 > python/hail/hail_version; echo 0.2.124 > python/hail/hail_pip_version; cp -f python/hail/hail_version python/hailtop/hail_version; printf 'hail_version=""0.2.124-13536b531342"";' > python/hail/docs/_static/hail_version.js; printf 'hail_pip_version=""0.2.124""' >> python/hail/docs/_static/hail_version.js; cloud_base is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:2960,echo,echo,2960,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,t variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ di,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:5568,echo,echo,5568,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,"t(j.counts['tsv.gz.tbi'], output_index_path); ```. (Trying to use `b.write_output(j.counts, output_base)` to write out the whole ResourceGroup fails because the `…/counts.counts.tsv` file no longer exists because it was removed by `bgzip`. In any case, we don't want to write that one to the final bucket anyway. Hence the two separate `write_output` invocations for the two desired output files.). This fails in the second `write_output` with a fairly mysterious exception:. ```; File ""…"", line 92, in test; b.write_output(j.counts['tsv.gz.tbi'], output_index_path); File ""…/lib/python/site-packages/hailtop/batch/batch.py"", line 595, in write_output; name = resource._source._resources_inverse[resource]; KeyError: __RESOURCE_FILE__11; ```. Checking that line of _batch.py_, it is failing while trying to print an error message because `_resources_inverse` is not set up for the JobResourceFiles within ResourceGroups. PR #13192 is a suggested fix for this. With that PR applied, this results in a more useful hail error message exception:. ```; hailtop.batch.exceptions.BatchException: undefined resource 'counts[""tsv.gz.tbi""]'; Hint: resources must be defined within the job methods 'command' or 'declare_resource_group'; ```. This can be worked around by mentioning the filename in the commands to be run — in a comment, because none of the commands actually need to specify the `.tbi` output filename:. ```python; …; j.command(f""""""; gatk SubCommand … --output {j.counts['tsv']}; bgzip {j.counts['tsv']}; gatk IndexFeatureFile --input {j.counts['tsv.gz']}; : {j.counts['tsv.gz.tbi']}; """"""); …; ```. This produces the desired two files — compressed data and the associated index — written to the final bucket. Is it kosher to use `write_output` on the individual items within a ResourceGroup like this?. However this resource **was** defined by a `declare_resource_group` invocation. So it seems to me that this too (namely, the BatchException encountered) is a bug and it should not be necessary ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13191:1847,error,error,1847,https://hail.is,https://github.com/hail-is/hail/issues/13191,1,['error'],['error']
Availability,"t(mt); ped = hl.Pedigree.read(fam_file); tdt_table = hl.tdt(mt, ped); _, _, per_sample, per_variant = hl.mendel_errors(mt, ped); family_stats = Struct(mendel=per_variant[(mt.locus, mt.alleles)],; tdt=tdt_table[(mt.locus, mt.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(mt.fam.pat_id),; mt.GT.num_alt_alleles())),; meta={'group': 'raw'}); mt = mt.annotate_rows(family_stats=[family_stats]). mt = filter_to_adj(mt); adj_tdt_table = hl.tdt(mt, ped); _, _, adj_per_sample, adj_per_variant = hl.mendel_errors(mt, ped); family_stats_adj = Struct(mendel=adj_per_variant[(mt.locus, mt.alleles)],; tdt=adj_tdt_table[(mt.locus, mt.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(mt.fam.pat_id),; mt.GT.num_alt_alleles())),; meta={'group': 'adj'}). per_sample = per_sample.annotate(adj=adj_per_sample[per_sample.s]). mt = mt.annotate_rows(family_stats=mt.family_stats.append(family_stats_adj)); mt.write(); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:394); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1433); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1421); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60$$anonfun$apply$4.apply$mcV$sp(MatrixTable.scala:1723); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:197); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:186); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTabl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3074:1265,error,error,1265,https://hail.is,https://github.com/hail-is/hail/issues/3074,1,['error'],['error']
Availability,"t(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 73 try:; ---> 74 result = json.loads(self._jhc.backend().executeJSON(jir)); 75 value = ir.typ._from_json(result['value']); 76 timings = result['timings']. /usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 1302; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19) (my-first-hail-cluster-w-0.c.open-targets-eu-dev.internal executor 1): java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:5986,failure,failure,5986,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['failure'],['failure']
Availability,"t-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,662	job.py	schedule_job:443	schedule job (100, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,663	job.py	schedule_job:443	schedule job (98, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (101, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (102, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:33,456	hail_logging.py	log:40	https POST /pr-11438-default-g6cibyji6520/batch-driver/api/v1alpha/instances/activate done in 3.2369999999998527s: 200; ERROR	2022-03-02 19:06:33,492	job.py	schedule_job:473	error while scheduling job (95, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 521, in _request\n req, traces=traces, timeout=real_timeout\n F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:2975,error,error,2975,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['error'],['error']
Availability,"t.github.com/fonttools/fonttools/issues/2993"">#2993</a>).</li>; <li>[glyf] Added experimental support for reading and writing Variable Composites/Components as defined in glyf v1 spec proposal (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2958"">#2958</a>):<br />; <a href=""https://github.com/harfbuzz/boring-expansion-spec/blob/main/glyf1-varComposites.md"">https://github.com/harfbuzz/boring-expansion-spec/blob/main/glyf1-varComposites.md</a>.</li>; <li>[pens]: Added <code>addVarComponent</code> method to pen protocols' base classes, which pens can implement to handle varcomponents (by default they get decomposed).</li>; <li>[misc.transform] Added DecomposedTransform class which implements an affine transformation with separate translate, rotation, scale, skew, and transformation-center components (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2598"">#2598</a>)</li>; <li>[sbix] Ensure Glyph.referenceGlyphName is set; fixes error after dumping and re-compiling sbix table with 'dupe' glyphs (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2984"">#2984</a>).</li>; <li>[feaLib] Be cleverer when merging chained single substitutions into same lookup when they are specified using the inline notation (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2150"">#2150</a>, <a href=""https://redirect.github.com/fonttools/fonttools/issues/2974"">#2974</a>).</li>; <li>[instancer] Clamp user-inputted axis ranges to those of fvar (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2959"">#2959</a>).</li>; <li>[otBase/subset] Define <code>__getstate__</code> for BaseTable so that a copied/pickled 'lazy' object gets its own OTTableReader to read from; incidentally fixes a bug while subsetting COLRv1 table containing ClipBoxes on python 3.11 (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2965"">#2965</a>, <a href=""https://redirect.github.com/fonttools/fonttools/issues/2968"">#2968</a>).</li>; <li>[sbix",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:7019,error,error,7019,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['error'],['error']
Availability,"t.primitives.serialization.load_pem_private_key</code>,; :func:<code>~cryptography.hazmat.primitives.serialization.load_der_private_key</code>,; and; :meth:<code>~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateNumbers.private_key</code>.; This speeds up key loading but is :term:<code>unsafe</code> if you are loading potentially; attacker supplied keys.</li>; <li>Significantly improved performance for; :class:<code>~cryptography.hazmat.primitives.ciphers.aead.ChaCha20Poly1305</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyca/cryptography/commit/d6951dca25de45abd52da51b608055371fbcde4e""><code>d6951dc</code></a> changelog + security fix backport (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8231"">#8231</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/138da90c8450446b19619e3faa77b9da54c34be3""><code>138da90</code></a> workaround scapy bug in downstream tests (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8218"">#8218</a>) (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8228"">#8228</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/69527bc79095c9646d7e839121f0783477892ecc""><code>69527bc</code></a> bookworm is py311 now (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8200"">#8200</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/111deefb659b8d73c56d3ce89458f2df973d60e4""><code>111deef</code></a> backport main branch CI to 39.0.x (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8153"">#8153</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/338a65a7df74e189f6b5d1d3a6315ffa911b21c2""><code>338a65a</code></a> 39.0.0 version bump (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/7954"">#7954</a>)</li>; <li><a href=""https://g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12668:3755,down,downstream,3755,https://hail.is,https://github.com/hail-is/hail/pull/12668,4,['down'],['downstream']
Availability,"t.scala:65); at is.hail.backend.service.ServiceBackend.$anonfun$withExecuteContext$1(ServiceBackend.scala:426); at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); at is.hail.backend.service.ServiceBackend.withExecuteContext(ServiceBackend.scala:415); at is.hail.backend.service.ServiceBackendAPI.$anonfun$doAction$1(ServiceBackend.scala:608); at is.hail.services.package$.retryTransientErrors(package.scala:186); at is.hail.backend.service.ServiceBackendAPI.doAction(ServiceBackend.scala:585); at is.hail.backend.service.ServiceBackendAPI.executeOneCommand(ServiceBackend.scala:662); at is.hail.backend.service.ServiceBackendAPI$.main(ServiceBackend.scala:497); at is.hail.backend.service.Main$.main(Main.scala:10); at is.hail.backend.service.Main.main(Main.scala); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.base/java.lang.reflect.Method.invoke(Method.java:566); at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515); at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264); at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515); at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.128-17247d8990c6; Error summary: RuntimeException: IR is.hail.expr.ir.StreamFlatMap of type stream<struct{oldContext: str, nRows: int64, nCols: int64}> is not realizable; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14537:12045,Error,Error,12045,https://hail.is,https://github.com/hail-is/hail/issues/14537,1,['Error'],['Error']
Availability,t.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:77); 	at is.hail.backend.Backend.executeJSON(Backend.scala:96); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). is.hail.utils.HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:210); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:974); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:967); 	at is.hail.utils.FlipbookIterator$$anon$5.<init>(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:174); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:145); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:967); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:963); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:147); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:146); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$19.apply(ContextRD,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:87701,Error,ErrorHandling,87701,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['Error'],['ErrorHandling']
Availability,"t/Dockerfile; /usr/local/share/google/dataproc/bdutil/components/initialize/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/post-install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/activate/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh; /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/docker-ce.done; /tmp/dataproc/components/pre-uninstall/docker-ce.running; /tmp/dataproc/components/pre-uninstall/docker-ce.done; /etc/apt/preferences.d/docker-ce.pref; /etc/apt/preferences.d/docker-ce-cli.pref; /etc/apt/sources.list.d/docker.list; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_InRelease; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_stable_binary-amd64_Packages; ```. </details>. There is a `/run/docker.sock` but notice it is not `/var/run/...`. However, if I install Docker by hand into this worker of a *non-Hail* Dataproc cluster, it just works. ---. I also tried to replicate the failure using an initialization action, but that also just worked.; ```; gcloud dataproc clusters create dk-test2 --initialization-actions=gs://hail-common/dk-test.sh; ```; `gs://hail-common/dk-test.sh`:; ```; apt-get update; apt-get -y install \; apt-transport-https \; ca-certificates \; curl \; gnupg2 \; software-properties-common \; tabix; curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -; sudo add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable""; apt-get update; apt-get install -y --allow-unauthenticated docker-ce; ```. ---. Our ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:12973,down,download,12973,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['down'],['download']
Availability,"t/chardet) from 4.0.0 to 5.0.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/chardet/chardet/releases"">chardet's releases</a>.</em></p>; <blockquote>; <h2>chardet 5.0.0</h2>; <p>⚠️ This release is the first release of chardet that no longer supports Python &lt; 3.6 ⚠️</p>; <p>In addition to that change, it features the following user-facing changes:</p>; <ul>; <li>Added a prober for Johab Korean (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/207"">#207</a>, <a href=""https://github.com/grizlupo""><code>@​grizlupo</code></a>)</li>; <li>Added a prober for UTF-16/32 BE/LE (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/109"">#109</a>, <a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/206"">#206</a>, <a href=""https://github.com/jpz""><code>@​jpz</code></a>)</li>; <li>Added test data for Croatian, Czech, Hungarian, Polish, Slovak, Slovene, Greek, and Turkish, which should help prevent future errors with those languages</li>; <li>Improved XML tag filtering, which should improve accuracy for XML files (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/208"">#208</a>)</li>; <li>Tweaked <code>SingleByteCharSetProber</code> confidence to match latest uchardet (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/209"">#209</a>)</li>; <li>Made <code>detect_all</code> return child prober confidences (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/210"">#210</a>)</li>; <li>Updated examples in docs (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/223"">#223</a>, <a href=""https://github.com/domdfcoding""><code>@​domdfcoding</code></a>)</li>; <li>Documentation fixes (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/212"">#212</a>, <a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/224"">#224</a>, <a href=""https://github-redirect.dependabot.com/chardet/c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12107:1054,error,errors,1054,https://hail.is,https://github.com/hail-is/hail/pull/12107,1,['error'],['errors']
Availability,"t/chardet/commit/57abbca866a41758f7c26e1bb26a0126e28575c2""><code>57abbca</code></a> Rebased and cleaned up version of UTF-16/32 BE/LE PR (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/206"">#206</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/eca9558cf7569c1f7689bd66e5aaf965a56e903c""><code>eca9558</code></a> Fix missing black formatting</li>; <li><a href=""https://github.com/chardet/chardet/commit/f1f9d4280e11fb3a9b2d9eaf1827dac9263cb1cb""><code>f1f9d42</code></a> slight increase in performance (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/252"">#252</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/f9ef56cfd6c9b24b9c865eae6dc2285c67ffb75c""><code>f9ef56c</code></a> Use Python-3 super() syntax in Latin1Prober (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/240"">#240</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/c5e5d5a8f1b6e135a8bffd8d60b2f726bb168339""><code>c5e5d5a</code></a> Simple maintenance improvements (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/244"">#244</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/49b8341f507bed68f7d3ff7138bb97047a0e04f0""><code>49b8341</code></a> Configure setuptools using the declarative syntax in setup.cfg (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/239"">#239</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/5c73bfcdf819251d1a1d0de672e34480ebafbe1f""><code>5c73bfc</code></a> Run all pre-commit hooks on pull requests (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/236"">#236</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/chardet/chardet/compare/4.0.0...5.0.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=chardet&package-manager=pip&previous-version=4.0.0&new-v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12107:4754,mainten,maintenance,4754,https://hail.is,https://github.com/hail-is/hail/pull/12107,1,['mainten'],['maintenance']
Availability,"t/issues/2324"">#2324</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; </ul>; <h2>pre-commit v2.18.0</h2>; <h3>Features</h3>; <ul>; <li>Keep <code>GIT_HTTP_PROXY_AUTHMETHOD</code> in git environ.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2272"">#2272</a> PR by <a href=""https://github.com/VincentBerthier""><code>@​VincentBerthier</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2271"">#2271</a> issue by <a href=""https://github.com/VincentBerthier""><code>@​VincentBerthier</code></a>.</li>; </ul>; </li>; <li>Support both <code>cs</code> and <code>coursier</code> executables for coursier hooks.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2293"">#2293</a> PR by <a href=""https://github.com/Holzhaus""><code>@​Holzhaus</code></a>.</li>; </ul>; </li>; <li>Include more information in errors for <code>language_version</code> / <code>additional_dependencies</code> for languages which do not support them.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2315"">#2315</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; <li>Have autoupdate preferentially pick tags which look like versions when there are multiple equivalent tags.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2312"">#2312</a> PR by <a href=""https://github.com/mblayman""><code>@​mblayman</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2311"">#2311</a> issue by <a href=""https://github.com/mblayman""><code>@​mblayman</code></a>.</li>; </ul>; </li>; <li>Upgrade <code>ruby-build</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2319"">#2319</a> PR by <a href=""https://github.com/jalessio""><code>@​jalessio</code></a>.</li>; </ul>; </",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11731:1447,error,errors,1447,https://hail.is,https://github.com/hail-is/hail/pull/11731,1,['error'],['errors']
Availability,"t: sigmaG2 = 0.13829390418697945; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: sigmaE2 = 0.8304138510277874; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: delta = 6.004703214575758; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: h2 = 0.1427612233333665; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: seH2 = 0.13770872661270844; 2017-08-28 21:47:48 Hail: INFO: Reading table with no type imputation; Loading column `Sample' as type `String' (type not specified); Loading column `Cov1' as type `Float64' (user-specified); Loading column `Cov2' as type `Float64' (user-specified). 2017-08-28 21:47:48 Hail: INFO: Reading table with no type imputation; Loading column `Sample' as type `String' (type not specified); Loading column `Pheno' as type `Float64' (user-specified). 2017-08-28 21:47:48 Hail: INFO: No multiallelics detected.; 2017-08-28 21:47:48 Hail: INFO: Coerced sorted dataset; 2017-08-28 21:47:48 Hail: WARN: called redundant `filtermulti' on an already split or multiallelic-filtered VDS; 2017-08-28 21:47:48 Hail: INFO: rrm: Computing Realized Relationship Matrix...; 2017-08-28 21:47:49 Hail: INFO: rrm: RRM computed using 3 variants.; 2017-08-28 21:47:49 Hail: WARN: 1 of 8 samples have a missing phenotype or covariate.; 2017-08-28 21:47:49 Hail: INFO: lmmreg: running lmmreg on 7 samples with 3 sample covariates including intercept...; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Computing eigendecomposition of kinship matrix...; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Estimating delta using REML... ; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Evals 1 to 7: 3.09757, 2.66667, 2.23576, 0.00000, 0.00000, -0.00000, -0.00000; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Evals 7 to 1: -0.00000, -0.00000, 0.00000, 0.00000, 2.23576, 2.66667, 3.09757; 2017-08-28 21:47:50 Hail: INFO: lmmreg: global model fit: beta = Map(intercept -> 0.48721539559123606, sa.cov.Cov1 -> 0.5924758486080468, sa.cov.Cov2 -> -0.23132255249379874); 2017-08-28 21:47:50 Ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835:5741,redundant,redundant,5741,https://hail.is,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835,1,['redundant'],['redundant']
Availability,"t; it properly due to <code>reasons listed in this issue &lt;https://github.com/urllib3/urllib3/issues/2282&gt;</code>_.; If you are a user of this module please leave a comment.</li>; <li>Changed <code>HTTPConnection.request_chunked()</code> to not erroneously emit multiple; <code>Transfer-Encoding</code> headers in the case that one is already specified.</li>; <li>Fixed typo in deprecation message to recommend <code>Retry.DEFAULT_ALLOWED_METHODS</code>.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/b1f60e44d43b13e5272d5b6003f125af9c25c8ad""><code>b1f60e4</code></a> Release 1.26.8</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/72e72b8ca8353318f60f418554c1150418bcb3a4""><code>72e72b8</code></a> [1.26] Add explicit support for Python 3.11 to packaging metadata</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/0435b0cb97a368e1641b623251a9ab5de4266027""><code>0435b0c</code></a> Wrap HTTPS/HTTP proxy mismatch error into ProxyError</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/25d04547377fbd4403acfbbe03dbc010585a1605""><code>25d0454</code></a> Revert commit bd816ee8 '[1.26] Add exception wrapper for HTTPS proxy connecti...</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/109b4456742ff8a4a6637cc2c7c45fa003147bf7""><code>109b445</code></a> Deprecate BACKOFF_MAX in favor of DEFAULT_BACKOFF_MAX</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/dc279bd9bf3b410f204e20b104392f0126f05389""><code>dc279bd</code></a> [1.26] Point setuptools to use standard library distutils as a fallback</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/dc1e68f544ea53d46b14756c5b76826101cfa1cb""><code>dc1e68f</code></a> [1.26] Upload coverage report if check failed</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/a472d6d7c64ce80fa89abbf1cc4ad491a4ea9819""><code>a472d6d</code></a> [1.26] Remove codecov in favor of GHA artifacts to ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11532:5962,error,error,5962,https://hail.is,https://github.com/hail-is/hail/pull/11532,1,['error'],['error']
Availability,"t@a49185acd9b0, date=20190525-12:28:13)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:324 host_details=""(Linux 4.14.127+ #1 SMP Tue Jun 18 18:32:10 PDT 2019 x86_64 prometheus-0 (none))""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:325 fd_limits=""(soft=1048576, hard=1048576)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:326 vm_limits=""(soft=unlimited, hard=unlimited)""; level=info ts=2019-07-31T15:45:51.993Z caller=main.go:645 msg=""Starting TSDB ...""; level=info ts=2019-07-31T15:45:51.994Z caller=web.go:417 component=web msg=""Start listening for connections"" address=0.0.0.0:9090; level=info ts=2019-07-31T15:45:51.996Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563105600000 maxt=1563170400000 ulid=01DFTDRJHCX1S9B0KPJTG8CRGW; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563170400000 maxt=1563235200000 ulid=01DFWBK0336Z71ZCRRKS79T18P; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563235200000 maxt=1563300000000 ulid=01DFY9C92NRA1S7FDVHFRFMFPF; level=info ts=2019-07-31T15:45:51.998Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:3743,repair,repair,3743,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['repair'],['repair']
Availability,"tCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). is.hail.utils.HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.stats.HistogramCombiner.merge(HistogramCombiner.scala:42); 	at is.hail.annotations.aggregators.RegionValueHistogramAggregator.seqOp(RegionValueHistogramAggregator.scala:31); 	at is.hail.codegen.generated.C95.apply(Unknown Source); 	at is.hail.codegen.generated.C95.apply(Unknown Source); 	at is.hail.expr.ir.Interpret$$anonfun$27.apply(Interpret.scala:809); 	at is.hail.expr.ir.Interpret$$anonfun$27.apply(Interpret.scala:808); 	at is.hail.rvd.RVD$$anonfun$20$$anonfun$apply$11.apply(RVD.scala:551); 	at is.hail.rvd.RVD$$anonfun$20$$anonfun$apply$11.apply(RVD.scala:550); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$20.apply(RVD.scala:550); 	at is.hail.rvd.RVD$$anonfun$20.apply(RVD.scala:547); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$appl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:10345,Error,ErrorHandling,10345,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['Error'],['ErrorHandling']
Availability,"t_dataproc. Cal also reported it.; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 56 in stage 4.0 failed 20 times, most recent failure: Lost task 56.19 in stage 4.0 (TID 48622) (jsealock-schema-sw-43bq.c.daly-neale-sczmeta.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 125; VEP Error output:; docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.; See 'docker run --help'. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.methods.VEP$.waitFor(VEP.scala:73); 	at is.hail.methods.VEP.$anonfun$execute$5(VEP.scala:231); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.hasNext(RichContextRDD.scala:69); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at is.h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:1173,Error,ErrorHandling,1173,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['Error'],['ErrorHandling']
Availability,"taFrameWriter.scala:494); at is.hail.variant.VariantDatasetFunctions$.write$extension(VariantDataset.scala:751); at is.hail.variant.VariantDatasetFunctions.write(VariantDataset.scala:721); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Job aborted due to stage failure: Task 754 in stage 1.0 failed 1 times, most recent failure: Lost task 754.0 in stage 1.0 (TID 1625, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.H",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2407:4187,failure,failure,4187,https://hail.is,https://github.com/hail-is/hail/issues/2407,1,['failure'],['failure']
Availability,"table.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```. 2. gs://hail-common/gencode_and_production_intervals.merged.hg19.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683:2211,Error,Error,2211,https://hail.is,https://github.com/hail-is/hail/issues/1683,1,['Error'],['Error']
Availability,"table.Table.aggregate(Table.scala:373); 	at is.hail.table.Table.aggregate(Table.scala:369); 	at is.hail.table.Table.aggregateJSON(Table.scala:364); 	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-414f3f183bd5; Error summary: RuntimeException: Class file too large!; ```; Code was:; ```; cutoff = 10. agg_expr = {; 'downsampling': hl.agg.collect(ht.downsamplings)[0]; }; locations = list(zip(('syn', 'mis', 'lof'), ('', '', '_classic_hc'))); agg_expr.update({; f'median_expected_{var}_{pop}': [hl.median(hl.agg.collect(ht[f'exp_{var}_{pop}{var_loc}'][i])) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'median_observed_{var}_{pop}': [hl.median(hl.agg.collect(ht[f'obs_{var}_{pop}{var_loc}'][i])) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'mean_expected_{var}_{pop}': [hl.agg.mean(ht[f'exp_{var}_{pop}{var_loc}'][i]) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'mean_observed_{var}_{pop}': [hl.agg.mean(ht[f'obs_{var}_{pop}{var_loc}'][i]) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'fraction_expected_{var}_{pop}': [hl.agg.fraction(ht[f'exp_{var}_{pop}{var_loc}'][i] > cutoff) for i in range(length)]; for length, pop in pop_len",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4516:1964,down,downsamplings,1964,https://hail.is,https://github.com/hail-is/hail/issues/4516,1,['down'],['downsamplings']
Availability,"table.py L433; ```; row = hl.bind(lambda v: self.key.annotate(**v), value_struct) if self.key else value_struct; ```. table.py L1612; ```; row_key = '[' + ', '.join(""'{name}'"".format(name=f) for f in self.key) + ']' \; if self.key else None; ```. I'm surprised these don't raise errors about the implicit __nonzero__ call, so I assume that Python is using the `Mapping` StructExpression superclass for __nonzero__ after hitting a `NotImplementedError`. Anyway, these should be fixed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3923:279,error,errors,279,https://hail.is,https://github.com/hail-is/hail/issues/3923,1,['error'],['errors']
Availability,table.show() errors when table is empty,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5173:13,error,errors,13,https://hail.is,https://github.com/hail-is/hail/issues/5173,1,['error'],['errors']
Availability,tablejoin require error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3346:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/issues/3346,1,['error'],['error']
Availability,"tadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^. Java stack trace:; is.hail.utils.HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2655:2461,Error,ErrorHandling,2461,https://hail.is,https://github.com/hail-is/hail/issues/2655,1,['Error'],['ErrorHandling']
Availability,"tails>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aioredis-py/blob/master/CHANGELOG.md"">aioredis's changelog</a>.</em></p>; <blockquote>; <h2>2.0.1 - (2021-12-20)</h2>; <h3>Features</h3>; <ul>; <li>Added Python 3.10 to CI &amp; Updated the Docs; (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1160"">#1160</a>)</li>; <li>Enable mypy in CI (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1101"">#1101</a>)</li>; <li>Synchronized reading the responses from a connection; (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1106"">#1106</a>)</li>; </ul>; <h3>Fixes</h3>; <ul>; <li>Remove <strong>del</strong> from Redis (Fixes <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1115"">#1115</a>); (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1227"">#1227</a>)</li>; <li>fix socket.error raises (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1129"">#1129</a>)</li>; <li>Fix buffer is closed error when using PythonParser class; (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1213"">#1213</a>)</li>; </ul>; <h2>2.0.0 - (2021-03-18)</h2>; <h3>Features</h3>; <ul>; <li>; <p>Port redis-py's client implementation to aioredis.<br />; (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/891"">#891</a>)</p>; </li>; <li>; <p>Make hiredis an optional dependency.<br />; (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/917"">#917</a>)</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/224f843bd4b33d657770bded6f86ce33b881257c""><code>224f843</code></a> Release version 2.0.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1247"">#1247</a>)</li>; <li><a href=""htt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11569:2936,error,error,2936,https://hail.is,https://github.com/hail-is/hail/pull/11569,1,['error'],['error']
Availability,take down jobs page,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4903:5,down,down,5,https://hail.is,https://github.com/hail-is/hail/issues/4903,1,['down'],['down']
Availability,"taproc:dataproc.logging.stackdriver.enable=true,dataproc:dataproc.monitoring.stackdriver.enable=true; ```; We are currently receiving a spark error when using this cluster for our larger dataset. ```; [Stage 10:=====> (69 + 656) / 729]; raise err; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 98, in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); File ""/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1304, in __call__; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 31, in deco; raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.project-.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.gbsc-project.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:1662,heartbeat,heartbeat,1662,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['heartbeat'],['heartbeat']
Availability,"tations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:7); at is.hail.backend.Backend.execute(Backend.scala:86); at is.hail.backend.Backend.executeJSON(Backend.scala:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.19-c6ec8b76eb26; Error summary: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the maximum allowaamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_1565788829616Exception.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOExcee.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBlockResolvt org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.NettyBloction.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamMt org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.TransportractChannelHandlerContext.java:362) at io.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:30238,Error,Error,30238,https://hail.is,https://github.com/hail-is/hail/issues/8106,2,"['Error', 'failure']","['Error', 'failure']"
Availability,"tbrot""><code>@​beatbrot</code></a> for spotting this, see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/205"">#205</a>)</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1b5d69760d19cb7f88cbc837ee46456c494c0696""><code>1b5d697</code></a> Bump up version number to 5.2.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/7d6de83037ca41cd2f2f31830b43e43720e45b3a""><code>7d6de83</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1da8f078e22412475b694ce07b890148b8a5e4fc""><code>1da8f07</code></a> Add comment</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/9703f764df56c52626f7d6f44bca8b1d51312389""><code>9703f76</code></a> Use pooling connection manager instead of basic one</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/306172e4c6532e185c8a6a9998bca7d22d2d0c63""><code>306172e</code></a> Bump up version number to 5.2.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b9df0c0daa080450772c365f16a9406fe0ca607a""><code>b9df0c0</code></a> Document eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/05a4433770f7020ff845add9348bdc12c82793dd""><code>05a4433</code></a> Add eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/09d1eca91afbf21ace3672be24c68d9028ee1e33""><code>09d1eca</code></a> Document runAsync method</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/800e3df1647c5ce65bffdd25c3240dfa5244e6c5""><code>800e3df</code></a> Add runAsync method to download extension</li>; <li><a href=""https://github.com/michel-kraem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:3556,down,download-task,3556,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['down'],['download-task']
Availability,"tch/worker/worker.py"", line 460, in timed_out_f; return await asyncio.wait_for(f(*args, **kwargs), timeout); File ""/usr/lib/python3.9/asyncio/tasks.py"", line 479, in wait_for; return fut.result(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 484, in _pull_with_auth_refresh; return await docker.images.pull(image_ref_str, auth=credentials); File ""/usr/local/lib/python3.9/dist-packages/aiodocker/images.py"", line 133, in _handle_list; async with cm as response:; File ""/usr/local/lib/python3.9/dist-packages/aiodocker/utils.py"", line 309, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.9/dist-packages/aiodocker/docker.py"", line 275, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(500, 'Head ""https://us-docker.pkg.dev/v2/1/does-not-exist/manifests/latest"": denied: Permission ""artifactregistry.repositories.downloadArtifacts"" denied on resource ""projects/1/locations/us/repositories/does-not-exist"" (or it may not exist)'). The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 915, in run; await self.create(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 840, in create; await self._run_until_done_or_deleted(self.image.pull); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1012, in _run_until_done_or_deleted; return await run_until_done_or_deleted(self.deleted_event, f, *args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 682, in run_until_done_or_deleted; return step.result(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 657, in pull; await asyncio.shield(self._localize_rootfs()); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 634, in _localize_rootfs; await self._pull_i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13907:1537,down,downloadArtifacts,1537,https://hail.is,https://github.com/hail-is/hail/issues/13907,1,['down'],['downloadArtifacts']
Availability,"td=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/FS.d -MT build/FS.o -c FS.cpp; g++ -fvisibility=default -rdynamic -shared -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/Na; tiveCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/FS.o -o lib/linux-x86-64/libhail.so; cp -p -f lib/linux-x86-64/libboot.so lib/linux-x86-64/libhail.so ../../../prebuilt/lib/linux-x86-64/; make[1]: Leaving directory `/mnt/tmp/hail/hail/src/main/c'; ./gradlew shadowJar -Dscala.version=2.12.15 -Dspark.version=3.3.2 -Delasticsearch.major-version=7; Downloading https://services.gradle.org/distributions/gradle-8.3-bin.zip; ............10%............20%.............30%............40%.............50%............60%.............70%............80%.............90%............100%. Welcome to Gradle 8.3!. Here are the highlights of this release:; - Faster Java compilation; - Reduced memory usage; - Support for running on Java 20. For more details see https://docs.gradle.org/8.3/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; WARNING: Hail primarily tested with Spark 3.3.0, use other versions at your own risk. > Task :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:15405,Down,Downloading,15405,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Down'],['Downloading']
Availability,"te explain samflags script to python3 (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1585"">#1585</a>); 4ba4c0678 Update to new version of the snappy library which will work with M1 macs (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1580"">#1580</a>); e92706452 add predicate to GFF3Codec to give a chance to filter out some unused attributes (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1575"">#1575</a>); c647764b0 Some long reads tests using PacBio data. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1564"">#1564</a>); 57c3f03eb remove hardcoded .idx (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1568"">#1568</a>); a94a32512 Add file extension to missing index error message <a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1512"">#1512</a> (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1567"">#1567</a>); 74b827b67 Improve error message in IntervalTree (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1545"">#1545</a>); 7719274fe Htsget POST request support (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1529"">#1529</a>)</p>; <p>VCF:; aac46ee6d Added GVCF mode for VariantContext type determination (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1544"">#1544</a>); d72d73b01 Add context to exception when the vcf file is invalid <a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1565"">#1565</a> (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1566"">#1566</a>); 8466c82dc Respect genotype filtering when calculating AC/AN/AF (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1554"">#1554</a>)</p>; <p>User API:; a4f1f04c8 Allow fluent chaining setters for SAMSequenceRecord (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1563"">#1563</a>)</p>; <!-- raw HTML omit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12229:4984,error,error,4984,https://hail.is,https://github.com/hail-is/hail/pull/12229,1,['error'],['error']
Availability,"te</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0f43ce67de72bd511d849c07bd7728c0d6f2e6dd""><code>0f43ce6</code></a> Document path and relativePath properties</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a8504f9d60d0264808894e4bb80d4a73b8086a3e""><code>a8504f9</code></a> Bump up version number to 5.3.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/708067cd11c4a013da7a8c15d91f7f946967cf94""><code>708067c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0fdebf3c7ad43ed4739d0400c333a72b32f5d514""><code>0fdebf3</code></a> Improve verify example</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/019089b9554692674d6baee7df7d4d884f310cc9""><code>019089b</code></a> Correctly create list of output files</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/fa2739ded05333ba46d8f50bb3b2a3721cf0ca86""><code>fa2739d</code></a> Create target directories at a central place</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/02b8e1a79d9e00acd61f9ac42e5555619fe2247a""><code>02b8e1a</code></a> Prevent duplicate destination files</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0b65ca2f17c8890a3ec34cf80cde52ee5413cbec""><code>0b65ca2</code></a> Call eachFile action only once per source</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/717877121299cea8f216d3a595eaa56731a6acd3""><code>7178771</code></a> Support changing a target file's relative path i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:3754,down,download-task,3754,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['down'],['download-task']
Availability,"te_rows(beta = hl.case(); .when(((variants.alleles[0] == variants.ss.nt1) &; (variants.alleles[1] == variants.ss.nt2)) | ; ((flip_text(variants.alleles[0]) == variants.ss.nt1) & ; (flip_text(variants.alleles[1]) == variants.ss.nt2)),; (-1*variants.ss.ldpred_inf_beta)); .when(((variants.alleles[0] == variants.ss.nt2) &; (variants.alleles[1] == variants.ss.nt1)) | ; ((flip_text(variants.alleles[0]) == variants.ss.nt2) & ; (flip_text(variants.alleles[1]) == variants.ss.nt1)),; variants.ss.ldpred_inf_beta); .or_missing()). variants = variants.filter_rows(hl.is_defined(variants.beta)); variants.beta.show(); ```. ### What went wrong (all error messages here, including the full java stack trace): When I went to try to show the beta column, Scala ""crashed"" such that I had to type in ""localhost:4040"" to reconnect and go into my application history to see what happened. I didn't get any errors in the Notebook I was using--it just stopped doing any work. . In the Scala tasks console, all of my workers had the following error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:975); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:964); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508:2936,error,error,2936,https://hail.is,https://github.com/hail-is/hail/issues/3508,1,['error'],['error']
Availability,"tebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-name; secret:; defaultMode: 420; secretName: gsa-key-j7gwm; - name: user-kmpnh-token-hbdd4; secret:; defaultMode: 420; secretName: user-kmpnh-token-hbdd4. hostIP: 10.128.0.32; phase: Running; podIP: 10.32.19.165; qosClass: Burstable; startTime: ""2019-04-02T19:50:21Z""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:3079,toler,tolerations,3079,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611,3,['toler'],"['tolerationSeconds', 'tolerations']"
Availability,"ted a fix so that cythonized functions can be decorated.; Brian McFee pointed out an issue in the <code>decorator_apply</code> example and; Wim Glenn pointed out that the &quot;fix&quot; in version 5.1 broke; <code>decorator.contextmanager</code> even more. Both issues are now solved.</p>; <h2>5.1.0 (2021-09-11)</h2>; <p>Added a function <code>decoratorx</code> using the <code>FunctionMaker</code> and thus; preserving the signature of <code>__code__</code> objects. Then fixed three small bugs:</p>; <ul>; <li>Sphinx was printing a few warnings when building the documentation, as; signaled by Tomasz Kłoczko</li>; <li>functions decorated with <code>decorator.contextmanager</code> were one-shot,; as discovered by Alex Pizarro.</li>; <li><code>decorator.decorator</code> was not passing the kwsyntax argument.</li>; </ul>; <h2>5.0.9 (2021-05-16)</h2>; <p>Fixed a test breaking PyPy. Restored support for Sphinx.</p>; <h2>5.0.8 (2021-05-15)</h2>; <p>Made the decorator module more robust when decorating builtin functions; lacking dunder attributes, like <code>dict.__setitem__</code>.</p>; <h2>5.0.7 (2021-04-14)</h2>; <p>The decorator module was not passing correctly the defaults inside the; <code>*args</code> tuple, thanks to Dan Shult for the fix. Also fixed some mispellings; in the documentation and integrated codespell in the CI, thanks to; Christian Clauss.</p>; <h2>5.0.6 (2021-04-08)</h2>; <p>The decorator module was not copying the <strong>module</strong> attribute anymore.; Thanks to Nikolay Markov for the notice.</p>; <h2>5.0.5 (2021-04-04)</h2>; <p>Dropped support for Python &lt; 3.5 with a substantial simplification of; the code base (now building a decorator does not require calling &quot;exec&quot;).; Added a way to mimic functools.wraps-generated decorators.; Ported the Continuous Integration from Travis to GitHub.</p>; <h2>4.4.2 (2020-02-29)</h2>; <p>Sylvan Mosberger (<a href=""https://github.com/Infinisil"">https://github.com/Infinisil</a>) contributed a patch to;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11490:1331,robust,robust,1331,https://hail.is,https://github.com/hail-is/hail/pull/11490,2,['robust'],['robust']
Availability,"tedFileSystemException: No FileSystem for scheme ""gs"". Java stack trace:; org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""gs""; 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.isDir(FS.scala:175); 	at is.hail.io.fs.FS.isDir$(FS.scala:173); 	at is.hail.io.fs.HadoopFS.isDir(HadoopFS.scala:70); 	at is.hail.expr.ir.RelationalSpec$.readMetadata(AbstractMatrixTableSpec.scala:30); 	at is.hail.expr.ir.RelationalSpec$.readReferences(AbstractMatrixTableSpec.scala:68); 	at is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:596); 	at is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.67-a673309b0445; Error summary: UnsupportedFileSystemException: No FileSystem for scheme ""gs""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10530:5132,Error,Error,5132,https://hail.is,https://github.com/hail-is/hail/issues/10530,1,['Error'],['Error']
Availability,"tempt to type everything, I discovered a really perplexing function `_add_resource_to_set`. I *was* able to give it a type but it was really complicated, involved `@overload`, and did not seem to help me reason about that function. I presumed the root cause was that the Resource class had an insufficient interface. Here is how I understand what that function was trying to achieve: We need to track the set of resources that are produced by, produced-by-and-later-consumed, and consumed by a job:. 1. ""Produced by"" corresponds to `Job._valid`.; 2. ""Produced-by-and-later-consumed"" corresponds to `Job._internal_outputs` and `Job._external_outputs`.; 3. ""Consumed by"" corresponds to `Job._inputs`. There is also `Job._mentioned` which I do not fully understand but which does not use `_add_resource_to_set`. There is an important distinction between the latter two and the first kind of resource set. The latter two must be what I am now calling `SingleResource`s. These are actual single files that need to be uploaded or downloaded. In contrast the ""produced by"" set (and, I think `Job._mentioned`) might include `ResourceGroup`s which are composite: one or more `SingleResource`s that must be transmitted as a group. For example, a VCF file and its TBI index file must always be transmitted as a group, even if a job only references one of those two files. That's the essential functionality of a `ResourceGroup`. I introduced three operations to `Resource` which I think make this system simpler:. 1. `get_resource_group`: if a `Resource` is a group or a member thereof, return it. 2. `component_resources`: if a `Resource` is composite, return the components, otherwise just return the resource itself. 3. `bonded_resources`: the minimum set of resources that must be transmitted if this resource is transmitted. In particular, a member of a resource group returns itself and the other resources in the group. A resource group just returns its components. We can now add a type annotation which",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14319:1183,down,downloaded,1183,https://hail.is,https://github.com/hail-is/hail/pull/14319,1,['down'],['downloaded']
Availability,"tension(RichIterator.scala:32); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:204); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:35); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:32); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:32); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:32); ```; and keeps going for a while. So maybe this Python thing is fixed, and we need a new ticket for this Java error?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5262#issuecomment-555090024:2491,error,error,2491,https://hail.is,https://github.com/hail-is/hail/issues/5262#issuecomment-555090024,1,['error'],['error']
Availability,terator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9148,Error,ErrorHandling,9148,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Error'],['ErrorHandling']
Availability,"tes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""1c6dbf20333a""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:4b4139c73fe9be3bee6c2895aa74059e157eb861d2bdac7d2304ba44b5421f88"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""labels"":map[""app"":""batch""] ""name"":""batch""] ""spec"":map[""ports"":[map[""port"":'P' ""protocol"":""TCP"" ""targetPort"":'\u1388']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; make: *** [deploy-batch] Error 1; Makefile:45: recipe for target 'deploy-batch' failed; ```; [deploy.log](https://github.com/hail-is/hail/files/2504429/deploy.log). Service accounts:; ```; error: the server doesn't have a resource type ""service-accounts""; # kubectl get serviceaccounts ; NAME SECRETS AGE; batch-svc 1 9h; default 1 113d; # kubectl get serviceaccounts -n batch-pods; NAME SECRETS AGE; default 1 7d; deploy-svc 1 1d; test-svc 1 1d; # kubectl get serviceaccounts -n test ; NAME SECRETS AGE; default 1 1d; ```. Apparently this is caused by a lack of permissions",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:5096,Error,Error,5096,https://hail.is,https://github.com/hail-is/hail/issues/4609,2,"['Error', 'error']","['Error', 'error']"
Availability,test against central1-b to ensure AVX2 instructions are available,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2164:56,avail,available,56,https://hail.is,https://github.com/hail-is/hail/pull/2164,1,['avail'],['available']
Availability,test against central1-b to ensure AVX2 instructions are available (0.1),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2165:56,avail,available,56,https://hail.is,https://github.com/hail-is/hail/pull/2165,1,['avail'],['available']
Availability,test failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8231#issuecomment-594308654:5,failure,failures,5,https://hail.is,https://github.com/hail-is/hail/pull/8231#issuecomment-594308654,3,['failure'],['failures']
Availability,"test failures related to locals, ugh.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8394#issuecomment-606100914:5,failure,failures,5,https://hail.is,https://github.com/hail-is/hail/pull/8394#issuecomment-606100914,1,['failure'],['failures']
Availability,test failures were caused by PType inference bugs that are now fixed,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8297#issuecomment-600361269:5,failure,failures,5,https://hail.is,https://github.com/hail-is/hail/pull/8297#issuecomment-600361269,1,['failure'],['failures']
Availability,"test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5087,down,downloads,5087,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,1,['down'],['downloads']
Availability,"test/commit/4a8f8ada431974f2837260af3ed36299fd382814""><code>4a8f8ad</code></a> build(deps): Bump django from 4.0.2 to 4.0.3 in /testing/plugins_integration ...</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/c0fd2d883940f1292d5e8234803beaacd08315e6""><code>c0fd2d8</code></a> build(deps): Bump pytest-asyncio from 0.18.1 to 0.18.2 in /testing/plugins_in...</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/843e01824c257c3190792a9df430289c3abe349d""><code>843e018</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9732"">#9732</a> from nicoddemus/9730-toml-failure</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/bc43d66b47b917d43a22e0c703ecfe4eea342263""><code>bc43d66</code></a> [automated] Update plugin list (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9733"">#9733</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/e38d1cac489e42f4bdbecbb50f9f25dc9c36c19f""><code>e38d1ca</code></a> Improve error message for malformed pyproject.toml files</li>; <li>Additional commits viewable in <a href=""https://github.com/pytest-dev/pytest/compare/6.2.5...7.1.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pytest&package-manager=pip&previous-version=6.2.5&new-version=7.1.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11571:6542,error,error,6542,https://hail.is,https://github.com/hail-is/hail/pull/11571,2,['error'],['error']
Availability,test/hail/methods/test_impex.py::BGENTests::test_import_bgen_no_entries FAILED. a deserialization error,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8282#issuecomment-598486137:98,error,error,98,https://hail.is,https://github.com/hail-is/hail/pull/8282#issuecomment-598486137,1,['error'],['error']
Availability,"test_ld_score_regression is failing now, caused by . ```scala; case GetTupleElement(o, idx) =>; infer(o); val t = coerce[PTuple](o.pType2); assert(idx >= 0 && idx < t.size); ```. in inferPType throwing. Will track down. I've seen this before, once. edit:. t.size 1, idx: 1; GetTupleElement(Ref(__iruid_28388,tuple(1:float64))),1)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586753124:214,down,down,214,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586753124,1,['down'],['down']
Availability,"testdir, 'example.sample'); # make index; self.hc.index_bgen(bgen_file); # load to vds; bgen_vds = self.hc.import_bgen(bgen_file, sample_file=sample_file); # export vcf; out_path = 'file://' + os.path.join(self.tmpdir, 'test_vcf_export.vcf.bgz'); > bgen_vds.export_vcf(out_path, export_pp=False, parallel=False). tests/hail/test_hail.py:55:; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; <decorator-gen-398>:2: in export_vcf; ???; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. func = <function export_vcf at 0x7fa13c4d9938>, args = (<hail.dataset.VariantDataset object at 0x7fa13c3c9390>, 'file:///scratch/test_vcf_export.vcf.bgz', None, False, False), kwargs = {}; e = Py4JJavaError(u'An error occurred while calling o160.exportVCF.\n', JavaObject id=o162), tpl = JavaObject id=o210; deepest = 'ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found'; full = 'java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.map...mmand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). '. @decorator; def handle_py4j(func, *args, **kwargs):; try:; r = func(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); E FatalError: ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E; E Java stack trace:; E java.lang.RuntimeException: java.lang.RuntimeEx",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:2633,error,error,2633,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['error'],['error']
Availability,"text_mt_path).drop('a_index', 'was_split'); context_mt = context_mt.annotate_rows(vep=context_mt.vep.drop('colocated_variants')); context_mt = hl.filter_intervals(context_mt, [hl.parse_locus_interval('1-22')]); ht.annotate(**context_mt[ht.key, :]).write(output_ht_path, overwrite); ```; gives:; ```; Java stack trace:; java.lang.ArrayIndexOutOfBoundsException: 1; at is.hail.annotations.Annotation$$anonfun$copy$1.apply(Annotation.scala:46); at is.hail.annotations.Annotation$$anonfun$copy$1.apply(Annotation.scala:46); at scala.Array$.tabulate(Array.scala:331); at is.hail.annotations.Annotation$.copy(Annotation.scala:46); at is.hail.rvd.OrderedRVDPartitioner.enlargeToRange(OrderedRVDPartitioner.scala:133); at is.hail.rvd.KeyedOrderedRVD.orderedJoin(KeyedOrderedRVD.scala:35); at is.hail.rvd.OrderedRVD.orderedJoin(OrderedRVD.scala:119); at is.hail.expr.TableJoin.execute(Relational.scala:1828); at is.hail.expr.TableMapRows.execute(Relational.scala:1859); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:465); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:15); at is.hail.table.Table.write(Table.scala:899); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: devel-ce257b532e2c; Error summary: ArrayIndexOutOfBoundsException: 1; ```; (this doesn't happen on master)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3637#issuecomment-392147738:2025,Error,Error,2025,https://hail.is,https://github.com/hail-is/hail/pull/3637#issuecomment-392147738,1,['Error'],['Error']
Availability,thank goodness for random batch failures!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6312#issuecomment-501000473:32,failure,failures,32,https://hail.is,https://github.com/hail-is/hail/pull/6312#issuecomment-501000473,1,['failure'],['failures']
Availability,"thank you very much for the reply,; when I upgrade the decorator from 3.4.0 to 4.1.2 , this error disappears：. ```; Installing collected packages: decorator; Found existing installation: decorator 3.4.0; Uninstalling decorator-3.4.0:; Successfully uninstalled decorator-3.4.0; Successfully installed decorator-4.1.2. ```. But there is another error, as follows：. ```; bash-4.2$ pyspark; WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).; WARNING: Running pyspark from user-defined location.; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.2.0.cloudera1; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = hail.Context(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; NameError: name 'hail' is not defined; >>> hc = hail.HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; NameError: name 'hail' is not defined; >>> hc = HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-470>"", line 2, in __init__; File ""/opt/Software/hail/python/hail/typecheck/check.py"", line 245, in _typecheck; return f(*args, **kwargs); File ""/opt/Software/hail/python/hail/context.py"", line 88, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); TypeError: 'JavaPackage' object is not callable; ```; My Java version; ```; [root@mg opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (bui",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337132579:92,error,error,92,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337132579,2,['error'],['error']
Availability,thanks. I had an initial version of the artifacts stuff using `.log` extensions but that wasn't enough! I hypothesized that .log was auto downloaded and never changed it back from no-ext. :(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4536#issuecomment-429328072:138,down,downloaded,138,https://hail.is,https://github.com/hail-is/hail/pull/4536#issuecomment-429328072,1,['down'],['downloaded']
Availability,that is a crazy error though,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4052#issuecomment-409700517:16,error,error,16,https://hail.is,https://github.com/hail-is/hail/pull/4052#issuecomment-409700517,1,['error'],['error']
Availability,that's what ~I suggested~ I almost suggested until I was shot down 😄,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5528#issuecomment-469475986:62,down,down,62,https://hail.is,https://github.com/hail-is/hail/pull/5528#issuecomment-469475986,1,['down'],['down']
Availability,"the browser, and nodejs, IO functions are (mostly?) asynchronous.; * For NodeJS: Transparently to the user, blocking operations (IO) are executed from kernel threads that Node maintains in the background, effectively making these operations non-blocking (until the thread pool is exhausted). Browsers and NodeJS use different event loops:. NodeJS: libuv event loop; * Node maintains a hidden worker thread pool (kernel threads) through which it issues sys calls, to avoid blocking the event loop. Web: depends on the underlying Javascript Engine; * Chromium: V8: libevent: https://stackoverflow.com/questions/25750884/are-there-significant-differences-between-the-chrome-browser-event-loop-versus-t; * Firefox: Spidermonkey: ?; * https://developer.mozilla.org/en-US/docs/Web/JavaScript/EventLoop#Event_loop. #### Using callbacks; ```js. # Callback-based; function asyncCall(arg, cb => {; const (err, result) = someSynchronousOperation();. cb(err, result);; }. asyncCall(arg,(r, err) => { if(err){ throw new Error(err); doSomething(r)} ); ```. #### Using async/await; Deeply nested callbacks are hard to follow. This is called ""callback hell"". To help combat this, JS, in both NodeJS and Web context, developed Promises. Promises flatten the callback tree. ```js; function asyncPromise(arg) {; return new Promise((resolve, reject) => {; const (err, result) = someSynchronousOperation();; ; if(err) {; reject(err);; return;; } ; ; resolve(result);; });; }. asyncPromise(arg).then( r => doSomething(r) ).catch( err => throw new Error(err) ); ```. This has one problem. Chaining promises leads to a potentially hard to follow chain of `.then` `.catch`. As in many other languages, the solution to ""transforming"" async call syntax to sync ones, is to color async functions with a ""async"" and ""await"" clauses. This can be used with any functions that return promises (but not those that just return a callback). Luckily again, JS libraries have been moving towards the Promise-land (sorry) for ~5 years, bef",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:5123,Error,Error,5123,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['Error'],['Error']
Availability,"the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; sphinx 5.3.0 has requirement docutils<0.20,>=0.14, but you have docutils 0.20.1.; sphinx-rtd-theme 1.3.0 has requirement docutils<0.19, but you have docutils 0.20.1. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **554/1000** <br/> **Why?** Has a fix available, CVSS 6.8 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CERTIFI-3164749](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-3164749) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![critical severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/c.png ""critical severity"") | **704/1000** <br/> **Why?** Has a fix available, CVSS 9.8 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **624/1000** <br/> **Why?** Has a fix available, CVSS 8.2 | Arbitrary Code Execution <br/>[SNYK-PYTHON-IPYTHON-2348630](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-2348630) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | No Known Exploit ;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14108:1074,avail,available,1074,https://hail.is,https://github.com/hail-is/hail/pull/14108,1,['avail'],['available']
Availability,"the error in #4529 was:; ```; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 11, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 76, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 237, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-nhxn5owt', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.jar,ZIP=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.zip', '--properties=spark:spark.driver.memory=41g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-init-docker.sh', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-highmem-8', '--",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,the error message? or what?. Can you write a docstring for that function?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1854#issuecomment-302792319:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/1854#issuecomment-302792319,1,['error'],['error']
Availability,"the following tests causes a segfault:. ```python; def test_agg_table_take(self):; ht = hl.utils.range_table(10).annotate(x = 'a'); ht.aggregate(agg.take(ht.x, 2)); ```. *only* as long as you run the test `test_init_hail_context_twice` in the same execution, i.e. ```; hail/python $ pytest -k 'test_init_hail_context_twice or test_agg_table_take'; platform darwin -- Python 3.6.0, pytest-4.5.0, py-1.8.0, pluggy-0.12.0; collected 653 items / 651 deselected / 2 selected ; test/hail/test_context.py . [ 50%]; test/hail/expr/test_expr.py F [100%]. .... # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ac8bbe2, pid=92110, tid=0x0000000000013d03; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 5335 C1 is.hail.annotations.Region$.loadInt(J)I (5 bytes) @ 0x000000010ac8bbe2 [0x000000010ac8bb40+0xa2]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid92110.log; Compiled method (c1) 4061 5335 3 is.hail.annotations.Region$::loadInt (5 bytes); total in heap [0x000000010ac8b9d0,0x000000010ac8bd78] = 936; relocation [0x000000010ac8baf8,0x000000010ac8bb28] = 48; main code [0x000000010ac8bb40,0x000000010ac8bc60] = 288; stub code [0x000000010ac8bc60,0x000000010ac8bcf0] = 144; oops [0x000000010ac8bcf0,0x000000010ac8bcf8] = 8; metadata [0x000000010ac8bcf8,0x000000010ac8bd08] = 16; scopes data [0x000000010ac8bd08,0x000000010ac8bd30] = 40; scopes pcs [0x000000010ac8bd30,0x000000010ac8bd70] = 64; dependencies [0x000000010ac8bd70,0x000000010ac8bd78] = 8; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp. ....; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6860:561,error,error,561,https://hail.is,https://github.com/hail-is/hail/issues/6860,2,['error'],['error']
Availability,"the gcloud command write to `stderr` which gets interpreted as errors in the logs and causes [a lot of noise](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aseverity%3DERROR%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22image-fetcher%22;timeRange=PT3H?project=hail-vdc&folder=true&organizationId=548622027621&query=%0A). . Added `gcr.io` because without it `configure-docker` would configure all the registries it can find, which is unnecessary. Also removed `-x` from the site script since each line of that goes to error logs and none of it is helpful. If errors occur they will show up in some other form.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9991:63,error,errors,63,https://hail.is,https://github.com/hail-is/hail/pull/9991,3,['error'],"['error', 'errors']"
Availability,the real error is in there at the top: ; ```; ExpressionException: Hail cannot impute the type of 'None'; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5700#issuecomment-478923407:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/issues/5700#issuecomment-478923407,1,['error'],['error']
Availability,then make sure the error says number of eigenvectors and not ``n_eigs``,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1929#issuecomment-313825166:19,error,error,19,https://hail.is,https://github.com/hail-is/hail/pull/1929#issuecomment-313825166,1,['error'],['error']
Availability,"things like VEP properties locations, hail launch script locations, available clusters, etc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/567:68,avail,available,68,https://hail.is,https://github.com/hail-is/hail/issues/567,1,['avail'],['available']
Availability,"this IR caused an error in the C++ emitter because `Streamify` ""skipped over"" the `ArraySort` and didn't streamify it properly (since ArraySort expects a stream as input, not an array); ```scala; ArrayMap(; ArraySort(; MakeArray(; Seq(I32(1), I32(2), I32(3)),; TArray(TInt32()))),; ""x"",; Ref(""x"", TInt32()) * 5); ```. this PR fixes this by hopefully making Streamify more straightforward",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6885:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/6885,1,['error'],['error']
Availability,"this error disappears：. ```; Installing collected packages: decorator; Found existing installation: decorator 3.4.0; Uninstalling decorator-3.4.0:; Successfully uninstalled decorator-3.4.0; Successfully installed decorator-4.1.2. ```. But there is another error, as follows：. ```; bash-4.2$ pyspark; WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).; WARNING: Running pyspark from user-defined location.; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.2.0.cloudera1; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = hail.Context(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; NameError: name 'hail' is not defined; >>> hc = hail.HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; NameError: name 'hail' is not defined; >>> hc = HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-470>"", line 2, in __init__; File ""/opt/Software/hail/python/hail/typecheck/check.py"", line 245, in _typecheck; return f(*args, **kwargs); File ""/opt/Software/hail/python/hail/context.py"", line 88, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); TypeError: 'JavaPackage' object is not callable; ```; My Java version; ```; [root@mg opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (build 1.8.0_91-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337132579:1116,avail,available,1116,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337132579,1,['avail'],['available']
Availability,"this is a python error, with analyze probably",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4110#issuecomment-411901041:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/issues/4110#issuecomment-411901041,1,['error'],['error']
Availability,"this is not a great error message. The problem is you're missing the lz4 dependency, see here:; https://hail.is/docs/0.2/getting_started.html?highlight=lz4#requirements",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7464#issuecomment-550295122:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/issues/7464#issuecomment-550295122,1,['error'],['error']
Availability,"this is now failing (previously passed, no issues on output) due to subsetTo assertion in master. edit: Strange the subsetTo commit was made 19 days ago. The assertion failure originates from subsetTo, but must have been caused by something else. Ah, my local master branch was old.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8008#issuecomment-582140545:168,failure,failure,168,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-582140545,1,['failure'],['failure']
Availability,this is ready for review. the tests failed spectacularly for reasons I don't understand with py4j errors. https://ci.hail.is/batches/7644244,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1638415522:98,error,errors,98,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1638415522,1,['error'],['errors']
Availability,"this small PR addresses #452 . alternatively, we could only have inParX and inParY, but this introduces redundant contig checks in two of three usages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/499:104,redundant,redundant,104,https://hail.is,https://github.com/hail-is/hail/pull/499,1,['redundant'],['redundant']
Availability,"this the exception (attached the log):. ``` ; File ""/tmp/7f8f775e-2ec3-40ee-ad5b-3e4df5649682/annotate_and_generate_scores_cloud.py"", line 24, in <module>; .vep(config='/vep/vep-gcloud.properties', root='va.vep', force=True); File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/dataset.py"", line 3095, in vep; File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/context.py"", line 81, in run_command; File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/java.py"", line 5, in jarray; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_collections.py"", line 228, in __setitem__; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_collections.py"", line 211, in __set_item; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 323, in get_return_value; py4j.protocol.Py4JError: An error occurred while calling None.None. Trace:; java.lang.IllegalArgumentException: array element type mismatch; at java.lang.reflect.Array.set(Native Method); at py4j.commands.ArrayCommand.setArray(ArrayCommand.java:141); at py4j.commands.ArrayCommand.execute(ArrayCommand.java:94); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745) ; ```. [hail.txt](https://github.com/hail-is/hail/files/725407/hail.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1284:874,error,error,874,https://hail.is,https://github.com/hail-is/hail/issues/1284,1,['error'],['error']
Availability,"thon/hail/backend/backend.py:188, in Backend.execute(self, ir, timed); 186 payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); 187 try:; --> 188 result, timings = self._rpc(ActionTag.EXECUTE, payload); 189 except FatalError as e:; 190 raise e.maybe_user_error(ir) from None. File ~/projects/hail/hail/python/hail/backend/py4j_backend.py:223, in Py4JBackend._rpc(self, action, payload); 221 if resp.status_code >= 400:; 222 error_json = orjson.loads(resp.content); --> 223 raise fatal_error_from_java_error_triplet(; 224 error_json['short'], error_json['expanded'], error_json['error_id']; 225 ); 226 return resp.content, resp.headers.get('X-Hail-Timings', ''). FatalError: NoSuchElementException: Ref with name __iruid_1834 could not be resolved in env BindingEnv((__iruid_1832 -> struct{},__iruid_2157 -> struct{}),None,None,()). Java stack trace:; is.hail.utils.HailException: error after applying LowerToDistributedArray; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:23); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:23); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:32); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:205); 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:249); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$2(LocalBackend.scala:314); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(Lo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245:3423,Error,ErrorHandling,3423,https://hail.is,https://github.com/hail-is/hail/issues/14245,1,['Error'],['ErrorHandling']
Availability,throw an error at large non-splittable files instead of trying to read on one core,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1173:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/issues/1173,1,['error'],['error']
Availability,throw different error in SparkBackend,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6053:16,error,error,16,https://hail.is,https://github.com/hail-is/hail/pull/6053,1,['error'],['error']
Availability,throws `Error summary: FileNotFoundException: Item not found` which leads to confusion when the file (directory) obviously exists,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5129:8,Error,Error,8,https://hail.is,https://github.com/hail-is/hail/issues/5129,1,['Error'],['Error']
Availability,"tic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html</a></p>; <h2>Elasticsearch Hadoop 8.2.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html</a></p>; <h2>Elasticsearch Hadoop 8.2.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html</a></p>; <h2>Elasticsearch Hadoop 8.1.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/a7873eb46985d849286ab89fa0a51f8b5374e02e""><code>a7873eb</code></a> Update index.adoc (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2000"">#2000</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2002"">#2002</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/9f44fe66d0ff82f18a13a38cae6abf3f72183a94""><code>9f44fe6</code></a> Bump to version 8.4.3</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/c9e3b114b98bb0e340555311c82e2d9f32c880b6""><code>c9e3b11</code></a> [DOCS] Add 8.4.2 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1998"">#1998</a>) (<a href=""https://github-redirect.de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:4298,down,downloads,4298,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"tic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html</a></p>; <h2>Elasticsearch Hadoop 8.2.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html</a></p>; <h2>Elasticsearch Hadoop 8.2.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html</a></p>; <h2>Elasticsearch Hadoop 8.1.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/a7873eb46985d849286ab89fa0a51f8b5374e02e""><code>a7873eb</code></a> Update index.adoc (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2000"">#2000</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2002"">#2002</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/9f44fe66d0ff82f18a13a38cae6abf3f72183a94""><code>9f44fe6</code></a> Bump t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:3972,down,downloads,3972,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"tic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html</a></p>; <h2>Elasticsearch Hadoop 8.2.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html</a></p>; <h2>Elasticsearch Hadoop 8.2.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html</a></p>; <h2>Elasticsearch Hadoop 8.1.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/a7873eb46985d849286ab89fa0a51f8b5374e02e""><code>a7873eb</code></a> Update index.adoc (<a href=""https://github-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:3646,down,downloads,3646,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"tic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html</a></p>; <h2>Elasticsearch Hadoop 8.2.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html</a></p>; <h2>Elasticsearch Hadoop 8.2.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html</a></p>; <h2>Elasticsearch Hadoop 8.1.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:3320,down,downloads,3320,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"tic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html</a></p>; <h2>Elasticsearch Hadoop 8.2.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html</a></p>; <h2>Elasticsearch Hadoop 8.2.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:2994,down,downloads,2994,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"tic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html</a></p>; <h2>Elasticsearch Hadoop 8.2.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:2668,down,downloads,2668,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"tic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:2342,down,downloads,2342,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"tic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:2016,down,downloads,2016,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"tic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html</a></p>; <h2>Elasticsearch Hadoop 8.4.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:1690,down,downloads,1690,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"tic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html</a></p>; <h2>Elasticsearch Hadoop 8.4.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html</a></p>; <h2>Elasticsearch Hadoop 8.4.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:1364,down,downloads,1364,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['down'],['downloads']
Availability,"tic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/da4f3c3f209aea47d69c4faf90029a6933bd3a60""><code>da4f3c3</code></a> [DOCS] Add 8.5.3 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2045"">#2045</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2050"">#2050</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/79d592abdce1cd90845d153afab8b66069e2a172""><code>79d592a</code></a> [DOCS] Add 8.5.2 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2042"">#2042</a>) (<a href=""https://github-redirect.dependabot.com/elas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12601:1690,down,downloads,1690,https://hail.is,https://github.com/hail-is/hail/pull/12601,1,['down'],['downloads']
Availability,"tic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/e665cd918aa8d0ba7806b92e16881edb96180d48""><code>e665cd9</code></a> Update Spark to 3.2.3 (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2056"">#2056</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2057"">#2057</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/07380b0e17c7d908d50d59fc69ac2953adfa5a0d""><code>07380b0</code></a> Use DRA repository for build-tools dependencies</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/77bce30bfefb39c39bd34a6f147b17fb0df4701c""><code>77bce30</code></a> Bump t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12623:2016,down,downloads,2016,https://hail.is,https://github.com/hail-is/hail/pull/12623,1,['down'],['downloads']
Availability,"tic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/da4f3c3f209aea47d69c4faf90029a6933bd3a60""><code>da4f3c3</code></a> [DOCS] Add 8.5.3 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2045"">#2045</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2050"">#2050</a>)</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12601:1364,down,downloads,1364,https://hail.is,https://github.com/hail-is/hail/pull/12601,1,['down'],['downloads']
Availability,"tic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/e665cd918aa8d0ba7806b92e16881edb96180d48""><code>e665cd9</code></a> Update Spark to 3.2.3 (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2056"">#2056</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2057"">#2057</a>)</li>; <li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12623:1690,down,downloads,1690,https://hail.is,https://github.com/hail-is/hail/pull/12623,1,['down'],['downloads']
Availability,"tic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.1.html</a></p>; <h2>Elasticsearch Hadoop 8.6.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elast",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12623:1364,down,downloads,1364,https://hail.is,https://github.com/hail-is/hail/pull/12623,1,['down'],['downloads']
Availability,"ticsearch-spark-20_2.12](https://github.com/elastic/elasticsearch-hadoop) from 8.4.3 to 8.6.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-20_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.6.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12601:999,Down,Downloads,999,https://hail.is,https://github.com/hail-is/hail/pull/12601,1,['Down'],['Downloads']
Availability,"ticsearch-spark-20_2.12](https://github.com/elastic/elasticsearch-hadoop) from 8.4.3 to 8.6.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-20_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.6.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.1.html</a></p>; <h2>Elasticsearch Hadoop 8.6.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12623:999,Down,Downloads,999,https://hail.is,https://github.com/hail-is/hail/pull/12623,1,['Down'],['Downloads']
Availability,"ticsearch-spark-30_2.12](https://github.com/elastic/elasticsearch-hadoop) from 8.0.0 to 8.4.3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-30_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.4.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html</a></p>; <h2>Elasticsearch Hadoop 8.4.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html</a></p>; <h2>Elasticsearch Hadoop 8.4.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:999,Down,Downloads,999,https://hail.is,https://github.com/hail-is/hail/pull/12319,1,['Down'],['Downloads']
Availability,til.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)sun.reflect.generics.reflectiveObjects.NotImplementedException: null; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Hail version: devel-438801a84105; Error summary: NotImplementedException: null; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:8179,Error,Error,8179,https://hail.is,https://github.com/hail-is/hail/issues/4215,1,['Error'],['Error']
Availability,tils$RichHadoopConfiguration$$using$extension(RichHadoopConfiguration.scala:226); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:251); 	at is.hail.variant.VariantSampleMatrix$.readFileMetadata(VariantSampleMatrix.scala:72); 	at is.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:51); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:434); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:433); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:433); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-41347fd; Error summary: HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2173:2659,Error,Error,2659,https://hail.is,https://github.com/hail-is/hail/pull/2173,1,['Error'],['Error']
Availability,"tils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); 	at is.hail.backend.local.LocalBackend.withExecuteContext(LocalBackend.scala:130); 	at is.hail.backend.local.LocalBackend.execute(LocalBackend.scala:308); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88); 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); 	at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83); 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82); 	at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:822); 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); 	at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:794); 	at sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:199); 	at sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:544); 	at sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:509); 	at java.lang.Thread.run(Thread.java:750). is.hail.utils.HailException: Error while typechecking IR:. %1 = Literal [Struct{}, <literal value>]; !2 = I64 [0] ; !3 = Str [""/tmp/__iru...""]; !4 = MakeStruct(partitionIndex: !2, partitionPath: !3); !s = ReadPartition(!4) [Struct{}, ""{\""name\"":\""PartitionNativeReader\"",\""spec\"":{\""name\"":\""TypedCodecSpec\"",\""_eType\"":\""+EBaseStruct{}\"",\""_vType\"":\""Struct{}\"",\""_bufferSpec\"":{\""name\"":\""LEB128BufferSpec\"",\""child\"":{\""name\"":\""BlockingBufferSpec\"",\""blockSize\"":65536,\""child\"":{\""name\"":\""ZstdBlockBufferSpec\"",\""blockSize\"":65536,\""child\"":{\""name\"":\""StreamBlockBufferSpec\""}}}}},\""uidFieldName\"":\""__row_uid\""}""]; !5 = ToArray(!s) ; !c0 = I32 [0]; %6 = ArrayRef(!5, !c0) [-1]; !7 = MakeArray(%1, %undefined_ref) [Array[Struct{}]]; %8 = MakeStruct(new_globals: !7); !9 = Literal [Array[Struct{partitionIndex:Int32,fileNum:Int32,file:String,start:Int64,end:Int64,split:Boolean}], <literal value>]; !s2 = ToStream(!9) [False]; %10 = ToArray(!s2); !11 = Literal [Array[Array[Int32]], <literal value>]; !s3 = ToStream(!11) [False]; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245:6106,Error,Error,6106,https://hail.is,https://github.com/hail-is/hail/issues/14245,1,['Error'],['Error']
Availability,tils/__init__.py'; adding 'hailtop/utils/filesize.py'; adding 'hailtop/utils/process.py'; adding 'hailtop/utils/rate_limiter.py'; adding 'hailtop/utils/rates.py'; adding 'hailtop/utils/rich_progress_bar.py'; adding 'hailtop/utils/serialization.py'; adding 'hailtop/utils/time.py'; adding 'hailtop/utils/utils.py'; adding 'hailtop/utils/validate/__init__.py'; adding 'hailtop/utils/validate/validate.py'; adding 'hail-0.2.124.dist-info/METADATA'; adding 'hail-0.2.124.dist-info/WHEEL'; adding 'hail-0.2.124.dist-info/entry_points.txt'; adding 'hail-0.2.124.dist-info/top_level.txt'; adding 'hail-0.2.124.dist-info/RECORD'; emoving build/bdist.linux-x86_64/wheel; python3 -m pip install 'pip-tools==6.13.0' && bash ../check_pip_requirements.sh python; Defaulting to user installation because normal site-packages is not writeable; Collecting pip-tools==6.13.0; Downloading pip_tools-6.13.0-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.2/53.2 kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:29452,Down,Downloading,29452,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Down'],['Downloading']
Availability,time to fix input error handling,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/96:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/issues/96,1,['error'],['error']
Availability,"timed); 186 payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); 187 try:; --> 188 result, timings = self._rpc(ActionTag.EXECUTE, payload); 189 except FatalError as e:; 190 raise e.maybe_user_error(ir) from None. File ~/projects/hail/hail/python/hail/backend/py4j_backend.py:223, in Py4JBackend._rpc(self, action, payload); 221 if resp.status_code >= 400:; 222 error_json = orjson.loads(resp.content); --> 223 raise fatal_error_from_java_error_triplet(; 224 error_json['short'], error_json['expanded'], error_json['error_id']; 225 ); 226 return resp.content, resp.headers.get('X-Hail-Timings', ''). FatalError: NoSuchElementException: Ref with name __iruid_1834 could not be resolved in env BindingEnv((__iruid_1832 -> struct{},__iruid_2157 -> struct{}),None,None,()). Java stack trace:; is.hail.utils.HailException: error after applying LowerToDistributedArray; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:23); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:23); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:32); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:205); 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:249); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$2(LocalBackend.scala:314); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(LocalBackend.scala:309); 	at is.hail.backend.local.LocalBackend.$a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245:3487,Error,ErrorHandling,3487,https://hail.is,https://github.com/hail-is/hail/issues/14245,1,['Error'],['ErrorHandling']
Availability,"timed); 97 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 98 try:; ---> 99 result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); 100 (result, timings) = (result_tuple._1(), result_tuple._2()); 101 value = ir.typ._from_encoding(result). /opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1321 answer = self.gateway_client.send_command(command); 1322 return_value = get_return_value(; -> 1323 answer, self.gateway_client, self.target_id, self.name); 1324 ; 1325 for temp_arg in temp_args:. /opt/conda/lib/python3.7/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 29 tpl = Env.jutils().handleForPython(e.java_exception); 30 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 31 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 32 except pyspark.sql.utils.CapturedException as e:; 33 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: co",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:3744,failure,failure,3744,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['failure'],['failure']
Availability,"ting a random vector `v`, and using `E(v' (A'A)^p v) = tr((A'A)^p) = 𝜇_p`. The estimator can be computed exactly using the Krylov factorization as above, i.e. `v' (A'A)^p v = V_1[0, :] S^{2p} V_1[0, :]'`. But this estimator has large variance, so we can just average over many independent estimators. We combine `k` random vectors into a random matrix `V_0`, compute `_krylov_factorization(A, V_0, p)`, and then `V'_0 (A'A)^p V_0 = V_1[:k, :] f(S^2) V_1[:k, :]'`, from which we can average the individual estimates. This spectral moments estimator is implemented in `KrylovFactorization.spectral_moments`. # SVD and moments; Finally, in practice we may need to average over too many estimators to get the desired accuracy, so we use one more trick to bring down the variance. This depends on the following property:; * If `Q` is an orthonormal matrix, then `tr(X) = tr(QQ'XQQ') + tr((I-QQ')X(I-QQ'))`, i.e. the trace decomposes into a sum of the traces on the projection onto the subspace spanned by `Q`, and the projection onto its complement. To see this, use the additivity and cyclicity of the trace (`tr(X+Y) = tr(X) + tr(Y)`, `tr(XY) = tr(YX)`). In particular, cyclicity implies `tr(QQ'XQQ') = tr(QQ'X) = tr(XQQ')`. Now we do two passes. In the first pass, we use some method (such as a Krylov factorization) to compute an orthonormal basis `V` for a subspace which approximates the dominant subspace. Even if this is not a very accurate approximation, the result is that `tr(QQ' X^p QQ')` is much larger than the complement `tr((I-QQ')X^p(I-QQ'))`. But the former we can compute exactly, and all the error comes from our estimate of the much smaller piece. This variance reduction trick is implemented in `_pca_and_moments`, which computes two Krylov factorizations. The first is used to compute the principal components, and the `V` factor is also used to decompose the moments estimator as above. The second factorization is used to estimate the remaining component of the moments estimator.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11045:4174,error,error,4174,https://hail.is,https://github.com/hail-is/hail/pull/11045,1,['error'],['error']
Availability,ting rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:234); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(OrderedRDD.scala:202); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(OrderedRDD.scala:195); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2407:5309,Error,ErrorHandling,5309,https://hail.is,https://github.com/hail-is/hail/issues/2407,1,['Error'],['ErrorHandling']
Availability,"tion files are now prevented. Specifying a duplicate destination file (e.g. in an <code>eachFile</code> action) will lead to an exception being thrown.</li>; </ul>; <p>Bug fixes:</p>; <ul>; <li>Call <code>eachFile</code> action only once per source</li>; <li>Correctly create list of output files (even if the destination is the project's build directory)</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Use pooling connection manager of Apache HttpClient instead of basic one. The basic one is not meant to be used by multiple threads. This fixes an issue that could cause an <code>IllegalStateException</code> with the message <code>Connection is still allocated</code>. Thanks to <a href=""https://github.com/dmarks2""><code>@​dmarks2</code></a> for spotting this.</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>eachFile</code> method that adds an action to be applied to each source URL before it is downloaded. The action can be used to modify the filename of the target file.</li>; <li>Add <code>runAsync</code> method to download extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:1661,down,downloaded,1661,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['down'],['downloaded']
Availability,"tion.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-a1d6ecc; Error summary: NoSuchElementException: key not found: GT; ```; The file has `GT` in the format field, but it's missing the corresponding header line. Passing a custom `header_file=` fixes the problem, but it's unfortunate that that's required (especially on such a widely used publicly available dataset).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:13960,Error,Error,13960,https://hail.is,https://github.com/hail-is/hail/issues/3467,2,"['Error', 'avail']","['Error', 'available']"
Availability,"tion</h3>; <ul>; <li>Do not accept bare carriage return line endings in pyproject.toml (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2408"">#2408</a>)</li>; <li>Add configuration option (<code>python-cell-magics</code>) to format cells with custom magics in Jupyter Notebooks (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2744"">#2744</a>)</li>; <li>Allow setting custom cache directory on all platforms with environment variable <code>BLACK_CACHE_DIR</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2739"">#2739</a>).</li>; <li>Enable Python 3.10+ by default, without any extra need to specify -<code>-target-version=py310</code>. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2758"">#2758</a>)</li>; <li>Make passing <code>SRC</code> or <code>--code</code> mandatory and mutually exclusive (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2804"">#2804</a>)</li>; </ul>; <h3>Output</h3>; <ul>; <li>Improve error message for invalid regular expression (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2678"">#2678</a>)</li>; <li>Improve error message when parsing fails during AST safety check by embedding the underlying SyntaxError (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2693"">#2693</a>)</li>; <li>No longer color diff headers white as it's unreadable in light themed terminals (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2691"">#2691</a>)</li>; <li>Text coloring added in the final statistics (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2712"">#2712</a>)</li>; <li>Verbose mode also now describes how a project root was discovered and which paths will be formatted. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2526"">#2526</a>)</li>; </ul>; <h3>Packaging</h3>; <ul>; <li>All upper version bounds on dependencies have been removed (<a href=""https://github-redirect.dependabot.com/psf/black/issues",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11468:5551,error,error,5551,https://hail.is,https://github.com/hail-is/hail/pull/11468,1,['error'],['error']
Availability,tionFailedException(TestNGSuite.scala:67); 	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); 	at is.hail.io.fs.FSSuite.largeDirectoryOperations(FSSuite.scala:413); 	at is.hail.io.fs.FSSuite.largeDirectoryOperations$(FSSuite.scala:398); 	at is.hail.io.fs.GoogleStorageFSSuite.largeDirectoryOperations(GoogleStorageFSSuite.scala:10); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333). test is.hail.io.fs.GoogleStorageFSSuite.largeDirectoryOperations FAILURE; ```. ### Version. 0.2.124. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13827:2596,FAILURE,FAILURE,2596,https://hail.is,https://github.com/hail-is/hail/issues/13827,1,['FAILURE'],['FAILURE']
Availability,tive_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$16(BackendUtils.scala:91); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90); 	at is.hail.backend.service.Worker$.$anonfun$main$12(Worker.scala:167); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$11(Worker.scala:166); 	at is.hail.backend.service.Worker$.$anonfun$main$11$adapted(Worker.scala:164); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.backend.service.Worker$.main(Worker.scala:164); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	... 11 more. Logs; Main; Log ; 2023-09-24 17:23:30.055 JVMEntryway: ERROR: Exception encountered in QoB cancel thread.; org.newsclub.net.unix.SocketClosedException: Not open; 	at org.newsclub.net.unix.AFCore.validFdOrException(AFCore.java:90) ~[jvm-entryway.jar:?]; 	at org.newsclub.net.unix.AFSocketImpl$AFInputStreamImpl.read(AFSocketImpl.java:510) ~[jvm-entryway.jar:?]; 	at java.io.DataInputStream.readInt(DataInputStream.java:388) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$2.run(JVMEntryway.java:136) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888:4453,ERROR,ERROR,4453,https://hail.is,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888,1,['ERROR'],['ERROR']
Availability,"tm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""49ddea8a-962c-4889-b800-3d64b82a0b38"",""prPublicId"":""49ddea8a-962c-4889-b800-3d64b82a0b38"",""dependencies"":[{""name"":""aiohttp"",""from"":""3.8.6"",""to"":""3.9.0""},{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.11.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-AIOHTTP-6091621"",""SNYK-PYTHON-AIOHTTP-6091622"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-JUPYTERSERVER-6099119"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[591,591,531,444,429,501,509,384,494,539],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Improper Input Validation](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14109:7761,avail,available,7761,https://hail.is,https://github.com/hail-is/hail/pull/14109,1,['avail'],['available']
Availability,"tml#reactmemo; 2. https://scotch.io/tutorials/react-166-reactmemo-for-functional-components-rendering-control. ### Typescript; 2. https://reactjs.org/docs/react-api.html#reactmemo. #### And React Component prop definitions; https://levelup.gitconnected.com/ultimate-react-component-patterns-with-typescript-2-8-82990c516935. ### NextJS; https://nextjs.org/docs/; Next has 4 deviations from normal react:; 1) _app.js: Can be omitted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />; <Component {...pageProps} />; <Footer />; </Container>; ```. 2) _document.js: Optional. Rendered only on the server, exactly one time. Wraps _app. Good place to define external resource you want to load, such as some external stylesheet, font, whatever. . 3) `getInitialProps`: a lifecycle method that is only available to components in the `pages/` folder. `getInitialProps ` runs once during server-side rendering, and again if you navigate to the page that defines it. Only components in pages can specify this property. This is because it is effectively a function triggered during routing and:; * `getInitialProps` is of course only available if you define a stateful component. See [functional components (just JSX wrapped in a function, rather than a class)](https://reactjs.org/docs/components-and-props.html). 4) NextJS includes a light, fast router. Routes are matched based on the names of files in `pages/`, with index.js mapping to `/`. For instance, to navigate to `domain.com/scorecard/users`, you'd make the folder structure:. pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal react components, except they expose `getInitialProps`, described above. Each page file must export 1 default component:. ```js; #Page file; import React from 'react",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:11745,avail,available,11745,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['avail'],['available']
Availability,"tnb/ukbiobank/ad/analysis/liftover/liftover.py"", line 29, in <module>; hl.export_vcf(mt,""/project/ukbiobank/imp/uk.v3.GRCh38/uk.v3.r38.chr""+chr+"".vcf.bgz""); File ""</share/pkg.7/hail/0.2.19/install/python/lib/python3.6/site-packages/decorator.py:decorator-gen-1289>"", line 2, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 585, in wrapper; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 513, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 108, in execute; File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 221, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the mmChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_ion(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrow.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBloa:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.Na.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOn) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.Read(AbstractChannelHandlerContext.java:362) at io.netty.channel.Abstra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:3395,failure,failure,3395,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['failure'],['failure']
Availability,"to an appropriate level. This issue manifests in a variety of inscrutable ways including RemoteDisconnectedError and socket closed. See issue #13960 for details. In Dataproc versions 1.5.74, 2.0.48, and 2.1.0, Dataproc introduced [""memory protection""](https://cloud.google.com/dataproc/docs/support/troubleshoot-oom-errors#memory_protection) which is a euphemism for a newly aggressive OOMKiller. When the OOMKiller kills the JVM driver process, there is no hs_err_pid...log file, no exceptional log statements, and no clean shutdown of any sockets. The process is simply SIGTERM'ed and then SIGKILL'ed. From Hail 0.2.83 through Hail 0.2.109 (released February 2023), Hail was pinned to Dataproc 2.0.44. From Hail 0.2.15 onwards, `hailctl dataproc`, by default, reserves 80% of the advertised memory of the driver node for the use of the Hail Query Driver JVM process. For example, Google advertises that an n1-highmem-8 has 52 GiB of RAM, so Hail sets the `spark:spark.driver.memory` property to 41g (we always round down). Before aggressive memory protection, this setting was sufficient to protect the driver from starving itself of memory. Unfortunately, Hail 0.2.110 upgraded to Dataproc 2.1.2 which enabled ""memory protection"". Moreover, in the years since Hail 0.2.15, the memory in use by system processes on Dataproc driver nodes appears to have increased. Due to these two circumstances, the driver VM's memory usage can grow high enough to trigger the OOMKiller before the JVM triggers a GC. Consider, for example, these slices of the syslog of the n1-highmem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14066:1131,down,down,1131,https://hail.is,https://github.com/hail-is/hail/pull/14066,1,['down'],['down']
Availability,"to answer (1), I think the right thing is for us to implement our own hashable immutable data structures (and use frozenset for sets, for instance) for results of Hail computations. I think we have yet to nail down whether this would be a breaking interface change, forcing us to wait until 0.3. To answer (2), you *may* be able to do `hl.stop(); hl.init()` to reset the session, but not sure this will work in every case. The driver should really only die for OOM and faults, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8076#issuecomment-584795090:210,down,down,210,https://hail.is,https://github.com/hail-is/hail/issues/8076#issuecomment-584795090,2,"['down', 'fault']","['down', 'faults']"
Availability,"to complete before it starts the next iteration of the scheduler. This leaves the scheduler vulnerable to problematic workers or workers that happen to be preempted during the scheduling process. So, the driver sets a [2 second timeout](https://github.com/hail-is/hail/blob/b27737f67bf9e69f1abed2fec07fc7c921790ef8/batch/batch/driver/job.py#L585) on the call to `/api/v1alpha/batches/jobs/create`. Additionally, this general design means that in the event of a request timeout or transient error, Batch cannot guarantee that there is always at most one concurrent running attempt for a given job. This ends up being a fine (and intentional) concession in practice because the idempotent design of preemptible jobs tends to cover this scenario, but it is regardless wasted compute and cost to users. Nevertheless, we strive to minimize cases where we might halt the scheduling loop or double-schedule work, and one way to do that in the current design is to minimize the variance in latency of `/api/v1alpha/batches/jobs/create`. The largest source of this latency is the request to blob storage. While GCS and ABS are relatively fast and highly available, Batch in Azure Terra requires first obtaining SAS tokens from the Terra control plane, which can introduce much higher and more variable latency. There have also been occurrences in the past of corrupted or deleted specs, which introduce unexpected failure modes that should error the job but instead disrupt the scheduling loop. Many of these problems would be mitigated by moving the read from object storage outside of the `/api/v1alpha/batches/jobs/create` endpoint. The endpoint should push this read into the asynchronous task that ultimately runs the job and therefore return its acknowledgement to the driver faster. If the worker encounters errors later on while reading the spec, those should result in `error`ing the job instead of raising a 500 in the scheduling request. ### Version. 0.2.129. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14456:2090,avail,available,2090,https://hail.is,https://github.com/hail-is/hail/issues/14456,5,"['avail', 'error', 'failure']","['available', 'error', 'errors', 'failure']"
Availability,"to elaborate, I'm seeing this test case hit the same error:; ```; t = hl.utils.range_table(10).annotate(**{f'f{i}': 0 for i in range(1300)}); mt = hl.utils.range_matrix_table(10, 10). mt.annotate_cols(foo=t[mt.col_idx])._force_count_rows(); ```. with the following stack trace: ; ```; 2019-10-28 17:15:08 root: ERROR: Verify Output 1 for is/hail/codegen/generated/C_etypeDecode_9:; 2019-10-28 17:15:08 root: ERROR: RuntimeException: Method code too large!; From java.lang.RuntimeException: Method code too large!; 	at is.hail.relocated.org.objectweb.asm.MethodWriter.a(Unknown Source); 	at is.hail.relocated.org.objectweb.asm.ClassWriter.toByteArray(Unknown Source); 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:333); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:372); 	at is.hail.expr.types.encoded.EType$.buildDecoder(EType.scala:199); 	at is.hail.io.TypedCodecSpec.buildDecoder(RowStore.scala:36); 	at is.hail.rvd.RVD.collect(RVD.scala:694); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:750); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:90); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:59); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:1125); 	at is.hail.expr.ir.Interpr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7396#issuecomment-547153723:53,error,error,53,https://hail.is,https://github.com/hail-is/hail/issues/7396#issuecomment-547153723,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; sphinx 5.3.0 has requirement docutils<0.20,>=0.14, but you have docutils 0.20.1.; sphinx-rtd-theme 1.3.0 has requirement docutils<0.19, but you have docutils 0.20.1.; notebook 6.5.6 has requirement pyzmq<25,>=17, but you have pyzmq 25.1.2. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14244:1142,avail,available,1142,https://hail.is,https://github.com/hail-is/hail/pull/14244,1,['avail'],['available']
Availability,"to replicate:; ```python; In [2]: mt = hl.import_vcf('src/test/resources/sample.vcf'); In [3]: mt.aggregate_entries(hl.agg.counter(hl.agg.explode(mt.PL))); ```; ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.NullPointerException; 	at is.hail.codegen.generated.C1.apply(Unknown Source); 	at is.hail.codegen.generated.C1.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:270); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:268); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:304); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:300); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1743); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1741); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at is.hail.annotations.UnsafeIndexedSeq.foreach(UnsafeRow.scala:51); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1741); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1734); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1734); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1728); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:79",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3276:243,failure,failure,243,https://hail.is,https://github.com/hail-is/hail/issues/3276,2,['failure'],['failure']
Availability,to reproduce:. ```; import hail as hl; t = hl.utils.range_table(10); t = t.annotate(**{f'f{i}': 0 for i in range(1300)}); t.write('foo.ht'); hl.read_table('foo.ht')._force_count(); ```. MethodCodeTooLarge error is thrown inside the decoder.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7396:205,error,error,205,https://hail.is,https://github.com/hail-is/hail/issues/7396,1,['error'],['error']
Availability,"to reproduce:. ```; l = hl.Locus('1', 1); rows = [; 	hl.Struct(locus=l, alleles=[""A"", ""T""]),; 	hl.Struct(locus=l, alleles=[""T"", ""T""]),; 	hl.Struct(locus=l, alleles=[""A"", ""CC"", ""TT""])]. t = hl.Table.parallelize(rows, hl.tstruct(locus=hl.tlocus(), alleles=hl.tarray(hl.tstr)), ['locus', 'alleles']); split = hl.split_multi(t); print(split.collect()); ```; should error, but does not. Writing/reading/showing `split` should also error, but instead prints:; ```; 2019-06-04 18:55:11 Hail: INFO: wrote table with 4 rows in 1 partition to foo; +---------------+------------+---------+-----------+---------------+-----------------+; | locus | alleles | a_index | was_split | old_locus | old_alleles |; +---------------+------------+---------+-----------+---------------+-----------------+; | locus<GRCh37> | array<str> | int32 | bool | locus<GRCh37> | array<str> |; +---------------+------------+---------+-----------+---------------+-----------------+; | 1:1 | [""A"",""T""] | 1 | false | 1:1 | [""A"",""T""] |; | 1:1 | [""T"",""T""] | 1 | false | 1:1 | [""T"",""T""] |; | 1:1 | [""A"",""CC""] | 1 | true | 1:1 | [""A"",""CC"",""TT""] |; | 1:1 | [""A"",""TT""] | 2 | true | 1:1 | [""A"",""CC"",""TT""] |; +---------------+------------+---------+-----------+---------------+-----------------+; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6223#issuecomment-498872887:361,error,error,361,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-498872887,2,['error'],['error']
Availability,to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:3113,echo,echo,3113,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['echo'],['echo']
Availability,to_expr should check integer values and use long / error appropriately,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2745:51,error,error,51,https://hail.is,https://github.com/hail-is/hail/issues/2745,1,['error'],['error']
Availability,"to_java_table_ir(t._tir))); 286 ; 287 def unpersist_table(self, t):. /fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 14 import pyspark; 15 try:; ---> 16 return f(*args, **kwargs); 17 except py4j.protocol.Py4JJavaError as e:; 18 s = e.java_exception.toString(). /fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 334 raise Py4JError(; 335 ""An error occurred while calling {0}{1}{2}"".; --> 336 format(target_id, ""."", name)); 337 else:; 338 type = answer[1]. Py4JError: An error occurred while calling o1.pyPersistTable; --------------- end of error -------------------. The above is for running the GWAS tutorial. While running my own GWAS, I have encountered a similar error when running GWAS with the line :. gwas = hl.agg.linreg(hl.float(mt.phenos.height),; [1.0, mt.hapcounts0.x, mt.anc1dos.x, mt.anc2dos.x]). The error message is as follows:. -------------- start of error ------------------. ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38197); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 977, in _get_connection; connection = self.deque.pop(); IndexError: pop from an empty deque. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1115, in start; self.socket.connect((self.address, self.port)); ConnectionRefusedError: [Errno 111] Connection ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9939:7709,error,error,7709,https://hail.is,https://github.com/hail-is/hail/issues/9939,1,['error'],['error']
Availability,"top and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.908 [ERROR] [system.err] Ran 7 tests in 83.523s. I noticed that a previous issue #209 from early last year had the exact same",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419:1204,ERROR,ERROR,1204,https://hail.is,https://github.com/hail-is/hail/issues/1419,1,['ERROR'],['ERROR']
Availability,"top_level references are references to things that should be in the EvalContext. top_level=False are references intermediate variables. Inside the view_join_x stuff, we constructed top_level=False references to things like `row` and `va`, so these threw errors if used inside aggregators. I'll add a test for this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3055#issuecomment-369985720:254,error,errors,254,https://hail.is,https://github.com/hail-is/hail/pull/3055#issuecomment-369985720,1,['error'],['errors']
Availability,tor(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. Thanks!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:51160,Error,Error,51160,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['Error'],['Error']
Availability,"tor.py:decorator-gen-946>"", line 2, in write; File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/matrixtable.py"", line 2494, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 106, in execute; result = json.loads(Env.hail().backend.spark.SparkBackend.executeJSON(self._to_java_ir(ir))); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/utils/java.py"", line 240, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: ScalaSigParserError: Unexpected failure. Java stack trace:; org.json4s.scalap.ScalaSigParserError: Unexpected failure; 	at org.json4s.scalap.Rules$$anonfun$expect$1.apply(Rules.scala:73); 	at org.json4s.scalap.scalasig.ClassFileParser$.parse(ClassFileParser.scala:95); 	at org.json4s.reflect.ScalaSigReader$.parseClassFileFromByteCode(ScalaSigReader.scala:178); 	at org.json4s.reflect.ScalaSigReader$.findScalaSig(ScalaSigReader.scala:172); 	at org.json4s.reflect.ScalaSigReader$.findClass(ScalaSigReader.scala:53); 	at org.json4s.reflect.ScalaSigReader$.org$json4s$reflect$ScalaSigReader$$findField(ScalaSigReader.scala:100); 	at org.json4s.reflect.ScalaSigReader$.org$json4s$reflect$ScalaSigReader$$read$1(ScalaSigReader.scala:45); 	at org.json4s.reflect.ScalaSigReader$.readField(ScalaSigReader.scala:49); 	at org.json4s.reflect.Reflector$ClassDescriptorBuilder$$anonfun$3.apply(Reflector.scala:69); 	at org.json4s.reflect.Reflector$ClassDescriptorBuilder$$anonfun$3.apply(Reflector.scala:68); 	at scala.collect",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6299:1742,failure,failure,1742,https://hail.is,https://github.com/hail-is/hail/issues/6299,1,['failure'],['failure']
Availability,"tp_session/issues/281"">#281</a>)</li>; </ul>; <h1>2.4.0 (2018-05-04)</h1>; <ul>; <li>Fix a bug for session fixation (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_session/issues/272"">#272</a>)</li>; </ul>; <h1>2.3.0 (2018-02-13)</h1>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiohttp-session/commit/af0560812d3dc2043565de1108ac41b65caac7d0""><code>af05608</code></a> Release 2.11 (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_session/issues/673"">#673</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp-session/commit/16aa24292125aa59fed1ab4292c6576d800295f1""><code>16aa242</code></a> Bump pytest-mock from 3.6.1 to 3.7.0 (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_session/issues/674"">#674</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp-session/commit/72d199d40689cb0a83f2b911044ab0ed9f6cc08e""><code>72d199d</code></a> Fix error in example</li>; <li><a href=""https://github.com/aio-libs/aiohttp-session/commit/44e60f51bdb1ecfc22fa8bc87e8d025f2f17cd90""><code>44e60f5</code></a> Minor changes to typing. (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_session/issues/672"">#672</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp-session/commit/bf9a5f0b87470dd145cff326b0b05f898f775d94""><code>bf9a5f0</code></a> Fix session resetting before expiry. (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_session/issues/671"">#671</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp-session/commit/36b8a0a5ed2caaaba9d5d3ece8aaf03ca45b6c34""><code>36b8a0a</code></a> Allow passing Fernet to Encrypted Cookie Storage (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_session/issues/448"">#448</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp-session/commit/984decc496fe92e053c14949c8d3a60bacd62426""><code>984decc</code></a> Test on ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11577:3344,error,error,3344,https://hail.is,https://github.com/hail-is/hail/pull/11577,1,['error'],['error']
Availability,"tps://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2154"">#2154</a> PR by <a href=""https://github.com/kuviokelluja""><code>@​kuviokelluja</code></a>.</li>; </ul>; </li>; <li>upgrade supported ruby versions.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2205"">#2205</a> PR by <a href=""https://github.com/jalessio""><code>@​jalessio</code></a>.</li>; </ul>; </li>; <li>allow <code>language: conda</code> to use <code>mamba</code> or <code>micromamba</code> via <code>PRE_COMMIT_USE_MAMBA=1</code> or <code>PRE_COMMIT_USE_MICROMAMBA=1</code> respectively.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2204"">#2204</a> issue by <a href=""https://github.com/janjagusch""><code>@​janjagusch</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2207"">#2207</a> PR by <a href=""https://github.com/xhochy""><code>@​xhochy</code></a>.</li>; </ul>; </li>; <li>display <code>git --version</code> in error report.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2210"">#2210</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; <li>add <code>language: lua</code> as a supported language.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2158"">#2158</a> PR by <a href=""https://github.com/mblayman""><code>@​mblayman</code></a>.</li>; </ul>; </li>; </ul>; <h3>Fixes</h3>; <ul>; <li>temporarily add <code>setuptools</code> to the zipapp.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2122"">#2122</a> issue by <a href=""https://github.com/andreoliwa""><code>@​andreoliwa</code></a>.</li>; <li>a737d5f commit by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; <li>use <code>go install</code> instead of <code>go get</code> for go 1.18+ support.; <ul>; <li><a href=""https://github-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11460:1598,error,error,1598,https://hail.is,https://github.com/hail-is/hail/pull/11460,2,['error'],['error']
Availability,"tps://github.com/michel-kraemer/gradle-download-task/commit/a1858b494b5f3a51ccef7580c243c6dfdf520731""><code>a1858b4</code></a> Merge pull request <a href=""https://redirect.github.com/michel-kraemer/gradle-download-task/issues/295"">#295</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/c1e212c0fb41b3ea9185a9ea463fb1ea7142f748""><code>c1e212c</code></a> Add integration tests for Gradle 8.0 and 8.0.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/304f68e25f53633a92a4d2d6ce003a4986929503""><code>304f68e</code></a> Fix type inference issue</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/5.3.1...5.4.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=5.3.1&new-version=5.4.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12893:2928,down,download,2928,https://hail.is,https://github.com/hail-is/hail/pull/12893,1,['down'],['download']
Availability,"tps://github.com/thockin""><code>@​thockin</code></a>)</li>; <li>Added a feature gate <code>StatefulSetAutoDeletePVC</code>, which allows PVCs automatically created for StatefulSet pods to be automatically deleted. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/99728"">kubernetes/kubernetes#99728</a>, <a href=""https://github.com/mattcary""><code>@​mattcary</code></a>)</li>; <li>Client-go impersonation config can specify a UID to pass impersonated uid information through in requests. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104483"">kubernetes/kubernetes#104483</a>, <a href=""https://github.com/margocrawf""><code>@​margocrawf</code></a>)</li>; <li>Create HPA v2 from v2beta2 with some fields changed. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/102534"">kubernetes/kubernetes#102534</a>, <a href=""https://github.com/wangyysde""><code>@​wangyysde</code></a>) [SIG API Machinery, Apps, Auth, Autoscaling and Testing]</li>; <li>Ephemeral containers graduated to beta and are now available by default. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105405"">kubernetes/kubernetes#105405</a>, <a href=""https://github.com/verb""><code>@​verb</code></a>)</li>; <li>Fix kube-proxy regression on UDP services because the logic to detect stale connections was not considering if the endpoint was ready. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106163"">kubernetes/kubernetes#106163</a>, <a href=""https://github.com/aojea""><code>@​aojea</code></a>) [SIG API Machinery, Apps, Architecture, Auth, Autoscaling, CLI, Cloud Provider, Contributor Experience, Instrumentation, Network, Node, Release, Scalability, Scheduling, Storage, Testing and Windows]</li>; <li>If a conflict occurs when creating an object with <code>generateName</code>, the server now returns an &quot;AlreadyExists&quot; error with a retry option. (<a href=""https://github-redirect.depend",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:3475,avail,available,3475,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['avail'],['available']
Availability,"tps://redirect.github.com/python-pillow/Pillow/issues/7667"">#7667</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>; <li>Allow uncompressed TIFF images to be saved in chunks <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7650"">#7650</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Concatenate multiple JPEG EXIF markers <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7496"">#7496</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Changed IPTC tile tuple to match other plugins <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7661"">#7661</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Do not assign new fp attribute when exiting context manager <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7566"">#7566</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Support arbitrary masks for uncompressed RGB DDS images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7589"">#7589</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Support setting ROWSPERSTRIP tag <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7654"">#7654</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Apply ImageFont.MAX_STRING_LENGTH to ImageFont.getmask() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7662"">#7662</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Optimise <code>ImageColor</code> using <code>functools.lru_cache</code> <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7657"">#7657</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>; <li>Restricted environment keys for ImageMath.eval() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7655"">#7655</a> [<a href=""https://github.com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:2369,mask,masks,2369,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['mask'],['masks']
Availability,"tps://snyk.io/vuln/SNYK-PYTHON-IPYTHON-2348630) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **604/1000** <br/> **Why?** Has a fix available, CVSS 7.8 | Improper Privilege Management <br/>[SNYK-PYTHON-JUPYTERCORE-3063766](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERCORE-3063766) | `jupyter-core:` <br> `4.6.3 -> 4.11.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-MISTUNE-2940625](https://snyk.io/vuln/SNYK-PYTHON-MISTUNE-2940625) | `mistune:` <br> `0.8.4 -> 2.0.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **726/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 8.1 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-NBCONVERT-2979829](https://snyk.io/vuln/SNYK-PYTHON-NBCONVERT-2979829) | `nbconvert:` <br> `5.6.1 -> 6.3.0b0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **434/1000** <br/> **Why?** Has a fix available, CVSS 4.4 | Open Redirect <br/>[SNYK-PYTHON-NOTEBOOK-1041707](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-1041707) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:2924,avail,available,2924,https://hail.is,https://github.com/hail-is/hail/pull/13717,2,['avail'],['available']
Availability,transmuting keys doesn't error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3673:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/issues/3673,1,['error'],['error']
Availability,treat warnings like errors in the compiler,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/124:20,error,errors,20,https://hail.is,https://github.com/hail-is/hail/issues/124,1,['error'],['errors']
Availability,"tributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2023-01-26..2023-01-26&amp;type=Issues""><code>@​blink1073</code></a></p>; <h2>v8.0.0</h2>; <h2>8.0.0</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.3.5...760a7835d8b20a9daea3737759b1751d5e55dad8"">Full Changelog</a>)</p>; <p>This release is primarily focused on improving <code>asyncio</code> support, while aiming to have minimal API changes.</p>; <h3>Enhancements made</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_client/blob/main/CHANGELOG.md"">jupyter-client's changelog</a>.</em></p>; <blockquote>; <h2>8.0.2</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v8.0.1...717d36edcd9ce595f727d8b5a27e270c2a6e2c46"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Add papermill downstream check and fix kernel client replies <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/925"">#925</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Adopt more ruff rules <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/924"">#924</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Prefer print in kernelspecapp <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/923"">#923</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2023-01-26&amp;to=2023-01-30&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12656:3142,down,downstream,3142,https://hail.is,https://github.com/hail-is/hail/pull/12656,1,['down'],['downstream']
Availability,trio_matrix throws an inscrutable error if the col key has missing values,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5293:34,error,error,34,https://hail.is,https://github.com/hail-is/hail/issues/5293,1,['error'],['error']
Availability,"ts_expr('v => va.pass').count(); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-44-0380f72331b7> in <module>(); ----> 1 vds.filter_variants_expr('v => va.pass').count(). <decorator-gen-223> in filter_variants_expr(self, condition, keep). /Users/tpoterba/hail/python/hail/java.py in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: UnsupportedOperationException: null. Java stack trace:; java.lang.UnsupportedOperationException: null; 	at is.hail.expr.AST.typecheckThis(AST.scala:215); 	at is.hail.expr.AST.typecheckThis(AST.scala:213); 	at is.hail.expr.AST.typecheck(AST.scala:219); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:67); 	at is.hail.expr.Parser$.parseTypedExpr(Parser.scala:77); 	at is.hail.variant.VariantSampleMatrix.filterVariantsExpr(VariantSampleMatrix.scala:1229); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-a3d64be; Error summary: UnsupportedOperationException: null```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1623:2035,Error,Error,2035,https://hail.is,https://github.com/hail-is/hail/issues/1623,1,['Error'],['Error']
Availability,"tsdb msg=""found healthy block"" mint=1563105600000 maxt=1563170400000 ulid=01DFTDRJHCX1S9B0KPJTG8CRGW; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563170400000 maxt=1563235200000 ulid=01DFWBK0336Z71ZCRRKS79T18P; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563235200000 maxt=1563300000000 ulid=01DFY9C92NRA1S7FDVHFRFMFPF; level=info ts=2019-07-31T15:45:51.998Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:4427,repair,repair,4427,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['repair'],['repair']
Availability,"tsdb msg=""found healthy block"" mint=1563170400000 maxt=1563235200000 ulid=01DFWBK0336Z71ZCRRKS79T18P; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563235200000 maxt=1563300000000 ulid=01DFY9C92NRA1S7FDVHFRFMFPF; level=info ts=2019-07-31T15:45:51.998Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:4598,repair,repair,4598,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['repair'],['repair']
Availability,"tsdb msg=""found healthy block"" mint=1563235200000 maxt=1563300000000 ulid=01DFY9C92NRA1S7FDVHFRFMFPF; level=info ts=2019-07-31T15:45:51.998Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:4769,repair,repair,4769,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['repair'],['repair']
Availability,"tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:4940,repair,repair,4940,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['repair'],['repair']
Availability,"tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:5111,repair,repair,5111,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['repair'],['repair']
Availability,"tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:5282,repair,repair,5282,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['repair'],['repair']
Availability,"tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:5453,repair,repair,5453,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['repair'],['repair']
Availability,"tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564272000000 maxt=1564336800000 ulid=01DGX64B6GVNNF4P09GB4YM3TV; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:5624,repair,repair,5624,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['repair'],['repair']
Availability,"tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564272000000 maxt=1564336800000 ulid=01DGX64B6GVNNF4P09GB4YM3TV; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564401600000 maxt=1564408800000 ulid=01DGZ3XMWQ04MNAJTWJNXGHNZ5; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:5795,repair,repair,5795,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['repair'],['repair']
Availability,"tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564272000000 maxt=1564336800000 ulid=01DGX64B6GVNNF4P09GB4YM3TV; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564401600000 maxt=1564408800000 ulid=01DGZ3XMWQ04MNAJTWJNXGHNZ5; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564336800000 maxt=1564401600000 ulid=01DGZ426ED23NR5759BDAQM0H6; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:5966,repair,repair,5966,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['repair'],['repair']
Availability,"tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564272000000 maxt=1564336800000 ulid=01DGX64B6GVNNF4P09GB4YM3TV; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564401600000 maxt=1564408800000 ulid=01DGZ3XMWQ04MNAJTWJNXGHNZ5; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564336800000 maxt=1564401600000 ulid=01DGZ426ED23NR5759BDAQM0H6; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564408800000 maxt=1564416000000 ulid=01DGZASC8TRTFRM61J3MX4PHX4; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:6137,repair,repair,6137,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['repair'],['repair']
Availability,"tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564272000000 maxt=1564336800000 ulid=01DGX64B6GVNNF4P09GB4YM3TV; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564401600000 maxt=1564408800000 ulid=01DGZ3XMWQ04MNAJTWJNXGHNZ5; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564336800000 maxt=1564401600000 ulid=01DGZ426ED23NR5759BDAQM0H6; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564408800000 maxt=1564416000000 ulid=01DGZASC8TRTFRM61J3MX4PHX4; level=info ts=2019-07-31T15:45:52.009Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564416000000 maxt=1564423200000 ulid=01DGZHN38ENTPENE3MM35HVS42; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:6308,repair,repair,6308,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['repair'],['repair']
Availability,"tsnew/v1.5.1.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made this release possible.</p>; <h2>Pandas 1.5.0</h2>; <p>This release includes some new features, bug fixes, and performance improvements. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.0/whatsnew/v1.5.0.html"">full whatsnew</a> for a list of all the changes. pandas 1.5.0 supports Python 3.8 and higher.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <p><code>conda install -c conda-forge pandas</code></p>; <p>Or via PyPI:</p>; <p><code>python3 -m pip install --upgrade pandas</code></p>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.5.0rc0</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pandas-dev/pandas/commit/8dab54d6573f7186ff0c3b6364d5e4dd635ff3e7""><code>8dab54d</code></a> RLS: 1.5.2</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/d78c5e624936ea5bc30568fd7d6fc9b5f42d0beb""><code>d78c5e6</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49806"">#49806</a> on branch 1.5.x (DOC: Update what's new notes for 1.5.2 re...</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/98c6139ff12107b9aa34441d25ef1593b6a0adca""><code>98c6139</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12564:2113,avail,available,2113,https://hail.is,https://github.com/hail-is/hail/pull/12564,1,['avail'],['available']
Availability,"ttp://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076:2161,Error,ErrorHandling,2161,https://hail.is,https://github.com/hail-is/hail/issues/2076,1,['Error'],['ErrorHandling']
Availability,"ttps://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5914629](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Missing Cryptographic Step <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6036192](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6036192) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6050294](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6050294) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6092044](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6092044) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Information Exposure <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6126975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6126975) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | N",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:8248,avail,available,8248,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,"ture/authlib/issues/308"">lepture/authlib#308</a></p>; <h2>Version 0.15.2</h2>; <p>Fixed httpx authentication bug via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/283"">#283</a></p>; <h2>Version 0.15.1</h2>; <p>Backward compitable fix for using JWKs in JWT, via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/280"">#280</a>.</p>; <h2>Version 0.15</h2>; <p>This is the last release before v1.0. In this release, we added more RFCs; implementations and did some refactors for JOSE:</p>; <ul>; <li>RFC8037: CFRG Elliptic Curve Diffie-Hellman (ECDH) and Signatures in JSON Object Signing and Encryption (JOSE)</li>; <li>RFC7638: JSON Web Key (JWK) Thumbprint</li>; </ul>; <p>We also fixed bugs for integrations:</p>; <ul>; <li>Fixed support for HTTPX&gt;=0.14.3</li>; <li>Added OAuth clients of HTTPX back via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/270"">#270</a></li>; <li>Fixed parallel token refreshes for HTTPX async OAuth 2 client</li>; <li>Raise OAuthError when callback contains errors via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/275"">#275</a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/lepture/authlib/blob/master/docs/changelog.rst"">authlib's changelog</a>.</em></p>; <blockquote>; <h2>Version 0.15.5</h2>; <p><strong>Released on Oct 18, 2021.</strong></p>; <ul>; <li>Make Authlib compatible with latest httpx</li>; <li>Make Authlib compatible with latest werkzeug</li>; <li>Allow customize RFC7523 <code>alg</code> value</li>; </ul>; <h2>Version 0.15.4</h2>; <p><strong>Released on Jul 17, 2021.</strong></p>; <ul>; <li>Security fix when JWT claims is None.</li>; </ul>; <h2>Version 0.15.3</h2>; <p><strong>Released on Jan 15, 2021.</strong></p>; <ul>; <li>Fixed <code>.authorize_access_token</code> for OAuth 1.0 services, via :gh:<code>issue#308</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11483:2097,error,errors,2097,https://hail.is,https://github.com/hail-is/hail/pull/11483,1,['error'],['errors']
Availability,"turn await retry_transient_errors(request_and_wait); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 763, in retry_transient_errors; return await retry_transient_errors_with_debug_string('', 0, f, *args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 775, in retry_transient_errors_with_debug_string; return await f(*args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiocloud/aiogoogle/client/compute_client.py"", line 116, in request_and_wait; raise GCPOperationError(result['httpErrorStatusCode'],; hailtop.aiocloud.aiogoogle.client.compute_client.GCPOperationError: GCPOperationError: 400:BAD REQUEST ['RESOURCE_IN_USE_BY_ANOTHER_RESOURCE'] [""The disk resource 'projects/hail-vdc/zones/us-central1-b/disks/batch-disk-82XXXXX' is already being used by 'projects/hail-vdc/zones/us-central1-b/instances/batch-worker-default-standard-yjXXXX'""]; {'kind': 'compute#operation', 'id': 'XXXXX', 'name': 'operation-XXXXX', 'zone': 'https://www.googleapis.com/compute/v1/projects/hail-vdc/zones/us-central1-b', 'clientOperationId': 'XXXX', 'operationType': 'attachDisk', 'targetLink': 'https://www.googleapis.com/compute/v1/projects/hail-vdc/zones/us-central1-b/instances/batch-worker-default-standard-yjupd', 'targetId': 'XXXX', 'status': 'DONE', 'user': 'batch2-agent@hail-vdc.iam.gserviceaccount.com', 'progress': 100, 'insertTime': '2023-10-30T20:38:40.145-07:00', 'startTime': '2023-10-30T20:38:41.871-07:00', 'endTime': '2023-10-30T20:38:42.367-07:00', 'error': {'errors': [{'code': 'RESOURCE_IN_USE_BY_ANOTHER_RESOURCE', 'message': ""The disk resource 'projects/hail-vdc/zones/us-central1-b/disks/batch-disk-82XXXXX' is already being used by 'projects/hail-vdc/zones/us-central1-b/instances/batch-worker-default-standard-yjXXXX'""}]}, 'httpErrorStatusCode': 400, 'httpErrorMessage': 'BAD REQUEST', 'selfLink': 'https://www.googleapis.com/compute/v1/projects/hail-vdc/zones/us-central1-b/operations/operation-XXX'}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13955:2515,error,error,2515,https://hail.is,https://github.com/hail-is/hail/pull/13955,2,['error'],"['error', 'errors']"
Availability,"tute.hail.driver.Write$.run(Write.scala:35); 	at org.broadinstitute.hail.driver.Write$.run(Write.scala:6); 	at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); 	at org.broadinstitute.hail.driver.Command.run(Command.scala:264); 	at sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 35 in stage 7.0 failed 20 times, most recent failure: Lost task 35.19 in stage 7.0 (TID 6963, gnomad-prod-sw-m8lk.c.broad-mpg-gnomad.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at jav",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202:3951,failure,failure,3951,https://hail.is,https://github.com/hail-is/hail/issues/1202,1,['failure'],['failure']
Availability,"ty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/mem_unpack.h:471:8: required from ‘void simdpp::arch_avx2::detail::insn::v_mem_unpack4_impl16_128(T&, T&, T&, T&) [with T = simdpp::arch_avx2::uint16<16>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/mem_unpack.h:585:29: required from ‘void simdpp::arch_avx2::detail::insn::mem_unpack4(simdpp::arch_avx2::uint16<N>&, simdpp::arch_avx2::uint16<N>&, simdpp::arch_avx2::uint16<N>&, simdpp::arch_avx2::uint16<N>&) [with unsigned int N = 16]’; libsimdpp-2.0-rc2/simdpp/detail/insn/load_packed4.h:249:16: required from ‘void simdpp::arch_avx2::detail::insn::v256_load_packed4(V&, V&, V&, V&, const char*) [with V = simdpp::arch_avx2::uint16<16>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/load_packed4.h:85:36: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint64<4>’ with ‘private’ member ‘simdpp::arch_avx2::uint64<4>::d_’ from an array of ‘const class simdpp::arch_avx2::uint16<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:27,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:91:7: note: ‘class simdpp::arch_avx2::uint64<4>’ declared here; class uint64<4, void> : public any_int64<4, uint64<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::uint64<2>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:143513,error,error,143513,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"ub.com/michel-kraemer/gradle-download-task/commit/708067cd11c4a013da7a8c15d91f7f946967cf94""><code>708067c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0fdebf3c7ad43ed4739d0400c333a72b32f5d514""><code>0fdebf3</code></a> Improve verify example</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/019089b9554692674d6baee7df7d4d884f310cc9""><code>019089b</code></a> Correctly create list of output files</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/fa2739ded05333ba46d8f50bb3b2a3721cf0ca86""><code>fa2739d</code></a> Create target directories at a central place</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/02b8e1a79d9e00acd61f9ac42e5555619fe2247a""><code>02b8e1a</code></a> Prevent duplicate destination files</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0b65ca2f17c8890a3ec34cf80cde52ee5413cbec""><code>0b65ca2</code></a> Call eachFile action only once per source</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/717877121299cea8f216d3a595eaa56731a6acd3""><code>7178771</code></a> Support changing a target file's relative path in an eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/e5af1bd7f9daa8a9222aee0dd1b703727cb5e94e""><code>e5af1bd</code></a> Bump version number to 5.3.0-SNAPSHOT</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/3.2.0...5.3.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=3.2.0&new-version=5.3.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will reso",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:4488,down,download-task,4488,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['down'],['download-task']
Availability,"ub.com/python-jsonschema/jsonschema/compare/v4.10.3...v4.11.0"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.3...v4.11.0</a></p>; <h2>v4.10.3</h2>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.2...v4.10.3"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.2...v4.10.3</a></p>; <h2>v4.10.2</h2>; <ul>; <li>Fix a second place where subclasses may have added attrs attributes (<a href=""https://github-redirect.dependabot.com/python-jsonschema/jsonschema/issues/982"">#982</a>).</li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.1...v4.10.2"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.1...v4.10.2</a></p>; <h2>v4.10.1</h2>; <ul>; <li>Fix Validator.evolve (and APIs like <code>iter_errors</code> which call it) for cases; where the validator class has been subclassed. Doing so wasn't intended to be; public API, but given it didn't warn or raise an error it's of course; understandable. The next release however will make it warn (and a future one; will make it error). If you need help migrating usage of inheriting from a; validator class feel free to open a discussion and I'll try to give some; guidance (<a href=""https://github-redirect.dependabot.com/python-jsonschema/jsonschema/issues/982"">#982</a>).</li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.0...v4.10.1"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.0...v4.10.1</a></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/python-jsonschema/jsonschema/blob/main/CHANGELOG.rst"">jsonschema's changelog</a>.</em></p>; <blockquote>; <h1>v4.15.0</h1>; <ul>; <li>A specific API Reference page is now present in the documentation.</li>; <li><code>$ref</code> on ear",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12163:3299,error,error,3299,https://hail.is,https://github.com/hail-is/hail/pull/12163,1,['error'],['error']
Availability,"ub.com/saschagrunert""><code>@​saschagrunert</code></a>) [SIG Apps, Auth, CLI and Node]</li>; <li>Adds the ability to disable Accelerator/GPU metrics collected by Kubelet (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91930"">kubernetes/kubernetes#91930</a>, <a href=""https://github.com/RenaudWasTaken""><code>@​RenaudWasTaken</code></a>) [SIG Node]</li>; <li>Admission webhooks can now return warning messages that are surfaced to API clients, using the <code>.response.warnings</code> field in the admission review response. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92667"">kubernetes/kubernetes#92667</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>) [SIG API Machinery and Testing]</li>; <li>CertificateSigningRequest API conditions were updated:; <ul>; <li>a <code>status</code> field was added; this field defaults to <code>True</code>, and may only be set to <code>True</code> for <code>Approved</code>, <code>Denied</code>, and <code>Failed</code> conditions</li>; <li>a <code>lastTransitionTime</code> field was added</li>; <li>a <code>Failed</code> condition type was added to allow signers to indicate permanent failure; this condition can be added via the <code>certificatesigningrequests/status</code> subresource.</li>; <li><code>Approved</code> and <code>Denied</code> conditions are mutually exclusive</li>; <li><code>Approved</code>, <code>Denied</code>, and <code>Failed</code> conditions can no longer be removed from a CSR (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/90191"">kubernetes/kubernetes#90191</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>) [SIG API Machinery, Apps, Auth, CLI and Node]</li>; </ul>; </li>; <li>Cluster admins can now turn off /logs endpoint in kubelet by setting enableSystemLogHandler to false in their kubelet configuration file. enableSystemLogHandler can be set to true only when enableDebuggingHandlers is also set ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:4478,failure,failure,4478,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['failure'],['failure']
Availability,"ue, -1)); 98 try:; ---> 99 result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); 100 (result, timings) = (result_tuple._1(), result_tuple._2()); 101 value = ir.typ._from_encoding(result). /opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1321 answer = self.gateway_client.send_command(command); 1322 return_value = get_return_value(; -> 1323 answer, self.gateway_client, self.target_id, self.name); 1324 ; 1325 for temp_arg in temp_args:. /opt/conda/lib/python3.7/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 29 tpl = Env.jutils().handleForPython(e.java_exception); 30 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 31 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 32 except pyspark.sql.utils.CapturedException as e:; 33 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.te",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:3802,failure,failure,3802,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['failure'],['failure']
Availability,"ue, ; ...: sep=' ', ; ...: min_partitions=16) ; ...: m = m.key_rows_by(locus=hl.parse_locus(m.f0)) ; ...: m._force_count_rows() ; ```. `/tmp/foo.tsv.gz` is a gzipped (not blocked) 1GB of 1000 rows each containing one row column and 500k sample columns. The entries are the integers from 0 to 499,999. The first column is the first run (when the JIT is warmed) and the second column is the mean of two subsequent runs. All times in seconds. Everything is necessarily executed on one core. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Aside: using the staged stuff is hard, especially when using multiple methods. A couple thoughts:; - Because SRVB generates a fresh SRVB for arrays and structs, you must thread the srvb through your code gen rather than using a single field, this is annoying and error prone; - `init` is not a first class thing in `FunctionBuilder` and I've arguably made the whole situation more ugly by exposing `addInitInstructions`. Without the ability to place code in the constructor, it is hard to coordinate work between multiple methods.; - When using lots of methods, there's a lot of bookkeeping. I would like a way to define a ""staged class"" that wraps up some of the boilerplate. Not totally clear what I want here, just less boilerplate. Aside2: This is still pretty",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6987:1799,reliab,reliable,1799,https://hail.is,https://github.com/hail-is/hail/pull/6987,1,['reliab'],['reliable']
Availability,"ues/1547"">#1547</a>)</p>; <p>Misc Improvements; f461401e3 Silence AsciiLineReader warning when creating a FASTA sequence index (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1559"">#1559</a>); 8f82871c1 Update explain samflags script to python3 (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1585"">#1585</a>); 4ba4c0678 Update to new version of the snappy library which will work with M1 macs (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1580"">#1580</a>); e92706452 add predicate to GFF3Codec to give a chance to filter out some unused attributes (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1575"">#1575</a>); c647764b0 Some long reads tests using PacBio data. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1564"">#1564</a>); 57c3f03eb remove hardcoded .idx (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1568"">#1568</a>); a94a32512 Add file extension to missing index error message <a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1512"">#1512</a> (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1567"">#1567</a>); 74b827b67 Improve error message in IntervalTree (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1545"">#1545</a>); 7719274fe Htsget POST request support (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1529"">#1529</a>)</p>; <p>VCF:; aac46ee6d Added GVCF mode for VariantContext type determination (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1544"">#1544</a>); d72d73b01 Add context to exception when the vcf file is invalid <a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1565"">#1565</a> (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1566"">#1566</a>); 8466c82dc Respect genotype filtering when calculating AC/AN/AF (<a href=""https://github-redirect.dependabot.com/s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12229:4775,error,error,4775,https://hail.is,https://github.com/hail-is/hail/pull/12229,1,['error'],['error']
Availability,"ues/1822"">#1822</a>)</li>; <li><a href=""https://github.com/jupyter/nbconvert/commit/af4ee6ba3e4badbb42fdce4b34d1d34a4fccc655""><code>af4ee6b</code></a> Fix title (<a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1832"">#1832</a>)</li>; <li><a href=""https://github.com/jupyter/nbconvert/commit/4680a6db10338ea14c39fe0e6cc6a1959fedaf92""><code>4680a6d</code></a> escape_html: prevent escaping quotes on widgets JSON reprs (<a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1829"">#1829</a>) (<a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1830"">#1830</a>)</li>; <li><a href=""https://github.com/jupyter/nbconvert/commit/3520c03ae9f517c6a921d6b9689d44670e71a7dc""><code>3520c03</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1824"">#1824</a>)</li>; <li><a href=""https://github.com/jupyter/nbconvert/commit/efdba369ddc15451ca0fe58a011bafd5fa83db74""><code>efdba36</code></a> Remove downloaded CSS from repository (<a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1827"">#1827</a>)</li>; <li><a href=""https://github.com/jupyter/nbconvert/commit/4354373fdc5014a329200f0a12f1ce7fe0a55af9""><code>4354373</code></a> Fix linters (<a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1825"">#1825</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/jupyter/nbconvert/compare/6.5...7.0.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=nbconvert&package-manager=pip&previous-version=6.5.0&new-version=7.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12126:11397,down,downloaded,11397,https://hail.is,https://github.com/hail-is/hail/pull/12126,1,['down'],['downloaded']
Availability,"ugh so this is really annoying. Shading libraries with native libraries doesn't work out of the box. The SO file that indeed ships has a symbol like this:; ```; 0000000000001440 T _Java_com_indeed_util_mmap_MMapBuffer_mmap; ```; But after relocation, the java counterpart's package doesn't match the package you see above. Ergo, link error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8576#issuecomment-616207938:334,error,error,334,https://hail.is,https://github.com/hail-is/hail/pull/8576#issuecomment-616207938,1,['error'],['error']
Availability,"ugh, I'm still exhausted from shooting Konrad down.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5528#issuecomment-469477749:46,down,down,46,https://hail.is,https://github.com/hail-is/hail/pull/5528#issuecomment-469477749,1,['down'],['down']
Availability,"ugh, test failure in local backend. Will debug in a bit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9618#issuecomment-713077513:10,failure,failure,10,https://hail.is,https://github.com/hail-is/hail/pull/9618#issuecomment-713077513,1,['failure'],['failure']
Availability,"uint32<4>; T = simdpp::arch_avx2::float32<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::float32<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::float32<4>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:129:36: required from ‘simdpp::arch_avx2::uint32<4>::uint32(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::float32<4>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/test_bits.h:64:39: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint32<4>’ with ‘private’ member ‘simdpp::arch_avx2::uint32<4>::d_’ from an array of ‘const class simdpp::arch_avx2::float32<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:104:7: note: ‘class simdpp::arch_avx2::uint32<4>’ declared here; class uint32<4, void> : public any_int32<4, uint32<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::float64<2>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:92930,error,error,92930,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"uint32<4>; T = simdpp::arch_avx2::float64<2>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::float64<2>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::float64<2>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:129:36: required from ‘simdpp::arch_avx2::uint32<4>::uint32(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::float64<2>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/test_bits.h:73:39: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint32<4>’ with ‘private’ member ‘simdpp::arch_avx2::uint32<4>::d_’ from an array of ‘const class simdpp::arch_avx2::float64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:104:7: note: ‘class simdpp::arch_avx2::uint32<4>’ declared here; class uint32<4, void> : public any_int32<4, uint32<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<8>; T = simdpp::arch_avx2::uint32<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:94706,error,error,94706,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"uint32<8>; T = simdpp::arch_avx2::int16<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::int16<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::int16<16>]’; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:110:36: required from ‘simdpp::arch_avx2::uint32<8>::uint32(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int16<16>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_reduce_mul.h:181:30: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint32<8>’ with ‘private’ member ‘simdpp::arch_avx2::uint32<8>::d_’ from an array of ‘const class simdpp::arch_avx2::int16<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:24,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:91:7: note: ‘class simdpp::arch_avx2::uint32<8>’ declared here; class uint32<8, void> : public any_int32<8, uint32<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int8<16>; T = simdpp::arch_avx2::uint8<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:87560,error,error,87560,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"uite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4718:9649,error,error,9649,https://hail.is,https://github.com/hail-is/hail/issues/4718,1,['error'],['error']
Availability,"ulate_concordance; mt = unphase_mt(mt.filter_cols(hl.is_defined(dup_ht[mt.s]) | (mt.s == 'NA12878') | (mt.s == 'syndip'))); File ""<decorator-gen-510>"", line 2, in filter_cols; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/typecheck/check.py"", line 480, in _typecheck; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 1419, in filter_cols; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 2241, in _process_joins; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/table.py"", line 1233, in <lambda>; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 250 in stage 16.0 failed 20 times, most recent failure: Lost task 250.19 in stage 16.0 (TID 5993, exomes2-sw-znhp.c.broad-mpg-gnomad.internal, executor 1): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:751); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); 	at scala.collection.Iterator$$anon$1.head(Iterator.scala:997); 	at is.hail.utils.richUtils.RichIterator$$anon$5.value(RichIterator.scala:18); 	at is.hail.utils.StagingIterator.value(FlipbookIterator.scala:47); 	at is.hail.utils.FlipbookIterator$$anon$5.value(FlipbookIterator.scala:167); 	at is.hail.utils.FlipbookIterator$$anon$5.isValid(FlipbookIterator.scala:168); 	at is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:46); 	at is.hail.utils.FlipbookIterator.exhaust(FlipbookIterator.scala:110);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3235:1442,failure,failure,1442,https://hail.is,https://github.com/hail-is/hail/issues/3235,1,['failure'],['failure']
Availability,"uln/SNYK-PYTHON-PROMPTTOOLKIT-6141120) | `prompt-toolkit:` <br> `1.0.18 -> 3.0.13` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **696/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-1086606](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1086606) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-PYGMENTS-1088505](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1088505) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-5750273](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-5750273) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **519/1000** <br/> **Why?** Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![medium severit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:6547,avail,available,6547,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['avail'],['available']
Availability,"ummary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a0374fc7c895ae53309ea351e989571204e0ea5f""><code>a0374fc</code></a> Bump up version number to 5.3.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/612f57a382b8640cc730dc5e75d1c809e3e772bd""><code>612f57a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/291"">#291</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/53af1049f5514afe58e884d487d7c57dae47759d""><code>53af104</code></a> Bump http-cache-semantics from 4.1.0 to 4.1.1 in /screencast</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/398c14c05c6448b380ac35c6095598299c5e23c5""><code>398c14c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/15cf7eecfbc17d2466143828b9b69494c6cb6f2b""><code>15cf7ee</code></a> Bump up version number to 5.3.1-SNAPSHOT</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/e3c65ffcb49b9c5a33fde5f31fb63043dbf21134""><code>e3c65ff</code></a> Allow extensions to be created from tasks</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/34e2dd41477f18b1ae3d6d5a71dca5449d6cd1e0""><code>34e2dd4</code></a> Downgrade slf4j to fix warning on console about missing slf4j provider</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b3fa29f9ffb4d4544e13ef84601e371fb2778ddf""><code>b3fa29f</code></a> Revert &quot;Update Apache HttpClient to 5.2.1&quot;</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/01f05e046be0dca18f506723c79e88f208336e71""><code>01f05e0</code></a> Add integration tests for Gradle 6.9.3 and 7.6</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a998a544908a8b39f713f4526f717f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:1988,down,download-task,1988,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['down'],['download-task']
Availability,"ummary>⚠️ <b>Warning</b></summary>. ```; prometheus-async 19.2.0 requires prometheus-client, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIzMjkzZGUwOS01NmJjLTRkNWEtYWNkZC1iMzdlMDBkMzk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14034:1427,avail,available,1427,https://hail.is,https://github.com/hail-is/hail/pull/14034,1,['avail'],['available']
Availability,"un(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:18387,Error,Error,18387,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['Error'],['Error']
Availability,"un.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: hybrid.m37m.vcf.bgz: caught htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; offending line: 3	60830534	.	M	C	40	.	.	GT:AD	1/1:0,40; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:5957,Error,ErrorHandling,5957,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Error'],['ErrorHandling']
Availability,"unately, Hail 0.2.110 upgraded to Dataproc 2.1.2 which enabled ""memory protection"". Moreover, in the years since Hail 0.2.15, the memory in use by system processes on Dataproc driver nodes appears to have increased. Due to these two circumstances, the driver VM's memory usage can grow high enough to trigger the OOMKiller before the JVM triggers a GC. Consider, for example, these slices of the syslog of the n1-highmem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: All done; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. Notice:. 1. The total memory available on the machine is less than 52 GiB (= 53,248 MiB), indeed it is a full 1025 MiB below the advertised amount. 2. Once all the components of the Dataproc cluster have started (but before any Hail Query jobs are submitted) the total memory available is already depleted to 42760 MiB. Recall that Hail allocates 41 GiB (= 41,984 MiB) to its JVM. This leaves the Python process and all other daemons on the system only 776 MiB of excess RAM. For reference python3 -c 'import hail' needs 206 MiB. This PR modifies `hailctl dataproc start` and the meaning of `--master-memory-fraction`. Now, `--master-memory-fraction` is the precentage of the memory available to the master node after accounting for the missing 1GiB and the system daemons. We also increase the default memory fraction ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14066:2205,echo,echo,2205,https://hail.is,https://github.com/hail-is/hail/pull/14066,2,"['avail', 'echo']","['avail', 'echo']"
Availability,"unately, Hail 0.2.110 upgraded to Dataproc 2.1.2 which enabled ""memory protection"". Moreover, in the years since Hail 0.2.15, the memory in use by system processes on Dataproc driver nodes appears to have increased. Due to these two circumstances, the driver VM's memory usage can grow high enough to trigger the OOMKiller before the JVM triggers a GC. Consider, for example, these slices of the syslog of the n1-highmem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: All done; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. Notice:; 1. The total memory available on the machine is less than 52 GiB (= 53,248 MiB), indeed it is a full 1025 MiB below the advertised amount.; 2. Once all the components of the Dataproc cluster have started (but before any Hail Query jobs are submitted) the total memory available is already depleted to 42760 MiB. Recall that Hail allocates 41 GiB (= 41,984 MiB) to its JVM. This leaves the Python process and all other daemons on the system only 776 MiB of excess RAM. For reference `python3 -c 'import hail'` needs 206 MiB. ---. We must address this situation. It seems safe to assume that the system daemons will use a constant 9.5 GiB of RAM. Moreover the advertised RAM amount is at least 1 GiB larger than reality. I propose:; 1. The driver memory calculation in `hailctl dataproc` should take the advertis",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790:1914,echo,echo,1914,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790,2,"['avail', 'echo']","['avail', 'echo']"
Availability,"up dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:8008,down,download,8008,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['down'],['download']
Availability,updated error:; ```; is.hail.utils.HailException: OrderedRVD error! Unexpected key in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Key should be in partition 1: ([bar]-[foo]]; Invalid key: [quam]; ```; 🤔,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055#issuecomment-410039793:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/issues/4055#issuecomment-410039793,2,['error'],['error']
Availability,upload is a service that displays uploaded data. It uses Google authentication to only allow the Hail team to view uploads. Pipeline uploads can be enabled with `enable_pipeline_upload(email)` and you can upload the current Hail log with `hl.upload_log(email)`. Pipeline uploads happen in the background asynchronously (and errors are ignored). Log upload is synchronous. I generated certs for upload and this is the upload that is currently deployed.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4509:324,error,errors,324,https://hail.is,https://github.com/hail-is/hail/pull/4509,1,['error'],['errors']
Availability,"upyter-lsp/jupyterlab-lsp/issues/1016"">#1016</a></li>; <li>fix latex/Greek letters insertion and other completions which do not match prefix (do not pre-filter completions from kernel) <a href=""https://redirect.github.com/jupyter-lsp/jupyterlab-lsp/issues/1022"">#1022</a></li>; <li>fix completions in Console <a href=""https://redirect.github.com/jupyter-lsp/jupyterlab-lsp/issues/1023"">#1023</a></li>; <li>fix customising <code>priority</code> after pre-setting it with <code>overrides.json</code> <a href=""https://redirect.github.com/jupyter-lsp/jupyterlab-lsp/issues/1027"">#1027</a></li>; <li>fix jump to definitions in a file inside root in Pyright on Windows <a href=""https://redirect.github.com/jupyter-lsp/jupyterlab-lsp/issues/1024"">#1024</a></li>; <li>fix typos in setting title and help message <a href=""https://redirect.github.com/jupyter-lsp/jupyterlab-lsp/issues/999"">#999</a> and <a href=""https://redirect.github.com/jupyter-lsp/jupyterlab-lsp/issues/1010"">#1010</a></li>; </ul>; </li>; <li>maintenance:; <ul>; <li>fix bootstrap script <a href=""https://redirect.github.com/jupyter-lsp/jupyterlab-lsp/issues/1021"">#1021</a></li>; <li>bump axios from 1.2.1 to 1.6.2 <a href=""https://redirect.github.com/jupyter-lsp/jupyterlab-lsp/issues/1019"">#1019</a></li>; <li>bump <code>@​babel/traverse</code> from 7.22.5 to 7.23.4 <a href=""https://redirect.github.com/jupyter-lsp/jupyterlab-lsp/issues/1020"">#1020</a></li>; </ul>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/jupyter-lsp/jupyterlab-lsp/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jupyter-lsp&package-manager=pip&previous-version=2.2.1&new-version=2.2.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14171:2709,mainten,maintenance,2709,https://hail.is,https://github.com/hail-is/hail/pull/14171,1,['mainten'],['maintenance']
Availability,"upyter_client/issues/800"">#800</a> <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/801"">#801</a> (<a href=""https://github.com/utkonos""><code>@​utkonos</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2022-06-06&amp;to=2022-06-07&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Autkonos+updated%3A2022-06-06..2022-06-07&amp;type=Issues""><code>@​utkonos</code></a></p>; <h2>7.3.2</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.3.1...c81771416d9e09e0e92be799f3e8549d0db57e43"">Full Changelog</a>)</p>; <h3>Enhancements made</h3>; <ul>; <li>Correct <code>Any</code> type annotations. <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/791"">#791</a> (<a href=""https://github.com/joouha""><code>@​joouha</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>[pre-commit.ci] pre-commit autoupdate <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/792"">#792</a> (<a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a>)</li>; <li>Use hatch backend <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/789"">#789</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>[pre-commit.ci] pre-commit autoupdate <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/788"">#788</a> (<a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a>)</li>; <li>Use flit build backend <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/781"">#781</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12110:6531,Mainten,Maintenance,6531,https://hail.is,https://github.com/hail-is/hail/pull/12110,1,['Mainten'],['Maintenance']
Availability,"ure requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. Package Info:; Name: hail; Version: 0.2.93; Summary: Scalable library for exploring and analyzing genomic data.; Home-page: https://hail.is; Author: Hail Team; Author-email: hail@broadinstitute.org; License: UNKNOWN; Location: /Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages; Requires: dill, bokeh, scipy, azure-storage-blob, janus, parsimonious, botocore, google-cloud-storage, tabulate, Jinja2, python-json-logger, plotly, avro, azure-identity, PyJWT, orjson, tqdm, aiohttp-session, google-auth, nest-asyncio, uvloop, humanize, hurry.filesize, decorator, requests, Deprecated, aiohttp, asyncinit, numpy, pyspark, sortedcontainers, boto3, pandas. -----------------------------------------------------------------------------. Importing hail via the IPython console in Spyder causes the following error:. Python 3.8.12 (default, Oct 12 2021, 06:23:56) ; IPython 8.2.0 -- An enhanced Interactive Python. In [1]: `import hail`. > [SpyderKernelApp] ERROR | Exception in message handler:; > Traceback (most recent call last):; > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages/spyder_kernels/comms/frontendcomm.py"", line 164, in poll_one; > asyncio.run(handler(out_stream, ident, msg)); > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages/nest_asyncio.py"", line 36, in run; > task = asyncio.ensure_future(main); > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/asyncio/tasks.py"", line 684, in ensure_future; > raise TypeError('An asyncio.Future, a coroutine or an awaitable is '; > TypeError: An asyncio.Future, a coroutine or an awaitable is required; > [SpyderKernelApp] ERROR | Exception in message handler:; > Traceback (most recent call last):; > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages/spyder_kerne",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11758:1102,error,error,1102,https://hail.is,https://github.com/hail-is/hail/issues/11758,1,['error'],['error']
Availability,"ure: Task 0 in stage 24.0 failed 20 times, most recent failure: Lost task 0.19 in stage 24.0 (TID 1813, lfrani-sw-hqb8.c.broad-mpg-gnomad.internal, executor 159): is.hail.utils.HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.stats.HistogramCombiner.merge(HistogramCombiner.scala:42); 	at is.hail.annotations.aggregators.RegionValueHistogramAggregator.seqOp(RegionValueHistogramAggregator.scala:31); 	at is.hail.codegen.generated.C95.apply(Unknown Source); 	at is.hail.codegen.generated.C95.apply(Unknown Source); 	at is.hail.expr.ir.Interpret$$anonfun$27.apply(Interpret.scala:809); 	at is.hail.expr.ir.Interpret$$anonfun$27.apply(Interpret.scala:808); 	at is.hail.rvd.RVD$$anonfun$20$$anonfun$apply$11.apply(RVD.scala:551); 	at is.hail.rvd.RVD$$anonfun$20$$anonfun$apply$11.apply(RVD.scala:550); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$20.apply(RVD.scala:550); 	at is.hail.rvd.RVD$$anonfun$20.apply(RVD.scala:547); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:3803,Error,ErrorHandling,3803,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['Error'],['ErrorHandling']
Availability,"ure_medians': dict<str, struct {; variant_type: str, ; n_alt_alleles: int32, ; qd: float64, ; pab_max: float64, ; info_MQRankSum: float64, ; info_SOR: float64, ; info_InbreedingCoeff: float64, ; info_ReadPosRankSum: float64, ; info_FS: float64, ; info_QD: float64, ; info_MQ: float64, ; info_DP: int32; }> ; 'test_intervals': array<interval<locus<GRCh37>>> ; ----------------------------------------; Row fields:; 'locus': locus<GRCh37> ; 'alleles': array<str> ; 'variant_type': str ; 'allele_type': str ; 'n_alt_alleles': int32 ; 'was_mixed': bool ; 'has_star': bool ; 'qd': float64 ; 'pab_max': float64 ; 'info_MQRankSum': float64 ; 'info_SOR': float64 ; 'info_InbreedingCoeff': float64 ; 'info_ReadPosRankSum': float64 ; 'info_FS': float64 ; 'info_QD': float64 ; 'info_MQ': float64 ; 'info_DP': int32 ; 'transmitted_singleton': bool ; 'fail_hard_filters': bool ; 'info_POSITIVE_TRAIN_SITE': bool ; 'info_NEGATIVE_TRAIN_SITE': bool ; 'omni': bool ; 'mills': bool ; 'feature_imputed': struct {; n_alt_alleles: bool, ; qd: bool, ; pab_max: bool, ; info_MQRankSum: bool, ; info_SOR: bool, ; info_InbreedingCoeff: bool, ; info_ReadPosRankSum: bool, ; info_FS: bool, ; info_QD: bool, ; info_MQ: bool, ; info_DP: bool; } ; 'tp': bool; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------; ```. ### What went wrong (all error messages here, including the full java stack trace):. When run on our data with 30023341 rows where `tp` = `True`, `rand_bool` seems to be run twice as it only returns 4.2M rows where `tp` = `True`, `fp` = `False` and `train_expr` = `True` instead of 11M. If the expression is split as follows, I get the expected 11M rows:; ```; ht = ht.annotate(**{train_col: hl.cond(hl.or_else(ht[fp_col], False), hl.or_else(~ht[tp_col], True), ht[tp_col])}); train_expr = hl.cond(ht[tp_col] & hl.or_else(~ht[fp_col], True), hl.rand_bool(prob_tp), ht[train_col]); ht = ht.annotate(**{label_col: label_expr,; train_col: train_expr}); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3969:2001,error,error,2001,https://hail.is,https://github.com/hail-is/hail/issues/3969,1,['error'],['error']
Availability,"urlparse isn't really what we want for breaking down a blob storage URL. There are special characters that delimit different parts of a typical URL that don't translate to a blob. Specifically, if a blob name contains a `?` (which is totally valid if albeit horrendous), `parsed.path` will terminate before the `?` and drop the rest because it views those as query params. We really just want everything after the container to be the blob name, so we do a bit more manual work in the name of simplicity. Dan already handled this in the Google async fs but this was never changed in the azure implementation. They now resemble each other more closely.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11851:48,down,down,48,https://hail.is,https://github.com/hail-is/hail/pull/11851,1,['down'],['down']
Availability,us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgen,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:7218,echo,echo,7218,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,"use a Mac and try to install hail.; I use Mojave; I installed pyenv to modify my python versions.; I installed Python 3.7.9 since you recommend to use Python 3.7 as the latest version.; I then did a pip install hail, and it fails with pyspark:. Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1.tar.gz (215.7 MB); ERROR: Command errored out with exit status 1:; command: /Users/spascal/.pyenv/versions/3.7.9/bin/python3.7 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/setup.py'""'""'; __file__='""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-pip-egg-info-vlaj8k6d; cwd: /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/; Complete output (47 lines):; Could not import pypandoc - required to package PySpark; WARNING: The wheel package is not available.; ERROR: Command errored out with exit status 1:; command: /Users/spascal/.pyenv/versions/3.7.9/bin/python3.7 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""'; __file__='""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-ggmq8ipk; cwd: /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/; Complete output (8 lines):; no pandoc found, building platform unspecific wheel..",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9742:1183,avail,available,1183,https://hail.is,https://github.com/hail-is/hail/issues/9742,1,['avail'],['available']
Availability,use node selector instead of tolerations/node affinity,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636:29,toler,tolerations,29,https://hail.is,https://github.com/hail-is/hail/pull/7636,1,['toler'],['tolerations']
Availability,"use nodeSelector, tolerate preemptibles",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7784:18,toler,tolerate,18,https://hail.is,https://github.com/hail-is/hail/pull/7784,1,['toler'],['tolerate']
Availability,use nonExtremeDouble to avoid test failures,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2366:35,failure,failures,35,https://hail.is,https://github.com/hail-is/hail/pull/2366,1,['failure'],['failures']
Availability,"use-overlay, we might need to modify the VM image to include fuse-overlay. ```; + set +x; Using GOOGLE_APPLICATION_CREDENTIALS; + export TMPDIR=/io/; + TMPDIR=/io/; + retry buildah build -t us-docker.pkg.dev/hail-vdc/hail/git-make-bash:test-deploy-j6d7pph9mlzf -f /Dockerfile --cache-from us-docker.pkg.dev/hail-vdc/hail/cache --cache-to us-docker.pkg.dev/hail-vdc/hail/cache --layers /io; + buildah build -t us-docker.pkg.dev/hail-vdc/hail/git-make-bash:test-deploy-j6d7pph9mlzf -f /Dockerfile --cache-from us-docker.pkg.dev/hail-vdc/hail/cache --cache-to us-docker.pkg.dev/hail-vdc/hail/cache --layers /io; STEP 1/2: FROM us-docker.pkg.dev/hail-vdc/hail/ubuntu:20.04; Trying to pull us-docker.pkg.dev/hail-vdc/hail/ubuntu:20.04...; Getting image source signatures; Copying blob sha256:ca1778b6935686ad781c27472c4668fc61ec3aeb85494f72deb1921892b9d39e; Copying config sha256:88bd6891718934e63638d9ca0ecee018e69b638270fe04990a310e5c78ab4a92; Writing manifest to image destination; Storing signatures; time=\""2023-05-26T14:52:12Z\"" level=error msg=\""Unmounting /var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/merged: invalid argument\""; Error: mounting new container: mounting build container \""45e0ed631d22b6e1de7945266efcf0b802aa3b919d6b6ebd529ded6fedc11cf9\"": creating overlay mount to /var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/merged, mount_data=\""lowerdir=/var/lib/containers/storage/overlay/l/ZCKOX3GV2VWHWT4DMPLYJGMJWL,upperdir=/var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/diff,workdir=/var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/work,nodev,fsync=0,volatile\"": using mount program /usr/bin/fuse-overlayfs: unknown argument ignored: lazytime; fuse: device not found, try 'modprobe fuse' first; fuse-overlayfs: cannot mount: No such file or directory; : exit status 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13103#issuecomment-1564774692:1214,error,error,1214,https://hail.is,https://github.com/hail-is/hail/pull/13103#issuecomment-1564774692,2,"['Error', 'error']","['Error', 'error']"
Availability,"using ""chromosome"" when unexpected or leaving it out produces terrible error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2537:71,error,error,71,https://hail.is,https://github.com/hail-is/hail/issues/2537,1,['error'],['error']
Availability,"ut, overwrite, _codec_spec); 2111 """"""; 2112; -> 2113 self._jvds.write(output, overwrite, _codec_spec); 2114; 2115 def globals_table(self) -> Table:. /share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job 2 cancelled because SparkContext was shut down. Java stack trace:; org.apache.spark.SparkException: Job 2 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1920); at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581); at org.apache.spark.util.SparkShutdownHook.run(ShutdownHook",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755:3260,down,down,3260,https://hail.is,https://github.com/hail-is/hail/issues/4755,1,['down'],['down']
Availability,"uteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:339); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:483); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:482); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.105-3f053140ad00; Error summary: MethodTooLargeException: Method too large: __C444collect_distributed_array_table_text_writer.__m512split_InsertFields ()V; ```. It seems to get triggered here:; https://github.com/populationgenomics/gnomad-browser/blob/71a953b7094f103dd8915d805163f0768ba91791/data-pipeline/src/data_pipeline/data_types/gtex_tissue_expression.py#L14. According to @nawatts, this is part of ""a hacky way to convert `.gz` to `.bgz` within Hail because `import_matrix_table` doesn't have a `force` option."". However, it's worth noting that this used to work fine in earlier Hail versions, e.g. 0.2.85.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12531:11135,Error,Error,11135,https://hail.is,https://github.com/hail-is/hail/issues/12531,1,['Error'],['Error']
Availability,"uting eigendecomposition of kinship matrix...; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Estimating delta using REML... ; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Evals 1 to 7: 3.09757, 2.66667, 2.23576, 0.00000, 0.00000, -0.00000, -0.00000; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Evals 7 to 1: -0.00000, -0.00000, 0.00000, 0.00000, 2.23576, 2.66667, 3.09757; 2017-08-28 21:47:50 Hail: INFO: lmmreg: global model fit: beta = Map(intercept -> 0.48721539559123606, sa.cov.Cov1 -> 0.5924758486080468, sa.cov.Cov2 -> -0.23132255249379874); 2017-08-28 21:47:50 Hail: INFO: lmmreg: global model fit: sigmaG2 = 0.25961182888248313; 2017-08-28 21:47:50 Hail: INFO: lmmreg: global model fit: sigmaE2 = 0.02099926061895032; 2017-08-28 21:47:50 Hail: INFO: lmmreg: global model fit: delta = 0.08088714874566029; 2017-08-28 21:47:50 Hail: INFO: lmmreg: global model fit: h2 = 0.9251659631261897; 2017-08-28 21:47:50 Hail: INFO: lmmreg: global model fit: seH2 = 0.11525741506951385; 2017-08-28 21:47:50 Hail: INFO: baldingnichols: generating genotypes for 10 populations, 100 samples, and 300 variants...; 2017-08-28 21:47:50 Hail: INFO: Coerced sorted dataset; bash: line 12: 5047 Segmentation fault spark-submit --class org.testng.TestNG --jars ./hail-all-spark-test.jar --conf ""spark.driver.extraClassPath=./hail-all-spark-test.jar"" --conf 'spark.executor.extraClassPath=./hail-all-spark-test.jar' ./hail-all-spark-test.jar ./testng.xml; + TEST_EXIT_CODE=139; + set -e; + gcloud --project broad-ctsa compute scp --recurse cluster-ci-d1n0nk3vmzf4-m:test-output test-output; scp: test-output: No such file or directory; ERROR: (gcloud.compute.scp) [/usr/bin/scp] exited with return code [1].; + cleanup; + gcloud --project broad-ctsa -q dataproc clusters delete --async cluster-ci-d1n0nk3vmzf4; Deleting [https://dataproc.googleapis.com/v1/projects/broad-ctsa/regions/global/clusters/cluster-ci-d1n0nk3vmzf4] with operation [projects/broad-ctsa/regions/global/operations/3a35ffb5-aac5-4329-9762-67a8f4fadf66].; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835:7392,fault,fault,7392,https://hail.is,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835,2,"['ERROR', 'fault']","['ERROR', 'fault']"
Availability,"utions/hail-python.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548; 549 update_wrapper(wrapper, f). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, _codec_spec); 2111 """"""; 2112; -> 2113 self._jvds.write(output, overwrite, _codec_spec); 2114; 2115 def globals_table(self) -> Table:. /share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job 2 cancelled because SparkContext was shut down. Java stack trace:; org.apache.spark.SparkException: Job 2 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755:2898,Error,Error,2898,https://hail.is,https://github.com/hail-is/hail/issues/4755,1,['Error'],['Error']
Availability,"utomatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; jupyter 1.0.0 requires notebook, which is not installed.; beautifulsoup4 4.12.2 requires soupsieve, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **554/1000** <br/> **Why?** Has a fix available, CVSS 6.8 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CERTIFI-3164749](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-3164749) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![critical severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/c.png ""critical severity"") | **704/1000** <br/> **Why?** Has a fix available, CVSS 9.8 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **624/1000** <br/> **Why?** Has a fix available, CVSS 8.2 | Arbitrary Code Execution <br/>[SNYK-PYTHON-IPYTHON-2348630](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-2348630) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | No Known Exploit ;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:1039,avail,available,1039,https://hail.is,https://github.com/hail-is/hail/pull/13717,1,['avail'],['available']
Availability,"utor driver, partition 0, PROCESS_LOCAL, 4777 bytes); 2018-10-09 14:46:43 Executor: INFO: Running task 0.0 in stage 5.0 (TID 5); 2018-10-09 14:46:43 BlockManager: INFO: Found block rdd_9_0 locally; 2018-10-09 14:46:43 CodeGenerator: INFO: Code generated in 19.341759 ms; 2018-10-09 14:46:43 CodeGenerator: INFO: Code generated in 10.738625 ms; 2018-10-09 14:46:43 SparkContext: INFO: Invoking stop() from shutdown hook; 2018-10-09 14:46:43 AbstractConnector: INFO: Stopped Spark@31b6843e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2018-10-09 14:46:43 SparkUI: INFO: Stopped Spark web UI at http://10.32.119.167:4040; 2018-10-09 14:46:43 DAGScheduler: INFO: Job 3 failed: fold at RVD.scala:361, took 0.107081 s; 2018-10-09 14:46:43 DAGScheduler: INFO: ResultStage 5 (fold at RVD.scala:361) failed in 0.097 s due to Stage cancelled because SparkContext was shut down; 2018-10-09 14:46:43 LiveListenerBus: ERROR: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@58127452); 2018-10-09 14:46:43 LiveListenerBus: ERROR: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(3,1539121603334,JobFailed(org.apache.spark.SparkException: Job 3 cancelled because SparkContext was shut down)); 2018-10-09 14:46:43 MapOutputTrackerMasterEndpoint: INFO: MapOutputTrackerMasterEndpoint stopped!; 2018-10-09 14:46:43 MemoryStore: INFO: MemoryStore cleared; 2018-10-09 14:46:43 BlockManager: INFO: BlockManager stopped; 2018-10-09 14:46:43 BlockManagerMaster: INFO: BlockManagerMaster stopped; 2018-10-09 14:46:43 OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: INFO: OutputCommitCoordinator stopped!; 2018-10-09 14:46:43 SparkContext: INFO: Successfully stopped SparkContext; 2018-10-09 14:46:43 ShutdownHookManager: INFO: Shutdown hook called; 2018-10-09 14:46:43 ShutdownHookManager: INFO: Deleting directory /private/var/folders/w4/9k0my8pd6113d61pq05fvqlr0000gn/T/spark-02128b51-f37e-4798-84bb-d3e3819e51be; ```; </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:49006,ERROR,ERROR,49006,https://hail.is,https://github.com/hail-is/hail/issues/4513,2,"['ERROR', 'down']","['ERROR', 'down']"
Availability,"utor$2.run(SingleThreadEventExecutor.java:131); 	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); 	at java.lang.Thread.run(Thread.java:748); </details>. <details>; <summary>Working hail.log</summary>. ```; 2018-10-09 15:04:33 Hail: INFO: SparkUI: http://10.32.119.167:4040; 2018-10-09 15:04:33 Hail: INFO: Running Hail version devel-17a988f2a628; 2018-10-09 15:04:33 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 15:04:33 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 15:04:33 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@16ba3696{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2780d0b8{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7cea1161{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@696b1f0{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@14d32b0c{/static/sql,null,AVAILABLE,@Spark}; 2018-10-09 15:04:34 StateStoreCoordinatorRef: INFO: Registered StateStoreCoordinator endpoint; 2018-10-09 15:04:34 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:34 SparkSqlParser: INFO: Parsing command: SHOW TABLES; 2018-10-09 15:04:36 SparkContext: INFO: Starting job: collect at utils.scala:44; 2018-10-09 15:04:36 DAGScheduler: INFO: Got job 0 (collect at utils.scala:44) with 1 output partitions; 2018-10-09 15:0",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:14135,AVAIL,AVAILABLE,14135,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['AVAIL'],['AVAILABLE']
Availability,"v.alt on a multi-allelic variant currently crashed with the following error:; java.lang.IllegalArgumentException: requirement failed: called altAllele on a non-biallelic variant. The error message could be improved (""called alt / altAllele on a ...""). Or maybe v.alt on a multi-allelic variant could return a comma-delimited string of alt alleles?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/978:70,error,error,70,https://hail.is,https://github.com/hail-is/hail/issues/978,2,['error'],['error']
Availability,"v.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in parse; ir_map); /miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py:1257: in __call__; answer, self.gateway_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. args = ('xro1961', <py4j.java_gateway.GatewayClient object at 0x7ffa62e9e390>, 'z:is.hail.expr.ir.IRParser', 'parse_value_ir'), kwargs = {}; pyspark = <module 'pyspark' from '/miniconda3/lib/python3.7/site-packages/pyspark/__init__.py'>, s = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])'; tpl = JavaObject id=o1962, deepest = 'NoSuchElementException: next on empty iterator'; full = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])\n\tat is.hail.expr.ir.IR$class.typ(IR....a:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: NoSuchElementException: next on empty iterator; E ; E Java stack trace:; E java.lang.RuntimeException: typ: inference failure: ; E (MakeArray Array[Int32]); E 	at is.hail.expr.ir.IR$class.typ(IR.scala:34); E 	at is.hail.expr.ir.MakeArray.typ(IR.scala:135);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:2822,failure,failure,2822,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930,1,['failure'],['failure']
Availability,"v0.1.13+hail.zip; WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979ba910>, 'Connection to github.com timed out. (connect timeout=15)')': /hail-is/jgscm/archive/v0.1.13+hail.zip; WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979cc310>, 'Connection to github.com timed out. (connect timeout=15)')': /hail-is/jgscm/archive/v0.1.13+hail.zip; WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979cce10>, 'Connection to github.com timed out. (connect timeout=15)')': /hail-is/jgscm/archive/v0.1.13+hail.zip; ERROR: Could not install packages due to an OSError: HTTPSConnectionPool(host='github.com', port=443): Max retries exceeded with url: /hail-is/jgscm/archive/v0.1.13+hail.zip (Caused by ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979ce290>, 'Connection to github.com timed out. (connect timeout=15)')). Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 61, in <module>; safe_call(*command); File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 17, in safe_call; raise e; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 14, in safe_call; sp.check_output(args, stderr=sp.STDOUT, **kwargs); File ""/opt/conda/default/lib/python3.11/subprocess.py"", line 466, in check_output; return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/opt/conda/default/lib/python3.11/subprocess.py"", line 571, in run",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14652:4240,ERROR,ERROR,4240,https://hail.is,https://github.com/hail-is/hail/issues/14652,1,['ERROR'],['ERROR']
Availability,"va(self._jbackend.pyPersistTable(storage_level, self._to_java_table_ir(t._tir))); 297 ; 298 def unpersist_table(self, t):. /databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 1302 ; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306 . /databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw); 115 def deco(*a, **kw):; 116 try:; --> 117 return f(*a, **kw); 118 except py4j.protocol.Py4JJavaError as e:; 119 converted = convert_exception(e.java_exception). /databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client); 325 if answer[1] == REFERENCE_TYPE:; --> 326 raise Py4JJavaError(; 327 ""An error occurred while calling {0}{1}{2}.\n"".; 328 format(target_id, ""."", name), value). Py4JJavaError: An error occurred while calling o504.pyPersistTable.; : is.hail.utils.HailException: 1 samples and 12 covariates (including x) implies -11 degrees of freedom.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:11); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:11); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.methods.LinearRegressionRowsSingle.execute(LinearRegression.scala:51); 	at is.hail.expr.ir.functions.WrappedMatrixToTableFunction.execute(RelationalFunctions.scala:51); 	at is.hail.expr.ir.TableToTableApply.execute(TableIR.scala:2936); 	at is.hail.expr.ir.TableIR.analyzeAndExecute(TableIR.scala:57); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:27); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyPersistTable$2(SparkBackend.scala:502); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11413:3613,error,error,3613,https://hail.is,https://github.com/hail-is/hail/issues/11413,1,['error'],['error']
Availability,"variantqc: parse error: ""-o"" is not a valid option",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1017:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/issues/1017,1,['error'],['error']
Availability,varspark.hail unable to be imported due to SparkBackend error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12859:56,error,error,56,https://hail.is,https://github.com/hail-is/hail/issues/12859,1,['error'],['error']
Availability,"vds_path(data_type, 'truth_data'), args.overwrite); File ""<decorator-gen-528>"", line 2, in write; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/typecheck/check.py"", line 479, in _typecheck; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/matrixtable.py"", line 1807, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/utils/java.py"", line 238, in deco; hail.utils.java.FatalError: HailException: found non-left aligned variant: 18:76051965:C:G. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 56 in stage 3.0 failed 20 times, most recent failure: Lost task 56.19 in stage 3.0 (TID 685, exomes2-sw-8mf1.c.broad-mpg-gnomad.internal, executor 55): is.hail.utils.HailException: found non-left aligned variant: 18:76051965:C:G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.methods.SplitMultiPartitionContext.splitRow(SplitMulti.scala:98); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:226); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:225); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedJoinDistinctIterator.advanceRight1(OrderedJoinDistinctIterator.scala:36); 	at is.hail.sparkextras.OrderedJoinDistinctIterator.advanceRight(OrderedJoinDistinctIterator.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3040:1628,Error,ErrorHandling,1628,https://hail.is,https://github.com/hail-is/hail/issues/3040,1,['Error'],['ErrorHandling']
Availability,"ve(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions: Int, shuffle: Boolean)(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; match argument types (Int,shuffle: Boolean); Error occurred in an application involving default arguments.; start.copy(rdd = start.rdd.coalesce(k, shuffle = shuffle)(null).asOrderedRDD); ^; 5 errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. *",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:2407,error,error,2407,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,1,['error'],['error']
Availability,veMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: OrderedRVD error! Unexpected key in partition 7; Range bounds for partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Key should be in partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Invalid key: [0.9986274705095608]; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1031); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1011); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.io.RichContextRDDRegionValue$.writeRowsPartition(RowStore.scala:1071); 	at is.hail.io.RichContextRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:1096); 	at is.hail.io.RichContextRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:1096); 	at is.hail.utils.richUtils.RichContextRDD$$anonfun$1.apply(RichContextRDD.scala:42); 	at is.hail.utils.richUtils.RichContextRDD$$anonfun$1.apply(RichContextRDD.scala:27); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22.apply(ContextRDD.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4096:8297,Error,ErrorHandling,8297,https://hail.is,https://github.com/hail-is/hail/issues/4096,1,['Error'],['ErrorHandling']
Availability,"ved the code). - `.strip()` the GitHub token in case there are newlines. - print the SHA being deployed in the log statement. - add `hail-ci-build.sh` to CI, which just invokes `make test-in-cluster`(which in turn runs `test-in-cluster.sh`. - `test-in-cluster.sh` copies the secrets for testing to the expected locations and exposes the pod in which it is running with an internal service, recent changes to `site` [redirect sub URLs of ci.test.is to services named using this scheme](https://github.com/hail-is/hail/blob/master/site/hail.nginx.conf#L38-L41). GitHub uses these URLs to send updates to the CI under test about the watched repositories. - `test-locally.sh` now installs `../batch` into the currently running `pip` before testing (NB: if you edit batch and run the tests without committing the changes you've made to batch, this will pass tests but fail when pushed to a PR!). - `test-locally.sh` activates the `hail-ci` conda environment itself because it was not being propagated from the `Makefile`. I don't know why, but this is a simple fix. - `test-locally.sh` starts the ci after the repository is created. CI will print error messages if a watched repository doesn't exist. - `test/test-ci.py` now uses access tokens for all interaction with GitHub, previously it relied on the latent privileges that I and Cotton had in our environments. - `test/test-ci.py` uses a temporary, but not automatically deleted, directory when the environment variable `IN_CLUSTER` is set to `true` (to which it is set by `test-in-cluster.sh`). I noticed that, when running in a batch job pod, if an error occurred, `pytest` failed to print any error information and instead failed because the current working directory no longer existed. I found very little information on Google about this. It seems safe to not clean up temporary directories created in the batch job pod because pods are ephemeral. cc: @cseed. Assigning to @tpoterba since he has the most context on this stuff other than Cotton.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4474:1610,error,error,1610,https://hail.is,https://github.com/hail-is/hail/pull/4474,3,['error'],['error']
Availability,"vel). File ~/projects/hail/hail/python/hail/typecheck/check.py:587, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 584 @decorator; 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_). File ~/projects/hail/hail/python/hail/table.py:2112, in Table.persist(self, storage_level); 2076 @typecheck_method(storage_level=storage_level); 2077 def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; 2078 """"""Persist this table in memory or on disk.; 2079 ; 2080 Examples; (...); 2110 Persisted table.; 2111 """"""; -> 2112 return Env.backend().persist(self). File ~/projects/hail/hail/python/hail/backend/backend.py:208, in Backend.persist(self, dataset); 206 from hail.context import TemporaryFilename; 207 tempfile = TemporaryFilename(prefix=f'persist_{type(dataset).__name__}'); --> 208 persisted = dataset.checkpoint(tempfile.__enter__()); 209 self._persisted_locations[persisted] = (tempfile, dataset); 210 return persisted. File <decorator-gen-1232>:2, in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals). File ~/projects/hail/hail/python/hail/typecheck/check.py:587, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 584 @decorator; 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_). File ~/projects/hail/hail/python/hail/table.py:1331, in Table.checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals); 1328 hl.current_backend().validate_file(output); 1330 if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; -> 1331 self.write(output=output, overwrite=overwrite, stage_locally=stage_locally",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13788:2414,checkpoint,checkpoint,2414,https://hail.is,https://github.com/hail-is/hail/issues/13788,1,['checkpoint'],['checkpoint']
Availability,"veltejs/svelte/issues/6538"">#6538</a>)</li>; <li>Do not generate <code>unused-export-let</code> warning inside <code>&lt;script context=&quot;module&quot;&gt;</code> blocks (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7055"">#7055</a>)</li>; <li>Do not collapse whitespace-only CSS vars (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7152"">#7152</a>)</li>; <li>Add <code>aria-description</code> to the list of allowed ARIA attributes (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7301"">#7301</a>)</li>; <li>Fix attribute escaping during SSR (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7327"">#7327</a>)</li>; <li>Prevent <code>.innerHTML</code> optimization from being used when <code>style:</code> directive is present (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7386"">#7386</a>)</li>; </ul>; <h2>3.46.4</h2>; <ul>; <li>Avoid <code>maximum call stack size exceeded</code> errors on large components (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/4694"">#4694</a>)</li>; <li>Preserve leading space with <code>preserveWhitespace: true</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/4731"">#4731</a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/sveltejs/svelte/commit/52153dbce0237f0c36e4ff36377398d7f95276ef""><code>52153db</code></a> -&gt; v3.49.0</li>; <li><a href=""https://github.com/sveltejs/svelte/commit/3798808e7484b7eeee6acb2860c45bb2e59d84bd""><code>3798808</code></a> update changelog</li>; <li><a href=""https://github.com/sveltejs/svelte/commit/0fa0a38d5168a1767843fdb0a43c00aa30b8670f""><code>0fa0a38</code></a> [fix] export CompileOptions (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7658"">#7658</a>)</li>; <li><a href=""https://github.com/sveltejs/svelte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12032:5883,error,errors,5883,https://hail.is,https://github.com/hail-is/hail/pull/12032,3,['error'],['errors']
Availability,"ver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:817); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:15735,Error,Error,15735,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Error'],['Error']
Availability,"version: bfea6715901c2db13654f11bbc750e2fc037f831 (recent master branch). Ran:. ```; import hail as hl; hl.init(). t = hl.utils.range_table(1000); t = t.annotate(x = hl.rand_unif(0, 1)); t = t.key_by('x'); t.write('foo.ht', overwrite=True); ```. and got:. ```; Traceback (most recent call last):; File ""foo.py"", line 7, in <module>; t.write('foo.ht', overwrite=True); File ""/Users/cseed/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/cseed/hail/python/hail/table.py"", line 1183, in write; self._jt.write(output, overwrite, stage_locally, _codec_spec); File ""/Users/cseed/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/cseed/hail/python/hail/utils/java.py"", line 200, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: OrderedRVD error! Unexpected key in partition 7; Range bounds for partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Key should be in partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Invalid key: [0.9986274705095608]. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 2.0 failed 1 times, most recent failure: Lost task 7.0 in stage 2.0 (TID 23, localhost, executor driver): is.hail.utils.HailException: OrderedRVD error! Unexpected key in partition 7; Range bounds for partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Key should be in partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Invalid key: [0.9986274705095608]; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1031); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1011); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Itera",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4096:802,Error,Error,802,https://hail.is,https://github.com/hail-is/hail/issues/4096,2,"['Error', 'error']","['Error', 'error']"
Availability,"verwrite=True); ```. and got:. ```; Traceback (most recent call last):; File ""foo.py"", line 7, in <module>; t.write('foo.ht', overwrite=True); File ""/Users/cseed/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/cseed/hail/python/hail/table.py"", line 1183, in write; self._jt.write(output, overwrite, stage_locally, _codec_spec); File ""/Users/cseed/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/cseed/hail/python/hail/utils/java.py"", line 200, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: OrderedRVD error! Unexpected key in partition 7; Range bounds for partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Key should be in partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Invalid key: [0.9986274705095608]. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 2.0 failed 1 times, most recent failure: Lost task 7.0 in stage 2.0 (TID 23, localhost, executor driver): is.hail.utils.HailException: OrderedRVD error! Unexpected key in partition 7; Range bounds for partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Key should be in partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Invalid key: [0.9986274705095608]; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1031); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1011); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.io.RichContextRDDRegionValue$.writeRowsPartition(RowStore.scala:1071); 	at is.hail.io.RichContextRDDRegio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4096:1232,failure,failure,1232,https://hail.is,https://github.com/hail-is/hail/issues/4096,1,['failure'],['failure']
Availability,"voke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). org.apache.spark.SparkException: Job aborted due to stage failure: Task 40 in stage 7.0 failed 20 times, most recent failure: Lost task 40.19 in stage 7.0 (TID 3171, seqr-loading-cluster-sw-z91p.c.seqr-project.internal, executor 14): is.hail.utils.HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:210); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:974); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:967); 	at is.hail.utils.FlipbookIterator$$anon$5.<init>(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:174); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:145); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:967); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:963); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:147); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:146); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$19.apply(ContextRD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:51448,Error,ErrorHandling,51448,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['Error'],['ErrorHandling']
Availability,"vuln/SNYK-PYTHON-CRYPTOGRAPHY-3172287) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **454/1000** <br/> **Why?** Has a fix available, CVSS 4.8 | Expected Behavior Violation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3314966](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3314966) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Use After Free <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315324](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315324) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **584/1000** <br/> **Why?** Has a fix available, CVSS 7.4 | Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315328](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315328) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Timing Attack <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315331](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315331) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315452](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315452) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:3012,avail,available,3012,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"vuln/SNYK-PYTHON-CRYPTOGRAPHY-3172287) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **454/1000** <br/> **Why?** Has a fix available, CVSS 4.8 | Expected Behavior Violation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3314966](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3314966) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Use After Free <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315324](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315324) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **584/1000** <br/> **Why?** Has a fix available, CVSS 7.4 | Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315328](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315328) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Timing Attack <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315331](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315331) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315452](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315452) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:3004,avail,available,3004,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,"vx2::int8<32>]’; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:114:36: required from ‘simdpp::arch_avx2::uint8<32>& simdpp::arch_avx2::uint8<32>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int8<32>]’; libsimdpp-2.0-rc2/simdpp/core/bit_xor.h:38:8: required from ‘typename simdpp::arch_avx2::detail::get_expr2<V1, V2>::empty simdpp::arch_avx2::bit_xor(const simdpp::arch_avx2::any_vec<N, V>&, const simdpp::arch_avx2::any_vec<N, V2>&) [with unsigned int N = 32; V1 = simdpp::arch_avx2::int8<32>; V2 = simdpp::arch_avx2::uint8<32>; typename simdpp::arch_avx2::detail::get_expr2<V1, V2>::empty = simdpp::arch_avx2::uint8<32, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_avg.h:85:25: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint8<32>’ with ‘private’ member ‘simdpp::arch_avx2::uint8<32>::d_’ from an array of ‘const class simdpp::arch_avx2::int8<32>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:91:7: note: ‘class simdpp::arch_avx2::uint8<32>’ declared here; class uint8<32, void> : public any_int8<32, uint8<32,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::int32<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::ar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:113095,error,error,113095,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"vx2::uint32<4>; T = simdpp::arch_avx2::int16<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::int16<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::int16<8>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:129:36: required from ‘simdpp::arch_avx2::uint32<4>::uint32(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int16<8>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_reduce_mul.h:154:30: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint32<4>’ with ‘private’ member ‘simdpp::arch_avx2::uint32<4>::d_’ from an array of ‘const class simdpp::arch_avx2::int16<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:104:7: note: ‘class simdpp::arch_avx2::uint32<4>’ declared here; class uint32<4, void> : public any_int32<4, uint32<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::int16<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:85786,error,error,85786,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"vx2::uint8<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:62:35: required from ‘simdpp::arch_avx2::int16<8>& simdpp::arch_avx2::int16<8>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_bytes16.h:45:11: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int16<8>’ with ‘private’ member ‘simdpp::arch_avx2::int16<8>::d_’ from an array of ‘const class simdpp::arch_avx2::uint8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:33:7: note: ‘class simdpp::arch_avx2::int16<8>’ declared here; class int16<8, void> : public any_int16<8, int16<8,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int16<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:16254,error,error,16254,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,warnings are errors is the bees knees,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12322#issuecomment-1279306858:13,error,errors,13,https://hail.is,https://github.com/hail-is/hail/pull/12322#issuecomment-1279306858,1,['error'],['errors']
Availability,"was an issue with the input data. When using Hail 0.2.85, getting the following error (with code that worked with 0.2.78). ; Wondering what changes have been made between these versions that can help point to a solution?; Thanks!. code:; ```; phenotypes = phenotype_df.columns[1:]. y = list(map(lambda p: hl.float64(mt[p]), phenotypes)). covs = list(map(lambda cov: mt[cov], covariate_df.columns[1:])). x = mt.GT.n_alt_alleles(). hl_res = hl.methods.linear_regression_rows(y,x,[1]+covs). hl_res.checkpoint(quantitative_gwas_results_chk, overwrite=True); ```. error. `is.hail.utils.HailException: 1 samples and 12 covariates (including x) implies -11 degrees of freedom.; `. stacktrace. ```; Py4JJavaError Traceback (most recent call last); <[command-1581524108362637]()> in <module>; 4 covs = list(map(lambda cov: mt[cov], covariate_df.columns[1:])); 5 x = mt.GT.n_alt_alleles(); ----> 6 hl_res = hl.methods.linear_regression_rows(y,x,[1]+covs); 7 hl_res.checkpoint(quantitative_gwas_results_chk, overwrite=True); 8 n_filtered_variants = hl_res.count(). <decorator-gen-1734> in linear_regression_rows(y, x, covariates, block_size, pass_through, weights). /databricks/python/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. /databricks/python/lib/python3.8/site-packages/hail/methods/statgen.py in linear_regression_rows(y, x, covariates, block_size, pass_through, weights); 374 ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}); 375 ; --> 376 return ht_result.persist(); 377 ; 378 . <decorator-gen-1132> in persist(self, storage_level). /databricks/python/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 5",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11413:966,checkpoint,checkpoint,966,https://hail.is,https://github.com/hail-is/hail/issues/11413,1,['checkpoint'],['checkpoint']
Availability,we added a base case with nicer error,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2232#issuecomment-422371999:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/issues/2232#issuecomment-422371999,1,['error'],['error']
Availability,we have better error messages now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202#issuecomment-317747531:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/issues/1202#issuecomment-317747531,1,['error'],['error']
Availability,"we have some random failures of tests for some of our services related to resource contention, it's a big problem :(. If you push an empty commit, I can rerun the tests and hope they succeed this time...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6676#issuecomment-512935301:20,failure,failures,20,https://hail.is,https://github.com/hail-is/hail/pull/6676#issuecomment-512935301,1,['failure'],['failures']
Availability,"we print a 2 or 3 more than that). The computeGrammianMatrix function is used by Spark SVD for tall-skinny matrices. It's defined on RowMatrix as:. ```; def computeGramianMatrix(): Matrix = {; val n = numCols().toInt; checkNumColumns(n); // Computes n*(n+1)/2, avoiding overflow in the multiplication.; // This succeeds when n <= 65535, which is checked above; val nt: Int = if (n % 2 == 0) ((n / 2) * (n + 1)) else (n * ((n + 1) / 2)). // Compute the upper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack trace:. ```; hail: grm: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 45, localhost): java.lang.ArrayIndexOutOfBoundsException: 1048578; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:345); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:47); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:804); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:570); at org.apache.spark.serializer.KryoSerializationStream.writeObjec",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/801#issuecomment-247861703:1588,Down,Down,1588,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703,1,['Down'],['Down']
Availability,"we recently locked down our CI to run PRs only from the development team -- we intend to add a feature to our CI to let us run the test suite on a specific PR and SHA, but don't have that feature yet... 😦 . @cseed 	@danking this isn't hard, right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6262#issuecomment-498911743:19,down,down,19,https://hail.is,https://github.com/hail-is/hail/pull/6262#issuecomment-498911743,1,['down'],['down']
Availability,we should put this on top level expression. Someone could do something like `'NA12878' in mt.s` and that should have an error as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9164#issuecomment-665108669:120,error,error,120,https://hail.is,https://github.com/hail-is/hail/pull/9164#issuecomment-665108669,1,['error'],['error']
Availability,"we've moved in the opposite direction -- be tolerant as possible on input, let people minrep if they want",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1371#issuecomment-422369573:44,toler,tolerant,44,https://hail.is,https://github.com/hail-is/hail/issues/1371#issuecomment-422369573,1,['toler'],['tolerant']
Availability,"we; timeout substantially more frequently. I have observed this myself on my laptop. Just this; morning I saw it happen to Daniel. 2. When using an `aiohttp.AsyncIterablePayload`, it is *critical* to always check if the coroutine; which actually writes to GCS (which is stashed in the variable `request_task`) is still; alive. In the current `main`, we do not do this which causes hangs (in particular the timeout; exceptions are never thrown ergo we never retry). To understand the second problem, you must first recall how writing works in aiogoogle. There are; two Tasks and an `asyncio.Queue`. The terms ""writer"" and ""reader"" are somewhat confusing, so let's; use left and right. The left Task has the owning reference to both the source ""file"" and the; destination ""file"". In particular, it is the *left* Task which closes both ""files"". Moreover, the; left Task reads chunks from the source file and places those chunks on the `asyncio.Queue`. The; right Task takes chunks off the queue and writes those chunks to the destination file. This situation can go awry in two ways. First, if the right Task encounters any kind of failure, it will stop taking chunks off of the; queue. When the queue (which has a size limit of one) is full, the left Task will hang. The system; is stuck. The left Task will wait forever for the right Task to empty the queue. The second scenario is exactly the same except that the left Task is trying to add the ""stop""; message to the queue rather than a chunk. In either case, it is critical that the left Task waits simultaneously on the queue operation *and*; on the right Task completing. If the right Task has died, no further writes can occur and the left; Task must raise an exception. In the first scenario, we do not observe the right Task's exception; because that will be done when we close the `InsertObjectStream` (which represents the destination; ""file""). ---. I also added several types, assertions, and a few missing `async with ... as resp:` blocks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11830:1348,failure,failure,1348,https://hail.is,https://github.com/hail-is/hail/pull/11830,1,['failure'],['failure']
Availability,website search returns error 404,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13377:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/issues/13377,1,['error'],['error']
Availability,what do you think of it?. ---. We should at least keep the fix to list_batches.py. I am unsure why I discovered blatant type errors in ci.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12990:125,error,errors,125,https://hail.is,https://github.com/hail-is/hail/pull/12990,1,['error'],['errors']
Availability,what error do you get now?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8866#issuecomment-635292105:5,error,error,5,https://hail.is,https://github.com/hail-is/hail/pull/8866#issuecomment-635292105,1,['error'],['error']
Availability,"what the hell, bytecode verify errors from using locals? i saw another code-function creating locals so i assumed it was safe to do so, but apparently it isn't.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7453#issuecomment-550407768:31,error,errors,31,https://hail.is,https://github.com/hail-is/hail/pull/7453#issuecomment-550407768,1,['error'],['errors']
Availability,what was the error you saw?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5207#issuecomment-457740960:13,error,error,13,https://hail.is,https://github.com/hail-is/hail/issues/5207#issuecomment-457740960,1,['error'],['error']
Availability,"when I tried to import_vcf, I got an error：. Hail version: 0.2.14-8dcb6722c72a; Error summary: HailException: Invalid locus 'MT:0' found. Position '0' is not within the range [1-16569] for reference genome 'GRCh37'. However, telomeres are indicated by using postion 0 or N+1 in VCF 4.2.; please tell me how can I import this VCF correctly?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6339:37,error,error,37,https://hail.is,https://github.com/hail-is/hail/issues/6339,2,"['Error', 'error']","['Error', 'error']"
Availability,"which is not installed.; argon2-cffi-bindings 21.2.0 requires cffi, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **554/1000** <br/> **Why?** Has a fix available, CVSS 6.8 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CERTIFI-3164749](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-3164749) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![critical severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/c.png ""critical severity"") | **704/1000** <br/> **Why?** Has a fix available, CVSS 9.8 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **624/1000** <br/> **Why?** Has a fix available, CVSS 8.2 | Arbitrary Code Execution <br/>[SNYK-PYTHON-IPYTHON-2348630](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-2348630) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | Proof of Concept ; !",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:1745,avail,available,1745,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['avail'],['available']
Availability,"which will retry transient errors. router-resolver is part of the infrastructure, so I will hand-deploy once you're happy with this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7859:27,error,errors,27,https://hail.is,https://github.com/hail-is/hail/pull/7859,1,['error'],['errors']
Availability,"while the 1kg vcf that generated the assertion error looks like ; ```; ##fileformat=VCFv4.2; ##ApplyRecalibration=""analysis_type=ApplyRecalibration input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false input=[(RodBinding name=input source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.unfiltered.vcf)] recal_file=(RodBinding name=recal_file source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recal) tranches_file=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.tranches out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub ts_filter_level=9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:47,error,error,47,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['error'],['error']
Availability,"wice. The first write driver ends at 2023-10-13T01:17:55Z and the next write driver starts at 2023-10-13T01:18:11Z, just 16 seconds later. Batch: https://batch.hail.is/batches/8058522; Just the drivers: https://batch.hail.is/batches/8058522?q=name%3Dexecute%28...%29_driver. Driver & frontend logs indicate the first driver job completed and was almost immediately followed by a resubmission of the entire pipeline. https://cloudlogging.app.goo.gl/1344nayXTgaqKhCz8. # OLD. ### What happened?. NB: This is a development build 87398e1b514e. I think my comments below might be misleading. We purposely `WriteMetadata` multiple times, but with different `MetadataWriter`s. Unfortunately, this information does not appear in the SSA IR for some reason?. ---. The ""Relevant log output"" contains the last IR printed before the code was executed. The observed error was:. <details>; <summary>Expand me for the full trace. ```; Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```. </summary>. ```; Traceback (most recent call last):; File ""/Users/wlu/PycharmProjects/aou_gwas/scripts/pre_process_random_pheno.py"", line 345, in <module>; ); File ""/Users/wlu/PycharmProjects/aou_gwas/scripts/pre_process_random_pheno.py"", line 297, in main; mt = mt.filter_rows(mt.locus.in_autosome()); File ""<decorator-gen-1358>"", line 2, in write; File ""/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/hail/typecheck/check.py"", line 587, in wrapper; return __original_func(*args_, **kwargs_); File ""/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/hail/matrixtable.py"", line 2738, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/hail/backend/service_backend.py"", line 541, in execute; return self._cancel_on_ctrl_c(self._async_execute(ir, timed=timed, **kwargs)); File ""/usr/local/Caskroom/miniconda/base/lib/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:1016,Error,Error,1016,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['Error'],['Error']
Availability,"without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which internal namespaces (dev and PR) are currently active. This requires relatively small changes to CI to track active namespaces but overall is a pretty small change. Note that this does not introduce a dependency on CI to support production traffic, only development traffic.; 3. Deployments that run more than 1 replica (but really can be all of them) are run behind Headless Services, which expose the underlying pod IPs so Envoy can handle load-balancing instead of kube-proxy. This allows Envoy to make smart load-balancing decisions and correctly enforce rate-limiting when using connection pools. The namespace tracking in CI in Point 2 is possible before we make any changes to our networking, so that comes first in #12093. Point 3 is taken care of in #12094, and the rest of Point 2 and Point 1, everything to do with Envoy, is in this PR. ### Additional QoL improvements; - Envoy by default exposes Prometheus metrics that we can use to easily monitor things like rate-limiting, request failures and durations; - Since all Envoy configuration is in the configmap, we don't need to build any images. I suppose we could have done this with NGINX, so this isn't something to fault NGINX for. Just another small win buried in these changes. cc @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:5622,failure,failures,5622,https://hail.is,https://github.com/hail-is/hail/pull/12095,2,"['failure', 'fault']","['failures', 'fault']"
Availability,wrap vep error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4405:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/pull/4405,1,['error'],['error']
Availability,"writing a simple command such as read and write produces different type of error, but all related to memory issues. ; The most explicit error is: . ```; error='Cannot allocate memory' (errno=12); # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 7376207872 bytes for committing reserved memory.; ```. But also get this issue in other occasions:. ```; ExecutorLostFailure (executor 2265 exited caused by one of the running tasks) Reason: Container marked as failed: container_1481808189977_0001_01_002296 on host: gnomadpsychk-sw-wpt8.c.wgspd-147615.internal. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1481808189977_0001_01_002296; Exit code: 50; Stack trace: ExitCodeException exitCode=50:; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582); 	at org.apache.hadoop.util.Shell.run(Shell.java:479); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 50. Driver stacktrace:; ```. Log for this last error is here: https://storage.googleapis.com/wgspd_urv/hailNEW.log. I tried different projects and get the same error, both with pyhail and native hail. I don't see the same error when running on cray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186:75,error,error,75,https://hail.is,https://github.com/hail-is/hail/issues/1186,6,['error'],['error']
Availability,ws; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:234); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(OrderedRDD.scala:202); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(OrderedRDD.scala:195); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2407:12544,Error,ErrorHandling,12544,https://hail.is,https://github.com/hail-is/hail/issues/2407,1,['Error'],['ErrorHandling']
Availability,"x.key_by('y').show(); File ""<decorator-gen-598>"", line 2, in show; File ""/Users/konradk/Dropbox/src/python/hail/typecheck/check.py"", line 486, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1101, in show; print(self._show(n,width, truncate, types)); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1104, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/konradk/programs/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/Dropbox/src/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 49, localhost, executor driver): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3516:1818,failure,failure,1818,https://hail.is,https://github.com/hail-is/hail/issues/3516,1,['failure'],['failure']
Availability,"x2::float64<2, simdpp::arch_avx2::expr_empty>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:29,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/float32x4.h:32:7: note: ‘class simdpp::arch_avx2::float32<4>’ declared here; class float32<4, void> : public any_float32<4, float32<4,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint16<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint16<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint16<8>]’; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:129:36: required from ‘simdpp::arch_avx2::uint8<16>::uint8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint16<8>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:69:42: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint8<16>’ with ‘private’ member ‘simdpp::arch_avx2::uint8<16>::d_’ from an array of ‘const class simdpp::arch_avx2::uint16<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:4984,Mask,MaskCastOverride,4984,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"x2::int8<32>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::int8<32>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::int8<32>]’; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:115:37: required from ‘simdpp::arch_avx2::uint16<16>& simdpp::arch_avx2::uint16<16>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int8<32>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:61:15: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint16<16>’ with ‘private’ member ‘simdpp::arch_avx2::uint16<16>::d_’ from an array of ‘const class simdpp::arch_avx2::int8<32>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:22,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:92:7: note: ‘class simdpp::arch_avx2::uint16<16>’ declared here; class uint16<16, void> : public any_int16<16, uint16<16,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::int16<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:62188,error,error,62188,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"x2::uint8<16>; T = simdpp::arch_avx2::int16<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int16<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int16<8>]’; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:129:36: required from ‘simdpp::arch_avx2::uint8<16>::uint8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int16<8>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_bytes16.h:51:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint8<16>’ with ‘private’ member ‘simdpp::arch_avx2::uint8<16>::d_’ from an array of ‘const class simdpp::arch_avx2::int16<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: ‘class simdpp::arch_avx2::uint8<16>’ declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int16<16>; T = simdpp::arch_avx2::uint8<32>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:18019,error,error,18019,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"x2::uint8<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int8<16>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int8<16>; T = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:62:35: required from ‘simdpp::arch_avx2::int8<16>& simdpp::arch_avx2::int8<16>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_zbytes16.h:48:11: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int8<16>’ with ‘private’ member ‘simdpp::arch_avx2::int8<16>::d_’ from an array of ‘const class simdpp::arch_avx2::uint8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:33:7: note: ‘class simdpp::arch_avx2::int8<16>’ declared here; class int8<16, void> : public any_int8<16, int8<16,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int8<32>; T = simdpp::arch_avx2::uint8<32>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:89364,error,error,89364,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"x2::uint8<32>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int8<32>; T = simdpp::arch_avx2::uint8<32>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int8<32>; T = simdpp::arch_avx2::uint8<32>]’; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:55:35: required from ‘simdpp::arch_avx2::int8<32>& simdpp::arch_avx2::int8<32>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint8<32>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_zbytes16.h:87:11: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int8<32>’ with ‘private’ member ‘simdpp::arch_avx2::int8<32>::d_’ from an array of ‘const class simdpp::arch_avx2::uint8<32>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:32:7: note: ‘class simdpp::arch_avx2::int8<32>’ declared here; class int8<32, void> : public any_int8<32, int8<32,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::float32<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:91162,error,error,91162,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"xTable.write(MatrixTable.scala:2428); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz: caught java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anon",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:9430,Error,ErrorHandling,9430,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Error'],['ErrorHandling']
Availability,"xcept FatalError as e:; --> 180 raise e.maybe_user_error(ir) from None; 181 if ir.typ == tvoid:; 182 value = None. File /opt/conda/lib/python3.10/site-packages/hail/backend/backend.py:178, in Backend.execute(self, ir, timed); 176 payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); 177 try:; --> 178 result, timings = self._rpc(ActionTag.EXECUTE, payload); 179 except FatalError as e:; 180 raise e.maybe_user_error(ir) from None. File /opt/conda/lib/python3.10/site-packages/hail/backend/py4j_backend.py:213, in Py4JBackend._rpc(self, action, payload); 211 if resp.status_code >= 400:; 212 error_json = orjson.loads(resp.content); --> 213 raise fatal_error_from_java_error_triplet(error_json['short'], error_json['expanded'], error_json['error_id']); 214 return resp.content, resp.headers.get('X-Hail-Timings', ''). FatalError: HailException: cannot set missing field for required type +PFloat64. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 6.0 failed 4 times, most recent failure: Lost task 5.3 in stage 6.0 (TID 67) (saturn-machinenumber.c.terra-code.internal executor 4): is.hail.utils.HailException: gs://path/to/bucket/chrY.0002.hard_filtered_with_genotypes.vcf.gz:offset 23933331019603: error while parsing line; chrY	113	.	GG	G,*,AG,CG	596	PASS	AC=2,4,6,1;AF=1.23e-03,5.550e-05,4.44e-05,2.00e-04;AN=265;AS_AltDP=10,0,3,10;AS_BaseQRankSum=0.000,.,0.100,0.500;AS_FS=7.777,.,2.144,8.001;AS_MQ=55.75,.,38.98,40.20;AS_MQRankSum=0.200,.,-1.050,-0.500;AS_QD=0.50,0.00,0.25,0.52;AS_ReadPosRankSum=-0.200,.,0.500,-0.220;AS_SOR=2.300,.,1.600,3.000;BaseQRankSum=0.200;DP=600000;ExcessHet=0.0477;FS=0.900;MQ=55.02;MQRankSum=-0.553;QD=1.00;ReadPosRankSum=-0.162;SOR=0.792;VarDP=650	GT:AD:DP:GQ:PGT:PID:PL:PS:SB	0/0:.:21:30	0/0:.:300:20	0/0:.:30:72	0/0:.:31:98	0|1:29,3,0,0,0:33:78:0|1:113_GG_G:78,0,1100,140,1400,1200,172,1600,1200,1000,175,1100,1100,1300,1000:113:19,19,2,1	0/0:.:20:19	0/0:.:19:20	0/0:.:25:50		0|1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:6018,failure,failure,6018,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['failure'],['failure']
Availability,xception as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: RuntimeException: Method code too large!. Java stack trace:; java.lang.RuntimeException: Method code too large!; 	at is.hail.relocated.org.objectweb.asm.MethodWriter.a(Unknown Source); 	at is.hail.relocated.org.objectweb.asm.ClassWriter.toByteArray(Unknown Source); 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:306); 	at is.hail.expr.ir.EmitFunctionBuilder.result(EmitFunctionBuilder.scala:284); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:50); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:31); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:72); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:413); 	at is.hail.expr.ir.TableKeyBy.execute(TableIR.scala:161); 	at is.hail.table.Table.value$lzycompute(Table.scala:237); 	at is.hail.table.Table.value(Table.scala:232); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:240); 	at is.hail.table.Table.x$5(Table.scala:240); 	at is.hail.table.Table.rvd$lzycompute(Table.scala:240); 	at is.hail.table.Table.rvd(Table.scala:240); 	at is.hail.table.Table.take(Table.scala:875); 	at is.hail.table.Table.showString(Table.scala:911); 	at sun.reflect.GeneratedMethodAccessor108.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-eb1e04205793; Error summary: RuntimeException: Method code too large!; ```. [; [hail.zip](https://github.com/hail-is/hail/files/2189721/hail.zip). ](url),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3922:4237,Error,Error,4237,https://hail.is,https://github.com/hail-is/hail/issues/3922,1,['Error'],['Error']
Availability,"xception: Job aborted due to stage failure: Task 56 in stage 4.0 failed 20 times, most recent failure: Lost task 56.19 in stage 4.0 (TID 48622) (jsealock-schema-sw-43bq.c.daly-neale-sczmeta.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 125; VEP Error output:; docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.; See 'docker run --help'. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.methods.VEP$.waitFor(VEP.scala:73); 	at is.hail.methods.VEP.$anonfun$execute$5(VEP.scala:231); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.hasNext(RichContextRDD.scala:69); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at is.hail.io.RichContextRDDRegionValue$.writeRowsPartition(RichContext",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:1237,Error,ErrorHandling,1237,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['Error'],['ErrorHandling']
Availability,"xecute(MatrixIR.scala:1352); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixKeyRowsBy.execute(MatrixIR.scala:1317); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapRows.execute(MatrixIR.scala:1352); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapEntries.execute(MatrixIR.scala:1257); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:728); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:57); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:32); 	at is.hail.variant.MatrixTable.aggregateEntries(MatrixTable.scala:891); 	at is.hail.variant.MatrixTable.aggregateEntriesJSON(MatrixTable.scala:884); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/homes/nber/barronk-dua51929/local/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733:3884,ERROR,ERROR,3884,https://hail.is,https://github.com/hail-is/hail/issues/4733,1,['ERROR'],['ERROR']
Availability,"xecute(self, ir, timed); 186 payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); 187 try:; --> 188 result, timings = self._rpc(ActionTag.EXECUTE, payload); 189 except FatalError as e:; 190 raise e.maybe_user_error(ir) from None. File ~/projects/hail/hail/python/hail/backend/py4j_backend.py:223, in Py4JBackend._rpc(self, action, payload); 221 if resp.status_code >= 400:; 222 error_json = orjson.loads(resp.content); --> 223 raise fatal_error_from_java_error_triplet(; 224 error_json['short'], error_json['expanded'], error_json['error_id']; 225 ); 226 return resp.content, resp.headers.get('X-Hail-Timings', ''). FatalError: NoSuchElementException: Ref with name __iruid_1834 could not be resolved in env BindingEnv((__iruid_1832 -> struct{},__iruid_2157 -> struct{}),None,None,()). Java stack trace:; is.hail.utils.HailException: error after applying LowerToDistributedArray; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:23); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:23); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:32); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:205); 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:249); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$2(LocalBackend.scala:314); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(LocalBackend.scala:309); 	at is.hail.backend.loc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245:3466,Error,ErrorHandling,3466,https://hail.is,https://github.com/hail-is/hail/issues/14245,1,['Error'],['ErrorHandling']
Availability,"xecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: Task 9586 in stage 2.0 failed 4 times, most recent failure: Lost task 9586.3 in stage 2.0 (TID 40203, scc-q21.scc.bu.edu, executor 13): java.lang.IllegalArgumentException: Self-suppression not permitted; at java.lang.Throwable.addSuppressed(Throwable.java:1043); at java.io.FilterOutputStream.close(FilterOutputStream.java:159); at is.hail.utils.package$.using(package.scala:603); at is.hail.io.RichContextRDDRegionValue$.writeSplitRegion(RichContextRDDRegionValue.scala:99); at is.hail.rvd.RVD$$anonfun$25.apply(RVD.scala:939); at is.hail.rvd.RVD$$anonfun$25.apply(RVD.scala:937); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$18.apply(ContextRDD.scala:248); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$18.apply(ContextRDD.scala:248); at is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22); at is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22); at scala.col",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:5201,failure,failure,5201,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['failure'],['failure']
Availability,"xecuting exceeded memory limits. It seems likely that whole stage codegen has either (1) changed memory management in a way that uses more memory or (2) is newly lowering code that exposes a latent issue in memory management that uses too much (or leaks) memory.]. Reported by Ben Weisburd and Julia Goodrich. [Ben is] running the first step of readviz for gnomAD v4 and we are hitting a 137 error on a partition that includes a site that has 27374 alleles. His code is [here](https://github.com/broadinstitute/gnomad-readviz/blob/step1_optimizations/step1__select_samples.py). I was testing his code out on just that failing partition (just added mt = vds.variant_data._filter_partitions([41229])) and I was able to recreate the error using Hail 0.2.119 (this is what Ben was using when he hit the error on the full dataset). However, the first time I tried to recreate the error I was accidentally using a different version of Hail and it ran with no memory error. It seems that 0.2.117 runs without error, but 0.2.118 and 0.2.119 both hit the 137 error. I am currently rerunning these tests so I can get logs:. Test with Hail 0.2.118:. Cluster:; ```; hailctl dataproc start readviz-118 \; --requester-pays-allow-all \; --packages=""git+https://github.com/broadinstitute/gnomad_methods.git@main"",""git+https://github.com/broadinstitute/gnomad_qc.git@main"" \; --autoscaling-policy=max-20 \; --master-machine-type n1-highmem-16 \; --no-off-heap-memory \; --worker-machine-type n1-highmem-8 \; --max-idle 560m \; --labels gnomad_release=gnomad_v4,gnomad_v4_testing=readviz_test_118; ```; Command:; ```; hailctl dataproc submit readviz-118 /Users/jgoodric/PycharmProjects/gnomad-readviz/step1__select_samples.py --sample-metadata-tsv gs://gnomad-readviz/v4.0/gnomad.exomes.v4.0.metadata.tsv.gz --output-ht-path gs://gnomad-tmp/julia/readviz/gnomad.exomes.v4.0.readviz_crams.part_41229.hail_118.ht; Job Link: https://console.cloud.google.com/dataproc/jobs/4db24eb6f93b491f8f07babc25c0d9c9/monitoring?regio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13248:1120,error,error,1120,https://hail.is,https://github.com/hail-is/hail/issues/13248,1,['error'],['error']
Availability,xecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 10 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 10 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 10; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 8 on scc-q19.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:108548,ERROR,ERROR,108548,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,xecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 11 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 11 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 11; 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 18 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:145676,ERROR,ERROR,145676,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,"xes:</p>; <ul>; <li>Call <code>eachFile</code> action only once per source</li>; <li>Correctly create list of output files (even if the destination is the project's build directory)</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Use pooling connection manager of Apache HttpClient instead of basic one. The basic one is not meant to be used by multiple threads. This fixes an issue that could cause an <code>IllegalStateException</code> with the message <code>Connection is still allocated</code>. Thanks to <a href=""https://github.com/dmarks2""><code>@​dmarks2</code></a> for spotting this.</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>eachFile</code> method that adds an action to be applied to each source URL before it is downloaded. The action can be used to modify the filename of the target file.</li>; <li>Add <code>runAsync</code> method to download extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:1785,down,download,1785,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['down'],['download']
Availability,"xport_vcf; File ""/Users/dking/projects/hail/python/hail/typecheck/check.py"", line 546, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/hail/python/hail/methods/impex.py"", line 424, in export_vcf; joption(typ._convert_to_j(metadata))); File ""/Users/dking/borg/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/Users/dking/projects/hail/python/hail/utils/java.py"", line 210, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: in export_vcf: column key must be type 'str', found: [Lis.hail.expr.types.Type;@1e5a1972. Java stack trace:; is.hail.utils.HailException: in export_vcf: column key must be type 'str', found: [Lis.hail.expr.types.Type;@1e5a1972; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.MatrixTable.requireColKeyString(MatrixTable.scala:392); 	at is.hail.io.vcf.ExportVCF$.apply(ExportVCF.scala:202); 	at is.hail.io.vcf.ExportVCF.apply(ExportVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-17d79be1221e; Error summary: HailException: in export_vcf: column key must be type 'str', found: [Lis.hail.expr.types.Type;@1e5a1972. ``",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4283:2516,Error,Error,2516,https://hail.is,https://github.com/hail-is/hail/issues/4283,1,['Error'],['Error']
Availability,"xpr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^. Java stack trace:; is.hail.utils.HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-45429b1; Error summary: HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" el",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2653:2789,error,error,2789,https://hail.is,https://github.com/hail-is/hail/issues/2653,1,['error'],['error']
Availability,"xpr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.named_value_ir(Parser.scala:494); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:489); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:489); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:281); 	at is.hail.expr.ir.IRParser$.named_value_irs(Parser.scala:489); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:670); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:920); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:885); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:923); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:885); 	at is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$2.apply(Parser.scala:1023); 	at is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$2.apply(Parser.scala:1023); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1007); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1023); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1022); 	at is.hail.expr.ir.IRParser.parse_matrix_ir(Parser.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.5-e017bef1c26c; Error summary: AssertionError: assertion failed: cov1 not in struct{__y: float64, __cov0: float64}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4941:3126,Error,Error,3126,https://hail.is,https://github.com/hail-is/hail/issues/4941,1,['Error'],['Error']
Availability,xt$.$anonfun$scoped$3(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:63); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:350); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:495); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:494); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182); 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.120-f00f916faf78; Error summary: ClassCastException: class is.hail.types.physical.stypes.concrete.SIndexablePointer cannot be cast to class is.hail.types.physical.stypes.concrete.SJavaArrayString (is.hail.types.physical.stypes.concrete.SIndexablePointer and is.hail.types.physical.stypes.concrete.SJavaArrayString are in unnamed module of loader 'app'); ```. ### Version. 0.2.122. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:13591,Error,Error,13591,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['Error'],['Error']
Availability,xt(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-425cc84a9997; Error summary: HailException: array index out of bounds: 2 / 2; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3653:8048,Error,Error,8048,https://hail.is,https://github.com/hail-is/hail/issues/3653,1,['Error'],['Error']
Availability,"xt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; aiodocker 0.21.0 requires aiohttp, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIzODVlZjFmNi0zYjJhLTRjZTEtOTA5MS0xMWM1YzU3NDY",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14044:1404,avail,available,1404,https://hail.is,https://github.com/hail-is/hail/pull/14044,1,['avail'],['available']
Availability,"xtensions to be created on demand in custom tasks, so these tasks can be made compatible with Gradle's configuration cache (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/284"">#284</a>). Thanks to <a href=""https://github.com/liblit""><code>@​liblit</code></a> for testing!</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; <li>Improve documentation</li>; <li>Add integration tests for Gradle 6.9.3 and 7.6</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a0374fc7c895ae53309ea351e989571204e0ea5f""><code>a0374fc</code></a> Bump up version number to 5.3.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/612f57a382b8640cc730dc5e75d1c809e3e772bd""><code>612f57a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/291"">#291</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/53af1049f5514afe58e884d487d7c57dae47759d""><code>53af104</code></a> Bump http-cache-semantics from 4.1.0 to 4.1.1 in /screencast</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/398c14c05c6448b380ac35c6095598299c5e23c5""><code>398c14c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/15cf7eecfbc17d2466143828b9b69494c6cb6f2b""><code>15cf7ee</code></a> Bump up version number to 5.3.1-SNAPSHOT</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/e3c65ffcb49b9c5a33fde5f31fb63043dbf21134""><code>e3c65ff</code></a> Allow extensions to be created from tasks</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/34e2dd41477f18b1ae3d6d5a71dca5449d6cd1e0""><code>34e2dd4</code></a> Downgrade slf4j to fix warning on co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:1459,down,download-task,1459,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['down'],['download-task']
Availability,"xtualize/rich#3013</a></li>; </ul>; <h3>Added</h3>; <ul>; <li>Added Text.extend_style method.</li>; <li>Added Span.extend method.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Text.tab_size now defaults to <code>None</code> to indicate that Console.tab_size should be used.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/Textualize/rich/blob/master/CHANGELOG.md"">rich's changelog</a>.</em></p>; <blockquote>; <h2>[13.5.3] - 2023-09-17</h2>; <h3>Fixed</h3>; <ul>; <li>Markdown table rendering issue with inline styles and links <a href=""https://redirect.github.com/Textualize/rich/issues/3115"">Textualize/rich#3115</a></li>; <li>Fix Markdown code blocks on a light background <a href=""https://redirect.github.com/Textualize/rich/issues/3123"">Textualize/rich#3123</a></li>; </ul>; <h2>[13.5.2] - 2023-08-01</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs assertion error</li>; </ul>; <h2>[13.5.1] - 2023-07-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fix tilde character (<code>~</code>) not included in link regex when printing to console <a href=""https://redirect.github.com/Textualize/rich/issues/3057"">Textualize/rich#3057</a></li>; </ul>; <h2>[13.5.0] - 2023-07-29</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs not expanding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13651:3281,error,error,3281,https://hail.is,https://github.com/hail-is/hail/pull/13651,2,['error'],['error']
Availability,"y created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/hailtop/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiohttp 3.8.5 requires frozenlist, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **611/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 6.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5914629](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `41.0.3 -> 41.0.4` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJkNGViNWEyYS04NmZjLTRhZDQtYmM5MC1mZDViZWU4M",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13701:1060,avail,available,1060,https://hail.is,https://github.com/hail-is/hail/pull/13701,1,['avail'],['available']
Availability,"y curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, tim",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4548,error,errors,4548,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,1,['error'],['errors']
Availability,"y keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions) to multi-way zip join? I don't see where that happens.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5424:1051,error,error,1051,https://hail.is,https://github.com/hail-is/hail/pull/5424,4,"['down', 'error', 'failure']","['down', 'error', 'failures']"
Availability,"y of ‘const class simdpp::arch_avx2::float64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:29,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/float32x4.h:32:7: note: ‘class simdpp::arch_avx2::float32<4>’ declared here; class float32<4, void> : public any_float32<4, float32<4,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint16<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint16<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint16<8>]’; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:62:35: required from ‘simdpp::arch_avx2::int16<8>& simdpp::arch_avx2::int16<8>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint16<8>]’; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:55:73: required from ‘simdpp::arch_avx2::int16<8>::int16(const simdpp::arch_avx2::uint16<8, E>&) [with E = void]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:42:28: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int16<8>’ with ‘private’ member ‘simdpp::arch_avx2::int16<8>::d_’ from an array of ‘const class simdpp::arch_avx2::uint",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:104175,Mask,MaskCastOverride,104175,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"y uploaded a JAR for theee current SHA in the last 24 hours, you can use `make -C query; ipython-no-deps` to start a new session without uploading a new JAR. You can interact at ipython like this:. ```; In [1]: import hail as hl; ...: hl.utils.range_table(10).collect(); Initializing Hail with default parameters...; /Users/dking/projects/hail/hail/python/hail/utils/java.py:54: UserWarning: When using the query service backend, use `await Env._async_hc()'; warnings.warn('When using the query service backend, use `await Env._async_hc()\''); Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.81-edcde5c1b324; LOGGING: writing to /Users/dking/projects/hail/query/hail-20220113-1844-0.2.81-edcde5c1b324.log; Out[1]: ; [Struct(idx=0),; Struct(idx=1),; Struct(idx=2),; Struct(idx=3),; Struct(idx=4),; Struct(idx=5),; Struct(idx=6),; Struct(idx=7),; Struct(idx=8),; Struct(idx=9)]; ```. The very first time you execute this comand, it will run four batches to generate the four default; reference genomes and download those to your local machine. Those four reference genomes are cached; per-SHA on your local machine, so future Hail pipelines will have lower latency. If you have the ""standing working"" enabled, you should expect a latency of ~8s for the above job. I; think we can approximately halve this by re-using classloaders. Ask me more about this. After running this job a few times, the Batch UI looks like this:. ![Screen Shot 2022-01-14 at 11 59 41 AM](https://user-images.githubusercontent.com/106194/149554866-221e4243-1238-4d01-8944-0a6ed0b4c28c.png). When you call `collect`, the Python-side `ServiceBackend` writes a file to cloud storage containing; the temporary directory, billing project, and the IR. In addition to the `execute` command used for; `collect`, there are command for getting the table type and references; genomes. `ServiceBackend._rpc` defines this API on the Python-side and; `is.hail.backend.service.ServiceBackendSocketAPI2` d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194:1662,down,download,1662,https://hail.is,https://github.com/hail-is/hail/pull/11194,1,['down'],['download']
Availability,"y where there is a $CODE_HOME/hail/ and a $CODE_HOME/miniconda/ python installation, which all users PATHs are pointing to. This worked fine for both interactive and spark-submit uses with a single user, but today when I was testing with multiple users the HailContext would fail to form intermittently on a call to hc = HailContext() with either one of two errors. Note, each user today was ssh'ed into a different node and we were all using different jupyter notebooks simultaneously. There were five of us, and everytime we would all try to start HailContext at least one of us would fail out with these errors. Most of the time all five of us would fail out. Also note that concurrent calls to python only would be fine, with from hail import * working fine. Any help at all would be wonderful, as we would really like to work collaboratively on the cluster at the same time and all be referencing the same hail and python installations so we can keep our code synchronized. The first error that we would get would be. ---------; OSError Traceback (most recent call last); <ipython-input-11-2841f1963bb0> in <module>(); ----> 1 hc_rav = HailContext(). /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir); 45; 46 from pyspark import SparkContext; ---> 47 SparkContext._ensure_initialized(); 48; 49 self._gateway = SparkContext._gateway. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 254 with SparkContext._lock:; 255 if not SparkContext._gateway:; --> 256 SparkContext._gateway = gateway or launch_gateway(conf); 257 SparkContext._jvm = SparkContext._gateway.jvm; 258. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/java_gateway.py in launch_gateway(conf); 75 def preexec_func():; 76 signal.signal(signal.SIGINT, signal.SIG_IGN); ---> 77 proc ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1525:1218,error,error,1218,https://hail.is,https://github.com/hail-is/hail/issues/1525,1,['error'],['error']
Availability,"y"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowSto",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:3426,Error,ErrorHandling,3426,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['Error'],['ErrorHandling']
Availability,y$3.apply(RowStore.scala:808); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:807); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:807); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:804); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:804); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-df317f3; Error summary: HailException: found non-left aligned variant: 18:76051965:C:G; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3040:14808,Error,Error,14808,https://hail.is,https://github.com/hail-is/hail/issues/3040,1,['Error'],['Error']
Availability,"y4j_backend.py"", line 94, in execute; jir = self._to_java_value_ir(ir); File ""/home/edmund/.local/src/hail/hail/python/hail/backend/spark_backend.py"", line 280, in _to_java_value_ir; return self._to_java_ir(ir, self._parse_value_ir); File ""/home/edmund/.local/src/hail/hail/python/hail/backend/spark_backend.py"", line 276, in _to_java_ir; ir._jir = parse(r(finalize_randomness(ir)), ir_map=r.jirs); File ""/home/edmund/.local/src/hail/hail/python/hail/backend/spark_backend.py"", line 245, in _parse_value_ir; return self._jbackend.parse_value_ir(; File ""/home/edmund/.local/src/hail/.venv/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1304, in __call__; return_value = get_return_value(; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/py4j_backend.py"", line 21, in deco; return f(*args, **kwargs); File ""/home/edmund/.local/src/hail/.venv/lib/python3.8/site-packages/py4j/protocol.py"", line 326, in get_return_value; raise Py4JJavaError(; py4j.protocol.Py4JJavaError: An error occurred while calling o1.parse_value_ir.; : java.util.NoSuchElementException: key not found: __uid_4; at scala.collection.immutable.Map$Map1.apply(Map.scala:114); at is.hail.expr.ir.Env.apply(Env.scala:128); at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:890); at is.hail.expr.ir.IRParser$.$anonfun$ir_value_expr$1(Parser.scala:820); at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); at is.hail.utils.StackSafe$.run(StackSafe.scala:16); at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); at is.hail.expr.ir.IRParser$.$anonfun$parse_value_ir$1(Parser.scala:2072); at is.hail.expr.ir.IRParser$.parse(Parser.scala:2068); at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:2072); at is.hail.backend.spark.SparkBackend.$anonfun$parse_value_ir$2(SparkBackend.scala:710); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); at is.hail.utils.package$.using(package.scala:635); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteCont",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:9899,error,error,9899,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['error'],['error']
Availability,"yeah this was a simple error, forgot to do one upstream processing step",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11413#issuecomment-1050268863:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/issues/11413#issuecomment-1050268863,1,['error'],['error']
Availability,"yeah, there's a non-deterministic failure, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9106#issuecomment-664632674:34,failure,failure,34,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-664632674,1,['failure'],['failure']
Availability,"yep, run `./gradlew makeDocs`. It could be somehow related to another random failure (no space left on device)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6742#issuecomment-519554156:77,failure,failure,77,https://hail.is,https://github.com/hail-is/hail/pull/6742#issuecomment-519554156,1,['failure'],['failure']
Availability,"yk.io/vuln/SNYK-PYTHON-NOTEBOOK-2441824) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **449/1000** <br/> **Why?** Has a fix available, CVSS 4.7 | Access Restriction Bypass <br/>[SNYK-PYTHON-NOTEBOOK-2928995](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2928995) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **696/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-1086606](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1086606) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-PYGMENTS-1088505](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1088505) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-5750273](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-5750273) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **519/1000** <br/> **Why?** Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:5139,avail,available,5139,https://hail.is,https://github.com/hail-is/hail/pull/13717,2,['avail'],['available']
Availability,"ylint/issues/49"">#49</a>, <a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/50"">#50</a>). Thanks to <a href=""https://github.com/jayvdb""><code>@​jayvdb</code></a>.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/thibaudcolas/curlylint/blob/main/CHANGELOG.md"">curlylint's changelog</a>.</em></p>; <blockquote>; <h2><a href=""https://github.com/thibaudcolas/curlylint/releases/tag/v0.13.0"">v0.13.0</a> 2021-04-25</h2>; <p>This release comes with a blog post! Read on <a href=""https://www.curlylint.org/blog/quality-of-life-improvements"">Quality-of-life improvements</a>.</p>; <h3>Added</h3>; <ul>; <li>Implement --template-tags CLI flag (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/25"">#25</a>, <a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/77"">#77</a>).</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Add more descriptive error message for missing whitespace between HTML attributes (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/23#issuecomment-700622837"">#23 (comment)</a>, <a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/68"">#68</a>).</li>; <li>Move development dependencies from extras to separate <code>requirements.txt</code> (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/68"">#68</a>).</li>; <li>Declare support for Python 3.9.</li>; <li>Tentatively declare support for Python 3.10 (tested with <code>Python 3.10.0a6+</code>).</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix Python 3.10 deprecation warning by importing Iterable from collections.abc (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/68"">#68</a>).</li>; </ul>; <h2><a href=""https://github.com/thibaudcolas/curlylint/releases/tag/v0.12.2"">v0.12.2</a> 2021-03-06</h2>; <h3>Fixed</h3>; <ul>; <li>The <code>image_alt</code> rule no longer crashes when encou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11494:3427,error,error,3427,https://hail.is,https://github.com/hail-is/hail/pull/11494,2,['error'],['error']
Availability,"you have a doctest failure:; ```. =================================== FAILURES ===================================; ______________ [doctest] hail.experimental.ldscsim.get_cov_matrix ______________; 034 trait 1 & trait 4 :math:`r_g` = 0; 035 trait 2 & trait 3 :math:`r_g` = 0.9; 036 trait 2 & trait 4 :math:`r_g` = 0.15; 037 trait 3 & trait 4 :math:`r_g` = 1; 038 To obtain the covariance matrix corresponding to this scenario :math:`h^2` values are; 039 ordered according to user specification and :math:`r_g` values are ordered by the ; 040 order in which the corresponding genetic covariance terms will appear in the ; 041 covariance matrix, reading lines in the upper triangular matrix from left to; 042 right, top to bottom (read first row left to right, read second row left to ; 043 right, etc.), exluding the diagonal.; Differences (unified diff with -expected +actual):; @@ -1,4 +1,12 @@; -array([[0.1 , 0.06928203, 0.09899495, 0. ],; - [0.06928203, 0.3 , 0.22045408, 0.06363961],; - [0.09899495, 0.22045408, 0.2 , 0.34641016],; - [0. , 0.06363961, 0.34641016, 0.6 ]]); +covariance matrix is not positive semidefinite.; +adjusting rg values to make covariance matrix positive semidefinite; +0.4 -> 0.42023852645344634; +0.7 -> 0.5623351779264535; +0.0 -> 0.04812102869845767; +0.9 -> 0.7179252549815345; +0.15 -> 0.20103397396043554; +1.0 -> 0.7534523335539473; +(array([[0.1 , 0.07278745, 0.0795262 , 0.0117872 ],; + [0.07278745, 0.3 , 0.17585505, 0.08529149],; + [0.0795262 , 0.17585505, 0.2 , 0.26100354],; + [0.0117872 , 0.08529149, 0.26100354, 0.6 ]]), [0.42023852645344634, 0.5623351779264535, 0.04812102869845767, 0.7179252549815345, 0.20103397396043554, 0.7534523335539473]). ```. And you'll need to update `ldscsim.rst` because you changed the list of exposed functions, it looks like.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6825#issuecomment-519985328:19,failure,failure,19,https://hail.is,https://github.com/hail-is/hail/pull/6825#issuecomment-519985328,2,"['FAILURE', 'failure']","['FAILURES', 'failure']"
Availability,you've got a compilation error,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3840#issuecomment-401560032:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/pull/3840#issuecomment-401560032,1,['error'],['error']
Availability,"your intuition is exactly what we're doing. We look for bind and lambda AST nodes, add those to the declared scope. If we find a `top_level=False` reference not in that scope, we error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3055#issuecomment-369986005:179,error,error,179,https://hail.is,https://github.com/hail-is/hail/pull/3055#issuecomment-369986005,1,['error'],['error']
Availability,"ype$lzycompute(TextTableReader.scala:347); at is.hail.expr.ir.TextTableReader.fullType(TextTableReader.scala:347); at is.hail.expr.ir.IRParser$$anonfun$table_ir_1$2.apply(Parser.scala:1231); at is.hail.expr.ir.IRParser$$anonfun$table_ir_1$2.apply(Parser.scala:1231); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:1231); at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:1205); at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$2.apply(Parser.scala:1675); at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$2.apply(Parser.scala:1675); at is.hail.expr.ir.IRParser$.parse(Parser.scala:1664); at is.hail.expr.ir.IRParser$.parse_table_ir(Parser.scala:1675); at is.hail.expr.ir.IRParser$.parse_table_ir(Parser.scala:1674); at is.hail.expr.ir.IRParser.parse_table_ir(Parser.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.34-2684f0214a05; Error summary: NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; ```. This is caused by a backwards incompatible change introduced in Google's Hadoop connector in version 2.1.0 https://github.com/GoogleCloudDataproc/hadoop-connectors/issues/323. As of 2.1.0 Google's Hadoop connector relies on Hadoop 2.8.3. Unfortunately, there are no Spark releases with Hadoop 2.8.3 yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8343:7598,Error,Error,7598,https://hail.is,https://github.com/hail-is/hail/issues/8343,1,['Error'],['Error']
Availability,"ypes=True):. /home/hail/hail.zip/hail/expr/expressions/base_expression.py in _show(self, n, width, truncate, types); 684 if isinstance(source, hl.Table):; 685 if self is source.row:; --> 686 return source._show(n, width, truncate, types); 687 elif self is source.key:; 688 return source.select()._show(n, width, truncate, types). /home/hail/hail.zip/hail/table.py in _show(self, n, width, truncate, types); 1201 ; 1202 def _show(self, n=10, width=90, truncate=None, types=True):; -> 1203 return self._jt.showString(n, joption(truncate), types, width); 1204 ; 1205 def index(self, *exprs):. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: RuntimeException: Method code too large!. Java stack trace:; java.lang.RuntimeException: Method code too large!; 	at is.hail.relocated.org.objectweb.asm.MethodWriter.a(Unknown Source); 	at is.hail.relocated.org.objectweb.asm.ClassWriter.toByteArray(Unknown Source); 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:306); 	at is.hail.expr.ir.EmitFunctionBuilder.result(EmitFunctionBuilder.scala:284); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:50); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:31); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:72); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:413); 	at is.hail.expr.ir.TableKeyBy.execute(TableIR.scala:161); 	at is.hail.table.Table.value$lzycompute(Table.scala:237); 	at i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3922:2262,Error,Error,2262,https://hail.is,https://github.com/hail-is/hail/issues/3922,1,['Error'],['Error']
Availability,"ypescript-2-8-82990c516935. ### NextJS; https://nextjs.org/docs/; Next has 4 deviations from normal react:; 1) _app.js: Can be omitted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />; <Component {...pageProps} />; <Footer />; </Container>; ```. 2) _document.js: Optional. Rendered only on the server, exactly one time. Wraps _app. Good place to define external resource you want to load, such as some external stylesheet, font, whatever. . 3) `getInitialProps`: a lifecycle method that is only available to components in the `pages/` folder. `getInitialProps ` runs once during server-side rendering, and again if you navigate to the page that defines it. Only components in pages can specify this property. This is because it is effectively a function triggered during routing and:; * `getInitialProps` is of course only available if you define a stateful component. See [functional components (just JSX wrapped in a function, rather than a class)](https://reactjs.org/docs/components-and-props.html). 4) NextJS includes a light, fast router. Routes are matched based on the names of files in `pages/`, with index.js mapping to `/`. For instance, to navigate to `domain.com/scorecard/users`, you'd make the folder structure:. pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal react components, except they expose `getInitialProps`, described above. Each page file must export 1 default component:. ```js; #Page file; import React from 'react';. const index = () => <div>Hello World</div>; export default index;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:12073,avail,available,12073,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['avail'],['available']
Availability,"yption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a ne",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:6033,avail,available,6033,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['avail'],['available']
Availability,"yptography.hazmat.primitives.asymmetric.padding.PSS</code>; X.509 certificates via the new keyword-only argument <code>rsa_padding</code> on; :meth:<code>~cryptography.x509.CertificateBuilder.sign</code>.</li>; <li>Added support for; :class:<code>~cryptography.hazmat.primitives.ciphers.aead.ChaCha20Poly1305</code>; on BoringSSL.</li>; </ul>; <p>.. _v40-0-2:</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyca/cryptography/commit/d02de9f26e9a2353e89427c1cea8b9ed2bae969e""><code>d02de9f</code></a> changelog and version bump (<a href=""https://redirect.github.com/pyca/cryptography/issues/9008"">#9008</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/53dc686431f59658d892b83383a330d796105843""><code>53dc686</code></a> Backport null fix (<a href=""https://redirect.github.com/pyca/cryptography/issues/9007"">#9007</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/b99900596e65f31543d62cf1a52069c709ba7970""><code>b999005</code></a> Backport tolerate (<a href=""https://redirect.github.com/pyca/cryptography/issues/9006"">#9006</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/c4d494fd3ee907316bd846e90cbf4a8df75a25ac""><code>c4d494f</code></a> 41.0.0 version bump (<a href=""https://redirect.github.com/pyca/cryptography/issues/8991"">#8991</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/8708245ccdeaff21d65eea68a4f8d2a7c5949a22""><code>8708245</code></a> new openssl day (<a href=""https://redirect.github.com/pyca/cryptography/issues/8990"">#8990</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/31436a486661cd863d4c77e40facf93fbb2d9f54""><code>31436a4</code></a> admit to the existence of nuance in HKDF (<a href=""https://redirect.github.com/pyca/cryptography/issues/8987"">#8987</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/91e41898e6d1d2a9a6e980c39e2f8baa2fa8a1f8""><code>91e4189</code></a> Port DSA to Rust (<a href=""https://redir",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13146:2868,toler,tolerate,2868,https://hail.is,https://github.com/hail-is/hail/pull/13146,1,['toler'],['tolerate']
Availability,"ython tool that can serve as model. From Kaitlin:. The most recent version is attached (3.93). The biggest issue that I've yet to resolve is how to handle multi-allelic lines above tri-allelic. Gets into nightmare territory quickly. To run this, you'll need three things:; 1) VCF of interest; 2) PED file for the families in the VCF; 3) The ESP variant counts file that I made (.gz for the moment since it is so large); You can find this file here: /humgen/atgu1/fs03/wip/kaitlin/all_ESP_counts_5.28.13.txt. The command line argument should look like this:; python de_novo_finder_3.py <VCF> <PED> all_ESP_counts_5.28.13.txt. I suggest specifying an output file. There are a few optional flags that you can use to adjust things in the script. -v, --annotatevariants_VEP: If you have VEP annotations in the ANNOTATION column of the VCF, this will pull out and print the gene name and mutation type. -t, --thresh: The PL threshold set for the next most likely genotype in the child. This gets after how confident you want the het call to be in the child. Default is a PL threshold of 20, but you can adjust that up or down if you'd like. -c, --minchildAB: I require that the heterozygous child has at least 20% alternative reads. You can adjust that with this. -d, --depthratio: I require that the depth of coverage in the child is at least 1/10th that of the combined parental depth. You can adjust that with this (integer input for the 1/x). -m, --prob(dn)metric: The minimum p(DN) that you will accept. I have it set at 0.05 and you could adjust it up. Due to the validation likelihoods that were added, you won't get anything below 0.05. -p, --maxparentAB: I require that the parents, who should both be homozygous reference, have no more than 5% alternative reads. You can adjust that with this flag. -a, --annotatevariants: If you have SnpEff annotations in the ANNOTATION column of the VCF, this will pull out and print the gene name and mutation type. _Copied from original issue: cseed/hail#44_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/34:1183,down,down,1183,https://hail.is,https://github.com/hail-is/hail/issues/34,1,['down'],['down']
Availability,"ython/hail/hail_version; echo 0.2.124 > python/hail/hail_pip_version; cp -f python/hail/hail_version python/hailtop/hail_version; printf 'hail_version=""0.2.124-13536b531342"";' > python/hail/docs/_static/hail_version.js; printf 'hail_pip_version=""0.2.124""' >> python/hail/docs/_static/hail_version.js; cloud_base is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723e4b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:3080,echo,echo,3080,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,"zip/hail/table.py in distinct(self); 2607 """"""; 2608 hail.methods.misc.require_key(self, ""distinct""); -> 2609 return Table(self._jt.distinctByKey()); 2610 ; 2611 table_type.set(Table). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 182 import pyspark; 183 try:; --> 184 return f(*args, **kwargs); 185 except py4j.protocol.Py4JJavaError as e:; 186 s = e.java_exception.toString(). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 317 raise Py4JJavaError(; 318 ""An error occurred while calling {0}{1}{2}.\n"".; --> 319 format(target_id, ""."", name), value); 320 else:; 321 raise Py4JError(. Py4JJavaError: An error occurred while calling o1507.distinctByKey.; : java.util.NoSuchElementException: None.get; 	at scala.None$.get(Option.scala:347); 	at scala.None$.get(Option.scala:345); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2089); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2064); 	at is.hail.expr.TableExplode.execute(Relational.scala:2166); 	at is.hail.expr.TableUnkey.execute(Relational.scala:1857); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2064); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1820); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1820); 	at is.hail.table.Table.value$lzycompute(Table.scala:237); 	at is.hail.table.Table.value(Table.scala:232); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:240); 	at is.hail.table.Table.x$5(Table.scala:240); 	at is.hail.table.Table.rvd$lzycompute(Table.scala:240); 	at is.hail.table.Table.rvd(Table.scala:240); 	at is.hail.table.Table.distinctByKey(Table.scala:593); 	at sun.reflect.NativeMethodAccessor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3814:2617,error,error,2617,https://hail.is,https://github.com/hail-is/hail/issues/3814,1,['error'],['error']
Availability,"},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13932:11231,avail,available,11231,https://hail.is,https://github.com/hail-is/hail/pull/13932,1,['avail'],['available']
Availability,"~Actually, this does appear to be a gap in our test coverage. I put in a failing assertion, and both `BlockMatrixIRSuite.scala` and `test_linalg.py` passed.~ Never mind, just forgot to run in local mode. I didn't notice any obvious errors in your code, but I don't think this is the right way to do it, so I tried writing it myself. I'll try to write a test that exercises this path and test both implementations.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13807#issuecomment-1766662634:232,error,errors,232,https://hail.is,https://github.com/hail-is/hail/pull/13807#issuecomment-1766662634,1,['error'],['errors']
Availability,"~I verified this scales all the way to 50,000 partitions but the batch-driver can't schedule these fast enough to make the test fast. They take less than a second but more than 300ms. We'd need like 64 8-core (512 cores) nodes to bring test time down to a reasonable amount. We should strive to get there but batch would need to schedule at 512 jobs per second for that to make sense.~ Hmm, something went wrong. OK, we'll need to revisit 50k partition tables. But let's get this in, the current code is obviously wrong.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13065#issuecomment-1550142533:246,down,down,246,https://hail.is,https://github.com/hail-is/hail/pull/13065#issuecomment-1550142533,1,['down'],['down']
Availability,"~Stacked on #10770~; ~Stacked on #10791~. This PR attempts to allow linear algebra codegen methods, like the LAPACK wrappers and the local whitening methods I'm working on, to defensively assert shape compatibility preconditions, without generating redundant runtime checks. (I always hate when we're pushed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerce",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10783:249,redundant,redundant,249,https://hail.is,https://github.com/hail-is/hail/pull/10783,1,['redundant'],['redundant']
Availability,"~~Stacked on #9106.~~. Adds an enumeration to control the allocation strategy in emitted code. There are currently three options, `Default`, `OneRegion`, and `ManyRegions`. `OneRegion` replicates the current behavior, and is the default for now. `ManyRegions` always allocates new regions when given the choice. This PR makes Java tests use the `ManyRegions` strategy to catch more lifetime errors. `Default` is somewhere in the middle, and will become the default. It will use one region (at least a constant number of regions) per row of a table. `Default` currently behaves the same as `ManyRegions`. Future work will implement the per-row behavior.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9220:391,error,errors,391,https://hail.is,https://github.com/hail-is/hail/pull/9220,1,['error'],['errors']
Availability,"~~Stacked on #9490~~. With the stronger invariants enforced by the previous PRs, some of the work done in `asBytes` is redundant.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9491:119,redundant,redundant,119,https://hail.is,https://github.com/hail-is/hail/pull/9491,1,['redundant'],['redundant']
Availability,"~~Stride for a dimension should be calculated based off the length * stride of the previous innermost dimension with length > 1. I was remembering to do that for the stride but just taking the length of the adjacent dimension.~~. ~~Length-1 dimensions have stride 0 so broadcasting happens implicitly without the need for checks while indexing into the ndarray.~~. Broadcasting was previously handled implicitly by having 0 stride for dimensions of length 1. This doesn't actually work in all scenarios and conflates the concept of a multidimensional index with the underlying representation. Since we already compute the shapes of all intermediate NDArrays before doing any iteration, we can identify broadcasting ""statically"". Instead, loop variables associated with a braodcast are replaced with 0 when indexing into the backing array. This method is more robust and I've already seen works with slicing, which will follow this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6154:859,robust,robust,859,https://hail.is,https://github.com/hail-is/hail/pull/6154,1,['robust'],['robust']
Availability,"​n3011</code></a>]</li>; <li>Use &quot;/sbin/ldconfig&quot; if ldconfig is not found <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7068"">#7068</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Prefer screenshots using XCB over gnome-screenshot <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7143"">#7143</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>; <li>Fixed joined corners for ImageDraw rounded_rectangle() odd dimensions <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7151"">#7151</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/main/CHANGES.rst"">pillow's changelog</a>.</em></p>; <blockquote>; <h2>10.0.0 (2023-07-01)</h2>; <ul>; <li>; <p>Fixed deallocating mask images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7246"">#7246</a>; [radarhere]</p>; </li>; <li>; <p>Added ImageFont.MAX_STRING_LENGTH <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7244"">#7244</a>; [radarhere, hugovk]</p>; </li>; <li>; <p>Fix Windows build with pyproject.toml <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7230"">#7230</a>; [hugovk, nulano, radarhere]</p>; </li>; <li>; <p>Do not close provided file handles with libtiff <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7199"">#7199</a>; [radarhere]</p>; </li>; <li>; <p>Convert to HSV if mode is HSV in getcolor() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7226"">#7226</a>; [radarhere]</p>; </li>; <li>; <p>Added alpha_only argument to getbbox() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7123"">#7123</a>; [radarhere. hugovk]</p>; </li>; <li>; <p>Prioritise speed in <em>repr_png</em> <a href=""https://",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:9732,mask,mask,9732,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['mask'],['mask']
Availability,"–c ‘s.id’ –o sample_tmp.tsv ; you can get the sample_tmp file including the names of genes satisfying the screen , but how to check the output as a number in the terminal, like the format shown in showglobals command?. If you run the command:; hail read -i tmp.vds imputesex -m 0.01 exportsamples –o impute_tmp.tsv -c “ID=s.id” exportvcf –o impute_tmp.vcf ; how to obtain the inbreeding coefficient from the impute_tmp file?; 1. The Structure has no filed ***. During the test, there are some similar errors in different modules. For example, if you run the command , ; hail importvcf sample.vcf filtersamples expr --keep -c 'sa.qc.callRate > 0.99' write -o output.vds exportvcf -o sample1.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnotations.anno1,ANNO2=va.MyAnnotations.anno2' -o file.tsv -o sample.tsv ; hail read -i output.vds exportvariants -c 'v,va.pass,va.qc.AF' -o file.tsv ; hail read -i output.vds exportsamples -c 's.id, sa.qc.rTiTv' -o file.tsv; you will get the same fatal error: ‘Struct’ has no field ‘qc’. Is it because the qc isn`t defined in “sa” struct? ; The same problems appeared in sa.pheno, global.genes, va.Myannotations and va.qc . ; hail importvcf sample.vcf annotatevariants expr -c 'va.minorCase = gs.count(sa.pheno.Pheno1 == ""Case"" && g.isHet)’ )‘ exportvcf -o fet_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c ‘global.first10genens = global.genes[:10]‘ exportvcf -o global_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c 'global.nCase = samples.count(sa.pheno.isCase)’ exportvcf -o global_tmp.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnotations.anno1,ANNO2=va.MyAnnotations.anno2' -o file.tsv -o sample.tsv ; 2. PCA error; If you run the command:; hail read -i tmp.vds pca –o pca_tmp.tsv ; you will get the error information: hail: pca: caught exception: requirement failed: Requested k singular values but got k=10 and numCols=0. Why?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/683:1338,error,error,1338,https://hail.is,https://github.com/hail-is/hail/issues/683,3,['error'],['error']
Availability,"‘const class simdpp::arch_avx2::float32<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:32,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/float64x2.h:32:7: note: ‘class simdpp::arch_avx2::float64<2>’ declared here; class float64<2, void> : public any_float64<2, float64<2,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2>]’; libsimdpp-2.0-rc2/simdpp/types/float32x4.h:59:37: required from ‘simdpp::arch_avx2::float32<4>& simdpp::arch_avx2::float32<4>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:598:8: required from ‘void simdpp::arch_avx2::detail::insn::v_sse_transpose32x4(V&, V&, V&, V&) [with V = simdpp::arch_avx2::float32<4>; D = simdpp::arch_avx2::float64<2>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:549:63: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::float32<",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:102099,Mask,MaskCastOverride,102099,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"‘const class simdpp::arch_avx2::float32<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:33,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/float64x4.h:33:7: note: ‘class simdpp::arch_avx2::float64<4>’ declared here; class float64<4, void> : public any_float64<4, float64<4,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float32<8>; T = simdpp::arch_avx2::float64<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float32<8>; T = simdpp::arch_avx2::float64<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float32<8>; T = simdpp::arch_avx2::float64<4>]’; libsimdpp-2.0-rc2/simdpp/types/float32x8.h:53:37: required from ‘simdpp::arch_avx2::float32<8>& simdpp::arch_avx2::float32<8>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::float64<4, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:277:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::float32<8>’ with ‘private’ member ‘simdpp::arch_avx2::float32<8>::d_’ from an array of ‘const class simdpp::arch_avx2::float64<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:48595,Mask,MaskCastOverride,48595,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"’ from an array of ‘const class simdpp::arch_avx2::int32<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:26,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:33:7: note: ‘class simdpp::arch_avx2::int64<2>’ declared here; class int64<2, void> : public any_int64<2, int64<2,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::int64<2>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::int64<2>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::int64<2>]’; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:129:36: required from ‘simdpp::arch_avx2::uint64<2>& simdpp::arch_avx2::uint64<2>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int64<2>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:270:19: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint64<2>’ with ‘private’ member ‘simdpp::arch_avx2::uint64<2>::d_’ from an array of ‘const class simdpp::arch_avx2::int64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file incl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:70647,Mask,MaskCastOverride,70647,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,"’ from an array of ‘const class simdpp::arch_avx2::int32<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:27,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:32:7: note: ‘class simdpp::arch_avx2::int64<4>’ declared here; class int64<4, void> : public any_int64<4, int64<4,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::int32<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::int32<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::int32<8>]’; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:114:36: required from ‘simdpp::arch_avx2::uint64<4>& simdpp::arch_avx2::uint64<4>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int32<8>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:300:19: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint64<4>’ with ‘private’ member ‘simdpp::arch_avx2::uint64<4>::d_’ from an array of ‘const class simdpp::arch_avx2::int32<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file incl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:79578,Mask,MaskCastOverride,79578,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability,“gradle check” ERROR：undefined symbol: cblas_dgemv,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565:15,ERROR,ERROR,15,https://hail.is,https://github.com/hail-is/hail/issues/565,1,['ERROR'],['ERROR']
Availability,"…a2c778969c1422ba. The tabix line iterator works by building a list of virtual offsets from; the index that correspond to a requested interval. We recently; discovered an issue that would lead to a runtime exception if the; following conditions held:. * An offset pair ended exactly on a block boundary.; * The block boundary was exactly the start of a line. In a blocked file with virtual offsets, there are two ways to point to; the start of every block, (previous block start offset, previous block size) or; (current block start offset, 0). Because of the way curOff was calculated in TabixLineIterator, this would; lead to a situation where curOff was the (previous block start offset, previous block size); value, causing the jump to next chunk comparision `!TbiOrd.less64(curOff, offsets(i)._2)`; to fail when it should succeed, causing an extra line to be read, which; then makes the assertion check in the next iteration. Here we revert the improvements made earlier in; 3302850eb9d93c30793dc231a2c778969c1422ba which is where this logic error; was introduced.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9298:1047,error,error,1047,https://hail.is,https://github.com/hail-is/hail/pull/9298,1,['error'],['error']
Availability,"…other headers, error won't be thrown",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2247:16,error,error,16,https://hail.is,https://github.com/hail-is/hail/pull/2247,1,['error'],['error']
Availability,"…poses only. This was added to debug if the resource usage monitoring triggered out of disk space errors again. Since, there's been no issues, let's take this out since it's O(n_jobs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12541:98,error,errors,98,https://hail.is,https://github.com/hail-is/hail/pull/12541,1,['error'],['errors']
Availability,"…ransient_error a bit. I saw another `ConnectionResetError` that [was not retried](https://ci.hail.is/batches/7309548/jobs/105). It was hidden as the cause of a `ClientOSError`. This change ensures that both `is_retry_once_error` and `is_transient_error` both recur through the cause hierarchy regardless of the error type. I also unified the formatting of `is_transient_error` conditions and ensured that we fall through the `if`s in every case except docker. The docker error case was complex and, AFAIK, docker does not raise errors with causes that matter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12930:312,error,error,312,https://hail.is,https://github.com/hail-is/hail/pull/12930,3,['error'],"['error', 'errors']"
Availability,"━━━ 84.2/84.2 kB 22.0 MB/s eta 0:00:00; 904 | amazon-ebs: Installing build dependencies: started; 905 | amazon-ebs: Installing build dependencies: finished with status 'done'; 906 | amazon-ebs: Getting requirements to build wheel: started; 907 | amazon-ebs: Getting requirements to build wheel: finished with status 'done'; 908 | amazon-ebs: Preparing metadata (pyproject.toml): started; 909 | amazon-ebs: Preparing metadata (pyproject.toml): finished with status 'done'; 910 | amazon-ebs: Collecting azure-identity==1.6.0; 911 | amazon-ebs: Downloading azure_identity-1.6.0-py2.py3-none-any.whl (108 kB); 912 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.5/108.5 kB 28.5 MB/s eta 0:00:00; 913 | amazon-ebs: Collecting azure-storage-blob==12.11.0; 914 | amazon-ebs: Downloading azure_storage_blob-12.11.0-py3-none-any.whl (346 kB); 915 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.4/346.4 kB 41.0 MB/s eta 0:00:00; 916 | amazon-ebs: Collecting bokeh<2.0,>1.3; 917 | amazon-ebs: Downloading bokeh-1.4.0.tar.gz (32.4 MB); 918 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 32.4/32.4 MB 48.4 MB/s eta 0:00:00; 919 | amazon-ebs: Preparing metadata (setup.py): started; 920 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 921 | amazon-ebs: Requirement already satisfied: boto3<2.0,>=1.17 in /usr/local/lib/python3.7/site-packages (1.24.78); 922 | amazon-ebs: Requirement already satisfied: botocore<2.0,>=1.20 in /usr/local/lib/python3.7/site-packages (1.27.78); 923 | amazon-ebs: Collecting decorator<5; 924 | amazon-ebs: Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); 925 | amazon-ebs: Collecting Deprecated<1.3,>=1.2.10; 926 | amazon-ebs: Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB); 927 | amazon-ebs: Collecting dill<0.4,>=0.3.1.1; 928 | amazon-ebs: Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB); 929 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.8/95.8 kB 15.3 MB/s eta 0:00:00; 930 | amazon-ebs:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:3350,Down,Downloading,3350,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 113.0 MB/s eta 0:00:00; 976 | ==> amazon-ebs: ERROR: Ignored the following versions that require a different python version: 1.22.0 Requires-Python >=3.8; 1.22.0rc1 Requires-Python >=3.8; 1.22.0rc2 Requires-Python >=3.8; 1.22.0rc3 Requires-Python >=3.8; 1.22.1 Requires-Python >=3.8; 1.22.2 Requires-Python >=3.8; 1.22.3 Requires-Python >=3.8; 1.22.4 Requires-Python >=3.8; 1.23.0 Requires-Python >=3.8; 1.23.0rc1 Requires-Python >=3.8; 1.23.0rc2 Requires-Python >=3.8; 1.23.0rc3 Requires-Python >=3.8; 1.23.1 Requires-Python >=3.8; 1.23.2 Requires-Python >=3.8; 1.23.3 Requires-Python >=3.8; 1.4.0 Requires-Python >=3.8; 1.4.0rc0 Requires-Python >=3.8; 1.4.1 Requires-Python >=3.8; 1.4.2 Requires-Python >=3.8; 1.4.3 Requires-Python >=3.8; 1.4.4 Requires-Python >=3.8; 1.5.0 Requires-Python >=3.8; 1.5.0rc0 Requires-Python >=3.8; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11; 1.9.0 Requires-Python >=3.8,<3.12; 1.9.0rc1 Requires-Python >=3.8,<3.12; 1.9.0rc2 Requires-Python >=3.8,<3.12; 1.9.0rc3 Requires-Python >=3.8,<3.12; 1.9.1 Requires-Python >=3.8,<3.12; 3.0.0.dev10 Requires-Python >=3.8; 3.0.0.dev11 Requires-Python >=3.8; 3.0.0.dev13 Requires-Python >=3.8; 3.0.0.dev14 Requires-Python >=3.8; 3.0.0.dev15 Requires-Python >=3.8; 3.0.0.dev16 Requires-Python >=3.8; 3.0.0.dev17 Requires-Python >=3.8; 3.0.0.dev18 Requires-Python >=3.8; 3.0.0.dev3 Requires-Python >=3.8; 3.0.0.dev4 Requires-Python >=3.8; 3.0.0.dev5 Requires-Python >=3.8; 3.0.0.dev6 Requires-Python >=3.8; 3.0.0.dev7 Requires-Python >=3.8; 3.0.0.dev8 Requires-Python >=3.8; 3.0.0.dev9 Requires-Python >=3.8; 977 | ==> amazon-ebs: ERROR: Could not find a version that satisfies the requirement sys_platform!=win32 (from versions: none); 978 | ==> amazon-ebs: ERROR: No matching distribution found for sys_platform!=win32. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:9583,ERROR,ERROR,9583,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,2,['ERROR'],['ERROR']
Availability,"（1）yum info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:583,Avail,Available,583,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,1,['Avail'],['Available']
Availability,"👍. Either way seems fine to me. Slight preference for `echo` because it's a shell built-in, so no external executables are invoked. AFAICT, [a POSIX-compliant shell](http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html) will do what we want. If there is more than one py4j in that directory things will definitely break. AFAIK, more than one py4j in that directory is an error, correct? If not, we could do something like `$(ls ... | tr '\n' ':')` to list all of them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1573#issuecomment-287766096:55,echo,echo,55,https://hail.is,https://github.com/hail-is/hail/pull/1573#issuecomment-287766096,2,"['echo', 'error']","['echo', 'error']"
Deployability," ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/hail/python/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;. Java stack trace:; java.lang.NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.lambda$getPropsWithPrefix$3(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getLookupKey(HadoopConfigurationProperty.java:120); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getPropsWithPrefix(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsFsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:383); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1516); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(Goo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8343:2438,Configurat,Configuration,2438,https://hail.is,https://github.com/hail-is/hail/issues/8343,1,['Configurat'],['Configuration']
Deployability," ""/usr/lib/python3.9/threading.py"", line 917 in run; File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner; File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap. Thread 0x00007fa5273ff640 (most recent call first):; File ""/usr/local/lib/python3.9/dist-packages/py4j/clientserver.py"", line 58 in run; File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner; File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap. Current thread 0x00007fa52bd6b000 (most recent call first):; File ""/usr/local/lib/python3.9/dist-packages/hail/backend/py4j_backend.py"", line 217 in _rpc; File ""/usr/local/lib/python3.9/dist-packages/hail/backend/backend.py"", line 212 in table_type; ...; ```. Line 217 only does one thing: call `orjson.dumps`. https://github.com/hail-is/hail/blob/b3df76360f931f54688bb03bf5774643c0b8205e/hail/python/hail/backend/py4j_backend.py#L216-L218. Indeed, `orjson` has had [this issue since 3.9.12](https://github.com/ijl/orjson/issues/452) and we just recently updated orjson from 3.9.10 to 3.9.12:. ```; commit d2615543476bde5d01061499c92f26124b85caf3; Author: Dan King <daniel.zidan.king@gmail.com>; Date: Fri Feb 2 14:21:47 2024 -0500. [dependencies] mass update (#14233). ```; The [relevant part of the diff](https://github.com/hail-is/hail/commit/d2615543476bde5d01061499c92f26124b85caf3#diff-332ea445eb23998f4f4e34a9bb687fa533a063641eb05f791c105a187bf0c19bL101-R101; ):; ```diff; -orjson==3.9.10; +orjson==3.9.12; ```. orjson [reduced the frequency of this segfault in 3.9.13](https://github.com/ijl/orjson/commit/58a8bd3e31aa3b5fd3d962fb5b03479fa0014ee9) by eliding some of the code that caused buffer overheads; however, [the problem persists](https://github.com/ijl/orjson/issues/452#issuecomment-1943053799). I complete fix is currently awaiting [pull request review](https://github.com/ijl/orjson/pull/457). Reports:; - https://hail.zulipchat.com/#narrow/stream/127527-team/topic/seg.20faults.20in.20tests; - https://hail.zulipchat.com/#",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14299:1800,update,updated,1800,https://hail.is,https://github.com/hail-is/hail/issues/14299,1,['update'],['updated']
Deployability," ""gs://hail-common/hailctl/dataproc/0.2.129"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/init_notebook.py python/hailtop/hailctl/dataproc/resources/vep-GRCh37.sh python/hailtop/hailctl/dataproc/resources/vep-GRCh38.sh build/deploy/dist/hail-0.2.129-py3-none-any.whl gs://hail-common/hailctl/dataproc/0.2.129; gcloud storage objects update -r gs://hail-common/hailctl/dataproc/0.2.129 --add-acl-grant=entity=AllUsers,role=READER; gcloud storage objects update ""gs://hail-common/hailctl/datap",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:1440,deploy,deploy,1440,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,1,['deploy'],['deploy']
Deployability," $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile=rgb_shapeit); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline; p.run(); ```. ```bash; #! /usr/bash; set -ex. # define tmp directory; __TMP_DIR__=/tmp//pipeline.yG41vqpS/. # __TASK__0 write_input; cp gs://hail-jigold/random_file.txt ${__TMP_DIR__}/rsfKylng. # __TASK__1 write_input; cp gs://hail-jigold/input.bed ${__TMP_DIR__}/xJONBVn7.bed. # __TASK__2 write_input; cp gs://hail-jigold/input.bim ${__TMP_DIR__}/xJONBVn7.bim. # __TASK__3 write_input; cp gs://hail-jigold/input.fam ${__TMP_DIR__}/xJONBVn7.fam. # __TASK__4 subset; __RESOURCE_GROUP__0=${__TMP_DIR__}/xJONBVn7; __RESOURCE_GROUP__1=${__TMP_DIR__}/TB7ZUbj8; __RESOURCE__6=${__TMP_DIR__}/TB7ZUbj8.fam; __RESOURCE__10=${__TMP_DIR__}/EVeRHf7V; __RESOURCE__1=${__TMP_DIR__}/xJONBVn7.bed; __RESOURCE__2=${__TMP_DIR__}/xJONBVn7.bim; __RESOURCE__3=${__TMP_DIR__}/xJONBVn7.fam; __RESOURCE_GROUP__2=${__TMP_DIR__}/MXBQugBx; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}; awk '{ print $1, $2}' ${__RESOURCE__6} | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > ${__RESOURCE__10}; plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741:2203,pipeline,pipeline,2203,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741,1,['pipeline'],['pipeline']
Deployability," &lt;/details&gt;. &lt;br /&gt;; </code></pre>. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pyjwt&package-manager=pip&previous-version=1.7.1&new-version=2.4.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11866:16064,upgrade,upgrade,16064,https://hail.is,https://github.com/hail-is/hail/pull/11866,3,['upgrade'],['upgrade']
Deployability," &{map[""apiVersion"":""v1"" ""kind"":""Namespace"" ""metadata"":map[""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""name"":""batch-pods"" ""namespace"":""""]]}; from server for: ""deployment.yaml"": namespaces ""batch-pods"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get namespaces in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=serviceaccounts"", GroupVersionKind: ""/v1, Kind=ServiceAccount""; Name: ""batch-svc"", Namespace: ""batch-pods""; Object: &{map[""kind"":""ServiceAccount"" ""metadata"":map[""name"":""batch-svc"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""apiVersion"":""v1""]}; from server for: ""deployment.yaml"": serviceaccounts ""batch-svc"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get serviceaccounts in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=roles"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=Role""; Name: ""batch-pods-admin"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""Role"" ""metadata"":map[""name"":""batch-pods-admin"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""rules"":[map[""apiGroups"":[""""] ""resources"":[""pods""] ""verbs"":[""get"" ""list"" ""watch"" ""create"" ""update"" ""patch"" ""delete""]] map[""apiGroups"":[""""] ""resources"":[""pods/log""] ""verbs"":[""get""]]]]}; from server for: ""deployment.yaml"": roles.rbac.authorization.k8s.io ""batch-pods-admin"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get roles.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:1171,deploy,deploy-svc,1171,https://hail.is,https://github.com/hail-is/hail/issues/4609,3,"['configurat', 'deploy']","['configuration', 'deploy-svc']"
Deployability," (1)</li>; <li>Robert Kern (1)</li>; <li>Ilhan Polat (4)</li>; <li>Tyler Reddy (8)</li>; </ul>; <p>A total of 4 people contributed to this release.; People with a &quot;+&quot; by their names contributed a patch for the first time.; This list of names is automatically generated, and may not be fully complete.</p>; <h1>SciPy 1.11.0 Release Notes</h1>; <p>SciPy <code>1.11.0</code> is the culmination of 6 months of hard work. It contains; many new features, numerous bug-fixes, improved test coverage and better; documentation. There have been a number of deprecations and API changes; in this release, which are documented below. All users are encouraged to; upgrade to this release, as there are a large number of bug-fixes and; optimizations. Before upgrading, we recommend that users check that; their own code does not use deprecated SciPy functionality (to do so,; run your code with <code>python -Wd</code> and check for <code>DeprecationWarning</code> s).; Our development attention will now shift to bug-fix releases on the; 1.11.x branch, and on adding new features on the main branch.</p>; <p>This release requires Python <code>3.9+</code> and NumPy <code>1.21.6</code> or greater.</p>; <p>For running on PyPy, PyPy3 <code>6.0+</code> is required.</p>; <h1>Highlights of this release</h1>; <ul>; <li>Several <code>scipy.sparse</code> array API improvements, including <code>sparse.sparray</code>, a new; public base class distinct from the older <code>sparse.spmatrix</code> class,; proper 64-bit index support, and numerous deprecations paving the way to a; modern sparse array experience.</li>; <li><code>scipy.stats</code> added tools for survival analysis, multiple hypothesis testing,; sensitivity analysis, and working with censored data.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/scipy/scipy/commit/cfe80116aaa145061246b8aec0e98248fbefb835""><code>cfe8011</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13228:1564,release,releases,1564,https://hail.is,https://github.com/hail-is/hail/pull/13228,1,['release'],['releases']
Deployability," (2022-11-10)</h2>; <h3>Other Changes</h3>; <h4>Dependency Updates</h4>; <ul>; <li>Upgraded <code>azure-communication-common</code> to 1.2.3</li>; <li>Upgraded <code>azure-core</code> to 1.34.0</li>; </ul>; <h2>azure-data-schemaregistry_1.3.1</h2>; <h2>1.3.1 (2022-11-16)</h2>; <h3>Other Changes</h3>; <h4>Dependency Updates</h4>; <ul>; <li>Update <code>azure-core</code> dependency to <code>1.34.0</code>.</li>; <li>Update <code>azure-core-http-netty</code> dependency to <code>1.12.7</code>.</li>; </ul>; <h2>azure-sdk-bom_1.2.8</h2>; <p>No release notes provided.</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/89123194fe6a4a2f2cdc58535abea9d75d753a79""><code>8912319</code></a> Identity 1.7.1 patch (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32140"">#32140</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/fe0a0dad4771fb7c6105cc412cf9cc8d10722e59""><code>fe0a0da</code></a> Updated versions after patch release. (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32231"">#32231</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/4fc24620f120643d18f85aca1bf3ee53daf7f124""><code>4fc2462</code></a> Increment versions for eventgrid releases (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32230"">#32230</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/de48207d7f57407cebfa549c843135dadeeab15c""><code>de48207</code></a> Nikoudsi/batch data plane sdk release for 2022 10 01 API Change (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32162"">#32162</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/11f08f973490521552c80e876f23ecc1bde99ed7""><code>11f08f9</code></a> Update live test (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32229"">#32229<",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12508:1791,Update,Updated,1791,https://hail.is,https://github.com/hail-is/hail/pull/12508,3,"['Update', 'patch', 'release']","['Updated', 'patch', 'release']"
Deployability," (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/434"">#434</a>)</li>; <li><a href=""https://github.com/fsspec/gcsfs/commit/8f7115216346648bacc57f5910048cab5735b9b3""><code>8f71152</code></a> Add support to additional 'fixed-key-metadata' (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/429"">#429</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/fsspec/gcsfs/compare/2021.04.0...2022.02.0"">compare view</a></li>; </ul>; </details>; <br />. Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11575:3722,upgrade,upgrade,3722,https://hail.is,https://github.com/hail-is/hail/pull/11575,3,['upgrade'],['upgrade']
Deployability," (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1695"">#1695</a>) (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1760"">#1760</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/803a90b7747b8972f51d1407616c51084d97c589""><code>803a90b</code></a> deps: update dependency com.google.cloud:google-cloud-shared-dependencies to ...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/82aacd7922573d6f4779f21cdc83de10616d7a08""><code>82aacd7</code></a> feat: add Autoclass support and sample (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1697"">#1697</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/7e3175a56a06dac0aa0841f221a486bb69b5c9bf""><code>7e3175a</code></a> deps: update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.17...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/140e90911229c876de7b674dd1e61b278e8b07fd""><code>140e909</code></a> deps: update dependency net.jqwik:jqwik to v1.7.1 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1758"">#1758</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/303ba366cc706dd233d497618bb22f8b018a617b""><code>303ba36</code></a> chore: Set <code>rest_numeric_enums = False</code> for all gapic rules explicitly (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1756"">#1756</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/e2d3851e22adac0c2d3cc25a7c772de7ac9c05aa""><code>e2d3851</code></a> chore: require hashes when installing dependencies in owlbot postprocessor jo...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/a67683558eee7590f98a391d915eb3a19fb88f95""><code>a676835</code></a> chore(deps): update dependency com.google.cloud:libraries-bom to v26.1.4 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12456:12966,update,update,12966,https://hail.is,https://github.com/hail-is/hail/pull/12456,1,['update'],['update']
Deployability," (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Clarified relative resource names in gRPC IAM RPCs (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; <li>Clarified the object can be deleted via DeleteObject (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; <li>Updated the document link for <code>Naming Guidelines</code> (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.1.0 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1797"">#1797</a>) (<a href=""https://github.com/googleapis/java-storage/commit/b1d026608a5e3772e8bf77f25f1daf68b007427a"">b1d0266</a>)</li>; <li>Update dependency org.apache.httpcomponents:httpclient to v4.5.14 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1795"">#1795</a>) (<a href=""https://github.com/googleapis/java-storage/commit/cf900f4139f30f89e3c0784467ddc12cc00cf81c"">cf900f4</a>)</li>; <li>Update dependency org.apache.httpcomponents:httpcore to v4.4.16 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1786"">#1786</a>) (<a href=""https://github.com/googleapis/java-storage/commit/3bf403e94c035e6cf936e062a1ced2b5221b3912"">3bf403e</a>)</li>; <li>Update dependency org.apache.httpcomponents:httpmime to v4.5.14 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1796"">#1796</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c9ee3ca8820531cd709bb8f8a58a736813346861"">c9ee3ca</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.18 (<a href=""https://github-redi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12545:2266,Update,Update,2266,https://hail.is,https://github.com/hail-is/hail/pull/12545,2,['Update'],['Update']
Deployability," (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1539"">#1539</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/53ca116ef64123735e5e445258b8b103ad31a26e""><code>53ca116</code></a> Release 2.0.0rc4 (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1538"">#1538</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/4498e97b462688bac2ff3615ac1da1b867b21842""><code>4498e97</code></a> Fix AttributeError when one of <code>css_files</code> is a string (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1537"">#1537</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/84aea9b0ba78b7f07df4b624e5200ed4c842794c""><code>84aea9b</code></a> Increment for next potential release (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1536"">#1536</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/febde39c7cd5f56fe19979adeee05e1e16eadfe2""><code>febde39</code></a> Release 2.0.0rc3 (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1535"">#1535</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/32310a819012589211a68db8d8060fd1e3a499f5""><code>32310a8</code></a> Fix <a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1522"">#1522</a>: fix <code>'str' object has no attribute 'attributes'</code> (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1528"">#1528</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/8ce23cec96f628ac0d29737bf1cf8cc2e750f068""><code>8ce23ce</code></a> Version bump for 2.0rc3 development (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1521"">#1521</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/46f5307dbd32ebb339c3a76514ce5791826ec381""><code>46f5307</code></a> Release 2.0rc2 (<a href=""https://redirect.github.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14502:2478,Release,Release,2478,https://hail.is,https://github.com/hail-is/hail/pull/14502,1,['Release'],['Release']
Deployability," - 2020-09-26</h2>; <h3>Added</h3>; <ul>; <li>New Changelog</li>; <li>Added timezone support to timestamps - <a href=""https://github.com/lalten""><code>@​lalten</code></a></li>; <li>Refactored log record to function - <a href=""https://github.com/georgysavva""><code>@​georgysavva</code></a></li>; <li>Add python 3.8 support - <a href=""https://github.com/tommilligan""><code>@​tommilligan</code></a></li>; </ul>; <h3>Removed</h3>; <ul>; <li>Support for Python 2.7</li>; <li>Debian directory</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/madzak/python-json-logger/commit/076b407aa7f34bc64a729cc77da336fb159d7597""><code>076b407</code></a> Release 2.0.2</li>; <li><a href=""https://github.com/madzak/python-json-logger/commit/f51d8fe76154380cac2fe6a30a944d67dc09df2d""><code>f51d8fe</code></a> added test/build requirements to ci file</li>; <li><a href=""https://github.com/madzak/python-json-logger/commit/b07b580670c6c4e340c372c73d0e76cdddc8b456""><code>b07b580</code></a> moved release out of test workflow. setup.cfg specifies a proper wheel now</li>; <li><a href=""https://github.com/madzak/python-json-logger/commit/4df12f93928bb66b053d8693e5cd60f38588069a""><code>4df12f9</code></a> removing missed python3.4 support data</li>; <li><a href=""https://github.com/madzak/python-json-logger/commit/7c40f4abf46ee6f3b5f4290f12ff3e9872ff9892""><code>7c40f4a</code></a> removed typo around build badge</li>; <li><a href=""https://github.com/madzak/python-json-logger/commit/0837da313ea796e353b40b574e3f82591a698caf""><code>0837da3</code></a> added env to git ignore</li>; <li><a href=""https://github.com/madzak/python-json-logger/commit/c58d7d4660780e3d4872189f9889200917a8930f""><code>c58d7d4</code></a> manifest will now include readme for setup.py long description. tox and</li>; <li><a href=""https://github.com/madzak/python-json-logger/commit/03e831488ce55e4aaf886806b2c66082fb8c4027""><code>03e8314</code></a> Removed references to travis</li>;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11467:3170,release,release,3170,https://hail.is,https://github.com/hail-is/hail/pull/11467,1,['release'],['release']
Deployability," --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5772,deploy,deployments,5772,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,1,['deploy'],['deployments']
Deployability," 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --disable-tool-default-read-filters false --minimum-mapping-quality 20 --disable-tool-default-annotations false --enable-all-annotations false"",Version=4.0.7.0,Date=""December 21, 2018 6:32:37 PM EST"">; ##source=HaplotypeCaller; ##source=10X/pipelines/stages/snpindels/attach_bcs_snpindels 2.2.2; ##source=10X/pipelines/stages/snpindels/phase_snpindels 2.2.2; ##bcftools_filterVersion=1.1-3-g9058fce+htslib-1.1-1-g03a4427; ##bcftools_filterCommand=filter canonicalized.vcftmp.vcf; ##bcftools_filterCommand=filter -O v --soft-filter 10X_RESCUED_MOLECULE_HIGH_DIVERSITY -e '(((RESCUED+NOT_RESCUED) > 0 & RESCUED/(RESCUED+NOT_RESCUED) > 0.1) & (MMD == -1 | MMD >= 3.0)) ' -m '+' /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SNPINDEL_PHASER/_SNPINDEL_CALLER/POPULATE_INFO_FIELDS/fork0/chnk00-u77951dd300/files/default.vcf.gztmp2.vcf; ##bcftools_filterCommand=filter -O v --soft-filter 10X_QUAL_FILTER -e '(%QUAL <= 15 || (AF[0] > 0.5 && %QUAL < 50))' -m '+' /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SNPINDEL_PHASER/_SNPINDEL_CALLER/POPULATE_INFO_FIELDS/fork0/chnk00-u77951dd300/files/default.vcf.gztmp2.vcf; ##bcftools_filterCommand=filter -O v --soft-filter 10X_ALLELE_FRACTION_FILTER -e '(AO[0] < 2 || AO[0]/(AO[0] + RO) < 0.15)' -m '+' /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SN",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:5190,pipeline,pipelines,5190,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['pipeline'],['pipelines']
Deployability," 1.5.5.; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/erdewit/nest_asyncio/commit/3cfd2c8bc453174ec0be57cd3bb8ec16dbcde1b4""><code>3cfd2c8</code></a> Potential fix for issue <a href=""https://github-redirect.dependabot.com/erdewit/nest_asyncio/issues/65"">#65</a></li>; <li><a href=""https://github.com/erdewit/nest_asyncio/commit/616d9a5e15d8d75e3343422778e49af2e9ac80ea""><code>616d9a5</code></a> Patch asyncio.get_event_loop to not require a running loop, fixes <a href=""https://github-redirect.dependabot.com/erdewit/nest_asyncio/issues/70"">#70</a></li>; <li>See full diff in <a href=""https://github.com/erdewit/nest_asyncio/compare/v1.5.4...v1.5.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=nest-asyncio&package-manager=pip&previous-version=1.5.4&new-version=1.5.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12209:1094,update,updates,1094,https://hail.is,https://github.com/hail-is/hail/pull/12209,1,['update'],['updates']
Deployability," 1511, in import_table; t = Table(TableRead(tr)); File ""/hail/python/hail/table.py"", line 334, in __init__; self._type = self._tir.typ; File ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/hail/python/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;. Java stack trace:; java.lang.NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.lambda$getPropsWithPrefix$3(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getLookupKey(HadoopConfigurationProperty.java:120); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getPropsWithPrefix(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsFsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:383); at com.google.cloud.hadoop.fs.gcs.GoogleHadoop",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8343:2298,Configurat,Configuration,2298,https://hail.is,https://github.com/hail-is/hail/issues/8343,1,['Configurat'],['Configuration']
Deployability," 2.10 which; removed the dependency on setuptools and pkg_resources, and added; limited support for namespace packages. The changes caused issues; when using Pytest. Due to the difficulty in supporting Python 2 and; :pep:<code>451</code> simultaneously, the changes are reverted until 3.0.; :pr:<code>1182</code></li>; <li>Fix line numbers in error messages when newlines are stripped.; :pr:<code>1178</code></li>; <li>The special <code>namespace()</code> assignment object in templates works in; async environments. :issue:<code>1180</code></li>; <li>Fix whitespace being removed before tags in the middle of lines when; <code>lstrip_blocks</code> is enabled. :issue:<code>1138</code></li>; <li>:class:<code>~nativetypes.NativeEnvironment</code> doesn't evaluate; intermediate strings during rendering. This prevents early; evaluation which could change the value of an expression.; :issue:<code>1186</code></li>; </ul>; <h2>Version 2.11.1</h2>; <p>Released 2020-01-30</p>; <ul>; <li>Fix a bug that prevented looking up a key after an attribute; (<code>{{ data.items[1:] }}</code>) in an async template. :issue:<code>1141</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/jinja/commit/cf215390d4a4d6f0a4de27e2687eed176878f13d""><code>cf21539</code></a> release version 2.11.3</li>; <li><a href=""https://github.com/pallets/jinja/commit/15ef8f09b659f9100610583938005a7a10472d4d""><code>15ef8f0</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1343"">#1343</a> from pallets/urlize-speedup</li>; <li><a href=""https://github.com/pallets/jinja/commit/ef658dc3b6389b091d608e710a810ce8b87995b3""><code>ef658dc</code></a> speed up urlize matching</li>; <li><a href=""https://github.com/pallets/jinja/commit/eeca0fecc3318d43f61bc340ad61db641b861ade""><code>eeca0fe</code></a> Merge pull request <a href=""https://github-redirect.depen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10209:4777,Release,Released,4777,https://hail.is,https://github.com/hail-is/hail/pull/10209,1,['Release'],['Released']
Deployability," 2019-01-22 13:11:24 Client: INFO: Preparing resources for our AM container; 2019-01-22 13:11:24 HadoopFSCredentialProvider: INFO: getting token for: hdfs://scc/user/farrell; 2019-01-22 13:11:24 DFSClient: INFO: Created HDFS_DELEGATION_TOKEN token 11364 for farrell on ha-hdfs:scc; 2019-01-22 13:11:26 Client: WARN: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 2019-01-22 13:11:29 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_libs__5184408978318087972.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_libs__5184408978318087972.zip; 2019-01-22 13:11:30 Client: INFO: Uploading resource file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/hail-all-spark.jar; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/share/pkg/spark/2.2.1/install/python/lib/pyspark.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/pyspark.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); g",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:15787,install,install,15787,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability," 3.11 tests</li>; <li>Additional commits viewable in <a href=""https://github.com/python-parsy/parsy/compare/v1.1.0...v2.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=parsy&package-manager=pip&previous-version=1.1.0&new-version=2.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). You can trigger a rebase of this PR by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>> **Note**; > Automatic rebases have been disabled on this pull request as it has been open for over 30 days.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12934:5369,upgrade,upgrade,5369,https://hail.is,https://github.com/hail-is/hail/pull/12934,3,['upgrade'],['upgrade']
Deployability," 3.6. The `hail-ubuntu` image explicitly installs Python 3.7. I'm happy to drop linting for Python 3.6 from build.yaml if compilers team is OK with that (ask Tim). We are already using Ubuntu 20.04 for our services and tests. See below for details, but `hail-ubuntu` is based on 20.04. We explicitly install JDK 8 in the `base` image. ---. This PR doesn't change the hail-ubuntu image which is the basis for nearly all our images. `DOCKER_ROOT_IMAGE` seems to have been introduced [here](https://github.com/hail-is/hail/pull/9660). That's my bad for not flagging in the review that this isn't actually a ""root image"". `DOCKER_ROOT_IMAGE` is just some Linux image with the standard utils. It was introduced as a distinct global concept when Docker Hub began enforcing rate limits (so we needed all our test images to live in GCR). If you look at the occurrences of `DOCKER_ROOT_IMAGE` or `docker_root_image` you'll find it's almost exclusively used in the tests except for one occurrence in `build-batch-worker-image-startup-gcp.sh`. We could just remove that line. That line is an attempt to keep a relatively recent version of the ubuntu image cached on the worker VM so that we can save some time when pulling the worker Docker image. In practice, the ubuntu image is extraordinarily tiny and quickly becomes out of date (because we rarely rebuild the VM). hail-ubuntu uses a timestamped ubuntu 20.04 tag: `ubuntu:focal-20201106`. I did this because we kept getting screwed by new ubuntu images getting released which were incompatible with us. We would only find out later when we changed the hail-ubuntu Dockerfile and triggered a refetch of the latest image at the `ubuntu:18.04` tag which included the breaking changes. You're correct that this is technical debt of ours. DOCKER_ROOT_IMAGE should really be SOME_LINUX_IMAGE and we shouldn't bother trying to fetch it in the worker image startup script. You're fighting the good fight, but, man, keeping a growing software project clean is hard.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11046#issuecomment-965578805:1715,release,released,1715,https://hail.is,https://github.com/hail-is/hail/pull/11046#issuecomment-965578805,1,['release'],['released']
Deployability," 3.7+</li>; <li>When installing with pip, the <code>docker[tls]</code> extra is deprecated and a no-op,; use <code>docker</code> for same functionality (TLS support is always available now)</li>; <li>Native Python SSH client (used by default / <code>use_ssh_client=False</code>) will now; reject unknown host keys with <code>paramiko.ssh_exception.SSHException</code></li>; <li>Short IDs are now 12 characters instead of 10 characters (same as Docker CLI)</li>; <li>Version metadata is now exposed as <code>__version__</code></li>; </ul>; <h3>✨ Features</h3>; <ul>; <li>Python 3.10 support</li>; <li>Automatically negotiate most secure TLS version</li>; <li>Add <code>platform</code> (e.g. <code>linux/amd64</code>, <code>darwin/arm64</code>) to container create &amp; run</li>; <li>Add support for <code>GlobalJob</code> and <code>ReplicatedJobs</code> for Swarm</li>; <li>Add <code>remove()</code> method on <code>Image</code></li>; <li>Add <code>force</code> param to <code>disable()</code> on <code>Plugin</code></li>; </ul>; <h3>🐛 Bugfixes</h3>; <ul>; <li>Fix install issues on Windows related to <code>pywin32</code></li>; <li>Do not accept unknown SSH host keys in native Python SSH mode</li>; <li>Use 12 character short IDs for consistency with Docker CLI</li>; <li>Ignore trailing whitespace in <code>.dockerignore</code> files</li>; <li>Fix IPv6 host parsing when explicit port specified</li>; <li>Fix <code>ProxyCommand</code> option for SSH connections</li>; <li>Do not spawn extra subshell when launching external SSH client</li>; <li>Improve exception semantics to preserve context</li>; <li>Documentation improvements (formatting, examples, typos, missing params)</li>; </ul>; <h3>🔧 Miscellaneous</h3>; <ul>; <li>Upgrade dependencies in <code>requirements.txt</code> to latest versions</li>; <li>Remove extraneous transitive dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12475:3610,install,install,3610,https://hail.is,https://github.com/hail-is/hail/pull/12475,1,['install'],['install']
Deployability," 5.0.1 (released Jun 03, 2022)</h1>; <h2>Bugs fixed</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10498"">#10498</a>: gettext: TypeError is raised when sorting warning messages if a node; has no line number</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10493"">#10493</a>: html theme: :rst:dir:<code>topic</code> directive is rendered incorrectly with; docutils-0.18</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10495"">#10495</a>: IndexError is raised for a :rst:role:<code>kbd</code> role having a separator</li>; </ul>; <h1>Release 5.0.0 (released May 30, 2022)</h1>; <h2>Dependencies</h2>; <p>5.0.0 b1</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10164"">#10164</a>: Support <code>Docutils 0.18</code>_. Patch by Adam Turner.</li>; </ul>; <p>.. _Docutils 0.18: <a href=""https://docutils.sourceforge.io/RELEASE-NOTES.html#release-0-18-2021-10-26"">https://docutils.sourceforge.io/RELEASE-NOTES.html#release-0-18-2021-10-26</a></p>; <h2>Incompatible changes</h2>; <p>5.0.0 b1</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10031"">#10031</a>: autosummary: <code>sphinx.ext.autosummary.import_by_name()</code> now raises; <code>ImportExceptionGroup</code> instead of <code>ImportError</code> when it failed to import; target object. Please handle the exception if your extension uses the; function to import Python object. As a workaround, you can disable the; behavior via <code>grouped_exception=False</code> keyword argument until v7.0.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9962"">#9962</a>: texinfo: Customizing styles of emphasized text via <code>@definfoenclose</code>; command was not supported because the command was deprecated since texinfo 6.8</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/2068"">#2068</a>: :con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11885:2090,release,release-,2090,https://hail.is,https://github.com/hail-is/hail/pull/11885,1,['release'],['release-']
Deployability," 5.0.1.; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/python/typeshed/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=types-chardet&package-manager=pip&previous-version=4.0.4&new-version=5.0.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12001:1690,upgrade,upgrade,1690,https://hail.is,https://github.com/hail-is/hail/pull/12001,3,['upgrade'],['upgrade']
Deployability," :user:<code>anesabml</code>.</p>; <p><code>[#5704](https://github.com/aio-libs/aiohttp/issues/5704) &lt;https://github.com/aio-libs/aiohttp/issues/5704&gt;</code>_</p>; </li>; <li>; <p>Added a middleware type alias <code>aiohttp.typedefs.Middleware</code>.</p>; <p><code>[#5898](https://github.com/aio-libs/aiohttp/issues/5898) &lt;https://github.com/aio-libs/aiohttp/issues/5898&gt;</code>_</p>; </li>; <li>; <p>Exported <code>HTTPMove</code> which can be used to catch any redirection request; that has a location -- :user:<code>dreamsorcerer</code>.</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/45b2c2c5773f0ee0d35fce8ff5716c78e91d9135""><code>45b2c2c</code></a> Release v3.9.0 (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7843"">#7843</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/5d59d3d6ac073a7db5e5d2234e03a67da5dec48a""><code>5d59d3d</code></a> Release v3.9.0rc0 (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7840"">#7840</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/c806814a8aaad1661d75e6e2b8d619d6c44d331d""><code>c806814</code></a> Release v3.9.0rc0 (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7838"">#7838</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/e07a1bdaacfb83fda3ea8f668edacb36c6c125df""><code>e07a1bd</code></a> Use timestamp instead of datetime to achieve faster cookie expiration… (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7837"">#7837</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/53476dfd4ef4fb1bb74a267714bbc39eda71b403""><code>53476df</code></a> Disallow arbitrary sequence types in version (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7835"">#7835</a>) (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7836"">#7836</a>)</li>; <li><a href=""https",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14027:7228,Release,Release,7228,https://hail.is,https://github.com/hail-is/hail/pull/14027,6,['Release'],['Release']
Deployability," ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **496/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 4.2 | Information Exposure Through Sent Data <br/>[SNYK-PYTHON-URLLIB3-6002459](https://snyk.io/vuln/SNYK-PYTHON-URLLIB3-6002459) | `urllib3:` <br> `1.26.17 -> 1.26.18` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIxMjY5MWQyMS0wMzk1LTQxYjMtODBkMi1mMjEyODMwZjY2ZWEiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjEyNjkxZDIxLTAzOTUtNDFiMy04MGQyLWYyMTI4MzBmNjZlYSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13873:3785,upgrade,upgraded,3785,https://hail.is,https://github.com/hail-is/hail/pull/13873,1,['upgrade'],['upgraded']
Deployability," ;;; 3: len 7; hex 020000018e1add; asc ;;; 4: len 7; hex 52756e6e696e67; asc Running;;; 5: len 30; hex 5b5b5b2264676f6c64737465222c202264676f6c647374652d6773612d6b; asc [[[""dgoldste"", ""dgoldste-gsa-k; (total 69 bytes);; 6: len 1; hex 80; asc ;;; 7: len 4; hex 800003e8; asc ;;; 8: SQL NULL;; 9: len 4; hex 80000000; asc ;;; 10: len 1; hex 80; asc ;;; 11: len 8; hex 8000000000000000; asc ;;; 12: len 6; hex 395664676a4c; asc 9VdgjL;;; 13: len 8; hex 7374616e64617264; asc standard;;; 14: len 4; hex 80000001; asc ;;; 15: len 4; hex 80000001; asc ;;; 16: len 8; hex 8000000000000004; asc ;;; 17: SQL DEFAULT;. *** (3) TRANSACTION:; TRANSACTION 2486516, ACTIVE 0 sec starting index read; mysql tables in use 1, locked 1; LOCK WAIT 6 lock struct(s), heap size 1128, 3 row lock(s); MySQL thread id 519, OS thread handle 140330967815936, query id 4742236 10.32.3.39 dgoldste-batch-user statistics; SELECT 1 INTO dummy_lock FROM instances_free_cores_mcpu; WHERE instances_free_cores_mcpu.name = in_instance_name; FOR UPDATE. *** (3) HOLDS THE LOCK(S):; RECORD LOCKS space id 312 page no 896 n bits 168 index PRIMARY of table `dgoldste-batch`.`jobs` trx id 2486516 lock_mode X locks rec but not gap; Record lock, heap no 27 PHYSICAL RECORD: n_fields 18; compact format; info bits 128; 0: len 8; hex 8000000000000095; asc ;;; 1: len 4; hex 80002156; asc !V;;; 2: len 6; hex 00000025eff8; asc % ;;; 3: len 7; hex 020000018e1add; asc ;;; 4: len 7; hex 52756e6e696e67; asc Running;;; 5: len 30; hex 5b5b5b2264676f6c64737465222c202264676f6c647374652d6773612d6b; asc [[[""dgoldste"", ""dgoldste-gsa-k; (total 69 bytes);; 6: len 1; hex 80; asc ;;; 7: len 4; hex 800003e8; asc ;;; 8: SQL NULL;; 9: len 4; hex 80000000; asc ;;; 10: len 1; hex 80; asc ;;; 11: len 8; hex 8000000000000000; asc ;;; 12: len 6; hex 395664676a4c; asc 9VdgjL;;; 13: len 8; hex 7374616e64617264; asc standard;;; 14: len 4; hex 80000001; asc ;;; 15: len 4; hex 80000001; asc ;;; 16: len 8; hex 8000000000000004; asc ;;; 17: SQL DEFAULT;. *** (3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14380:7948,UPDATE,UPDATE,7948,https://hail.is,https://github.com/hail-is/hail/issues/14380,1,['UPDATE'],['UPDATE']
Deployability," </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ai/nanoid/commit/23b136929a6d58f32e31b269534a3ce3f680a086""><code>23b1369</code></a> Release 3.2 version</li>; <li><a href=""https://github.com/ai/nanoid/commit/967788efce880960512f969a56f8f22f3fc20bae""><code>967788e</code></a> Remove TS test tools</li>; <li><a href=""https://github.com/ai/nanoid/commit/27eaa90cd207a7782bbcf17343092ae87dd62164""><code>27eaa90</code></a> Simplify new binary tool</li>; <li><a href=""https://github.com/ai/nanoid/commit/a9d91239931dc77506381874826d297aee71d6ef""><code>a9d9123</code></a> Update dependencies</li>; <li><a href=""https://github.com/ai/nanoid/commit/32b9bdaab1fbc28576b17de8516164ce0360f292""><code>32b9bda</code></a> Allows passing size or custom alphabet via cli as args (<a href=""https://github-redirect.dependabot.com/ai/nanoid/issues/334"">#334</a>)</li>; <li><a href=""https://github.com/ai/nanoid/commit/246d5f87b6b34e23b5e401bdf3da1f80c810ac4c""><code>246d5f8</code></a> Update vite</li>; <li><a href=""https://github.com/ai/nanoid/commit/afdf9c92b41427f35476fbe14b5af5d73dd7fbdb""><code>afdf9c9</code></a> doc: Fixed Typo (<a href=""https://github-redirect.dependabot.com/ai/nanoid/issues/335"">#335</a>)</li>; <li><a href=""https://github.com/ai/nanoid/commit/90a446fef3ecaac78e5af2ea01025c4f40182e2b""><code>90a446f</code></a> Update benchmark results</li>; <li><a href=""https://github.com/ai/nanoid/commit/8ba2319b579895cc1f9060b9946a44852f97c509""><code>8ba2319</code></a> bench: add <code>@​napi-rs/uuid</code> v4 (<a href=""https://github-redirect.dependabot.com/ai/nanoid/issues/333"">#333</a>)</li>; <li><a href=""https://github.com/ai/nanoid/commit/f4257780ece488734a65c176e80c2fd8ab6aab8e""><code>f425778</code></a> Release 3.1.32 version</li>; <li>Additional commits viewable in <a href=""https://github.com/ai/nanoid/compare/3.1.23...3.2.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badg",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11284:2430,Update,Update,2430,https://hail.is,https://github.com/hail-is/hail/pull/11284,2,['Update'],['Update']
Deployability," </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/da4f3c3f209aea47d69c4faf90029a6933bd3a60""><code>da4f3c3</code></a> [DOCS] Add 8.5.3 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2045"">#2045</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2050"">#2050</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/79d592abdce1cd90845d153afab8b66069e2a172""><code>79d592a</code></a> [DOCS] Add 8.5.2 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2042"">#2042</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2049"">#2049</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/712f518822ff281abdd6f83c2e0ea97857dbf6ba""><code>712f518</code></a> [DOCS] Add 8.5.1 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2040"">#2040</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/0a8e0bca839408ba7cdd4e1e4ef669894f29e96f""><code>0a8e0bc</code></a> [DOCS] Add 8.5.0 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2032"">#2032</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/4d09926f840998a20d4f45380723d2d77b46544a""><code>4d09926</code></a> [DOCS] Add 8.4.3 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2028"">#2028</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/d498aa068ef552aef9de3325e9d105611e5adbba""><code>d498aa0</code></a> Update index.adoc (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2000"">#2000</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/e90ae30e6977b43eea3e08c8",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12601:2913,release,release,2913,https://hail.is,https://github.com/hail-is/hail/pull/12601,1,['release'],['release']
Deployability," </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jpadilla/pyjwt/blob/master/CHANGELOG.rst"">pyjwt's changelog</a>.</em></p>; <blockquote>; <h2><code>v2.4.0 &lt;https://github.com/jpadilla/pyjwt/compare/2.3.0...2.4.0&gt;</code>__</h2>; <p>Security</p>; <pre><code>; - [CVE-2022-29217] Prevent key confusion through non-blocklisted public key formats. https://github.com/jpadilla/pyjwt/security/advisories/GHSA-ffqj-6fqr-9h24; <p>Changed</p>; <pre><code>; - Explicit check the key for ECAlgorithm by @estin in https://github.com/jpadilla/pyjwt/pull/713; - Raise DeprecationWarning for jwt.decode(verify=...) by @akx in https://github.com/jpadilla/pyjwt/pull/742. Fixed; ~~~~~. - Don't use implicit optionals by @rekyungmin in https://github.com/jpadilla/pyjwt/pull/705; - documentation fix: show correct scope for decode_complete() by @sseering in https://github.com/jpadilla/pyjwt/pull/661; - fix: Update copyright information by @kkirsche in https://github.com/jpadilla/pyjwt/pull/729; - Don't mutate options dictionary in .decode_complete() by @akx in https://github.com/jpadilla/pyjwt/pull/743. Added; ~~~~~. - Add support for Python 3.10 by @hugovk in https://github.com/jpadilla/pyjwt/pull/699; - api_jwk: Add PyJWKSet.__getitem__ by @woodruffw in https://github.com/jpadilla/pyjwt/pull/725; - Update usage.rst by @guneybilen in https://github.com/jpadilla/pyjwt/pull/727; - Docs: mention performance reasons for reusing RSAPrivateKey when encoding by @dmahr1 in https://github.com/jpadilla/pyjwt/pull/734; - Fixed typo in usage.rst by @israelabraham in https://github.com/jpadilla/pyjwt/pull/738; - Add detached payload support for JWS encoding and decoding by @fviard in https://github.com/jpadilla/pyjwt/pull/723; - Replace various string interpolations with f-strings by @akx in https://github.com/jpadilla/pyjwt/pull/744; - Update CHANGELOG.rst by @hipertracker in https://github.com/jpadilla/pyjwt/p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11866:9848,Update,Update,9848,https://hail.is,https://github.com/hail-is/hail/pull/11866,1,['Update'],['Update']
Deployability," </li>; <li>; <p>Added <code>examples/bf.py</code> Brainf*ck parser/executor example. Illustrates using; a pyparsing grammar to parse language syntax, and attach executable AST nodes to; the parsed results.</p>; </li>; </ul>; <h2>Version 3.1.0b1 - April, 2023</h2>; <ul>; <li>; <p>Added support for Python 3.12.</p>; </li>; <li>; <p>API CHANGE: A slight change has been implemented when unquoting a quoted string; parsed using the <code>QuotedString</code> class. Formerly, when unquoting and processing; whitespace markers such as \t and \n, these substitutions would occur first, and</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/c09eb6e4bb283b375e53cfe851bb6a63ed3957bb""><code>c09eb6e</code></a> Minor update to HowToUsePyparsing.rst</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/395431ab3db9cfe59ddcb22b431caea74e623a1e""><code>395431a</code></a> Prep for release</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/54b39a52dce9a0e438b11c76bc6e1dddaf490333""><code>54b39a5</code></a> Fix regression in SkipTo when ignoring an ignoreExpr, and failed to also igno...</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/411c8ab6f697cb7b96f7fd1af37fefe9e3ce422b""><code>411c8ab</code></a> Handle case where Word(min &gt; 1) with differing init and body chars</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/1936b36823ae162698dff5184061d71e5d0bd39b""><code>1936b36</code></a> Handle case where Word(min &gt; 1) (fixes <a href=""https://redirect.github.com/pyparsing/pyparsing/issues/502"">#502</a>)</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/421e0fcdbc76fcfb43de9c97c89872bc485d8d40""><code>421e0fc</code></a> Update timestamp and CHANGES file to reflect recent PRs; added another test t...</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/1355e76c16f74b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13345:7381,release,release,7381,https://hail.is,https://github.com/hail-is/hail/pull/13345,1,['release'],['release']
Deployability," <a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/10544"">#10544</a> from deannagarcia/3.20.x</li>; <li><a href=""https://github.com/protocolbuffers/protobuf/commit/ae718b39020ae6e6f8f5568e357d6893fd0fd29c""><code>ae718b3</code></a> Add missing includes</li>; <li><a href=""https://github.com/protocolbuffers/protobuf/commit/b4c395aaedfacb32e2414d361fa85968c0991b34""><code>b4c395a</code></a> Apply patch</li>; <li><a href=""https://github.com/protocolbuffers/protobuf/commit/6439c5c01349e74d4deb57c844a7ad4b7b13a302""><code>6439c5c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/10531"">#10531</a> from protocolbuffers/deannagarcia-patch-7</li>; <li><a href=""https://github.com/protocolbuffers/protobuf/commit/22c79e6e4ca8be2bc2f700b2cdddca84d84659ce""><code>22c79e6</code></a> Update version.json</li>; <li><a href=""https://github.com/protocolbuffers/protobuf/commit/c1a2d2ec29314975e725021ffe4334926dbaa56c""><code>c1a2d2e</code></a> Fix python release on macos (<a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/10512"">#10512</a>)</li>; <li><a href=""https://github.com/protocolbuffers/protobuf/commit/a826282e15efe3ae3a2aebb040fb1691b2233a1e""><code>a826282</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/10505"">#10505</a> from deannagarcia/3.20.x</li>; <li><a href=""https://github.com/protocolbuffers/protobuf/commit/7639a710e10beb47bfc62f363680f7b04e8b3d26""><code>7639a71</code></a> Add version file</li>; <li>Additional commits viewable in <a href=""https://github.com/protocolbuffers/protobuf/compare/v3.20.1...v3.20.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=protobuf&package-manager=pip&previous-version=3.20.1&new-version=3.20.2)](https://docs.github.com/en/github/managing-security-vuln",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12223:2161,release,release,2161,https://hail.is,https://github.com/hail-is/hail/pull/12223,3,['release'],['release']
Deployability," <a href=""https://github.com/Azure/azure-sdk-for-python/compare/azure-storage-blob_12.8.1...azure-storage-blob_12.10.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=azure-storage-blob&package-manager=pip&previous-version=12.8.1&new-version=12.10.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11610:7275,upgrade,upgrade,7275,https://hail.is,https://github.com/hail-is/hail/pull/11610,3,['upgrade'],['upgrade']
Deployability," <a href=""https://github.com/Azure/azure-sdk-for-python/compare/azure-storage-blob_12.8.1...azure-storage-blob_12.11.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=azure-storage-blob&package-manager=pip&previous-version=12.8.1&new-version=12.11.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11703:7753,upgrade,upgrade,7753,https://hail.is,https://github.com/hail-is/hail/pull/11703,3,['upgrade'],['upgrade']
Deployability," <a href=""https://github.com/Jackenmen""><code>@​Jackenmen</code></a> in <a href=""https://github-redirect.dependabot.com/saghul/pycares/pull/174"">saghul/pycares#174</a></li>; <li>Fix tests that depended on external sites by <a href=""https://github.com/Jackenmen""><code>@​Jackenmen</code></a> in <a href=""https://github-redirect.dependabot.com/saghul/pycares/pull/180"">saghul/pycares#180</a></li>; <li>Complete the Python 3.11 support by <a href=""https://github.com/Jackenmen""><code>@​Jackenmen</code></a> in <a href=""https://github-redirect.dependabot.com/saghul/pycares/pull/179"">saghul/pycares#179</a></li>; <li>Drop CPython 3.6 by <a href=""https://github.com/saghul""><code>@​saghul</code></a> in <a href=""https://github-redirect.dependabot.com/saghul/pycares/pull/181"">saghul/pycares#181</a></li>; <li>Improve test compatibility with pytest by <a href=""https://github.com/saghul""><code>@​saghul</code></a> in <a href=""https://github-redirect.dependabot.com/saghul/pycares/pull/182"">saghul/pycares#182</a></li>; <li>Update c-ares submodule to 1.18.1 by <a href=""https://github.com/saghul""><code>@​saghul</code></a> in <a href=""https://github-redirect.dependabot.com/saghul/pycares/pull/183"">saghul/pycares#183</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/saghul/pycares/compare/pycares-4.2.2...pycares-4.3.0"">https://github.com/saghul/pycares/compare/pycares-4.2.2...pycares-4.3.0</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/saghul/pycares/commit/036aafd253b39f7459a86a5574daae8c5b681bc2""><code>036aafd</code></a> Bump version to 4.3.0</li>; <li><a href=""https://github.com/saghul/pycares/commit/22a37d760349787704f8901c19e149915a7f1b26""><code>22a37d7</code></a> Update c-ares submodule to 1.18.1</li>; <li><a href=""https://github.com/saghul/pycares/commit/b5165834724340d27da642f554431514fd62ccb4""><code>b516583</code></a> Improve test compatibility with pytest</li>; <li><a href=""https://githu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12559:1377,Update,Update,1377,https://hail.is,https://github.com/hail-is/hail/pull/12559,1,['Update'],['Update']
Deployability," <a href=""https://github.com/tkashem""><code>@​tkashem</code></a>) [SIG API Machinery]</li>; <li>Fixes using server-side apply with APIService resources (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/100713"">kubernetes/kubernetes#100713</a>, <a href=""https://github.com/kevindelgado""><code>@​kevindelgado</code></a>) [SIG API Machinery, Apps, Scheduling and Testing]</li>; <li>Regenerate protobuf code to fix CVE-2021-3121 (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/100515"">kubernetes/kubernetes#100515</a>, <a href=""https://github.com/joelsmith""><code>@​joelsmith</code></a>) [SIG API Machinery, Auth, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation, Node and Storage]</li>; <li>Kubernetes is now built using go1.15.8 (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/99093"">kubernetes/kubernetes#99093</a>, <a href=""https://github.com/cpanato""><code>@​cpanato</code></a>) [SIG Cloud Provider, Instrumentation, Release and Testing]</li>; <li>Fix conversions for custom metrics. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/94654"">kubernetes/kubernetes#94654</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>) [SIG Instrumentation]</li>; <li>A new alpha-level field, <code>SupportsFsGroup</code>, has been introduced for CSIDrivers to allow them to specify whether they support volume ownership and permission modifications. The <code>CSIVolumeSupportFSGroup</code> feature gate must be enabled to allow this field to be used. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92001"">kubernetes/kubernetes#92001</a>, <a href=""https://github.com/huffmanca""><code>@​huffmanca</code></a>) [SIG API Machinery, CLI and Storage]</li>; <li>Added pod version skew strategy for seccomp profile to synchronize the deprecated annotations with the new API Server fields. Please see the corresponding section <a href=""https://github.com/kube",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:2006,Release,Release,2006,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['Release'],['Release']
Deployability," <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.1/whatsnew/v1.4.1.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.4.0</h2>; <p>This release includes some new features, bug fixes, and performance improvements. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.0/whatsnew/v1.4.0.html"">full whatsnew</a> for a list of all the changes. pandas 1.4.0 supports Python 3.8 and higher.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install -c conda-forge pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.4.0rc0</h2>; <p>We are pleased to announce a release candidate for pandas 1.4.0. If all goes well, we'll release pandas 1.4.0 in about two weeks.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4/whatsnew/v1.4.0.html"">whatsnew</a> for a list of all the changes. pandas 1.4.0 supports Python 3.8 and higher.</p>; <p>The release will be available on conda-forge and PyPI.</p>; <p>The release can be installed from PyPI</p>; <pre><code>python -m pip install --upgrade --pre pandas==1.4.0rc0; </code></pre>; <p>Or from conda-forge</p>; <pre><code>conda install -c conda-forge/label/pandas_rc pandas==1.4.0rc0; </code></pre>; <p>Please report any issues with the release candidate on the pandas issue tracker.</p>; <h2>Pandas 1.3.5</h2>; <!-- raw HTM",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11539:1296,release,release,1296,https://hail.is,https://github.com/hail-is/hail/pull/11539,5,"['install', 'release', 'upgrade']","['install', 'release', 'upgrade']"
Deployability," <code>@quack4 &lt;https://github.com/quack4&gt;</code>_.</li>; <li>Added :class:<code>forward_declaration</code>.</li>; </ul>; <h2>1.3.0 - 2019-08-03</h2>; <ul>; <li>Documentation improvements.</li>; <li>Added :func:<code>peek</code> - thanks <code>@lisael &lt;https://github.com/lisael&gt;</code>_.</li>; <li>Removed Python 3.3 support</li>; <li>Added Python 3.7 support</li>; <li>:meth:<code>Parser.combine_dict</code> now strips keys that start with <code>_</code>.</li>; </ul>; <h2>1.2.0 - 2017-11-15</h2>; <ul>; <li>Added <code>transform</code> argument to :func:<code>string</code> and :func:<code>string_from</code>.</li>; <li>Made :meth:<code>Parser.combine_dict</code> accept lists of name value pairs,; and filter out keys with value <code>None</code>.</li>; <li>Added :func:<code>from_enum</code>.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python-parsy/parsy/commit/c4f01f1f44b8e0afb6d3467051ca1d220cc005e3""><code>c4f01f1</code></a> Updated release process</li>; <li><a href=""https://github.com/python-parsy/parsy/commit/03807bcef9cf0a5ff0583d6398f9d4a1eeb67ef2""><code>03807bc</code></a> Version bump - 1.4.0</li>; <li><a href=""https://github.com/python-parsy/parsy/commit/c38eda7dfe58e9619d3b1a7f2210cdbb112bac1b""><code>c38eda7</code></a> Docs improvement</li>; <li><a href=""https://github.com/python-parsy/parsy/commit/4486f90b4097805193d77c1c891817f281d89461""><code>4486f90</code></a> Fixed tests on Python 3.5</li>; <li><a href=""https://github.com/python-parsy/parsy/commit/87e622e145482d93b9bb4fe137c4c76b4ef2a2b8""><code>87e622e</code></a> Added forward_declaration</li>; <li><a href=""https://github.com/python-parsy/parsy/commit/2968580f9e98b44181d2c6c48c4890bc03081ebd""><code>2968580</code></a> Version bump for 1.4.0-dev1</li>; <li><a href=""https://github.com/python-parsy/parsy/commit/bbc1d75065108a4de64f10e169ea542b6f3167b6""><code>bbc1d75</code></a> Test example code as part of tests</li>; <li><a href=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12007:1597,Update,Updated,1597,https://hail.is,https://github.com/hail-is/hail/pull/12007,2,"['Update', 'release']","['Updated', 'release']"
Deployability," <code>protected-access</code> was not; raised on functions.</p>; <p>Fixes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5989"">#5989</a></p>; </li>; <li>; <p>Better error messages in case of crash if pylint can't write the issue template.</p>; <p>Refer to <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5987"">#5987</a></p>; </li>; </ul>; <h1>What's New in Pylint 2.13.1?</h1>; <p>Release date: 2022-03-26</p>; <ul>; <li>; <p>Fix a regression in 2.13.0 where <code>used-before-assignment</code> was emitted for; the usage of a nonlocal in a try block.</p>; <p>Fixes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5965"">#5965</a></p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/PyCQA/pylint/commit/7591ac04dcefc527c42fd7713c909d1319e83fab""><code>7591ac0</code></a> Bump pylint to 2.13.3, update changelog</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/a880bd6d85d2487f509d1505b5146d608b15d870""><code>a880bd6</code></a> Change 'nonexistent-operator' to allow repeated unary ops (with space or pare...</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/c73353064f934ae49472eb6138e1f8071b6b733e""><code>c733530</code></a> <code>unnecessary-ellipsis</code> false positive: allow ellipsis as default argument (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6"">#6</a>...</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/19e6531068cf95d602054ff8638adcb79971d552""><code>19e6531</code></a> Fix crash on unbalanced tuple unpacking</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/2066cab9bbe43341b84014ac9610e275db586431""><code>2066cab</code></a> Bump pylint to 2.13.2, update changelog</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/6a25d7048edadc18a05e999021049ade86ef2bd9""><code>6a25d70</code></a> Better error message when we cant write the crash file",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11702:2455,update,update,2455,https://hail.is,https://github.com/hail-is/hail/pull/11702,1,['update'],['update']
Deployability," <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/sphinx-doc/sphinx/releases"">sphinx's releases</a>.</em></p>; <blockquote>; <h2>v5.0.2</h2>; <p>Changelog: <a href=""https://www.sphinx-doc.org/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v5.0.1</h2>; <p>Changelog: <a href=""https://www.sphinx-doc.org/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v5.0.0</h2>; <p>No release notes provided.</p>; <h2>v5.0.0b1</h2>; <p>Changelog: <a href=""https://www.sphinx-doc.org/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v4.5.0</h2>; <p>Changelog: <a href=""https://www.sphinx-doc.org/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v4.4.0</h2>; <p>Changelog: <a href=""https://www.sphinx-doc.org/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v4.3.1</h2>; <p>No release notes provided.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/sphinx-doc/sphinx/blob/5.x/CHANGES"">sphinx's changelog</a>.</em></p>; <blockquote>; <h1>Release 5.0.2 (released Jun 17, 2022)</h1>; <h2>Features added</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10523"">#10523</a>: HTML Theme: Expose the Docutils's version info tuple as a template; variable, <code>docutils_version_info</code>. Patch by Adam Turner.</li>; </ul>; <h2>Bugs fixed</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10538"">#10538</a>: autodoc: Inherited class attribute having docstring is documented even; if :confval:<code>autodoc_inherit_docstring</code> is disabled</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10509"">#10509</a>: autosummary: autosummary fails with a shared library</li>; <li><a href=""https://github-redir",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11925:1070,release,release,1070,https://hail.is,https://github.com/hail-is/hail/pull/11925,1,['release'],['release']
Deployability," <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11722"">#11722</a> [component: tests] Update visual baselines on MacOS</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11724"">#11724</a> [NO SQUASH] More 3.0 -&gt; 2.4 backports</li>; </ul>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/bokeh/bokeh/commit/ad33147f5762af8830e68144419e31e46a024caf""><code>ad33147</code></a> Deployment updates for release 2.4.2</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/17578f3a7fce22af09cf105c67769890dfdb5705""><code>17578f3</code></a> also update latest=2.4.2</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/e3e182a740b1a88f6b13d83656df296e02616506""><code>e3e182a</code></a> Merge deployment staging branch staging-2.4.2rc1</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/6bcda8a0b0a6ff9c4449abb40082b63c3ea7e3e4""><code>6bcda8a</code></a> Deployment updates for release 2.4.2rc1</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/638d3ee438716feceac319c40fa6b17655457e5d""><code>638d3ee</code></a> Updates for release (<a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11824"">#11824</a>)</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/56eef3cdac7e195f3564110cbc3a8daab863f961""><code>56eef3c</code></a> Merge deployment staging branch staging-2.4.2dev1</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/99d1fc017eab9e3b8e66b8d40fdc5737c8c9f0fe""><code>99d1fc0</code></a> Deployment updates for release 2.4.2dev1</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/23b5134dee481632775b09a2d37d183a646026a8""><code>23b5134</code></a> Provide complete model context for deserialization of instances (<a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11469"">#11469</a>)</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/64fa0759bab7e2cc48663a2359093d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11540:7334,Deploy,Deployment,7334,https://hail.is,https://github.com/hail-is/hail/pull/11540,3,"['Deploy', 'release', 'update']","['Deployment', 'release', 'updates']"
Deployability," <li><a href=""https://github.com/PyCQA/mccabe/commit/4ba21d2e8db92534914a89f44b5dfd0fb2e29e9c""><code>4ba21d2</code></a> Travis CI: allow_failures in Python end of life branches</li>; <li><a href=""https://github.com/PyCQA/mccabe/commit/80794d37d7d3e35cf243877a396e53f70243e154""><code>80794d3</code></a> Apply suggestions from code review</li>; <li><a href=""https://github.com/PyCQA/mccabe/commit/e864119dca577a38552b0d32c66d0ef3dc7779e0""><code>e864119</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pycqa/mccabe/issues/86"">#86</a> from cclauss/patch-1</li>; <li>Additional commits viewable in <a href=""https://github.com/pycqa/mccabe/compare/0.6.1...0.7.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=mccabe&package-manager=pip&previous-version=0.6.1&new-version=0.7.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12449:2486,update,updates,2486,https://hail.is,https://github.com/hail-is/hail/pull/12449,1,['update'],['updates']
Deployability," <li><a href=""https://github.com/apache/spark/commit/7c465bc3154cdd0d578f837c9b82e4289caf0b14""><code>7c465bc</code></a> Preparing Spark release v3.3.1-rc3</li>; <li><a href=""https://github.com/apache/spark/commit/5fe895a65a4a9d65f81d43af473b5e3a855ed8c8""><code>5fe895a</code></a> [SPARK-40660][SQL][3.3] Switch to XORShiftRandom to distribute elements</li>; <li><a href=""https://github.com/apache/spark/commit/5dc9ba0d22741173bd122afb387c54d7ca4bfb6d""><code>5dc9ba0</code></a> [SPARK-40669][SQL][TESTS] Parameterize <code>rowsNum</code> in <code>InMemoryColumnarBenchmark</code></li>; <li>Additional commits viewable in <a href=""https://github.com/apache/spark/compare/v3.1.3...v3.3.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pyspark&package-manager=pip&previous-version=3.1.3&new-version=3.3.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12455:2488,update,updates,2488,https://hail.is,https://github.com/hail-is/hail/pull/12455,1,['update'],['updates']
Deployability," <li><a href=""https://github.com/jaraco/keyrings.alt/commit/799b8da31058a63fd47a1cf1d341e5acbe3a1e8a""><code>799b8da</code></a> Run crypto tests against both crypto implementations.</li>; <li><a href=""https://github.com/jaraco/keyrings.alt/commit/f36ec65595bc6b59243adc0cb9e5a1a367f1e50b""><code>f36ec65</code></a> Consolidate logic for resolving crypto lib.</li>; <li><a href=""https://github.com/jaraco/keyrings.alt/commit/3de7f4007c4cf749b97dd3acba4b00d7cf0b55a1""><code>3de7f40</code></a> Remove dependency on deprecated keyring.util.properties. Fixes <a href=""https://github-redirect.dependabot.com/jaraco/keyrings.alt/issues/47"">#47</a>.</li>; <li><a href=""https://github.com/jaraco/keyrings.alt/commit/010fe59c64ffacbc0f97405d3bf21072d811baf1""><code>010fe59</code></a> Merge <a href=""https://github.com/jaraco/skeleton"">https://github.com/jaraco/skeleton</a></li>; <li><a href=""https://github.com/jaraco/keyrings.alt/commit/47c2cb324e20f784289496ef3a7b19a1cd23d196""><code>47c2cb3</code></a> Also update release to v4</li>; <li>Additional commits viewable in <a href=""https://github.com/jaraco/keyrings.alt/compare/v3.5.2...v4.2.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=keyrings-alt&package-manager=pip&previous-version=3.5.2&new-version=4.2.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12448:3225,update,update,3225,https://hail.is,https://github.com/hail-is/hail/pull/12448,2,"['release', 'update']","['release', 'update']"
Deployability," <li><a href=""https://github.com/numpy/numpy/commit/c283859128b1a4b57014581570a23ed7950a24ea""><code>c283859</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20682"">#20682</a> from charris/backport-20416</li>; <li><a href=""https://github.com/numpy/numpy/commit/5399c03d4a069fe81a1616be0184c9749d7271ee""><code>5399c03</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20681"">#20681</a> from charris/backport-20954</li>; <li><a href=""https://github.com/numpy/numpy/commit/f9c45f8ebf31340b1a5a0371bfca25afcfc4794e""><code>f9c45f8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20680"">#20680</a> from charris/backport-20663</li>; <li><a href=""https://github.com/numpy/numpy/commit/794b36f7e1bf2a8c42774ab0db86a74bd32f674b""><code>794b36f</code></a> Update armccompiler.py</li>; <li><a href=""https://github.com/numpy/numpy/commit/d93b14e3d7abaa1d837825e51671f817788e120f""><code>d93b14e</code></a> Update test_public_api.py</li>; <li><a href=""https://github.com/numpy/numpy/commit/7662c0789cc6a70d5ad4d950ee2e95f3afef7df6""><code>7662c07</code></a> Update <strong>init</strong>.py</li>; <li><a href=""https://github.com/numpy/numpy/commit/311ab52488a7d096ac3bc4c2de0fdae17ecd13ef""><code>311ab52</code></a> Update armccompiler.py</li>; <li>Additional commits viewable in <a href=""https://github.com/numpy/numpy/compare/v1.21.6...v1.22.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=numpy&package-manager=pip&previous-version=1.21.6&new-version=1.22.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot reb",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11939:4573,Update,Update,4573,https://hail.is,https://github.com/hail-is/hail/pull/11939,2,['Update'],['Update']
Deployability," <li>Deprecate AFUNIXSocketCapability in favor of AFSocketCapability</li>; <li>Drop support for Java 7</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/8bfc43a332d5573397b72b778fed2b8c13d1dfc1""><code>8bfc43a</code></a> Fix PMD warning</li>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/53e668df0d279b368d81db8b67576342927ad892""><code>53e668d</code></a> native: Disable DEBUG by default</li>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/f8423eaee2871623113bd0200f510a292aa165d1""><code>f8423ea</code></a> docs: Update GraalVM instructions</li>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/90a31b6309e653d2714dd6a35d43b45bc8e94002""><code>90a31b6</code></a> Add security policy</li>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/0e296b9452d4097b15e81b2f39a15d840f79e3b6""><code>0e296b9</code></a> Update release instructions</li>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/81be9d63f29977814ec6746c54fd29f4f3382510""><code>81be9d6</code></a> docs: Update selftest output</li>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/015386550afb07aecc3ecb589cc37961f2be6166""><code>0153865</code></a> docs: DBeaver: Add explanation for dependencies</li>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/d207a133af6ac8f79eeb88ff20500dc6916c907f""><code>d207a13</code></a> Speed up CodeQL Java pipeline</li>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/1de756b48fbebfcd06fc4cbca7486ed7160729b6""><code>1de756b</code></a> Split CodeQL analysis jobs into Java and C to save time</li>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/f5af376908ccbcfc4120eb6ba5d371126f4026bc""><code>f5af376</code></a> Try one more time to fix CodeQL warning</li>; <li>Additional commits viewable in <a href=""https://github",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12483:4217,Update,Update,4217,https://hail.is,https://github.com/hail-is/hail/pull/12483,2,"['Update', 'release']","['Update', 'release']"
Deployability," <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h1>4.0.0 (2021-11-01)</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Dropped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/5128f7f4ff73b165579006a8336978efaeeca07a""><code>5128f7f</code></a> Fix CHANGES</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/18e718a56f9def7dc40bb9ce3a2962a8fb0c883c""><code>18e718a</code></a> Bump to 4.0.2</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/48d1c4ce923b1da75976d7e2a6ca9234b3092c16""><code>48d1c4c</code></a> Setup towncrier</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/2ede7d73e55f8d1a2279d78861af0009d96219fb""><code>2ede7d7</code></a> Fix annotations on <code>__exit__</code> and <code>__aexit__</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/8685c60dc0e82ee246fbe3d1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:3796,update,update,3796,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['update'],['update']
Deployability," <p>This is a feature release, which includes new features and removes previously deprecated features. The 2.2.x branch is now the supported bugfix branch, the 2.1.x branch will become a tag marking the end of support for that branch. We encourage everyone to upgrade, and to use a tool such as <a href=""https://pypi.org/project/pip-tools/"">pip-tools</a> to pin all dependencies and control upgrades.</p>; <ul>; <li>Changes: <a href=""https://werkzeug.palletsprojects.com/en/2.2.x/changes/#version-2-2-0"">https://werkzeug.palletsprojects.com/en/2.2.x/changes/#version-2-2-0</a></li>; <li>Milestone: <a href=""https://github.com/pallets/werkzeug/milestone/20?closed=1"">https://github.com/pallets/werkzeug/milestone/20?closed=1</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/werkzeug/blob/main/CHANGES.rst"">werkzeug's changelog</a>.</em></p>; <blockquote>; <h2>Version 2.2.2</h2>; <p>Released 2022-08-08</p>; <ul>; <li>Fix router to restore the 2.1 <code>strict_slashes == False</code> behaviour; whereby leaf-requests match branch rules and vice; versa. :pr:<code>2489</code></li>; <li>Fix router to identify invalid rules rather than hang parsing them,; and to correctly parse <code>/</code> within converter arguments. :pr:<code>2489</code></li>; <li>Update subpackage imports in :mod:<code>werkzeug.routing</code> to use the; <code>import as</code> syntax for explicitly re-exporting public attributes.; :pr:<code>2493</code></li>; <li>Parsing of some invalid header characters is more robust. :pr:<code>2494</code></li>; <li>When starting the development server, a warning not to use it in a; production deployment is always shown. :issue:<code>2480</code></li>; <li><code>LocalProxy.__wrapped__</code> is always set to the wrapped object when; the proxy is unbound, fixing an issue in doctest that would cause it; to fail. :issue:<code>2485</code></li>; <li>Address one <code>ResourceWarning</code> relat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12119:2196,Release,Released,2196,https://hail.is,https://github.com/hail-is/hail/pull/12119,1,['Release'],['Released']
Deployability," <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/googleapis/python-api-common-protos/commit/0ee8d805e9061b52a210d69d46e433c718ad18ff""><code>0ee8d80</code></a> chore(main): release 1.57.0 (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-common-protos/issues/139"">#139</a>)</li>; <li><a href=""https://github.com/googleapis/python-api-common-protos/commit/b9dbb219ea46abd9851af1fc41ea37f9d5631c0b""><code>b9dbb21</code></a> feat: add support for Python 3.11 (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-common-protos/issues/145"">#145</a>)</li>; <li><a href=""https://github.com/googleapis/python-api-common-protos/commit/63ca888512be84508fcf95e4d5d40df036a85e18""><code>63ca888</code></a> feat: add support for Python 3.10 (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-common-protos/issues/143"">#143</a>)</li>; <li><a href=""https://github.com/googleapis/python-api-common-protos/commit/6af21322879cba158e0a5992c9799e68c1744fac""><code>6af2132</code></a> chore(python): update dependencies in .kokoro/requirements.txt [autoapprove] ...</li>; <li><a href=""https://github.com/googleapis/python-api-common-protos/commit/9ea3530b459269e964fcc98db1c5025e05d6495f""><code>9ea3530</code></a> fix(deps): require protobuf &gt;=3.19.5 (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-common-protos/issues/141"">#141</a>)</li>; <li><a href=""https://github.com/googleapis/python-api-common-protos/commit/5cf4e0bbfed23d061600d64099f21fcf92ef0cf2""><code>5cf4e0b</code></a> chore: update dependency protobuf &gt;= 3.20.2 (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-common-protos/issues/138"">#138</a>)</li>; <li><a href=""https://github.com/googleapis/python-api-common-protos/commit/92d9f53f5525ecb9af97c93467a594d6b92095cd""><code>92d9f53</code></a> fix(deps): require protobuf&gt;=3.20.2 (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-common-protos/issues/137"">#",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12514:5603,update,update,5603,https://hail.is,https://github.com/hail-is/hail/pull/12514,1,['update'],['update']
Deployability," <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/tqdm/tqdm/releases"">tqdm's releases</a>.</em></p>; <blockquote>; <h2>tqdm v4.63.0 stable</h2>; <ul>; <li>add <code>__reversed__()</code></li>; <li>add efficient <code>__contains__()</code></li>; <li>improve CLI startup time (replace <code>pkg_resources</code> =&gt; <code>importlib</code>)</li>; <li><code>tqdm.autonotebook</code> warning &amp; <code>std</code> fallback on missing <code>ipywidgets</code> (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1218"">#1218</a> &lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1082"">#1082</a>, <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1217"">#1217</a>)</li>; <li>warn on positional CLI arguments</li>; <li>misc build/test framework updates; <ul>; <li>enable <code>py3.10</code> tests</li>; <li>add <code>conda</code> dependencies</li>; <li>update pre-commit hooks</li>; <li>fix <code>pytest</code> config (<code>nbval</code>, <code>asyncio</code>)</li>; <li>fix dependencies &amp; tests</li>; <li>fix site deployment</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.3 stable</h2>; <ul>; <li>fix minor typo (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>minor example fix (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>misc tidying &amp; refactoring</li>; <li>misc build/dev framework updates; <ul>; <li>update dependencies</li>; <li>update linters</li>; <li>update docs deployment branches</li>; </ul>; </li>; <li>misc test/ci updates; <ul>; <li>test forks</li>; <li>tidy OS &amp; Python version tests</li>; <li>bump primary python version 3.7 =&gt; 3.8</li>; <li>beta py3.10 testing</li>; <li>fix py2.7 tests</li>; <li>better timeout handling</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.2 stable</h2>; <ul>; <li>fix notebook memory leak (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1216"">#1216</a>)</li>; <l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11587:1005,update,update,1005,https://hail.is,https://github.com/hail-is/hail/pull/11587,2,"['deploy', 'update']","['deployment', 'update']"
Deployability," Add Windows support to CI (<a href=""https://redirect.github.com/bartdag/py4j/issues/487"">#487</a>)</li>; <li><a href=""https://github.com/py4j/py4j/commit/1c622faa81e983f5ceface5290859d6a49974849""><code>1c622fa</code></a> Migrate nosetest to pytest (<a href=""https://redirect.github.com/bartdag/py4j/issues/481"">#481</a>)</li>; <li><a href=""https://github.com/py4j/py4j/commit/64ba89c5a680218d682161a4a6d952a969d1299b""><code>64ba89c</code></a> Add explanations for releasing Py4J for eclipse. Convert .txt to .md (<a href=""https://redirect.github.com/bartdag/py4j/issues/479"">#479</a>)</li>; <li>See full diff in <a href=""https://github.com/bartdag/py4j/compare/0.10.9.5...0.10.9.7"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=py4j&package-manager=pip&previous-version=0.10.9.5&new-version=0.10.9.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12978:2704,update,updates,2704,https://hail.is,https://github.com/hail-is/hail/pull/12978,1,['update'],['updates']
Deployability," Add information about mathjax bundle</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11681"">#11681</a> [NO SQUASH] Batch of 3.0 -&gt; 2.4 backports</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11712"">#11712</a> [component: tests] Upgrade baselines to Chrome 94</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11722"">#11722</a> [component: tests] Update visual baselines on MacOS</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11724"">#11724</a> [NO SQUASH] More 3.0 -&gt; 2.4 backports</li>; </ul>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/bokeh/bokeh/commit/ad33147f5762af8830e68144419e31e46a024caf""><code>ad33147</code></a> Deployment updates for release 2.4.2</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/17578f3a7fce22af09cf105c67769890dfdb5705""><code>17578f3</code></a> also update latest=2.4.2</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/e3e182a740b1a88f6b13d83656df296e02616506""><code>e3e182a</code></a> Merge deployment staging branch staging-2.4.2rc1</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/6bcda8a0b0a6ff9c4449abb40082b63c3ea7e3e4""><code>6bcda8a</code></a> Deployment updates for release 2.4.2rc1</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/638d3ee438716feceac319c40fa6b17655457e5d""><code>638d3ee</code></a> Updates for release (<a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11824"">#11824</a>)</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/56eef3cdac7e195f3564110cbc3a8daab863f961""><code>56eef3c</code></a> Merge deployment staging branch staging-2.4.2dev1</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/99d1fc017eab9e3b8e66b8d40fdc5737c8c9f0fe""><code>99d1fc0</code></a> Deployment updates for release 2.4.2dev1</li>; <li><a href=""https://github.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11540:7017,update,update,7017,https://hail.is,https://github.com/hail-is/hail/pull/11540,1,['update'],['update']
Deployability," Amazon Redshift Data API service. This feature will support the Redshift destination connector on both public and private accessible Amazon Redshift Clusters and Amazon Redshift Serverless.</li>; <li>api-change:<code>kinesisanalyticsv2</code>: [<code>botocore</code>] Support for Apache Flink 1.15 in Kinesis Data Analytics.</li>; </ul>; <h1>1.26.14</h1>; <ul>; <li>api-change:<code>route53</code>: [<code>botocore</code>] Amazon Route 53 now supports the Asia Pacific (Hyderabad) Region (ap-south-2) for latency records, geoproximity records, and private DNS for Amazon VPCs in that region.</li>; </ul>; <h1>1.26.13</h1>; <ul>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow provides a new API called UpdateConnectorRegistration to update a custom connector that customers have previously registered. With this API, customers no longer need to unregister and then register a connector to make an update.</li>; <li>api-change:<code>auditmanager</code>: [<code>botocore</code>] This release introduces a new feature for Audit Manager: Evidence finder. You can now use evidence finder to quickly query your evidence, and add the matching evidence results to an assessment report.</li>; <li>api-change:<code>chime-sdk-voice</code>: [<code>botocore</code>] Amazon Chime Voice Connector, Voice Connector Group and PSTN Audio Service APIs are now available in the Amazon Chime SDK Voice namespace. See <a href=""https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html"">https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html</a> for more details.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/d872f34494e33473126a887499262c6d3139d0f3""><code>d872f34</code></a> Merge branch 'release-1.26.17'</li>; <li><a href=""https://github.com/boto/boto3/commit/c547ba545c4aeb40bc1848e50d9b89f54df8937c""><code>c547ba5</co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12507:5962,release,release,5962,https://hail.is,https://github.com/hail-is/hail/pull/12507,1,['release'],['release']
Deployability," Amazon Redshift Data API service. This feature will support the Redshift destination connector on both public and private accessible Amazon Redshift Clusters and Amazon Redshift Serverless.</li>; <li>api-change:<code>kinesisanalyticsv2</code>: [<code>botocore</code>] Support for Apache Flink 1.15 in Kinesis Data Analytics.</li>; </ul>; <h1>1.26.14</h1>; <ul>; <li>api-change:<code>route53</code>: [<code>botocore</code>] Amazon Route 53 now supports the Asia Pacific (Hyderabad) Region (ap-south-2) for latency records, geoproximity records, and private DNS for Amazon VPCs in that region.</li>; </ul>; <h1>1.26.13</h1>; <ul>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow provides a new API called UpdateConnectorRegistration to update a custom connector that customers have previously registered. With this API, customers no longer need to unregister and then register a connector to make an update.</li>; <li>api-change:<code>auditmanager</code>: [<code>botocore</code>] This release introduces a new feature for Audit Manager: Evidence finder. You can now use evidence finder to quickly query your evidence, and add the matching evidence results to an assessment report.</li>; <li>api-change:<code>chime-sdk-voice</code>: [<code>botocore</code>] Amazon Chime Voice Connector, Voice Connector Group and PSTN Audio Service APIs are now available in the Amazon Chime SDK Voice namespace. See <a href=""https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html"">https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html</a> for more details.</li>; <li>api-change:<code>cloudfront</code>: [<code>botocore</code>] CloudFront API support for staging distributions and associated traffic management policies.</li>; <li>api-change:<code>connect</code>: [<code>botocore</code>] Added AllowedAccessControlTags and TagRestrictedResource for Tag Based Access Control on Amazon Connect Webpage</li>; <li>api-change:<code>dynamodb</code>: [<code>botoco",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12498:1648,release,release,1648,https://hail.is,https://github.com/hail-is/hail/pull/12498,2,['release'],['release']
Deployability," Apache Atlas | 2.2.0 | 2.2.0 |; | Apache Flink | 1.15.4 | 1.15.0 |; | Apache Hadoop | 3.3.6 | 3.3.3 |; | Apache Hive | 3.1.3 | 3.1.3 |; | Apache Hive WebHCat | 3.1.3 | 3.1.3 |; | Apache Hudi | 0.12.3 | 0.12.0 |; | Apache Kafka | 3.1.0 | 3.1.0 |; | Apache Pig | 0.18.0-SNAPSHOT | 0.18.0-SNAPSHOT | | Apache Spark | 3.3.2 | 3.3.0 |; | Apache Sqoop v1 | 1.5.0-SNAPSHOT | 1.5.0-SNAPSHOT | | Apache Sqoop v2 | N/A | 1.99.6 |; | Apache Tez | N/A | 0.10.1 |; | Cloud Storage Connector | hadoop3-2.2.18 | hadoop3-2.2.9 | | Conscrypt | 2.5.2 | 2.5.2 |; | Docker | 20.10 | 20.10 |; | Hue | 4.10.0 | 4.10.0 |; | Java | 11 | 11 |; | JupyterLab Notebook | 3.4 | 3.4 |; | Oozie | 5.2.1 | 5.2.1 |; | Trino | 376 | 376 |; | Python | Python 3.10 | Python 3.10 |; | R | R 4.1 | R 4.1 |; | Ranger | 2.2.0 | 2.2.0 |; | Scala | 2.12.18 | 2.12.14 |; | Solr | 9.0.0 | 9.0.0 |; | Zeppelin Notebook | 0.10.1 | 0.10.1 |; | Zookeeper | 3.8.3 | 3.8.0 |. Here's a diff. I think the things that may affect us are:. 1. Scala patch version; 4. Cloud Storage Connector patch version (this is the motivation for this change); 5. Spark patch version; 6. Total daemon memory use at start-up. ```diff; diff -u /tmp/b /tmp/a; --- /tmp/b	2023-12-11 19:20:34; +++ /tmp/a	2023-12-11 19:22:42; @@ -1,17 +1,17 @@; -Component	2.1.2-debian11; +Component	2.1.33-debian11; Apache Atlas	2.2.0; -Apache Flink	1.15.0; -Apache Hadoop	3.3.3; +Apache Flink	1.15.4; +Apache Hadoop	3.3.6; Apache Hive	3.1.3; Apache Hive WebHCat	3.1.3; -Apache Hudi	0.12.0; +Apache Hudi	0.12.3; Apache Kafka	3.1.0; Apache Pig	0.18.0-SNAPSHOT; -Apache Spark	3.3.0; +Apache Spark	3.3.2; Apache Sqoop v1	1.5.0-SNAPSHOT; -Apache Sqoop v2	1.99.6; -Apache Tez	0.10.1; -Cloud Storage Connector	hadoop3-2.2.9; +Apache Sqoop v2	N/A; +Apache Tez	N/A; +Cloud Storage Connector	hadoop3-2.2.18; Conscrypt	2.5.2; Docker	20.10; Hue	4.10.0; @@ -22,7 +22,7 @@; Python	Python 3.10; R	R 4.1; Ranger	2.2.0; -Scala	2.12.14; +Scala	2.12.18; Solr	9.0.0; Zeppelin Notebook	0.10.1; -Zookeeper	3.8.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14093:1919,patch,patch,1919,https://hail.is,https://github.com/hail-is/hail/pull/14093,1,['patch'],['patch']
Deployability," Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13=8dRi0LwZ.haps; __RESOURCE__15=NIqfevqS.haps; __RESOURCE__17=GLxOwBss; cat ${__RESOURCE__11} ${__RESOURCE__13} ${__RESOURCE__15} >> ${__RESOURCE__17}'. # __TASK__9 write_output; __RESOURCE__17=GLxOwBss; cp ${__RESOURCE__17} gs://jigold/final_output.txt; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:1437,pipeline,pipeline,1437,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938,9,['pipeline'],['pipeline']
Deployability," CHANGES.rst to MANIFEST.in and sdist <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/402"">#402</a></li>; </ul>; <h2>aiohttp-jinja2 1.4.1 release</h2>; <h2>Changes</h2>; <ul>; <li>Document async rendering functions <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/396"">#396</a></li>; </ul>; <h2>aiohttp-jinja2 1.4.0 release</h2>; <h1>Changes</h1>; <ul>; <li>; <p>Fix type annotation for <code>context_processors</code> argument <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/354"">#354</a></p>; </li>; <li>; <p>Bump the minimal supported <code>aiohttp</code> version to 3.6.3 to avoid problems; with uncompatibility between <code>aiohttp</code> and <code>yarl</code></p>; </li>; <li>; <p>Add async rendering support <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/393"">#393</a></p>; </li>; </ul>; <h2>aiohttp-jinja2 1.3.0 release</h2>; <h1>Changes</h1>; <ul>; <li>Remove Any from template annotations <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/343"">#343</a></li>; <li>Fix type annotation for filters in aiohttp_jinja2.setup <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/330"">#330</a></li>; <li>Drop Python 3.5, support Python 3.9</li>; </ul>; <h2>aiohttp-jinja2 1.2.0 release</h2>; <h2>Changes</h2>; <ul>; <li>Add type hints <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/285"">#285</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp-jinja2/blob/master/CHANGES.rst"">aiohttp-jinja2's changelog</a>.</em></p>; <blockquote>; <h2>1.5 (2021-08-21)</h2>; <ul>; <li>Drop support for jinaj2 &lt;3. Add support for 3+.</li>; <li>Don't require <code>typing_extensions</code> on Python 3.8+.</li>; </ul>; <h2>1.4.2 (2020-11-23)</h2>; <ul>; <li>Add CHANGES.rst to MANIFEST.in and sdist <a h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11576:1284,release,release,1284,https://hail.is,https://github.com/hail-is/hail/pull/11576,1,['release'],['release']
Deployability," Call(alleles=[0, 0], phased=True),; E + Call(alleles=[0, 0], phased=True),; E + Call(alleles=[0, 0], phased=True),; E + Call(alleles=[0, 0], phased=True),; E + Call(alleles=[0, 0], phased=True),; E + Call(alleles=[0, 0], phased=True),; E Call(alleles=[0, 0], phased=True),; E Call(alleles=[1, 0], phased=True),; E - Call(alleles=[1, 1], phased=True),; E - Call(alleles=[0, 1], phased=True),; E - Call(alleles=[1, 1], phased=True),; E - Call(alleles=[0, 0], phased=True),; E - Call(alleles=[0, 1], phased=True),; E Call(alleles=[1, 0], phased=True),; E - Call(alleles=[0, 0], phased=True),; E - Call(alleles=[1, 0], phased=True),; E - Call(alleles=[0, 0], phased=True),; E - Call(alleles=[0, 0], phased=True),; E - Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 0], phased=True),; E Call(alleles=[0, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E + Call(alleles=[1, 1], phased=True),; E + Call(alleles=[1, 1], phased=True),; E + Call(alleles=[1, 1], phased=True),; E + Call(alleles=[1, 1], phased=True),; E ]. test/hail/methods/test_statgen.py:1642: AssertionError; ------------------------------ Captured log call -------------------------------; WARNING backend.service_backend:java.py:186 To ensure reproducible randomness across Hail sessions, you must set the ""global_seed"" parameter in hl.init(), in addition to the local seed in each random function.; INFO backend.service_backend:java.py:190 balding_nichols_model: generating genotypes for 1 populations, 5 samples, and 5 variants...; INFO batch_client.aioclient:aioclient.py:741 created batch 859; INFO batch_client.aioclient:aioclient.py:758 updated batch 859; INFO batch_client.aioclient:aioclient.py:758 updated batch 859; INFO batch_client.aioclient:aioclient.py:758 updated batch 859. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12588#issuecomment-1397330495:4663,update,updated,4663,https://hail.is,https://github.com/hail-is/hail/pull/12588#issuecomment-1397330495,3,['update'],['updated']
Deployability," Filtering Common Variants; [Stage 0:==================================================>(96600 + 1) / 96601]2018-04-27 20:54:43 Hail: INFO: wrote 11341822 items in 96601 partitions; Pruning LD Variants; [Stage 1:==================================================>(96598 + 3) / 96601]2018-04-27 21:19:04 Hail: INFO: Running LD prune with nSamples=4795, nVariants=11341822, nPartitions=96601, and maxQueueSize=429841.; [Stage 2:=========================================> (79823 + 18) / 96601]java.lang.OutOfMemoryError: Java heap spaceop""; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.toArray(ArrayList.java:376); at java.util.Collections$SynchronizedCollection.toArray(Collections.java:2024); at java.util.ArrayList.<init>(ArrayList.java:177); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:470); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:444); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1103); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1092); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1092); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1168); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1711); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); [Stage 2:=========================================> (79823 + 18) / 96601]. Used yarn application -kill to kill but driver still runs. Then use kill -KILL to terminate the driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3463:3588,update,updateAccumulators,3588,https://hail.is,https://github.com/hail-is/hail/issues/3463,2,['update'],['updateAccumulators']
Deployability," Fix python release on macos (<a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/10512"">#10512</a>)</li>; <li><a href=""https://github.com/protocolbuffers/protobuf/commit/a826282e15efe3ae3a2aebb040fb1691b2233a1e""><code>a826282</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/10505"">#10505</a> from deannagarcia/3.20.x</li>; <li><a href=""https://github.com/protocolbuffers/protobuf/commit/7639a710e10beb47bfc62f363680f7b04e8b3d26""><code>7639a71</code></a> Add version file</li>; <li>Additional commits viewable in <a href=""https://github.com/protocolbuffers/protobuf/compare/v3.20.1...v3.20.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=protobuf&package-manager=pip&previous-version=3.20.1&new-version=3.20.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12223:3168,update,updates,3168,https://hail.is,https://github.com/hail-is/hail/pull/12223,3,['update'],['updates']
Deployability," Hail (inside the terra container) is parsing information from the lines. I was not capable of running older versions of the Terra container (1.0.x) because the versions of Hail implemented there are not compatible with the current version of Spark on Terra. . I hope you may have a solution to this irritating problem. I have added the scripts and logs below. . Thanks in advance,; Sean Jurgens. ### Version. 0.2.126-ee77707f4fab. ### Relevant log output. ```shell; ## PLEASE NOTE: to protect privacy as much as possible, I have removed almost all entries shown by the code, and for the few line entries that remain I have changed/randomized the numeric values. The order and structure is preserved for enrtries nonetheless. `#import libraries; import os; import hail as hl; from pprint import pprint. #### Start hail; hl.init(); hl.spark_context()`. /opt/conda/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:44: UserWarning:. Reading spark-defaults.conf to determine GCS requester pays configuration. This is deprecated. Please use `hailctl config set gcs_requester_pays/project` and `hailctl config set gcs_requester_pays/buckets`. SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.3.0; SparkUI available at http://saturn-3f2d119c-05e5-496d-97b9-8f40efff98a3-m.c.terra-db12d060.internal:36235/; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ ver",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:1861,configurat,configuration,1861,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['configurat'],['configuration']
Deployability," INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 2524, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: SocketException: Too many open files. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(S",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:2690,install,install,2690,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['install'],['install']
Deployability," Instances to help customers optimize provisioning of Spot Instances via EC2 Auto Scaling, EC2 Fleet, and Spot Fleet. It allocates Spot Instances based on both spare capacity availability and Spot Instance price.</li>; <li>api-change:<code>ecs</code>: [<code>botocore</code>] This release adds support for task scale-in protection with updateTaskProtection and getTaskProtection APIs. UpdateTaskProtection API can be used to protect a service managed task from being terminated by scale-in events and getTaskProtection API to get the scale-in protection status of a task.</li>; <li>api-change:<code>es</code>: [<code>botocore</code>] Amazon OpenSearch Service now offers managed VPC endpoints to connect to your Amazon OpenSearch Service VPC-enabled domain in a Virtual Private Cloud (VPC). This feature allows you to privately access OpenSearch Service domain without using public IPs or requiring traffic to traverse the Internet.</li>; <li>api-change:<code>resource-explorer-2</code>: [<code>botocore</code>] Text only updates to some Resource Explorer descriptions.</li>; <li>api-change:<code>scheduler</code>: [<code>botocore</code>] AWS introduces the new Amazon EventBridge Scheduler. EventBridge Scheduler is a serverless scheduler that allows you to create, run, and manage tasks from one central, managed service.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/a177681a29a7dd039cf9dacce7bb810b748d27de""><code>a177681</code></a> Merge branch 'release-1.26.8'</li>; <li><a href=""https://github.com/boto/boto3/commit/894a5c591fa4b56f6e1dfa369948c3b6d25e4178""><code>894a5c5</code></a> Bumping version to 1.26.8</li>; <li><a href=""https://github.com/boto/boto3/commit/dde20184baf312a4f5ca7df08a0d7ce2c5c6e697""><code>dde2018</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/2d82a0c13d4510a5950dd24b4664e23584a5a364""><code>2d82a0c</code></a> Merge branch '",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12458:3402,update,updates,3402,https://hail.is,https://github.com/hail-is/hail/pull/12458,1,['update'],['updates']
Deployability," Instances to help customers optimize provisioning of Spot Instances via EC2 Auto Scaling, EC2 Fleet, and Spot Fleet. It allocates Spot Instances based on both spare capacity availability and Spot Instance price.</li>; <li>api-change:<code>ecs</code>: [<code>botocore</code>] This release adds support for task scale-in protection with updateTaskProtection and getTaskProtection APIs. UpdateTaskProtection API can be used to protect a service managed task from being terminated by scale-in events and getTaskProtection API to get the scale-in protection status of a task.</li>; <li>api-change:<code>es</code>: [<code>botocore</code>] Amazon OpenSearch Service now offers managed VPC endpoints to connect to your Amazon OpenSearch Service VPC-enabled domain in a Virtual Private Cloud (VPC). This feature allows you to privately access OpenSearch Service domain without using public IPs or requiring traffic to traverse the Internet.</li>; <li>api-change:<code>resource-explorer-2</code>: [<code>botocore</code>] Text only updates to some Resource Explorer descriptions.</li>; <li>api-change:<code>scheduler</code>: [<code>botocore</code>] AWS introduces the new Amazon EventBridge Scheduler. EventBridge Scheduler is a serverless scheduler that allows you to create, run, and manage tasks from one central, managed service.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/fa603d91ec4b97a31c17ee28318b3e0d691377ce""><code>fa603d9</code></a> Merge branch 'release-1.26.9'</li>; <li><a href=""https://github.com/boto/boto3/commit/6e6df92c650a91b013bd82df8549ebae9d0a56f7""><code>6e6df92</code></a> Bumping version to 1.26.9</li>; <li><a href=""https://github.com/boto/boto3/commit/4d51ef07becef163a8c4fe1af2be1ba8b63b9979""><code>4d51ef0</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/a177681a29a7dd039cf9dacce7bb810b748d27de""><code>a177681</code></a> Merge branch '",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12466:4705,update,updates,4705,https://hail.is,https://github.com/hail-is/hail/pull/12466,1,['update'],['updates']
Deployability," Keep Authorization header on subdomain redirects.</li>; <li><a href=""https://github.com/follow-redirects/follow-redirects/commit/2ad9e82b6277ae2104f7770e9ff1186cc6da29d4""><code>2ad9e82</code></a> Carry over Host header on relative redirects (<a href=""https://github-redirect.dependabot.com/follow-redirects/follow-redirects/issues/172"">#172</a>)</li>; <li><a href=""https://github.com/follow-redirects/follow-redirects/commit/77e2a581e1d1811674b7b74745a9c20a5b939488""><code>77e2a58</code></a> Release version 1.14.4 of the npm package.</li>; <li>Additional commits viewable in <a href=""https://github.com/follow-redirects/follow-redirects/compare/v1.14.1...v1.14.7"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=follow-redirects&package-manager=npm_and_yarn&previous-version=1.14.1&new-version=1.14.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11283:2603,update,updates,2603,https://hail.is,https://github.com/hail-is/hail/pull/11283,2,['update'],['updates']
Deployability," NDArray values, etc.); - body: the transformation of blockContext that represents the actual NDArray in each partition of the BlockMatrix.; - broadcastVals (currently unused, perhaps unnecessary): values that we'd potentially want to broadcast to all nodes to use in computation. I could see this being useful in specific broadcast operations, but I'd also be happy to take it out until we have a use case.; - ctxName and ctxType are used to reference the block context within the body of computation. Lowering each node would consist of two parts:; - Defining the transformation in LowerBlockMatrixIR; - Propagating the sparsity transformation correctly through each BlockMatrixIR node, so that lowering functions can use it. This looks pretty different, depending on the node being lowered; Filter, for example, will need to lift the sparsity propagation logic from the FilterRDD where it's currently defined, while for BlockMatrixMap it's mostly a matter of making explicit the implicit densification that (sometimes) happens within the node, and then propagating sparsity accordingly. I've lifted the matrix multiply as an illustration of how this would work, although we can't test it until we have at least one entrypoint and one exit; if this looks reasonable I can clean it up and PR it when I get back on Monday. Notes on testing:; I'm envisioning that this would be tested really similarly to how we currently test Table lowering; for now, adding an execution strategy in the Scala test framework and using it on specific BlockMatrix pipelines that we expect to be fully lowerable. Once we have a specific/non-trivial pipeline that is fully lowerable, we can add a feature flag that controls whether or not we attempt to lower the execution, and start benchmarking on the python end. I think the second threshold is probably most quickly achieved by lowering the BlockMatrixFromValue node (or BlockMatrixRead, for larger multiplies) and implementing/lowering a NDArrayFromBlockMatrix node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8051:2229,pipeline,pipelines,2229,https://hail.is,https://github.com/hail-is/hail/pull/8051,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability," NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.796 NotebookApp] All authentication is disabled. Anyone who can connect to this server will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, in init_server_extensions; Mar 01 19:59:04 dk-m python[5149]: func(self); Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/__init__.py"", line 30, in load_jupyter_server_extension; Mar 01 19:59:04 dk-m python[5149]: from .handlers import SparkHandler; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 8, in <module>; Mar 01 19:59:04 dk-m python[5149]: class SparkHandler(IPythonHandler):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5505:1595,configurat,configuration,1595,https://hail.is,https://github.com/hail-is/hail/issues/5505,1,['configurat'],['configuration']
Deployability," Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **489/1000** <br/> **Why?** Has a fix available, CVSS 5.5 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **519/1000** <br/> **Why?** Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIxM2UyYzQ2MC1mZTA2LTQwOTktYWRhYi1lMWY4ZmE5MzFkZTAiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjEzZTJjNDYwLWZlMDYtNDA5OS1hZGFiLWUxZjhmYTkzMWRlMCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14329:10301,upgrade,upgraded,10301,https://hail.is,https://github.com/hail-is/hail/pull/14329,1,['upgrade'],['upgraded']
Deployability," README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h2>3.9.11 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ijl/orjson/commit/a348f59f0b55d92a1364523560f52f5b3cf9c12a""><code>a348f59</code></a> 3.9.15</li>; <li><a href=""https://github.com/ijl/orjson/commit/b0e4d2c06ce06c6e63981bf0276e4b7c74e5845e""><code>b0e4d2c</code></a> yyjson 0eca326, recursion limit</li>; <li><a href=""https://github.com/ijl/orjson/commit/5067eadc84cf516e4eb33bcb09ad756bb59dc42e""><code>5067ead</code></a> impl_escape_unchecked() byte exact read</li>; <li><a href=""https://github.com/ijl/orjson/commit/e04ea735b087742b6cee738aa295d8b835c3a195""><code>e04ea73</code></a> cargo update, build misc</li>; <li><a href=""https://github.com/ijl/orjson/commit/ba8c701292e4720b4e10210b266be5666d098fb6""><code>ba8c701</code></a> 3.9.14</li>; <li><a href=""https://github.com/ijl/orjson/commit/a2f7b7bfa4987c102892793ab7c7483fcb8050a0""><code>a2f7b7b</code></a> impl_format_simd!() lift create from loop, rotate left</li>; <li><a href=""https://github.com/ijl/orjson/commit/528220fb0d18bbf0212de7f0ce5c7aec209bc6e7""><code>528220f</code></a> format_escaped_str() fast and slow paths depending on page boundary</li>; <li><a href=""https://github.com/ijl/orjson/commit/29884e617d35c6774f60b8fedf6de47d74edcd2f""><code>29884e6</code></a> Fix buffer overread in format_escaped_str</li>; <li><a href=""https://github.com/ijl/orjson/commit/c825472198c20f40064af63d7ef7b21eb2e3aaef""><code>c825472</code></a> cargo update</li>; <li><a href=""https://github.com/ijl/orjson/commit/4eb4f005a6f1b71609051770612a055b584b73d2""><code>4eb4f00</code></a> 3.9.13</li>; <li>Additional commits v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14357:3570,update,update,3570,https://hail.is,https://github.com/hail-is/hail/pull/14357,3,['update'],['update']
Deployability," The following is the IR for one invocation to TableMultiWayZipJoin in the old pipeline, this would be replicated for every vcf imported:; ```; (CastMatrixToTable ""__entries"" ""__cols""; (MatrixMapEntries; (MatrixMapRows; (MatrixMapEntries; (MatrixMapRows; (MatrixMapEntries; (MatrixLiteral); (InsertFields; (SelectFields (AD DP GQ GT MIN_DP PGT PID PL SB); (Ref g)); None; (END; (GetField END; (GetField info; (Ref va)))); (BaseQRankSum; (GetField BaseQRankSum; (GetField info; (Ref va)))); (ClippingRankSum; (GetField ClippingRankSum; (GetField info; (Ref va)))); (MQ; (GetField MQ; (GetField info; (Ref va)))); (MQRankSum; (GetField MQRankSum; (GetField info; (Ref va)))); (ReadPosRankSum; (GetField ReadPosRankSum; (GetField info; (Ref va)))); (LGT; (GetField GT; (Ref g))); (LAD; (If; (ApplyComparisonOp EQ; (ApplyIR indexArray; (GetField alleles; (Ref va)); (I32 -1)); (Str ""<NON_REF>"")); (ApplyIR `[:*]`; (GetField AD; (Ref g)); (I32 -1)); (GetField AD; (Ref g)))); (LPL; (If; (ApplyComparisonOp EQ; (ApplyIR indexArray; (GetField alleles; (Ref va)); (I32 -1)); (Str ""<NON_REF>"")); (If; (ApplyComparisonOp GT; (ArrayLen; (GetField alleles; (Ref va))); (I32 2)); (ApplyIR `[:*]`; (GetField PL; (Ref g)); (ApplyUnaryPrimOp Negate; (ArrayLen; (GetField alleles; (Ref va))))); (NA Array[Int32])); (If; (ApplyComparisonOp GT; (ArrayLen; (GetField alleles; (Ref va))); (I32 1)); (GetField PL; (Ref g)); (NA Array[Int32])))); (LPGT; (GetField PGT; (Ref g))); (RGQ; (If; (ApplyComparisonOp EQ; (ApplyIR indexArray; (GetField alleles; (Ref va)); (I32 -1)); (Str ""<NON_REF>"")); (ApplyIR indexArray; (GetField PL; (Ref g)); (Apply unphasedDiploidGtIndex; (Apply Call; (I32 0); (ApplyBinaryPrimOp Subtract; (ArrayLen; (GetField alleles; (Ref va))); (I32 1)); (False)))); (NA Int32))))); (InsertFields; (SelectFields (locus alleles rsid qual filters info); (Ref va)); None; (alleles; (If; (ApplyComparisonOp EQ; (ApplyIR indexArray; (GetField alleles; (Ref va))",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5527:127,pipeline,pipeline,127,https://hail.is,https://github.com/hail-is/hail/pull/5527,1,['pipeline'],['pipeline']
Deployability," Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-12728-job-287-742170; ReadOnly: false; batch-output-pod-token-8pkmz:; Type: Secret (a volume populated by a Secret); SecretName: batch-output-pod-token-8pkmz; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events: <none>; ```; ```; # k get pods -n batch-pods batch-12728-job-287-742170 -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: 2019-09-05T19:12:22Z; labels:; app: batch-job; batch_id: ""12728""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""287""; user: wang; uuid: ca985fd90f9d46968ab9c480af9c931c; name: batch-12728-job-287-742170; namespace: batch-pods; resourceVersion: ""116541360""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-12728-job-287-742170; uid: 1681dd05-d011-11e9-92a9-42010a800041; spec:; containers:; - command:; - /bin/bash; - -c; - set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac;; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv; ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz; && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources; -t read_with_index_p1000; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/broad-ctsa/benchmark_wang:latest; imagePullPolicy: Always; name: main; resources:; requests:; cpu: ""2""; memory: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/se",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7016:5488,pipeline,pipeline,5488,https://hail.is,https://github.com/hail-is/hail/issues/7016,6,['pipeline'],"['pipeline', 'pipeline-']"
Deployability," VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoot",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:5739,deploy,deployed,5739,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['deploy'],['deployed']
Deployability," What happened?. Hello,. I installed hail into an empty, new Python 3.12.2 virtual environment, and was not able to import it. I see a failure like this:. ```; (venv) (py312) alex@rpi400:~/hail $ python; Python 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:38:53) [GCC 12.3.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/alex/hail/venv/lib/python3.12/site-packages/hail/__init__.py"", line 2, in <module>; import pkg_resources; ModuleNotFoundError: No module named 'pkg_resources'; ```. It looks like in Python 3.12, the bundled setuptools was removed and new virtual environments will not have setuptools in them, it needs to be specifically installed through pip: https://github.com/python/cpython/issues/95299. This could be fixed either by adding `setuptools` to hail's requirements so that it will be installed when users install hail, or hail could remove usage of setuptools & its associated modules (`pkg_resources`) at runtime, as some other projects have done: https://github.com/TDAmeritrade/stumpy/issues/950. At a glance, the cleanest thing to do here may be to move off of the deprecated `pkg_resources` and to the recommended `importlib` if it has what you need: https://setuptools.pypa.io/en/latest/pkg_resources.html. I also have to admit that I discovered this while playing around with hail on a Raspberry Pi 4, so it is possible that something else broken caused this failure, but I believe I understand what's happening. Here's my full `pip freeze` for reference:. ```; (venv) (py312) alex@rpi400:~/hail $ pip freeze; aiodns==2.0.0; aiohttp==3.9.3; aiosignal==1.3.1; attrs==23.2.0; avro==1.11.3; azure-common==1.1.28; azure-core==1.30.1; azure-identity==1.15.0; azure-mgmt-core==1.4.0; azure-mgmt-storage==20.1.0; azure-storage-blob==12.19.1; bokeh==3.4.0; boto3==1.34.73; botocore==1.34.73; cachetools==5.3.3; certifi==2024.2.2;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14428:960,install,installed,960,https://hail.is,https://github.com/hail-is/hail/issues/14428,2,['install'],"['install', 'installed']"
Deployability," When we declare a Service for say, batch in default, Kubernetes adds a DNS record for `batch.default` that resolves to a single IP pointing at kube-proxy. When a new TCP connection is established with kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which inte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:3704,configurat,configuration,3704,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['configurat'],['configuration']
Deployability," [<code>botocore</code>] This release includes a new feature for customers to calculate the position of their devices by adding three new APIs: UpdateResourcePosition, GetResourcePosition, and GetPositionEstimate.</li>; <li>api-change:<code>kendra</code>: [<code>botocore</code>] Amazon Kendra now supports preview of table information from HTML tables in the search results. The most relevant cells with their corresponding rows, columns are displayed as a preview in the search result. The most relevant table cell or cells are also highlighted in table preview.</li>; <li>api-change:<code>logs</code>: [<code>botocore</code>] Updates to support CloudWatch Logs data protection and CloudWatch cross-account observability</li>; <li>api-change:<code>mgn</code>: [<code>botocore</code>] This release adds support for Application and Wave management. We also now support custom post-launch actions.</li>; <li>api-change:<code>oam</code>: [<code>botocore</code>] Amazon CloudWatch Observability Access Manager is a new service that allows configuration of the CloudWatch cross-account observability feature.</li>; <li>api-change:<code>organizations</code>: [<code>botocore</code>] This release introduces delegated administrator for AWS Organizations, a new feature to help you delegate the management of your Organizations policies, enabling you to govern your AWS organization in a decentralized way. You can now allow member accounts to manage Organizations policies.</li>; <li>api-change:<code>rds</code>: [<code>botocore</code>] This release enables new Aurora and RDS feature called Blue/Green Deployments that makes updates to databases safer, simpler and faster.</li>; <li>api-change:<code>textract</code>: [<code>botocore</code>] This release adds support for classifying and splitting lending documents by type, and extracting information by using the Analyze Lending APIs. This release also includes support for summarized information of the processed lending document package, in addition to",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12507:2895,configurat,configuration,2895,https://hail.is,https://github.com/hail-is/hail/pull/12507,1,['configurat'],['configuration']
Deployability," [sphinxcontrib-katex](https://github.com/hagenw/sphinxcontrib-katex) from 0.5.1 to 0.9.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/hagenw/sphinxcontrib-katex/releases"">sphinxcontrib-katex's releases</a>.</em></p>; <blockquote>; <h2>Release v0.9.0</h2>; <ul>; <li>Added: local KaTeX server; to dramatically speed up pre-rendering</li>; <li>Added: <code>katex.min.js</code> and <code>auto-render.min.js</code>; are now included in the Python package</li>; <li>Added: support for Python 3.10</li>; <li>Changed: use KaTeX 0.16.0</li>; <li>Removed: support for Python 3.6</li>; </ul>; <h2>Release v0.8.6</h2>; <ul>; <li>Fixed: allow to work with Sphinx&gt;=4.0.0</li>; </ul>; <h2>Release v0.8.5</h2>; <ul>; <li>Fixed: remove extra space after inline math when using pre-rendering</li>; </ul>; <h2>Release v0.8.4</h2>; <ul>; <li>Changed: increase top padding of equations by 2px</li>; </ul>; <h2>Release v0.8.3</h2>; <ul>; <li>Fixed: building of documentation on RTD</li>; </ul>; <h2>Release v0.8.2</h2>; <ul>; <li>Fixed: PyPI package version number</li>; </ul>; <h2>Release v0.8.0</h2>; <ul>; <li>Added: support for Python 3.9</li>; <li>Added: support for Sphinx&gt;=4.0.0</li>; <li>Added: tests for Windows and macOS</li>; <li>Changed: switch to KaTeX 0.13.11</li>; <li>Changed: switched CI tests from Travis to Github Actions</li>; <li>Changed: running sphinx will now fail in pre-render mode; if KaTeX fails</li>; <li>Removed: support for Python 2.7, 3.4, 3.5</li>; </ul>; <h2>sphinxcontrib-katex 0.7.2</h2>; <ul>; <li>Fixed: Sphinx&gt;=4.0.0 is not supported at the moment</li>; </ul>; <h2>sphinxcontrib-katex 0.7.1</h2>; <ul>; <li>Fixed: label of fraction example in docs</li>; </ul>; <h2>sphinxcontrib-katex 0.7.0</h2>; <ul>; <li>Added: fraction example to docs</li>; <li>Changed: switch to KaTeX 0.12.0</li>; <li>Changed: add small top and bottom padding to equations</li>; </ul>; <h2>sphinxcontrib-katex 0.6.1</h2>; <p><a href=""https://pypi.or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12241:1037,Release,Release,1037,https://hail.is,https://github.com/hail-is/hail/pull/12241,1,['Release'],['Release']
Deployability," __TASK__0 read_input; cp gs://hail-jigold/random_file.txt DWRmR1Lh. # __TASK__1 read_input; cp gs://hail-jigold/input.bed Aw2arWP9.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim Aw2arWP9.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:1154,pipeline,pipeline,1154,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938,1,['pipeline'],['pipeline']
Deployability," a <code>ProxyError</code> recommending configuring the proxy as HTTP; instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/aa3def7d242525e6e854991247c4b68583d15135""><code>aa3def7</code></a> Release 1.26.11</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/6f93b8f450b18b4c9f4c6333d759a911a63d15ae""><code>6f93b8f</code></a> Fix <code>OverflowError</code> when TLS is used on some Python versions</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/0a5f34d2c2ee6457e8365543243eccd3d1dc9430""><code>0a5f34d</code></a> Set GHA token permissions to be read-only</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/ac61b73da703df53707c31030b4ea51aab22d43c""><code>ac61b73</code></a> Backport publish workflow and process to 1.26.x</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1fd77edc1a1373c9a7e762de148f19f1e2edd418""><code>1fd77ed</code></a> Release 1.26.10</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/37ba00248424ea3cdf556cc3e7aa81ce0bf40382""><code>37ba002</code></a> [1.26] Update paid contributor program with early feedback</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/dddbab3612ead7d39d1dc33a5a504703a8d0eecf""><code>dddbab3</code></a> [1.26] Bump RECENT_DATE</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/6dd01c74102db0d608687953e351e31df3f31d9f""><code>6dd01c7</code></a> [1.26] Update docs for re-using HTTP connections after streaming</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2049c91f732ae4fec0216c0697dee7822c25db10""><code>2049c91</code></a> Adds changing branches for installing from git docs for 1.26.x</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/cb4950545be4d427557ce863539c08655c9bdd6e""><code>cb49505</code></a> [1.26] Improve testing for IPv6 scoped addresses</li>; <li>Additional commits viewable in <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12104:3130,Release,Release,3130,https://hail.is,https://github.com/hail-is/hail/pull/12104,1,['Release'],['Release']
Deployability," a long or tuple of longs which is guaranteed to be distinct on every execution of `child`.; * Uids are typically created at the leaves of pipelines (`TableRead`, `StreamRange`, etc.), and propagated upwards. There was a phase-ordering conflict that had to be worked around:; * IRs must be given explict rng state and uid semantics as early as possible, to ensure determinism.; * The transformation to explicitly pass rng states and uids must happen during IR construction. If it happened later, it would create new IR objects, which would defeat the python CSE pass (which only recognizes equivalent subexpressions when they are represented by the same python object).; * The rng explication requires some type information.; * Types on the python IR are assigned after the IR is fully constructed. To fix this:; * `Ref`'s must be given a type at construction; * `TopLevelReference`s are the only case that needs to be constructed before a type is known. But they are always constructed wrapped in a `SelectFields` or `GetField`, whose type is known at construction. I added new IR classes `SelectedTopLevelReference` and `ProjectedTopLevelReference` for these two cases, which are thin wrappers which don't appear in the rendered IR.; * `construct_expr` always assigns a type to the ir. Bottom-up type construction will later assert equality with the assigned type. This caught some existing bugs, where expression type and ir type didn't agree.; * At construction of the root node of a stream/table/matrixtable pipeline (i.e. a non-stream value ir with at least one stream/table/matrixtable child), recursively rewrite the contained pipeline(s) to make rng states and uids explicit. This is safe, since stream/table/matrixtable IRs won't be CSE'd, because they may only be evaluated once. Contained value IRs are not rewritten, only wrapped with bindings which define the rng state. These are currently non-functional changes, as `ApplySeeded` still uses the old rng, and will ignore the rng state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11847:1980,pipeline,pipeline,1980,https://hail.is,https://github.com/hail-is/hail/pull/11847,2,['pipeline'],['pipeline']
Deployability," addition to these; widely used encoders and decoders, the codec package also maintains a; collection of phonetic encoding utilities.</p>; <p>Feature and fix release.</p>; <p>Changes in this version include:</p>; <p>New features:; o CODEC-290: Base16Codec and Base16Input/OutputStream. Thanks to Adam Retter.; o CODEC-291: Hex encode/decode with existing arrays. Thanks to Adam Retter.</p>; <p>Fixed Bugs:; o CODEC-264: MurmurHash3: Ensure hash128 maintains the sign extension bug.; Thanks to Andy Seaborne.</p>; <p>Changes:; o CODEC-280: Base32/Base64/BCodec: Added strict decoding property to control; handling of trailing bits. Default lenient mode discards them; without error. Strict mode raise an exception.; o CODEC-289: Base32/Base64 Input/OutputStream: Added strict decoding property; to control handling of trailing bits. Default lenient mode; discards them without error. Strict mode raise an exception.; o Update tests from JUnit 4.12 to 4.13. Thanks to Gary Gregory.; o Update actions/checkout from v1 to v2.3.2 <a href=""https://github-redirect.dependabot.com/apache/commons-codec/issues/50"">#50</a>, <a href=""https://github-redirect.dependabot.com/apache/commons-codec/issues/56"">#56</a>.; Thanks to Dependabot.; o Update actions/setup-java from v1.4.0 to v1.4.1 <a href=""https://github-redirect.dependabot.com/apache/commons-codec/issues/57"">#57</a>.; Thanks to Dependabot.</p>; <p>For complete information on Apache Commons Codec, including instructions on how; to submit bug reports, patches, or suggestions for improvement, see the; Apache Commons Codec website:</p>; <p><a href=""https://commons.apache.org/proper/commons-codec/"">https://commons.apache.org/proper/commons-codec/</a></p>; <p>Download page: <a href=""https://commons.apache.org/proper/commons-codec/download_codec.cgi"">https://commons.apache.org/proper/commons-codec/download_codec.cgi</a></p>; <hr />; <pre><code> Apache Commons Codec 1.14 RELEASE NOTES; December 30 2019; </code></pre>; <p>The Apache Commons Codec p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12385:1492,Update,Update,1492,https://hail.is,https://github.com/hail-is/hail/pull/12385,1,['Update'],['Update']
Deployability," already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py\"", line 188, in start\n data=kwargs\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py\"", line 166, in _query\n json.loads(what.decode('utf8')))\naiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown')\n""; }; },; ""start_time"": 1580760856630,; ""end_time"": 1580760867038; },; ""spec"": {; ""command"": [; ""/bin/bash"",; ""-c"",; ""set -e; mkdir -p /io/pipeline/pipeline-3dea50d54013/__TASK__18/; /bin/true""; ],; ""image"": ""ubuntu:18.04"",; ""job_id"": 19,; ""mount_docker_socket"": false,; ""resources"": {; ""cpu"": ""0.001"",; ""memory"": ""375M""; },; ""secrets"": [; {; ""namespace"": ""dking"",; ""name"": ""dking-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""env"": []; },; ""attributes"": {; ""name"": ""18""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8029:2500,pipeline,pipeline,2500,https://hail.is,https://github.com/hail-is/hail/issues/8029,2,['pipeline'],"['pipeline', 'pipeline-']"
Deployability," and 3.7 (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/14"">#14</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/9ec54091547330aaf994e82ba759cb1fe071e070""><code>9ec5409</code></a> Drop support for EOL Python 2.7 (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/12"">#12</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/252ac00bf1e119a044cc579ffade30164e2cdfff""><code>252ac00</code></a> Add support for Python 3.12 (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/11"">#11</a>)</li>; <li>See full diff in <a href=""https://github.com/pyasn1/pyasn1-modules/compare/v0.3.0...v0.4.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pyasn1-modules&package-manager=pip&previous-version=0.3.0&new-version=0.4.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14472:2433,update,updates,2433,https://hail.is,https://github.com/hail-is/hail/pull/14472,1,['update'],['updates']
Deployability," and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.5</h2>; <p>Maintenance:</p>; <ul>; <li>Publish signed artifacts to Gradle plugin portal</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.4</h2>; <p>Bug fixes:</p>; <ul>; <li>Fix deadlock in <code>DownloadExtension</code> if <code>max-workers</code> equals 1 (thanks to <a href=""https://github.com/beatbrot""><code>@​beatbrot</code></a> for spotting this, see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/205"">#205</a>)</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1b5d69760d19cb7f88cbc837ee46456c494c0696""><code>1b5d697</code></a> Bump up version number to 5.2.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/7d6de83037ca41cd2f2f31830b43e43720e45b3a""><code>7d6de83</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1da8f078e22412475b694ce07b890148b8a5e4fc""><code>1da8f07</code></a> Add comment</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/9703f764df56c52626f7d6f44bca8b1d51312389""><code>9703f76</code></a> Use pooling connection manager instead of basic one</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/306172e4c6532e185c8a6a9998bca7d22d2d0c63""><code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:2838,Update,Update,2838,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['Update'],['Update']
Deployability," and lazy=True</li>; <li><a href=""https://github.com/fonttools/fonttools/commit/1d5feb81e597db8faa53695315befbccf0075b2e""><code>1d5feb8</code></a> ttFont_test: add reproducer for SpooledTemporaryFile has no seekable</li>; <li><a href=""https://github.com/fonttools/fonttools/commit/f1c609aa57fa11ab98f2152275f2c709e06c0680""><code>f1c609a</code></a> .readthedocs.yml: don't use 'legacy' build specification</li>; <li><a href=""https://github.com/fonttools/fonttools/commit/f9b941d226242c6b481e752036654aa346409036""><code>f9b941d</code></a> use python3.10 for ReadTheDocs</li>; <li>Additional commits viewable in <a href=""https://github.com/fonttools/fonttools/compare/4.38.0...4.39.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=fonttools&package-manager=pip&previous-version=4.38.0&new-version=4.39.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:17377,update,updates,17377,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['update'],['updates']
Deployability," and multiple in option</li>; <li><a href=""https://github.com/pallets/click/commit/afdfb120fff5cb5f8d0184d411369f5dddaed5b3""><code>afdfb12</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/click/issues/2256"">#2256</a> from Jonxslays/patch/typing-fix</li>; <li><a href=""https://github.com/pallets/click/commit/4695370071b543f6261fcaa26705323ae064050b""><code>4695370</code></a> use verbose Callable for <a href=""https://github.com/command""><code>@​command</code></a> and <a href=""https://github.com/group""><code>@​group</code></a></li>; <li><a href=""https://github.com/pallets/click/commit/6f35455ddf500d57aa746a4dc06d812ca535dd0e""><code>6f35455</code></a> start version 8.1.3</li>; <li><a href=""https://github.com/pallets/click/commit/4883c93a786f561108a806cbae7873e5c2b604a3""><code>4883c93</code></a> pre-commit updates latest release branch</li>; <li><a href=""https://github.com/pallets/click/commit/0f833b6e5ecf2307c23050823fc70644476b4bdb""><code>0f833b6</code></a> update requirements</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/click/compare/8.0.4...8.1.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=click&package-manager=pip&previous-version=8.0.4&new-version=8.1.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11808:6414,update,update,6414,https://hail.is,https://github.com/hail-is/hail/pull/11808,1,['update'],['update']
Deployability," and multiple in option</li>; <li><a href=""https://github.com/pallets/click/commit/afdfb120fff5cb5f8d0184d411369f5dddaed5b3""><code>afdfb12</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/click/issues/2256"">#2256</a> from Jonxslays/patch/typing-fix</li>; <li><a href=""https://github.com/pallets/click/commit/4695370071b543f6261fcaa26705323ae064050b""><code>4695370</code></a> use verbose Callable for <a href=""https://github.com/command""><code>@​command</code></a> and <a href=""https://github.com/group""><code>@​group</code></a></li>; <li><a href=""https://github.com/pallets/click/commit/6f35455ddf500d57aa746a4dc06d812ca535dd0e""><code>6f35455</code></a> start version 8.1.3</li>; <li><a href=""https://github.com/pallets/click/commit/4883c93a786f561108a806cbae7873e5c2b604a3""><code>4883c93</code></a> pre-commit updates latest release branch</li>; <li><a href=""https://github.com/pallets/click/commit/0f833b6e5ecf2307c23050823fc70644476b4bdb""><code>0f833b6</code></a> update requirements</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/click/compare/8.1.2...8.1.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=click&package-manager=pip&previous-version=8.1.2&new-version=8.1.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11973:3281,update,update,3281,https://hail.is,https://github.com/hail-is/hail/pull/11973,1,['update'],['update']
Deployability," and upkeep improvements</h3>; <ul>; <li>Update ruff config and typing <a href=""https://redirect.github.com/jupyter/notebook/pull/7145"">#7145</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Clean up lint handling <a href=""https://redirect.github.com/jupyter/notebook/pull/7142"">#7142</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Adopt ruff format <a href=""https://redirect.github.com/jupyter/notebook/pull/7132"">#7132</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>[7.0.x] Install stable JupyterLab 4.0 in the releaser hook <a href=""https://redirect.github.com/jupyter/notebook/pull/7183"">#7183</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; <li>Update publish-release workflow for PyPI trusted publisher <a href=""https://redirect.github.com/jupyter/notebook/pull/7176"">#7176</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/notebook/graphs/contributors?from=2023-10-17&amp;to=2024-01-19&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Abrichet+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​brichet</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ad5423197+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​d5423197</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Agithub-actions+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ajtpio+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Akrassowski+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​k",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14182:1708,release,release,1708,https://hail.is,https://github.com/hail-is/hail/pull/14182,2,['release'],['release']
Deployability, argument validation and removed the test-dataproc and wheel dependencies in the Makefile to demonstrate the functionality in these examples:. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \ ; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE= \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + ',MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:1033,deploy,deploy-,1033,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability, at scala.tools.nsc.interpreter.ILoop.$anonfun$chooseReader$3(ILoop.scala:926); at scala.tools.nsc.interpreter.ILoop.chooseReader(ILoop.scala:926); at org.apache.spark.repl.SparkILoop.$anonfun$process$1(SparkILoop.scala:138); at scala.Option.fold(Option.scala:251); at org.apache.spark.repl.SparkILoop.newReader$1(SparkILoop.scala:138); at org.apache.spark.repl.SparkILoop.preLoop$1(SparkILoop.scala:142); at org.apache.spark.repl.SparkILoop.$anonfun$process$10(SparkILoop.scala:203); at org.apache.spark.repl.SparkILoop.withSuppressedSettings$1(SparkILoop.scala:189); at org.apache.spark.repl.SparkILoop.startup$1(SparkILoop.scala:201); at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:236); at org.apache.spark.repl.Main$.doMain(Main.scala:78); at org.apache.spark.repl.Main$.main(Main.scala:58); at org.apache.spark.repl.Main.main(Main.scala); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.base/java.lang.reflect.Method.invoke(Method.java:566); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958); at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203); at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90); at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. Nothing strike my attention apart the fact that `$SPARK_SCALA_VERSION` is null and that `scala` in not found by `which` in both scenario (hail installed or not),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045:11229,deploy,deploy,11229,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045,10,"['deploy', 'install']","['deploy', 'installed']"
Deployability," authentication and CSRF tokens leak in JupyterLab (<a href=""https://github.com/jupyterlab/jupyterlab/security/advisories/GHSA-44cc-43rp-5947"">GHSA-44cc-43rp-5947</a>)</li>; <li>SXSS in Markdown Preview (<a href=""https://github.com/jupyterlab/jupyterlab/security/advisories/GHSA-4m77-cmpx-vjc4"">GHSA-4m77-cmpx-vjc4</a>)</li>; </ul>; <h3>Bugs fixed</h3>; <ul>; <li>Fixes focus indicator on input checkbox for Firefox <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15612"">#15612</a> (<a href=""https://github.com/alden-ilao""><code>@​alden-ilao</code></a>)</li>; </ul>; <h3>Documentation improvements</h3>; <ul>; <li>Fix link to yarn docs in extension migration guide <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15640"">#15640</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab/graphs/contributors?from=2023-12-29&amp;to=2024-01-19&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Abrichet+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​brichet</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Afcollonval+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​fcollonval</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Agithub-actions+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajtpio+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajupyterlab-probot+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​jupyterlab-probot</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+invol",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14184:1519,release,release,1519,https://hail.is,https://github.com/hail-is/hail/pull/14184,1,['release'],['release']
Deployability," backport outdated optimized dep removed from module graph (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8534"">#8534</a>)</li>; <li><a href=""https://github.com/vitejs/vite/commit/078a7dcabc8ffc93a06c84063fba04e0e2157f3b""><code>078a7dc</code></a> release: v2.9.11</li>; <li><a href=""https://github.com/vitejs/vite/commit/01fa8070fab5faa590fbe312d2465897a0e6c6a2""><code>01fa807</code></a> fix(dev): avoid FOUC when swapping out link tag (fix <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7973"">#7973</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8495"">#8495</a>)</li>; <li><a href=""https://github.com/vitejs/vite/commit/ab7dc1c4405ce2814ccc38d5979b51ad2f37d4e6""><code>ab7dc1c</code></a> fix: backport respect server.headers in static middlewares (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8481"">#8481</a>)</li>; <li><a href=""https://github.com/vitejs/vite/commit/ced0374b867db3c01b910275fda6b76548d72f47""><code>ced0374</code></a> release: v2.9.10</li>; <li><a href=""https://github.com/vitejs/vite/commit/9fdd0a3ae8caaf8a3633b9e2cc81a350ed5cef63""><code>9fdd0a3</code></a> feat: backport treat Astro file scripts as TS (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8151"">#8151</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/vitejs/vite/commits/v2.9.13/packages/vite"">compare view</a></li>; </ul>; </details>; <details>; <summary>Maintainer changes</summary>; <p>This version was pushed to npm by <a href=""https://www.npmjs.com/~vitebot"">vitebot</a>, a new releaser for vite since your current version.</p>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=vite&package-manager=npm_and_yarn&previous-version=2.3.2&new-version=2.9.13)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12142:12981,release,release,12981,https://hail.is,https://github.com/hail-is/hail/pull/12142,1,['release'],['release']
Deployability," backport outdated optimized dep removed from module graph (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8534"">#8534</a>)</li>; <li><a href=""https://github.com/vitejs/vite/commit/078a7dcabc8ffc93a06c84063fba04e0e2157f3b""><code>078a7dc</code></a> release: v2.9.11</li>; <li><a href=""https://github.com/vitejs/vite/commit/01fa8070fab5faa590fbe312d2465897a0e6c6a2""><code>01fa807</code></a> fix(dev): avoid FOUC when swapping out link tag (fix <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7973"">#7973</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8495"">#8495</a>)</li>; <li><a href=""https://github.com/vitejs/vite/commit/ab7dc1c4405ce2814ccc38d5979b51ad2f37d4e6""><code>ab7dc1c</code></a> fix: backport respect server.headers in static middlewares (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8481"">#8481</a>)</li>; <li><a href=""https://github.com/vitejs/vite/commit/ced0374b867db3c01b910275fda6b76548d72f47""><code>ced0374</code></a> release: v2.9.10</li>; <li><a href=""https://github.com/vitejs/vite/commit/9fdd0a3ae8caaf8a3633b9e2cc81a350ed5cef63""><code>9fdd0a3</code></a> feat: backport treat Astro file scripts as TS (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8151"">#8151</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/vitejs/vite/commits/v2.9.13/packages/vite"">compare view</a></li>; </ul>; </details>; <details>; <summary>Maintainer changes</summary>; <p>This version was pushed to npm by <a href=""https://www.npmjs.com/~vitebot"">vitebot</a>, a new releaser for vite since your current version.</p>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=vite&package-manager=npm_and_yarn&previous-version=2.3.3&new-version=2.9.13)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12143:12981,release,release,12981,https://hail.is,https://github.com/hail-is/hail/pull/12143,1,['release'],['release']
Deployability," batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Re",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:16980,pipeline,pipeline,16980,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,4,['pipeline'],"['pipeline', 'pipeline-']"
Deployability," below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; Running on Apache Spark version 2.2.0 version devel-cfdbb87; ### What you did:; This error message is related to this filter:; passed=passed.filter_rows((passed.variant_qc.AC>= 10)); Without this filter it runs OK. This file is a merged vcf file from Lumpy. Some sites may have no alternate alleles called (all 0/0 or ./.). ### What went wrong (all error messages here, including the full java stack trace):; [Stage 2:> (0 + 72) / 125]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/hail/lumpy/models.all.py"", line 80, in <module>; print(""Filtered Passed Rows:"",passed.count_rows()); File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2072, in count_rows; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 2.0 failed 4 times, most recent failure: Lost task 30.3 in stage 2.0 (TID 91, scc-q05.scc.bu.edu, executor 9): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:649); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:246); 	at is.hail.HailContex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3901:1028,install,install,1028,https://hail.is,https://github.com/hail-is/hail/issues/3901,1,['install'],['install']
Deployability," botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/96da06d3dc41d4d04503929a7b5894d82c71c08f""><code>96da06d</code></a> Merge branch 'release-1.26.14'</li>; <li><a href=""https://github.com/boto/boto3/commit/61de529b5f9a7bdcc8c76debb472a7f934d048e6""><code>61de529</code></a> Merge branch 'release-1.26.14' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/c92111ed1a7346642060fd7f6dfedcb3770a9650""><code>c92111e</code></a> Bumping version to 1.26.14</li>; <li><a href=""https://github.com/boto/boto3/commit/53c38e2c6849af7c0c47a4d21ce41bfbad7d80cb""><code>53c38e2</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/1de404aff4ecb1c5560b4e023f0614d8149622ed""><code>1de404a</code></a> fix typo: 'are specified the' should be 'are specified in the' (<a href=""https://github-redirect.dependabot.com/boto/boto3/issues/3499"">#3499</a>)</li>; <li><a href=""https://github.com/boto/boto3/commit/47f20744b3c57223da5e14e185120586f8212af8""><code>47f2074</code></a> Merge branch 'release-1.26.13'</li>; <li><a href=""https://github.com/boto/boto3/commit/3d4081ccb7c350538ba3d6a5073c59575a812eb0""><code>3d4081c</code></a> Merge branch 'release-1.26.13' into develop</li>; <li>Additional commits viewable in <a href=""https://github.com/boto/boto3/compare/1.26.7...1.26.15"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=boto3&package-manager=pip&previous-version=1.26.7&new-version=1.26.15)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and opti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12498:8985,release,release-,8985,https://hail.is,https://github.com/hail-is/hail/pull/12498,1,['release'],['release-']
Deployability," by Adam Turner.</li>; </ul>; <h2>Bugs fixed</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10538"">#10538</a>: autodoc: Inherited class attribute having docstring is documented even; if :confval:<code>autodoc_inherit_docstring</code> is disabled</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10509"">#10509</a>: autosummary: autosummary fails with a shared library</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10497"">#10497</a>: py domain: Failed to resolve strings in Literal. Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10523"">#10523</a>: HTML Theme: Fix double brackets on citation references in Docutils 0.18+.; Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10534"">#10534</a>: Missing CSS for nav.contents in Docutils 0.18+. Patch by Adam Turner.</li>; </ul>; <h1>Release 5.0.1 (released Jun 03, 2022)</h1>; <h2>Bugs fixed</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10498"">#10498</a>: gettext: TypeError is raised when sorting warning messages if a node; has no line number. Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10493"">#10493</a>: HTML Theme: :rst:dir:<code>topic</code> directive is rendered incorrectly with; Docutils 0.18. Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10495"">#10495</a>: IndexError is raised for a :rst:role:<code>kbd</code> role having a separator.; Patch by Adam Turner.</li>; </ul>; <h1>Release 5.0.0 (released May 30, 2022)</h1>; <h2>Dependencies</h2>; <p>5.0.0 b1</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10164"">#10164</a>: Support <code>Docutils 0.18</code>_. Patch by Adam Turner.</li>; </ul>; <p>.. _Docutils 0.1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11925:2597,Release,Release,2597,https://hail.is,https://github.com/hail-is/hail/pull/11925,1,['Release'],['Release']
Deployability," can be created is similar to the pool control loop. The total number of instances we can create is fed to the fair share allocator.; - I added an asyncio.wait(15) at the end of the instance creation loop body to make sure we didn't run past our GCE limits.; - The scheduling loop iterates over all attempts with active instances in order of time of activation (no user fair share here -- FIFO); - There is no possibility of double scheduling because there must only be one active instance per job based on the create instances loop. **Canceller:**; - There's a new canceller loop that looks for jobs that need to be cancelled in the creating state. It marks these jobs as complete ""cancelled"" in the database and then calls GCE to delete the instance. **Mark Job Complete:**; - I modified this function to kill a job private instance if the job is marked as complete and the instance is active. **Worker:**; - I added a kill function; - Note: I did not change how storage is computed. For job private instances, it's possible to be billed for 10Gi but only get 5 Gi if you requested 5 Gi in the XFS quotas. I decided that thinking through the storage here can be delayed until the storage PR since no one is going to be using this functionality yet. **Testing:**; - I added three new job private instance tests: preemptible, non preemptible, preemptible with cancellation in the creating state; - I also added the creating state to the check_incremental background test.; - I ran a chaos script and made sure everything was working with regards to cancellation.; - I looked at the UI pages in my dev deploy and made sure they were working. **Other:**; - The free cores for an instance is set to 0 at the time of instance creation. I wanted this behavior because we're billing for that time and the job has been allocated to that instance. So the free cores should be 0. It might be confusing. I also think of free cores as a resource we're wasting where as in this case, we're not wasting resources.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9972:4642,deploy,deploy,4642,https://hail.is,https://github.com/hail-is/hail/pull/9972,1,['deploy'],['deploy']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI0NmViNDQ4ZS1kYjUyLTRhMjEtYjU5Ni0zZTNhZDQyYWFlYTEiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjQ2ZWI0NDhlLWRiNTItNGEyMS1iNTk2LTNlM2FkNDJhYWVhMSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""46eb448e-db52-4a21-b596-3e3ad42aaea1"",""prPublicId"":""46eb448e-db52-4a21-b596-3e3ad42aaea1"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""42.0.2""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14296:9694,patch,patches-to-fix-vulnerabilities,9694,https://hail.is,https://github.com/hail-is/hail/pull/14296,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI0OWRkZWE4YS05NjJjLTQ4ODktYjgwMC0zZDY0YjgyYTBiMzgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjQ5ZGRlYThhLTk2MmMtNDg4OS1iODAwLTNkNjRiODJhMGIzOCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""49ddea8a-962c-4889-b800-3d64b82a0b38"",""prPublicId"":""49ddea8a-962c-4889-b800-3d64b82a0b38"",""dependencies"":[{""name"":""aiohttp"",""from"":""3.8.6"",""to"":""3.9.0""},{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.11.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-AIOHTTP-6091621"",""SNYK-PYTHON-AIOHTTP-6091622"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-JUPYTERSERVER-6099119"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14109:6172,patch,patches-to-fix-vulnerabilities,6172,https://hail.is,https://github.com/hail-is/hail/pull/14109,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI0Yzg3NGFkNy01NjNmLTQ5Y2QtOTc3My04YjlmMTA5NWUzNmMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjRjODc0YWQ3LTU2M2YtNDljZC05NzczLThiOWYxMDk1ZTM2YyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""4c874ad7-563f-49cd-9773-8b9f1095e36c"",""prPublicId"":""4c874ad7-563f-49cd-9773-8b9f1095e36c"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""41.0.6""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:10233,patch,patches-to-fix-vulnerabilities,10233,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI0ZDFlNzI4ZS0yNjljLTQ5YTItYTJkMC1iZjFjMDQ5NjZlMjkiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjRkMWU3MjhlLTI2OWMtNDlhMi1hMmQwLWJmMWMwNDk2NmUyOSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""4d1e728e-269c-49a2-a2d0-bf1c04966e29"",""prPublicId"":""4d1e728e-269c-49a2-a2d0-bf1c04966e29"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14026:4847,patch,patches-to-fix-vulnerabilities,4847,https://hail.is,https://github.com/hail-is/hail/pull/14026,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI0ZGE0MDY2My1hNjY3LTRhNzktOWE2NS0zMWE5NzQxMGZhZjEiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjRkYTQwNjYzLWE2NjctNGE3OS05YTY1LTMxYTk3NDEwZmFmMSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""4da40663-a667-4a79-9a65-31a97410faf1"",""prPublicId"":""4da40663-a667-4a79-9a65-31a97410faf1"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""ipython"",""from"":""5.10.0"",""to"":""8.10.0""},{""name"":""jinja2"",""from"":""2.11.3"",""to"":""3.1.3""},{""name"":""jupyter-core"",""from"":""4.6.3"",""to"":""4.11.2""},{""name"":""mistune"",""from"":""0.8.4"",""to"":""2.0.3""},{""name"":""nbconvert"",""from"":""5.6.1"",""to"":""6.3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""prompt-toolkit"",""from"":""1.0.18"",""to"":""3.0.13""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14257:9723,patch,patches-to-fix-vulnerabilities,9723,https://hail.is,https://github.com/hail-is/hail/pull/14257,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI1MGUxY2NlOC1kNjhlLTQxMzMtYTU5MS1lMmQxYTYyNTczMzciLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjUwZTFjY2U4LWQ2OGUtNDEzMy1hNTkxLWUyZDFhNjI1NzMzNyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""50e1cce8-d68e-4133-a591-e2d1a6257337"",""prPublicId"":""50e1cce8-d68e-4133-a591-e2d1a6257337"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""41.0.4""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13887:8159,patch,patches-to-fix-vulnerabilities,8159,https://hail.is,https://github.com/hail-is/hail/pull/13887,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI1MTRiNWVkZS0yNmZhLTQxMDYtODMxMC1jMmNlZWQ3YzA4YTkiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjUxNGI1ZWRlLTI2ZmEtNDEwNi04MzEwLWMyY2VlZDdjMDhhOSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""514b5ede-26fa-4106-8310-c2ceed7c08a9"",""prPublicId"":""514b5ede-26fa-4106-8310-c2ceed7c08a9"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.11.2""},{""name"":""setuptools"",""from"":""40.5.0"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-JUPYTERSERVER-6099119"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14365:5506,patch,patches-to-fix-vulnerabilities,5506,https://hail.is,https://github.com/hail-is/hail/pull/14365,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI1NDBhNTVlYS05Y2JkLTRlZWEtYmJmZi00ZWU2NjlhZWJmYWQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjU0MGE1NWVhLTljYmQtNGVlYS1iYmZmLTRlZTY2OWFlYmZhZCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""540a55ea-9cbd-4eea-bbff-4ee669aebfad"",""prPublicId"":""540a55ea-9cbd-4eea-bbff-4ee669aebfad"",""dependencies"":[{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,509],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13516:3195,patch,patches-to-fix-vulnerabilities,3195,https://hail.is,https://github.com/hail-is/hail/pull/13516,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI1ZGE3Y2E3NS1mMTYxLTRmN2EtYWU3Zi1jOTJiYjc0N2VjYTkiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjVkYTdjYTc1LWYxNjEtNGY3YS1hZTdmLWM5MmJiNzQ3ZWNhOSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""5da7ca75-f161-4f7a-ae7f-c92bb747eca9"",""prPublicId"":""5da7ca75-f161-4f7a-ae7f-c92bb747eca9"",""dependencies"":[{""name"":""aiohttp"",""from"":""3.8.6"",""to"":""3.9.0""},{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.11.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-AIOHTTP-6091621"",""SNYK-PYTHON-AIOHTTP-6091622"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-JUPYTERSERVER-6099119"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14074:5350,patch,patches-to-fix-vulnerabilities,5350,https://hail.is,https://github.com/hail-is/hail/pull/14074,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI2NDhhMGFlYS05MjJjLTQ2YzktYjg1MS02NmM3ZTI4MmQyZTUiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjY0OGEwYWVhLTkyMmMtNDZjOS1iODUxLTY2YzdlMjgyZDJlNSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""648a0aea-922c-46c9-b851-66c7e282d2e5"",""prPublicId"":""648a0aea-922c-46c9-b851-66c7e282d2e5"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""41.0.6""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14196:8769,patch,patches-to-fix-vulnerabilities,8769,https://hail.is,https://github.com/hail-is/hail/pull/14196,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI2NTQzMzZlYi02MmRmLTQ0ODAtOTFkOS0xZDg4N2FmNmQwMTUiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjY1NDMzNmViLTYyZGYtNDQ4MC05MWQ5LTFkODg3YWY2ZDAxNSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""654336eb-62df-4480-91d9-1d887af6d015"",""prPublicId"":""654336eb-62df-4480-91d9-1d887af6d015"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""ipython"",""from"":""5.10.0"",""to"":""8.10.0""},{""name"":""jinja2"",""from"":""2.11.3"",""to"":""3.1.3""},{""name"":""jupyter-core"",""from"":""4.6.3"",""to"":""4.11.2""},{""name"":""mistune"",""from"":""0.8.4"",""to"":""2.0.3""},{""name"":""nbconvert"",""from"":""5.6.1"",""to"":""6.3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""prompt-toolkit"",""from"":""1.0.18"",""to"":""3.0.13""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:11806,patch,patches-to-fix-vulnerabilities,11806,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI2ZTAyYTQ3Zi02MzNlLTQ2MDUtYjM1OS1mN2RjOGIyMDk1YTYiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjZlMDJhNDdmLTYzM2UtNDYwNS1iMzU5LWY3ZGM4YjIwOTVhNiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""6e02a47f-633e-4605-b359-f7dc8b2095a6"",""prPublicId"":""6e02a47f-633e-4605-b359-f7dc8b2095a6"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494],""r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13933:4456,patch,patches-to-fix-vulnerabilities,4456,https://hail.is,https://github.com/hail-is/hail/pull/13933,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3NTkyMDJiMS1hZTUwLTQxMjUtYjNhNS1iZjFmOTM3NTU1YWMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6Ijc1OTIwMmIxLWFlNTAtNDEyNS1iM2E1LWJmMWY5Mzc1NTVhYyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""759202b1-ae50-4125-b3a5-bf1f937555ac"",""prPublicId"":""759202b1-ae50-4125-b3a5-bf1f937555ac"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494],""r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13836:4456,patch,patches-to-fix-vulnerabilities,4456,https://hail.is,https://github.com/hail-is/hail/pull/13836,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3YmFjNzAzOC00ZmQzLTQ3YmItOGUwMy0yNjRmYTUxNDRlNGQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjdiYWM3MDM4LTRmZDMtNDdiYi04ZTAzLTI2NGZhNTE0NGU0ZCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""7bac7038-4fd3-47bb-8e03-264fa5144e4d"",""prPublicId"":""7bac7038-4fd3-47bb-8e03-264fa5144e4d"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""ipython"",""from"":""5.10.0"",""to"":""8.10.0""},{""name"":""jupyter-core"",""from"":""4.6.3"",""to"":""4.11.2""},{""name"":""mistune"",""from"":""0.8.4"",""to"":""2.0.3""},{""name"":""nbconvert"",""from"":""5.6.1"",""to"":""6.3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14108:10790,patch,patches-to-fix-vulnerabilities,10790,https://hail.is,https://github.com/hail-is/hail/pull/14108,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI4OGE5ZjRlMS0yNGNjLTQ5NDYtYWYwYy03OWJlZTNkNTg3YzUiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6Ijg4YTlmNGUxLTI0Y2MtNDk0Ni1hZjBjLTc5YmVlM2Q1ODdjNSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""88a9f4e1-24cc-4946-af0c-79bee3d587c5"",""prPublicId"":""88a9f4e1-24cc-4946-af0c-79bee3d587c5"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""ipython"",""from"":""5.10.0"",""to"":""8.10.0""},{""name"":""jupyter-core"",""from"":""4.6.3"",""to"":""4.11.2""},{""name"":""mistune"",""from"":""0.8.4"",""to"":""2.0.3""},{""name"":""nbconvert"",""from"":""5.6.1"",""to"":""6.3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13835:8565,patch,patches-to-fix-vulnerabilities,8565,https://hail.is,https://github.com/hail-is/hail/pull/13835,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI4ZjJmN2FlNC0wY2VjLTQ3ZTYtODIyZi1lODFiMTA2N2RhMjIiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjhmMmY3YWU0LTBjZWMtNDdlNi04MjJmLWU4MWIxMDY3ZGEyMiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""8f2f7ae4-0cec-47e6-822f-e81b1067da22"",""prPublicId"":""8f2f7ae4-0cec-47e6-822f-e81b1067da22"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""numpy"",""from"":""1.21.3"",""to"":""1.22.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-NUMPY-2321964"",""SNYK-PYTHON-NUMPY-2321966"",""SNYK-PYTHON-NUMPY-2321970"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""en",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13871:5537,patch,patches-to-fix-vulnerabilities,5537,https://hail.is,https://github.com/hail-is/hail/pull/13871,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI5MmJjZjUxZi1jNzEwLTRhODUtOWFmMS01YWUxNzBhODc5N2EiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjkyYmNmNTFmLWM3MTAtNGE4NS05YWYxLTVhZTE3MGE4Nzk3YSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""92bcf51f-c710-4a85-9af1-5ae170a8797a"",""prPublicId"":""92bcf51f-c710-4a85-9af1-5ae170a8797a"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""41.0.5""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13938:8527,patch,patches-to-fix-vulnerabilities,8527,https://hail.is,https://github.com/hail-is/hail/pull/13938,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI5ZjJhMGZlMy1kYmVkLTQ2YzAtYmQyMC0yMjM3NzFiYzE0OTciLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjlmMmEwZmUzLWRiZWQtNDZjMC1iZDIwLTIyMzc3MWJjMTQ5NyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""9f2a0fe3-dbed-46c0-bd20-223771bc1497"",""prPublicId"":""9f2a0fe3-dbed-46c0-bd20-223771bc1497"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""42.0.2""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:11376,patch,patches-to-fix-vulnerabilities,11376,https://hail.is,https://github.com/hail-is/hail/pull/14327,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIxM2UyYzQ2MC1mZTA2LTQwOTktYWRhYi1lMWY4ZmE5MzFkZTAiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjEzZTJjNDYwLWZlMDYtNDA5OS1hZGFiLWUxZjhmYTkzMWRlMCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""13e2c460-fe06-4099-adab-e1f8fa931de0"",""prPublicId"":""13e2c460-fe06-4099-adab-e1f8fa931de0"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""42.0.2""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14329:11364,patch,patches-to-fix-vulnerabilities,11364,https://hail.is,https://github.com/hail-is/hail/pull/14329,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIxMjY5MWQyMS0wMzk1LTQxYjMtODBkMi1mMjEyODMwZjY2ZWEiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjEyNjkxZDIxLTAzOTUtNDFiMy04MGQyLWYyMTI4MzBmNjZlYSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""12691d21-0395-41b3-80d2-f212830f66ea"",""prPublicId"":""12691d21-0395-41b3-80d2-f212830f66ea"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""},{""name"":""urllib3"",""from"":""1.26.17"",""to"":""1.26.18""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-URLLIB3-6002459""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-w",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13873:4848,patch,patches-to-fix-vulnerabilities,4848,https://hail.is,https://github.com/hail-is/hail/pull/13873,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIxMjk3MjE5NC04YzAyLTRhMjQtYTA0Ni0yZjIxMjk4YjQ2NmEiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjEyOTcyMTk0LThjMDItNGEyNC1hMDQ2LTJmMjEyOThiNDY2YSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/e7c92c7b-5282-49ea-940f-7a5797e2a45a?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/e7c92c7b-5282-49ea-940f-7a5797e2a45a?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""12972194-8c02-4a24-a046-2f21298b466a"",""prPublicId"":""12972194-8c02-4a24-a046-2f21298b466a"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""41.0.6""},{""name"":""pyjwt"",""from"":""1.7.1"",""to"":""2.4.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""rsa"",""from"":""4.5"",""to"":""4.7""}],""packageManager"":""pip"",""projectPublicId"":""e7c92c7b-5282-49ea-940f-7a5797e2a45a"",""projectUrl"":""https://app.snyk.io/org/danking/project/e7c92c7b-5282-49ea-940f-7a5797e2a45a?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRA",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14134:9088,patch,patches-to-fix-vulnerabilities,9088,https://hail.is,https://github.com/hail-is/hail/pull/14134,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIxZDhjNDI0MS1hOTllLTQwZDktOTM5Yy0zZWMzM2NkNTI0ZjkiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjFkOGM0MjQxLWE5OWUtNDBkOS05MzljLTNlYzMzY2Q1MjRmOSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""1d8c4241-a99e-40d9-939c-3ec33cd524f9"",""prPublicId"":""1d8c4241-a99e-40d9-939c-3ec33cd524f9"",""dependencies"":[{""name"":""cryptography"",""from"":""41.0.7"",""to"":""42.0.2""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CRYPTOGRAPHY-6149518"",""SNYK-PYTHON-CRYPTOGRAPHY-6157248""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[581,581],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Use of a Broken or Risky Cryptographic Algorithm](https://learn.snyk.io/lesson/insecure-hash/?loc&#x3D;fix-pr); 🦉 [Uncontrolled Resource Consumption (&#x27;Resourc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14230:3180,patch,patches-to-fix-vulnerabilities,3180,https://hail.is,https://github.com/hail-is/hail/pull/14230,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIyMjlkNGUyNC0xMDE4LTQ5ZDItYTQ3NC04MmViZDVhNzZlMDEiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjIyOWQ0ZTI0LTEwMTgtNDlkMi1hNDc0LTgyZWJkNWE3NmUwMSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""229d4e24-1018-49d2-a474-82ebd5a76e01"",""prPublicId"":""229d4e24-1018-49d2-a474-82ebd5a76e01"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""ipython"",""from"":""5.10.0"",""to"":""8.10.0""},{""name"":""jupyter-core"",""from"":""4.6.3"",""to"":""4.11.2""},{""name"":""mistune"",""from"":""0.8.4"",""to"":""2.0.3""},{""name"":""nbconvert"",""from"":""5.6.1"",""to"":""6.3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:10037,patch,patches-to-fix-vulnerabilities,10037,https://hail.is,https://github.com/hail-is/hail/pull/13717,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIyN2MzNWY4NC0yNDIyLTRmNzUtYWMxYy1mODQxOGJmNzRlMzciLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjI3YzM1Zjg0LTI0MjItNGY3NS1hYzFjLWY4NDE4YmY3NGUzNyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""27c35f84-2422-4f75-ac1c-f8418bf74e37"",""prPublicId"":""27c35f84-2422-4f75-ac1c-f8418bf74e37"",""dependencies"":[{""name"":""cryptography"",""from"":""41.0.7"",""to"":""42.0.2""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CRYPTOGRAPHY-6149518"",""SNYK-PYTHON-CRYPTOGRAPHY-6157248"",""SNYK-PYTHON-CRYPTOGRAPHY-6210214""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[509,581,451],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Use of a Broken or Risky Cryptographic Algorithm](https://learn.snyk.io/lesson/insecure-hash/?loc&#x3D;fix-pr); 🦉 [Uncontrol",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14234:3537,patch,patches-to-fix-vulnerabilities,3537,https://hail.is,https://github.com/hail-is/hail/pull/14234,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJhMmM5NWY4MC0wNmQyLTRkNWYtODk4NS00MzBmOTdiOGY2NDMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImEyYzk1ZjgwLTA2ZDItNGQ1Zi04OTg1LTQzMGY5N2I4ZjY0MyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""a2c95f80-06d2-4d5f-8985-430f97b8f643"",""prPublicId"":""a2c95f80-06d2-4d5f-8985-430f97b8f643"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""ipython"",""from"":""5.10.0"",""to"":""8.10.0""},{""name"":""jupyter-core"",""from"":""4.6.3"",""to"":""4.11.2""},{""name"":""mistune"",""from"":""0.8.4"",""to"":""2.0.3""},{""name"":""nbconvert"",""from"":""5.6.1"",""to"":""6.3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14024:9130,patch,patches-to-fix-vulnerabilities,9130,https://hail.is,https://github.com/hail-is/hail/pull/14024,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJhZGQ5ZWQxOC00MjJhLTRkZWUtYWI4Yy01MTkyYmQ4ZmYxMzIiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImFkZDllZDE4LTQyMmEtNGRlZS1hYjhjLTUxOTJiZDhmZjEzMiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/b72ce54d-5de3-48e5-a1d4-6f8967681a12?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/b72ce54d-5de3-48e5-a1d4-6f8967681a12?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""add9ed18-422a-4dee-ab8c-5192bd8ff132"",""prPublicId"":""add9ed18-422a-4dee-ab8c-5192bd8ff132"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""protobuf"",""from"":""3.17.3"",""to"":""3.18.3""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""rsa"",""from"":""4.5"",""to"":""4.7""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""}],""packageManager"":""pip"",""projectPublicId"":""b72ce54d-5de3-48e5-a1d4-6f8967681a12"",""projectUrl"":""https://app.snyk.io/org/danking/project/b72ce54d-5de3-48e5-a1d4-6f8967681a12?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-PROTOBUF-3031740"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-RSA-1038401"",""SNYK-PYTHON-SETUPTOOLS-3180412""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13975:3981,patch,patches-to-fix-vulnerabilities,3981,https://hail.is,https://github.com/hail-is/hail/pull/13975,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJhZWIyYjAwNS1lYjhhLTRiMzgtYjkwMS04YzRmNTY2OGM3ZDYiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImFlYjJiMDA1LWViOGEtNGIzOC1iOTAxLThjNGY1NjY4YzdkNiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""aeb2b005-eb8a-4b38-b901-8c4f5668c7d6"",""prPublicId"":""aeb2b005-eb8a-4b38-b901-8c4f5668c7d6"",""dependencies"":[{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,509],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13517:3019,patch,patches-to-fix-vulnerabilities,3019,https://hail.is,https://github.com/hail-is/hail/pull/13517,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJiMTFhNTg5Ny0yYzUzLTQ3MmEtOWY1NS1kMjcwNjYxNWNkMjgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImIxMWE1ODk3LTJjNTMtNDcyYS05ZjU1LWQyNzA2NjE1Y2QyOCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""b11a5897-2c53-472a-9f55-d2706615cd28"",""prPublicId"":""b11a5897-2c53-472a-9f55-d2706615cd28"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""ipython"",""from"":""5.10.0"",""to"":""8.10.0""},{""name"":""jinja2"",""from"":""2.11.3"",""to"":""3.1.3""},{""name"":""jupyter-core"",""from"":""4.6.3"",""to"":""4.11.2""},{""name"":""mistune"",""from"":""0.8.4"",""to"":""2.0.3""},{""name"":""nbconvert"",""from"":""5.6.1"",""to"":""6.3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""prompt-toolkit"",""from"":""1.0.18"",""to"":""3.0.13""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""40.5.0"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.32.2"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14364:10076,patch,patches-to-fix-vulnerabilities,10076,https://hail.is,https://github.com/hail-is/hail/pull/14364,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJiOTY3NzQ2Ny0zYWIwLTQxZDYtYmUyMC01MmIzOTNjOTRiYWUiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImI5Njc3NDY3LTNhYjAtNDFkNi1iZTIwLTUyYjM5M2M5NGJhZSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""b9677467-3ab0-41d6-be20-52b393c94bae"",""prPublicId"":""b9677467-3ab0-41d6-be20-52b393c94bae"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""ipython"",""from"":""5.10.0"",""to"":""8.10.0""},{""name"":""jupyter-core"",""from"":""4.6.3"",""to"":""4.11.2""},{""name"":""mistune"",""from"":""0.8.4"",""to"":""2.0.3""},{""name"":""nbconvert"",""from"":""5.6.1"",""to"":""6.3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13932:8871,patch,patches-to-fix-vulnerabilities,8871,https://hail.is,https://github.com/hail-is/hail/pull/13932,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJiYzkzNDY4ZC02OGU5LTRmYWMtYTMzNS1mODcyNjE3MDZmNDgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImJjOTM0NjhkLTY4ZTktNGZhYy1hMzM1LWY4NzI2MTcwNmY0OCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""bc93468d-68e9-4fac-a335-f87261706f48"",""prPublicId"":""bc93468d-68e9-4fac-a335-f87261706f48"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.11.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-JUPYTERSERVER-6099119"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14211:5436,patch,patches-to-fix-vulnerabilities,5436,https://hail.is,https://github.com/hail-is/hail/pull/14211,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJjMjFkYTE5Ny1lMDgzLTRiNzEtODc1Yi0xZmY0MjNhZWZmOWEiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImMyMWRhMTk3LWUwODMtNGI3MS04NzViLTFmZjQyM2FlZmY5YSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""c21da197-e083-4b71-875b-1ff423aeff9a"",""prPublicId"":""c21da197-e083-4b71-875b-1ff423aeff9a"",""dependencies"":[{""name"":""cryptography"",""from"":""41.0.7"",""to"":""42.0.2""}],""packageManager"":""pip"",""projectPublicId"":""701495b8-b53d-48af-82fe-1a6c57aa56cb"",""projectUrl"":""https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CRYPTOGRAPHY-6149518"",""SNYK-PYTHON-CRYPTOGRAPHY-6157248"",""SNYK-PYTHON-CRYPTOGRAPHY-6210214""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[509,581,451],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Use of a Broken or Risky Cryptographic Algorithm](https://learn.snyk.io/lesson/insecure-hash/?loc&#x3D;fix-pr); 🦉 [Uncontrolled Resource Consum",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14236:3271,patch,patches-to-fix-vulnerabilities,3271,https://hail.is,https://github.com/hail-is/hail/pull/14236,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJkMzg0ZDAwZS1iMThiLTQxYmMtODcxZi00Y2YyYTU3YWQ5MzgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImQzODRkMDBlLWIxOGItNDFiYy04NzFmLTRjZjJhNTdhZDkzOCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""d384d00e-b18b-41bc-871f-4cf2a57ad938"",""prPublicId"":""d384d00e-b18b-41bc-871f-4cf2a57ad938"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494],""r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13718:4388,patch,patches-to-fix-vulnerabilities,4388,https://hail.is,https://github.com/hail-is/hail/pull/13718,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJkOTAwZjdjYS1mY2U3LTQxZDQtYTE2ZC1iYWQxMDkzMzhiZWIiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImQ5MDBmN2NhLWZjZTctNDFkNC1hMTZkLWJhZDEwOTMzOGJlYiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""d900f7ca-fce7-41d4-a16d-bad109338beb"",""prPublicId"":""d900f7ca-fce7-41d4-a16d-bad109338beb"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""41.0.4""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13736:8159,patch,patches-to-fix-vulnerabilities,8159,https://hail.is,https://github.com/hail-is/hail/pull/13736,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlZjJlMTU4YS01YTI0LTQ2NjgtYjY2My1iMmYzYmNmZjM3NmUiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImVmMmUxNThhLTVhMjQtNDY2OC1iNjYzLWIyZjNiY2ZmMzc2ZSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""ef2e158a-5a24-4668-b663-b2f3bcff376e"",""prPublicId"":""ef2e158a-5a24-4668-b663-b2f3bcff376e"",""dependencies"":[{""name"":""numpy"",""from"":""1.21.3"",""to"":""1.22.2""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-NUMPY-2321964"",""SNYK-PYTHON-NUMPY-2321966"",""SNYK-PYTHON-NUMPY-2321970""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown""],""priorityScoreList"":[null,null,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [NULL Pointer Dereference](https://learn.snyk.io/lessons/null-dereference/cpp/?loc&#x3D;fix-pr); 🦉 [Denial of Service (DoS)](https://learn.snyk.io/lessons/redos/javas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12895:2915,patch,patches-to-fix-vulnerabilities,2915,https://hail.is,https://github.com/hail-is/hail/pull/12895,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJmMDZmMzgzNi1lYTNhLTQxNDMtYmE5OS0xMmI3YWQzMzc1M2QiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImYwNmYzODM2LWVhM2EtNDE0My1iYTk5LTEyYjdhZDMzNzUzZCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""f06f3836-ea3a-4143-ba99-12b7ad33753d"",""prPublicId"":""f06f3836-ea3a-4143-ba99-12b7ad33753d"",""dependencies"":[{""name"":""aiohttp"",""from"":""3.8.6"",""to"":""3.9.2""},{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.11.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-AIOHTTP-6091621"",""SNYK-PYTHON-AIOHTTP-6091622"",""SNYK-PYTHON-AIOHTTP-6209406"",""SNYK-PYTHON-AIOHTTP-6209407"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-JUPYTERSERVER-6099119"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14259:6006,patch,patches-to-fix-vulnerabilities,6006,https://hail.is,https://github.com/hail-is/hail/pull/14259,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJmY2JlODM3Mi04NzYwLTQyYjEtOGU0ZS1jZDZlNGZkNjNhYzYiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImZjYmU4MzcyLTg3NjAtNDJiMS04ZTRlLWNkNmU0ZmQ2M2FjNiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""fcbe8372-8760-42b1-8e4e-cd6e4fd63ac6"",""prPublicId"":""fcbe8372-8760-42b1-8e4e-cd6e4fd63ac6"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""ipython"",""from"":""5.10.0"",""to"":""8.10.0""},{""name"":""jupyter-core"",""from"":""4.6.3"",""to"":""4.11.2""},{""name"":""mistune"",""from"":""0.8.4"",""to"":""2.0.3""},{""name"":""nbconvert"",""from"":""5.6.1"",""to"":""6.3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13866:8591,patch,patches-to-fix-vulnerabilities,8591,https://hail.is,https://github.com/hail-is/hail/pull/13866,1,['patch'],['patches-to-fix-vulnerabilities']
Deployability," changelog</a>.</em></p>; <blockquote>; <h2>2.26.0</h2>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab_server/compare/v2.25.4...2cc9672e751943f5c51af9d4174f0b4d986e74a0"">Full Changelog</a>)</p>; <h3>New features added</h3>; <ul>; <li>For v2 extensions parse entrypoint data <a href=""https://redirect.github.com/jupyterlab/jupyterlab_server/pull/445"">#445</a> (<a href=""https://github.com/AshokChoudhary11""><code>@​AshokChoudhary11</code></a>)</li>; </ul>; <h3>Bugs fixed</h3>; <ul>; <li>Ignore pageconfig file if JSON is zero-length <a href=""https://redirect.github.com/jupyterlab/jupyterlab_server/pull/444"">#444</a> (<a href=""https://github.com/holzman""><code>@​holzman</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab_server/graphs/contributors?from=2024-03-11&amp;to=2024-04-08&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab_server+involves%3AAshokChoudhary11+updated%3A2024-03-11..2024-04-08&amp;type=Issues""><code>@​AshokChoudhary11</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab_server+involves%3Aholzman+updated%3A2024-03-11..2024-04-08&amp;type=Issues""><code>@​holzman</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab_server+involves%3Amanics+updated%3A2024-03-11..2024-04-08&amp;type=Issues""><code>@​manics</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab_server+involves%3Awelcome+updated%3A2024-03-11..2024-04-08&amp;type=Issues""><code>@​welcome</code></a></p>; <!-- raw HTML omitted -->; <h2>2.25.4</h2>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab_server/compare/v2.25.3...15e796699f04e06db9ed23a689d454feae36ffbd"">Full Changelog</a>)</p>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Use updated releaser workflows <a href=""https://redirect.github.com/jupyterlab/jupyterlab_server/pull/442"">#442</a> (",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14483:4510,update,updated,4510,https://hail.is,https://github.com/hail-is/hail/pull/14483,1,['update'],['updated']
Deployability," checking of node.docstring (<a href=""https://github-redirect.dependabot.com/PyCQA/pyflakes/issues/704"">#704</a>)</li>; <li><a href=""https://github.com/PyCQA/pyflakes/commit/405a0906c8debafaae419472d3f51b84b7ba5c49""><code>405a090</code></a> simplify PYPY check (<a href=""https://github-redirect.dependabot.com/PyCQA/pyflakes/issues/703"">#703</a>)</li>; <li><a href=""https://github.com/PyCQA/pyflakes/commit/30ec8589e183f76f40764a8dd78591719f521943""><code>30ec858</code></a> remove unused WIN (<a href=""https://github-redirect.dependabot.com/PyCQA/pyflakes/issues/702"">#702</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/PyCQA/pyflakes/compare/2.4.0...2.5.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pyflakes&package-manager=pip&previous-version=2.4.0&new-version=2.5.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12149:3876,update,updates,3876,https://hail.is,https://github.com/hail-is/hail/pull/12149,1,['update'],['updates']
Deployability," come with 2 CPU and 7.5 GB each, but not all of that is allocatable to our pods. In reality, somewhere between 5.7-5.9GB ([GCP Docs](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture) say 5.7, GKE console says 5.9) of memory and 1.9CPU are available for us to use. Some of our Big services request 1 CPU and 3.75GB, but then we can never fit two big pods on one node. This is an attempt to standardize our requests so their easier to reason about while hopefully getting better packing. . | resource | Big | Medium | Small |; | --- | --- | --- | --- |; | CPU | 600m | 100m | 20m |; | Memory | 2G | 200M | 20M |. The intentions here are:; - always be able to comfortably get 2 Big pods on a node; - medium pods shouldn't have to force new nodes to spin up just because there's a Big pod there already; - small pods take up minimal resources; - there's ample room for small pods (which are mostly on HPA) to scale up considering most nodes shouldn't be at their medium pod capacity. The ratios don't match exactly, because I didn't want to assign CPU lower than 20m to prevent HPA thrashing we saw with auth and see a bit now with router. Setting it to 20m should hopefully convince k8s that idle small apps don't need to be scaled up under normal fluctuation. ## Big; - query; - batch-driver; - shuffler; - memory. ## Medium; - grafana; - ukbb-browser; - ukbb-static; - blog; - ci; - internal-gateway. ## Small; - amundsen; - router; - gateway; - site; - batch; - address; - atgu; - router-resolver; - ci/test statefulset & deployment; - auth-driver; - echo; - benchmark; - image-fetcher. ### Fun surprises I found along the way; - CI test statefulsets and deployment are getting .5GB and .5 CPU each; - We run a lot of image fetchers because a daemon set gets added per PR namespace. EDIT: It seems that discrepancy between GCP docs and GKE console is the console counts kube-system pods in ""Allocatable Memory"", and it really does take 2GB to run the Kubernetes Engine 🙃",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10117:1565,deploy,deployment,1565,https://hail.is,https://github.com/hail-is/hail/pull/10117,2,['deploy'],['deployment']
Deployability," configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Conf",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:8165,deploy,deploy-ago,8165,https://hail.is,https://github.com/hail-is/hail/pull/8561,2,"['deploy', 'upgrade']","['deploy-ago', 'upgrade']"
Deployability," create_connection; raise exceptions[0]; File ""/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py"", line 1050, in create_connection; sock = await self._connect_sock(; File ""/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py"", line 944, in _connect_sock; sock = socket.socket(family=family, type=type_, proto=proto); File ""/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socket.py"", line 232, in __init__; _socket.socket.__init__(self, family, type, proto, fileno); OSError: [Errno 24] Too many open files. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/weisburd/code/sma_finder/sma_finder_pipeline.py"", line 473, in <module>; main(); File ""/Users/weisburd/code/sma_finder/sma_finder_pipeline.py"", line 393, in main; bp.run(); File ""/Users/weisburd/code/step-pipeline/step_pipeline/batch.py"", line 300, in run; result = self._run_batch_obj(); File ""/Users/weisburd/code/step-pipeline/step_pipeline/batch.py"", line 368, in _run_batch_obj; result = self._batch.run(; File ""/usr/local/lib/python3.9/site-packages/hailtop/batch/batch.py"", line 712, in run; run_result = self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs) # pylint: disable=assignment-from-no-return; File ""/usr/local/lib/python3.9/site-packages/hailtop/batch/backend.py"", line 595, in _run; return async_to_blocking(; File ""/usr/local/lib/python3.9/site-packages/hailtop/utils/utils.py"", line 156, in async_to_blocking; return loop.run_until_complete(task); File ""/usr/local/lib/python3.9/site-packages/nest_asyncio.py"", line 81, in run_until_complete; return f.result(); File ""/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/futures.py"", line 201, in result; raise self._exception; File ""/usr/local/Cella",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13940:1915,pipeline,pipeline,1915,https://hail.is,https://github.com/hail-is/hail/issues/13940,1,['pipeline'],['pipeline']
Deployability," debugging of JVM crashing in production until the JVM logs are shown on a per-worker page. 2. JVMEntryway is now a real gradle project. I need to compile against log4j, and I didn't want to do that by hand with `javac`. Ignore gradlew, gradlew.bat, and gradle/wrapper, they're programmatically generated by gradle. 3. Add logging to JVMEntryway. JVMEntryway now logs its arguments into the QoB job log. I also log exceptions from the main thread or the cancel thread into the job log. We also flush the logs after the main thread completes, the cancel thread completes, and when the try-catch exits. This should ensure that regardless of what goes wrong (even if both threads fail to start) we at least see the arguments that the JVMEntryway received. 4. Use log4j2 programmatic reconfiguration after every job. This restores log4j2 to well enough working order that, *if you do not try to reconfigure it using log4j1 programmatic configuration*, logs will work. All old versions of Hail use log4j1 programmatic configuration. As a result, **all old versions of Hail will still have no logs**. However, new versions of Hail will log correctly even if an old version of Hail used the JVM before it. 5. `QoBAppender`. This is how we always should have done logging. A custom appender which we can flush and then redirect to a new file at our whim. I followed the log4j2 best practices for creating a new appender. All these annotations, factory methods, and managers are The Right Way, for better or worse. If we ever ban old versions of Hail from the cluster, then we can also eliminate the log4j2 reconfiguration. New versions of Hail work fine without any runtime log configuration (thanks to `QoBAppender`). I would like to eliminate reconfiguration because log4j2 reconfiguration leaves around oprhaned appenders and appender managers. Maybe I'm implementing the Appender or Appender Manager interfaces wrong, but I've read over that code a bunch of times and I cannot sort out what I am missing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941:2015,configurat,configuration,2015,https://hail.is,https://github.com/hail-is/hail/pull/12941,1,['configurat'],['configuration']
Deployability," deep record data; Python: AVRO-2914 Drop Python 2 support; Python: AVRO-3004 Drop Python 3.5 support; Ruby: AVRO-3108 Drop Ruby 2.5 support</p>; <p>For the first time, the 1.11.0 release includes experimental support for; Rust. Work is continuing on this donated SDK, but we have not versioned and; published official artifacts for this release.</p>; <p>Python: The avro package fully supports Python 3. We will no longer publish a; separate avro-python3 package</p>; <p>And of course upgraded dependencies to latest versions, CVE fixes and more:; <a href=""https://issues.apache.org/jira/issues/?jql=project%3DAVRO%20AND%20fixVersion%3D1.11.0"">https://issues.apache.org/jira/issues/?jql=project%3DAVRO%20AND%20fixVersion%3D1.11.0</a></p>; <p>The link to all fixed JIRA issues and a brief summary can be found at:; <a href=""https://github.com/apache/avro/releases/tag/release-1.11.0"">https://github.com/apache/avro/releases/tag/release-1.11.0</a></p>; <p>In addition, language-specific release artifacts are available:</p>; <ul>; <li>C#: <a href=""https://www.nuget.org/packages/Apache.Avro/1.11.0"">https://www.nuget.org/packages/Apache.Avro/1.11.0</a></li>; <li>Java: from Maven Central,</li>; <li>Javascript: <a href=""https://www.npmjs.com/package/avro-js/v/1.11.0"">https://www.npmjs.com/package/avro-js/v/1.11.0</a></li>; <li>Perl: <a href=""https://metacpan.org/release/Avro"">https://metacpan.org/release/Avro</a></li>; <li>Python 3: <a href=""https://pypi.org/project/avro/1.11.0"">https://pypi.org/project/avro/1.11.0</a></li>; <li>Ruby: <a href=""https://rubygems.org/gems/avro/versions/1.11.0"">https://rubygems.org/gems/avro/versions/1.11.0</a></li>; </ul>; <p>Thanks to everyone for contributing!</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/apache/avro/compare/release-1.10.0...release-1.11.0"">compare view</a></li>; </ul>; </details>; <br />. Dependabot will resolve any conflicts with this PR as long as you don'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11475:2054,release,release,2054,https://hail.is,https://github.com/hail-is/hail/pull/11475,1,['release'],['release']
Deployability," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI1ZDhmZDhmZC1mZGUxLTRiYmMtYWMzMi0xOTE1NmY0ZDFjZjIiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjVkOGZkOGZkLWZkZTEtNGJiYy1hYzMyLTE5MTU2ZjRkMWNmMiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/7fad328c-8d01-4768-8813-73d6c644e2d4?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/7fad328c-8d01-4768-8813-73d6c644e2d4?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""5d8fd8fd-fde1-4bbc-ac32-19156f4d1cf2"",""prPublicId"":""5d8fd8fd-fde1-4bbc-ac32-19156f4d1cf2"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""7fad328c-8d01-4768-8813-73d6c644e2d4"",""projectUrl"":""https://app.snyk.io/org/danking/project/7fad328c-8d01-4768-8813-73d6c644e2d4?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13112:2349,upgrade,upgrade,2349,https://hail.is,https://github.com/hail-is/hail/pull/13112,6,"['patch', 'update', 'upgrade']","['patch', 'patches-to-fix-vulnerabilities', 'updated-fix-title', 'upgrade']"
Deployability," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI2YzU3NmY1Yi1lNGM5LTQ4ZjctYmYxNy04YjEzOTIxODlmZDQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjZjNTc2ZjViLWU0YzktNDhmNy1iZjE3LThiMTM5MjE4OWZkNCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""6c576f5b-e4c9-48f7-bf17-8b1392189fd4"",""prPublicId"":""6c576f5b-e4c9-48f7-bf17-8b1392189fd4"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13097:2272,upgrade,upgrade,2272,https://hail.is,https://github.com/hail-is/hail/pull/13097,6,"['patch', 'update', 'upgrade']","['patch', 'patches-to-fix-vulnerabilities', 'updated-fix-title', 'upgrade']"
Deployability," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3ZGVlZGFlMy1mZmE3LTQxYmUtOGY4MS1lNmYwZTA5YTczOTMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjdkZWVkYWUzLWZmYTctNDFiZS04ZjgxLWU2ZjBlMDlhNzM5MyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""7deedae3-ffa7-41be-8f81-e6f0e09a7393"",""prPublicId"":""7deedae3-ffa7-41be-8f81-e6f0e09a7393"",""dependencies"":[{""name"":""urllib3"",""from"":""1.26.16"",""to"":""1.26.17""}],""packageManager"":""pip"",""projectPublicId"":""701495b8-b53d-48af-82fe-1a6c57aa56cb"",""projectUrl"":""https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-URLLIB3-5926907""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[581],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13770:2364,upgrade,upgrade,2364,https://hail.is,https://github.com/hail-is/hail/pull/13770,6,"['patch', 'update', 'upgrade']","['patch', 'patches-to-fix-vulnerabilities', 'updated-fix-title', 'upgrade']"
Deployability," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIwMzdiOGRmZS1hZDA4LTRmZjUtYTFkOC1hNGM4Nzg2N2NkYjAiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjAzN2I4ZGZlLWFkMDgtNGZmNS1hMWQ4LWE0Yzg3ODY3Y2RiMCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""037b8dfe-ad08-4ff5-a1d8-a4c87867cdb0"",""prPublicId"":""037b8dfe-ad08-4ff5-a1d8-a4c87867cdb0"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13159:2364,upgrade,upgrade,2364,https://hail.is,https://github.com/hail-is/hail/pull/13159,6,"['patch', 'update', 'upgrade']","['patch', 'patches-to-fix-vulnerabilities', 'updated-fix-title', 'upgrade']"
Deployability," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIyNWE2ZGYzMi1kYmEzLTQzOTctYmIyNC0zNjdlMzhmZWQ3ZmUiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjI1YTZkZjMyLWRiYTMtNDM5Ny1iYjI0LTM2N2UzOGZlZDdmZSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""25a6df32-dba3-4397-bb24-367e38fed7fe"",""prPublicId"":""25a6df32-dba3-4397-bb24-367e38fed7fe"",""dependencies"":[{""name"":""urllib3"",""from"":""1.26.17"",""to"":""1.26.18""}],""packageManager"":""pip"",""projectPublicId"":""701495b8-b53d-48af-82fe-1a6c57aa56cb"",""projectUrl"":""https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-URLLIB3-6002459""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[496],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13848:2364,upgrade,upgrade,2364,https://hail.is,https://github.com/hail-is/hail/pull/13848,6,"['patch', 'update', 'upgrade']","['patch', 'patches-to-fix-vulnerabilities', 'updated-fix-title', 'upgrade']"
Deployability," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIzM2VkMzM4Ny0zZTVmLTRkZDgtYjIxYy1iYzIyNzk4ODViZjMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjMzZWQzMzg3LTNlNWYtNGRkOC1iMjFjLWJjMjI3OTg4NWJmMyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/b72ce54d-5de3-48e5-a1d4-6f8967681a12?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/b72ce54d-5de3-48e5-a1d4-6f8967681a12?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""33ed3387-3e5f-4dd8-b21c-bc2279885bf3"",""prPublicId"":""33ed3387-3e5f-4dd8-b21c-bc2279885bf3"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""b72ce54d-5de3-48e5-a1d4-6f8967681a12"",""projectUrl"":""https://app.snyk.io/org/danking/project/b72ce54d-5de3-48e5-a1d4-6f8967681a12?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13107:2257,upgrade,upgrade,2257,https://hail.is,https://github.com/hail-is/hail/pull/13107,6,"['patch', 'update', 'upgrade']","['patch', 'patches-to-fix-vulnerabilities', 'updated-fix-title', 'upgrade']"
Deployability," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIzYWEwNDk2OC02NDIxLTRmODktYTBjYy03MjE4MzExNDNiZGQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjNhYTA0OTY4LTY0MjEtNGY4OS1hMGNjLTcyMTgzMTE0M2JkZCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""3aa04968-6421-4f89-a0cc-721831143bdd"",""prPublicId"":""3aa04968-6421-4f89-a0cc-721831143bdd"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13100:2360,upgrade,upgrade,2360,https://hail.is,https://github.com/hail-is/hail/pull/13100,6,"['patch', 'update', 'upgrade']","['patch', 'patches-to-fix-vulnerabilities', 'updated-fix-title', 'upgrade']"
Deployability," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJiNzM2NzI0Yi1hY2RiLTRiOTUtYWQwMy1hYWI3MjkyZGNlYzQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImI3MzY3MjRiLWFjZGItNGI5NS1hZDAzLWFhYjcyOTJkY2VjNCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""b736724b-acdb-4b95-ad03-aab7292dcec4"",""prPublicId"":""b736724b-acdb-4b95-ad03-aab7292dcec4"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13116:2264,upgrade,upgrade,2264,https://hail.is,https://github.com/hail-is/hail/pull/13116,6,"['patch', 'update', 'upgrade']","['patch', 'patches-to-fix-vulnerabilities', 'updated-fix-title', 'upgrade']"
Deployability," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlNDQxZTBmNS1jZDQ4LTQzZDUtYTdkMy1kMTM4YzQ2ZTc2NTgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImU0NDFlMGY1LWNkNDgtNDNkNS1hN2QzLWQxMzhjNDZlNzY1OCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""e441e0f5-cd48-43d5-a7d3-d138c46e7658"",""prPublicId"":""e441e0f5-cd48-43d5-a7d3-d138c46e7658"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13158:2356,upgrade,upgrade,2356,https://hail.is,https://github.com/hail-is/hail/pull/13158,6,"['patch', 'update', 'upgrade']","['patch', 'patches-to-fix-vulnerabilities', 'updated-fix-title', 'upgrade']"
Deployability," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlZjQxMWYxOC1hM2JiLTQ1YzgtODFjOS1hNmNhNjI4MWI1ZjMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImVmNDExZjE4LWEzYmItNDVjOC04MWM5LWE2Y2E2MjgxYjVmMyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""ef411f18-a3bb-45c8-81c9-a6ca6281b5f3"",""prPublicId"":""ef411f18-a3bb-45c8-81c9-a6ca6281b5f3"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""701495b8-b53d-48af-82fe-1a6c57aa56cb"",""projectUrl"":""https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13108:2347,upgrade,upgrade,2347,https://hail.is,https://github.com/hail-is/hail/pull/13108,6,"['patch', 'update', 'upgrade']","['patch', 'patches-to-fix-vulnerabilities', 'updated-fix-title', 'upgrade']"
Deployability," determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout d",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:1830,update,update,1830,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,1,['update'],['update']
Deployability," equations by 2px</li>; </ul>; <h2>Version 0.8.3 (2021-05-18)</h2>; <ul>; <li>Fixed: building of documentation on RTD</li>; </ul>; <h2>Version 0.8.2 (2021-05-18)</h2>; <ul>; <li>Fixed: PyPI package version number</li>; </ul>; <h2>Version 0.8.1 (2021-05-18)</h2>; <ul>; <li>Fixed: PyPI package had wrong version number</li>; </ul>; <h2>Version 0.8.0 (2021-05-18)</h2>; <ul>; <li>Added: support for Python 3.9</li>; <li>Added: support for Sphinx&gt;=4.0.0</li>; <li>Added: tests for Windows and macOS</li>; <li>Changed: switch to KaTeX 0.13.11</li>; <li>Changed: switched CI tests from Travis to Github Actions</li>; <li>Changed: running sphinx will now fail in pre-render mode; if KaTeX fails</li>; <li>Removed: support for Python 2.7, 3.4, 3.5</li>; </ul>; <p>Version 0.7.2 (2021-04-28)</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/ce89a95b3b330a19ad4562b87aacc69ddb6742f2""><code>ce89a95</code></a> Release 0.8.6</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/c230f938a2d3b5913004d004613f67b69ebaf526""><code>c230f93</code></a> Allow sphinx&gt;=4.0.0 in setup.cfg (<a href=""https://github-redirect.dependabot.com/hagenw/sphinxcontrib-katex/issues/64"">#64</a>)</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/c6456022c32c540ffc5db6c684d8f8bf70a966f3""><code>c645602</code></a> Release 0.8.5</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/598efcf53499f2d0a58cccc1942120dee07de3f1""><code>598efcf</code></a> Remove unintentional whitespace in prerender mode (<a href=""https://github-redirect.dependabot.com/hagenw/sphinxcontrib-katex/issues/62"">#62</a>)</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/da43ec0d98471d0cf02292cc03c05ed526777bf0""><code>da43ec0</code></a> Release 0.8.4</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/35888",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11458:3665,Release,Release,3665,https://hail.is,https://github.com/hail-is/hail/pull/11458,2,['Release'],['Release']
Deployability," error.... ipython vcf2mt.py 22; ```; Running on Apache Spark version 2.2.1; SparkUI available at http://10.48.225.55:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181113-2115-0.2-721af83bc30a.log; Converting vcf /project/ukbiobank/imp/ad.v1/vcf/ukbb.hg38.imputed.chr22.dose.vcf.gz to mt /project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr22.mt; [Stage 1:====> (59 + 24) / 741]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>; 6 mt=""/project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr""+chr+"".mt""; 7 print(""Converting vcf ""+vcf+"" to mt ""+ mt); ----> 8 hl.import_vcf(vcf,force_bgz=True).write(mt). <decorator-gen-891> in write(self, output, overwrite, stage_locally, _codec_spec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:1057,install,install,1057,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['install'],['install']
Deployability," exact trigger is . ```sql; DROP TRIGGER IF EXISTS aggregated_job_resources_v2_after_update $$; CREATE TRIGGER aggregated_job_resources_v2_after_update AFTER UPDATE ON aggregated_job_resources_v2; FOR EACH ROW; BEGIN; DECLARE new_deduped_resource_id INT;. IF OLD.migrated = 0 AND NEW.migrated = 1 THEN; SELECT deduped_resource_id INTO new_deduped_resource_id FROM resources WHERE resource_id = OLD.resource_id;. INSERT INTO aggregated_job_resources_v3 (batch_id, job_id, resource_id, `usage`); VALUES (NEW.batch_id, NEW.job_id, new_deduped_resource_id, NEW.usage); ON DUPLICATE KEY UPDATE; `usage` = `usage` + NEW.usage;; END IF;; END $$; ```. What this PR does is find the keys of all rows in the `aggregated_jobs_resources_v2` table in intervals of 100 rows. This is a ""chunk"". The reason is because we want to keep the transactions small and fast. I optimized this and found 100 rows worked best for performance. We then want to set `migrated=1` for all rows in the given chunk which activates the trigger and also maintains idempotency so we only run the update for each chunk once. . Most of the code in this PR is identifying the bounds of each chunk and then doing the update. We have a burn-in period at the beginning where we migrate chunks serially. Then we migrate the chunks in 10-way parallel. This is to get rid of deadlock errors due to row locks with the ""birthday problem"". Lastly, once all of the updates are complete, we run an audit that makes sure the ""v2"" and ""v3"" tables are equivalent and have the same total aggregate resource usage. I believe I also run this audit in chunks here as these tables are massive and a single audit query would take hours. The bounds of the audit for these chunks are on the order of `(batch_id, job_id)` rather than `(batch_id, job_id, resource_id)` which was used for the actual updates. This is because the resource_ids can differ between ""v2"" and ""v3"", so we just check the overall job adds up to the same usage after deduplicating the resourc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782:2700,update,update,2700,https://hail.is,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782,1,['update'],['update']
Deployability," extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0f43ce67de72bd511d849c07bd7728c0d6f2e6dd""><code>0f43ce6</code></a> Document path and relativePath properties</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a8504f9d60d0264808894e4bb80d4a73b8086a3e""><code>a8504f9</code></a> Bump up version number to 5.3.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/708067cd11c4a013da7a8c15d91f7f946967cf94""><code>708067c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0fdebf3c7ad43ed4739",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:2951,integrat,integration,2951,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['integrat'],['integration']
Deployability," extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.5</h2>; <p>Maintenance:</p>; <ul>; <li>Publish signed artifacts to Gradle plugin portal</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.4</h2>; <p>Bug fixes:</p>; <ul>; <li>Fix deadlock in <code>DownloadExtension</code> if <code>max-workers</code> equals 1 (thanks to <a href=""https://github.com/beatbrot""><code>@​beatbrot</code></a> for spotting this, see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/205"">#205</a>)</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1b5d69",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:2232,integrat,integration,2232,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['integrat'],['integration']
Deployability," extra dependency</li>; <li><a href=""https://github.com/tqdm/tqdm/commit/4a1d10e19fdca00db47fd50725715dc5e4aa68e6""><code>4a1d10e</code></a> consistent ordering</li>; <li><a href=""https://github.com/tqdm/tqdm/commit/bf6c960f60f8a390b47ac55d2ece3ffc419e5dcd""><code>bf6c960</code></a> emoji bars</li>; <li><a href=""https://github.com/tqdm/tqdm/commit/7994aa8285743b351cf1a3b36275335d8d0730b7""><code>7994aa8</code></a> warn once on error</li>; <li><a href=""https://github.com/tqdm/tqdm/commit/a1d4401f186dc5a79b4ad452f38cae75e1f2e6da""><code>a1d4401</code></a> remove unneeded variable</li>; <li>Additional commits viewable in <a href=""https://github.com/tqdm/tqdm/compare/v4.42.1...v4.64.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tqdm&package-manager=pip&previous-version=4.42.1&new-version=4.64.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12260:6015,update,updates,6015,https://hail.is,https://github.com/hail-is/hail/pull/12260,1,['update'],['updates']
Deployability," fairly certain I know understand this and the AoU VDS creation issue. In Dataproc versions 1.5.74, 2.0.48, and 2.1.0, Dataproc introduced ""memory protection"" which is a euphemism for a newly aggressive OOMKiller. When the OOMKiller kills the JVM driver process, there is no hs_err_pid...log file, no exceptional log statements, and no clean shutdown of any sockets. The process is simply SIGTERM'ed and then SIGKILL'ed. From Hail 0.2.83 through Hail 0.2.109 (released February 2023), Hail was pinned to Dataproc 2.0.44. From Hail 0.2.15 onwards, `hailctl dataproc`, by default, reserves 80% of the advertised memory of the driver node for the use of the Hail Query Driver JVM process. For example, Google advertises that an n1-highmem-8 has 52 GiB of RAM, so Hail sets the `spark:spark.driver.memory` property to `41g` (we always round down). Before aggressive memory protection, this setting was sufficient to protect the driver from starving itself of memory. Unfortunately, Hail 0.2.110 upgraded to Dataproc 2.1.2 which enabled ""memory protection"". Moreover, in the years since Hail 0.2.15, the memory in use by system processes on Dataproc driver nodes appears to have increased. Due to these two circumstances, the driver VM's memory usage can grow high enough to trigger the OOMKiller before the JVM triggers a GC. Consider, for example, these slices of the syslog of the n1-highmem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790:994,upgrade,upgraded,994,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790,1,['upgrade'],['upgraded']
Deployability," features; compared to <code>1.11.1</code>. Python <code>3.12</code> and musllinux wheels; are provided with this release.</p>; <h1>Authors</h1>; <ul>; <li>Name (commits)</li>; <li>Evgeni Burovski (2)</li>; <li>CJ Carey (3)</li>; <li>Dieter Werthmüller (1)</li>; <li>elbarso (1) +</li>; <li>Ralf Gommers (2)</li>; <li>Matt Haberland (1)</li>; <li>jokasimr (1) +</li>; <li>Thilo Leitzbach (1) +</li>; <li>LemonBoy (1) +</li>; <li>Ellie Litwack (2) +</li>; <li>Sturla Molden (1)</li>; <li>Andrew Nelson (5)</li>; <li>Tyler Reddy (39)</li>; <li>Daniel Schmitz (6)</li>; <li>Dan Schult (2)</li>; <li>Albert Steppi (1)</li>; <li>Matus Valo (1)</li>; <li>Stefan van der Walt (1)</li>; </ul>; <p>A total of 18 people contributed to this release.; People with a &quot;+&quot; by their names contributed a patch for the first time.; This list of names is automatically generated, and may not be fully complete.</p>; <h1>SciPy 1.11.1 Release Notes</h1>; <p>SciPy <code>1.11.1</code> is a bug-fix release with no new features; compared to <code>1.11.0</code>. In particular, a licensing issue; discovered after the release of <code>1.11.0</code> has been addressed.</p>; <h1>Authors</h1>; <ul>; <li>Name (commits)</li>; <li>h-vetinari (1)</li>; <li>Robert Kern (1)</li>; <li>Ilhan Polat (4)</li>; <li>Tyler Reddy (8)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/scipy/scipy/commit/686422c4f0a71be1b4258309590fd3e9de102e18""><code>686422c</code></a> REL: 1.11.2 release [wheel build]</li>; <li><a href=""https://github.com/scipy/scipy/commit/dede0d3cb7f271a1b015fa2308701c9c1b5ff77d""><code>dede0d3</code></a> Merge pull request <a href=""https://redirect.github.com/scipy/scipy/issues/19040"">#19040</a> from tylerjereddy/treddy_backports_1_11_2</li>; <li><a href=""https://github.com/scipy/scipy/commit/bbf69b28f8b78e22d2fa4025cf525bff961fc717""><code>bbf69b2</code></a> TST: PR 19040 revisions",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13461:1314,release,release,1314,https://hail.is,https://github.com/hail-is/hail/pull/13461,1,['release'],['release']
Deployability," fix default value of app.env</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/flask/compare/2.0.3...2.2.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask&package-manager=pip&previous-version=2.0.3&new-version=2.2.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12206:10981,upgrade,upgrade,10981,https://hail.is,https://github.com/hail-is/hail/pull/12206,3,['upgrade'],['upgrade']
Deployability," fixed bugs for integrations:</p>; <ul>; <li>Fixed support for HTTPX&gt;=0.14.3</li>; <li>Added OAuth clients of HTTPX back via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/270"">#270</a></li>; <li>Fixed parallel token refreshes for HTTPX async OAuth 2 client</li>; <li>Raise OAuthError when callback contains errors via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/275"">#275</a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/lepture/authlib/blob/master/docs/changelog.rst"">authlib's changelog</a>.</em></p>; <blockquote>; <h2>Version 0.15.5</h2>; <p><strong>Released on Oct 18, 2021.</strong></p>; <ul>; <li>Make Authlib compatible with latest httpx</li>; <li>Make Authlib compatible with latest werkzeug</li>; <li>Allow customize RFC7523 <code>alg</code> value</li>; </ul>; <h2>Version 0.15.4</h2>; <p><strong>Released on Jul 17, 2021.</strong></p>; <ul>; <li>Security fix when JWT claims is None.</li>; </ul>; <h2>Version 0.15.3</h2>; <p><strong>Released on Jan 15, 2021.</strong></p>; <ul>; <li>Fixed <code>.authorize_access_token</code> for OAuth 1.0 services, via :gh:<code>issue#308</code>.</li>; </ul>; <h2>Version 0.15.2</h2>; <p><strong>Released on Oct 18, 2020.</strong></p>; <ul>; <li>Fixed HTTPX authentication bug, via :gh:<code>issue#283</code>.</li>; </ul>; <h2>Version 0.15.1</h2>; <p><strong>Released on Oct 14, 2020.</strong></p>; <ul>; <li>Backward compitable fix for using JWKs in JWT, via :gh:<code>issue#280</code>.</li>; </ul>; <h2>Version 0.15</h2>; <p><strong>Released on Oct 10, 2020.</strong></p>; <p>This is the last release before v1.0. In this release, we added more RFCs; implementations and did some refactors for JOSE:</p>; <ul>; <li>RFC8037: CFRG Elliptic Curve Diffie-Hellman (ECDH) and Signatures in JSON Object Signing and Encryption (JOSE)</li>; <li>RFC7638: JSON Web Key (JWK",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11483:2759,Release,Released,2759,https://hail.is,https://github.com/hail-is/hail/pull/11483,1,['Release'],['Released']
Deployability," flush (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2125"">#2125</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c099a2f4f8ea9afa6953270876653916b021fd9f"">c099a2f</a>)</li>; <li>Update GrpcStorageImpl.createFrom(BlobInfo, Path) to use RewindableContent (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2112"">#2112</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c80505129baa831e492a5514e937875407211595"">c805051</a>)</li>; </ul>; <h3>Documentation</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/googleapis/java-storage/commit/d0b4ef7ac4a1c05658f8c6c3aac4a84e9691a732""><code>d0b4ef7</code></a> chore(main): release 2.26.1 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2161"">#2161</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/a35d4ce1dc8492ecf2b5db76c137b3bcf5b7b0ca""><code>a35d4ce</code></a> chore(deps): update dependency com.google.cloud:libraries-bom to v26.22.0 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2"">#2</a>...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/4f5682a4f6d6d5372a2d382ae3e47dace490ca0d""><code>4f5682a</code></a> deps: update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.24...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/8627f7b141a53ca90a997f1f70126b1d1272784a""><code>8627f7b</code></a> chore(deps): update dependency com.google.cloud:google-cloud-storage to v2.26...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/95df758d6753005226556177e68a3e9c630c789b""><code>95df758</code></a> fix: update gRPC writeAndClose to only set finish_write on the last message (...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/e9746f856e9204c1c0ec62f19e6f71ff8a0b9750""><code>e9746f8</code></a> fix: make use of Immutab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13605:9507,update,update,9507,https://hail.is,https://github.com/hail-is/hail/pull/13605,1,['update'],['update']
Deployability," for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-yrxul/batch/tmp; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. Use an existing bucket and give permissions:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init ; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp/foo; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. User does not give permissions to existing remote tmpdir:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: n ; WARNING: Please verify service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com has the role ""roles/storage.objectAdmin"" or both ""roles/storage.objectViewer"" and ""roles/storage.objectCreator"" roles for bucket hail-batch-jigold-oxmmp.; Which ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:2431,CONFIGURAT,CONFIGURATION,2431,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['CONFIGURAT'],['CONFIGURATION']
Deployability," for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/Textualize/rich/commit/7f580bdcf07a3b269a0e786b6a3aa9c804f393cf""><code>7f580bd</code></a> Merge pull request <a href=""https://redirect.github.com/Textualize/rich/issues/3293"">#3293</a> from Textualize/bump1371</li>; <li><a href=""https://github.com/Textualize/rich/commit/705bc464cb8a25ff550a6e6e8b0f16e338576db6""><code>705bc46</code></a> bump</li>; <li><a href=""https://github.com/Textualize/rich/commit/f4a7ed38ebd6aecfe1077ad75f334b74beea4cc2""><code>f4a7ed3</code></a> Merge pull request <a href=""https://redirect.github.com/Textualize/rich/issues/3289"">#3289</a> from Textualize/update-wcwidth</li>; <li><a href=""https://github.com/Textualize/rich/commit/18ebb427bcd36e0b0b200cc41bf165d5fb758e95""><code>18ebb42</code></a> Update wcwidth and cell widths.</li>; <li><a href=""https://github.com/Textualize/rich/commit/26152e9cc95eef9c8f363d7bf1dfda426275348d""><code>26152e9</code></a> Export TextType into the docs. (<a href=""https://redirect.github.com/Textualize/rich/issues/3257"">#3257</a>)</li>; <li><a href=""https://github.com/Textualize/rich/commit/fd981823644ccf50d685ac9c0cfe8e1e56c9dd35""><code>fd98182</code></a> Merge pull request <a href=""https://redirect.github.com/Textualize/rich/issues/3202"">#3202</a> from Textualize/bump1370</li>; <li><a href=""https://github.com/Textualize/rich/commit/1f04c9c2f7bfbeaf42f40d8013ac6eba38bfffda""><code>1f04c9c</code></a> new version</li>; <li><a href=""https://github.com/Textualize/rich/commit/59b1aca63bf9ec69ada93af960d8b2a7bd920477""><code>59b1aca</code></a> Fix double-width characters disappearing when wrapping (<a href=""https://redirect.github.com/Textualize/rich/issues/3180"">#3180</a>)</li>; <li><a href=""https://github.com/Textualize/ri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14376:5071,Update,Update,5071,https://hail.is,https://github.com/hail-is/hail/pull/14376,2,['Update'],['Update']
Deployability, for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE=origin make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE=origin \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z buil,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:10985,release,release,10985,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability," for: ""deployment.yaml"": serviceaccounts ""batch-svc"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get serviceaccounts in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=roles"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=Role""; Name: ""batch-pods-admin"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""Role"" ""metadata"":map[""name"":""batch-pods-admin"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""rules"":[map[""apiGroups"":[""""] ""resources"":[""pods""] ""verbs"":[""get"" ""list"" ""watch"" ""create"" ""update"" ""patch"" ""delete""]] map[""apiGroups"":[""""] ""resources"":[""pods/log""] ""verbs"":[""get""]]]]}; from server for: ""deployment.yaml"": roles.rbac.authorization.k8s.io ""batch-pods-admin"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get roles.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=rolebindings"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=RoleBinding""; Name: ""batch-pods-admin-binding"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""RoleBinding"" ""metadata"":map[""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""name"":""batch-pods-admin-binding"" ""namespace"":""batch-pods""] ""roleRef"":map[""apiGroup"":"""" ""kind"":""Role"" ""name"":""batch-pods-admin""] ""subjects"":[map[""kind"":""ServiceAccount"" ""name"":""batch-svc"" ""namespace"":""default""]]]}; from server for: ""deployment.yaml"": rolebindings.rbac.authorization.k8s.io ""batch-pods-admin-binding"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:2088,deploy,deploy-svc,2088,https://hail.is,https://github.com/hail-is/hail/issues/4609,1,['deploy'],['deploy-svc']
Deployability," for; ```; GET http://localhost:8123/spark/api/v1/applications; ```; which happened repeatedly if you try to evaluate a cell. On the leader node of the spark cluster, `journalctl -u jupyter` shows:; ```; -- Logs begin at Fri 2019-03-01 19:54:49 UTC, end at Fri 2019-03-01 20:11:51 UTC. --; Mar 01 19:59:03 dk-m systemd[1]: Started Jupyter Notebook.; Mar 01 19:59:04 dk-m python[5149]: [I 19:59:04.630 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.796 NotebookApp] All authentication is disabled. Anyone who can connect to this server will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, in init_server_extensions; Mar 01 19:59:04 dk-m python[5149]: func(self); Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/__init__.py"", line 30, in load_jupyter",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5505:1175,configurat,configuration,1175,https://hail.is,https://github.com/hail-is/hail/issues/5505,1,['configurat'],['configuration']
Deployability," from 2.3.2 to 2.3.3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/python-cloud-core/releases"">google-cloud-core's releases</a>.</em></p>; <blockquote>; <h2>v2.3.3</h2>; <h2><a href=""https://github.com/googleapis/python-cloud-core/compare/v2.3.2...v2.3.3"">2.3.3</a> (2023-06-29)</h2>; <h3>Documentation</h3>; <ul>; <li>Update docs structure for c.g.c usage (<a href=""https://redirect.github.com/googleapis/python-cloud-core/issues/226"">#226</a>) (<a href=""https://github.com/googleapis/python-cloud-core/commit/b805f4a273de53017f74ca693ba7243fb4694848"">b805f4a</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/python-cloud-core/blob/main/CHANGELOG.md"">google-cloud-core's changelog</a>.</em></p>; <blockquote>; <h2><a href=""https://github.com/googleapis/python-cloud-core/compare/v2.3.2...v2.3.3"">2.3.3</a> (2023-06-29)</h2>; <h3>Documentation</h3>; <ul>; <li>Update docs structure for c.g.c usage (<a href=""https://redirect.github.com/googleapis/python-cloud-core/issues/226"">#226</a>) (<a href=""https://github.com/googleapis/python-cloud-core/commit/b805f4a273de53017f74ca693ba7243fb4694848"">b805f4a</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/googleapis/python-cloud-core/commit/c5f86620142ff1e1b0d200bfb2c22f97f3163363""><code>c5f8662</code></a> chore(main): release 2.3.3 (<a href=""https://redirect.github.com/googleapis/python-cloud-core/issues/236"">#236</a>)</li>; <li><a href=""https://github.com/googleapis/python-cloud-core/commit/ef0c83e7d3e49ab82d8610521ef07b25e2985012""><code>ef0c83e</code></a> chore: store artifacts in placer [autoapprove] (<a href=""https://redirect.github.com/googleapis/python-cloud-core/issues/237"">#237</a>)</li>; <li><a href=""https://github.com/googleapis/python-cloud-core/commit/b805f4a273de53017f74ca693ba7243fb4694848""><code>b80",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13311:1090,Update,Update,1090,https://hail.is,https://github.com/hail-is/hail/pull/13311,1,['Update'],['Update']
Deployability," from <a href=""https://github.com/pytest-dev/pytest-metadata/blob/master/CHANGES.rst"">pytest-metadata's changelog</a>.</em></p>; <blockquote>; <h2>2.0.2 (2022-07-15)</h2>; <ul>; <li>Allow all python versions above 3.7</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/6688a6f21c7bef1aaea5f8ac171be4a5df2eb527""><code>6688a6f</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-metadata/issues/50"">#50</a> from BeyondEvil/release-v2.0.2</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/f0b5503452f922a84faa213224a4970c57d8a654""><code>f0b5503</code></a> Release v2.0.2</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/ff493afce81b1bbe7a2f866cd27510e5d9b8feef""><code>ff493af</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-metadata/issues/47"">#47</a> from pytest-dev/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/4591db1fa5546ff372ae95155caed99ce8dc4842""><code>4591db1</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/0414bb9f81cc1856ea021504eecd22d202462f1d""><code>0414bb9</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-metadata/issues/46"">#46</a> from pytest-dev/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/025be8999a22ae395b0e2b8ae4e7c9fa2334f874""><code>025be89</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/429840f4de26276560961929f21aab79ed305875""><code>429840f</code></a> Avoid running nightly on forks</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/c1968f39609978ec9c6a4bcf91c37c6164483f04""><code>c1968f3</code></a> Fix nightly",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12188:1164,update,update-config,1164,https://hail.is,https://github.com/hail-is/hail/pull/12188,1,['update'],['update-config']
Deployability," from <a href=""https://github.com/sass/libsass-python/blob/main/docs/changes.rst"">libsass's changelog</a>.</em></p>; <blockquote>; <h2>Version 0.22.0</h2>; <p>Released on November 12, 2022.</p>; <ul>; <li>Remove python 2.x support [:issue:<code>373</code> by anthony sottile].</li>; <li>Remove deprecated <code>sassc</code> cli [:issue:<code>379</code> by anthony sottile].</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/sass/libsass-python/commit/b18db090672676d7c58fcd52e6ae0eb505993886""><code>b18db09</code></a> 0.22.0</li>; <li><a href=""https://github.com/sass/libsass-python/commit/22adb66fac69d058e8dccc0014563cd76e78349e""><code>22adb66</code></a> correct version number</li>; <li><a href=""https://github.com/sass/libsass-python/commit/b2436e282ae19ccb7be9318f3ddd0eb6cdb48be3""><code>b2436e2</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/406"">#406</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/980b41f462ae07939515993781e72654b117bdce""><code>980b41f</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/cfffd417e56b7fd3aaf6034fa49083185714f6b7""><code>cfffd41</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/405"">#405</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/20b3cdade8a199e832521d0e44a2507bc75315e0""><code>20b3cda</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/940ef2e9f9dd4143d642a29156c94d0a3133a691""><code>940ef2e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/404"">#404</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/5f8470b48cb576f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12482:1523,update,update-config,1523,https://hail.is,https://github.com/hail-is/hail/pull/12482,1,['update'],['update-config']
Deployability," from <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/releases"">sphinx-autodoc-typehints's releases</a>.</em></p>; <blockquote>; <h2>1.18.2</h2>; <h2>What's Changed</h2>; <ul>; <li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/230"">tox-dev/sphinx-autodoc-typehints#230</a></li>; <li>Support and require nptyping 2.1.1 by <a href=""https://github.com/gaborbernat""><code>@​gaborbernat</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/232"">tox-dev/sphinx-autodoc-typehints#232</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2"">https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2</a></p>; <h2>1.18.1</h2>; <p>No release notes provided.</p>; <h2>1.18.0</h2>; <p>No release notes provided.</p>; <h2>1.17.1</h2>; <p>No release notes provided.</p>; <h2>typehints_use_rtype support and handle TypeError</h2>; <p>No release notes provided.</p>; <h2>1.16.0</h2>; <p>No release notes provided.</p>; <h2>1.15.3</h2>; <p>No release notes provided.</p>; <h2>1.15.2</h2>; <p>No release notes provided.</p>; <h2>1.15.1</h2>; <p>No release notes provided.</p>; <h2>1.15.0</h2>; <p>No release notes provided.</p>; <h2>1.14.1</h2>; <p>No release notes provided.</p>; <h2>Added document_defaults config option</h2>; <p>No release notes provided.</p>; <h2>Fix NewType is inserting a reference as first argument</h2>; <p>No release notes provided.</p>; <h2>Python 3.10 support and PEP-563, drop 3.6</h2>; <p>No release notes provided.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/blob/main/CHANGELOG.md"">sphinx-autodoc-typehints's changelog</a>.</em></p>; <blockquote>; <h2>1.18.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11893:1165,release,release,1165,https://hail.is,https://github.com/hail-is/hail/pull/11893,1,['release'],['release']
Deployability," from showtraceback()</li>; <li>Additional commits viewable in <a href=""https://github.com/ipython/ipython/compare/7.34.0...8.10.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=ipython&package-manager=pip&previous-version=7.34.0&new-version=8.10.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12683:3639,upgrade,upgrade,3639,https://hail.is,https://github.com/hail-is/hail/pull/12683,3,['upgrade'],['upgrade']
Deployability," green</li>; <li><a href=""https://github.com/aio-libs/janus/commit/ec94b35b2ae095dcb97827f1369c0cd31b7e8e5e""><code>ec94b35</code></a> Fix CI again</li>; <li><a href=""https://github.com/aio-libs/janus/commit/2303208c2f972e38445e7ecec54fda0f3203f566""><code>2303208</code></a> Fix CI</li>; <li><a href=""https://github.com/aio-libs/janus/commit/dff507895bf8d77efea2c4cc1d8b04a8a2986a0b""><code>dff5078</code></a> Bump to 1.0.0</li>; <li><a href=""https://github.com/aio-libs/janus/commit/3421545f3954b7ef6d90e02b7653a7ab685f3e78""><code>3421545</code></a> Bump mypy from 0.910 to 0.920 (<a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/384"">#384</a>)</li>; <li><a href=""https://github.com/aio-libs/janus/commit/56b2d1d8dbd10cce28302a4e1c4224ce219c6246""><code>56b2d1d</code></a> Bump black from 21.11b1 to 21.12b0 (<a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/383"">#383</a>)</li>; <li><a href=""https://github.com/aio-libs/janus/commit/883e82bea0af1d12a68e92148b75b3344b31227a""><code>883e82b</code></a> Update README.rst</li>; <li><a href=""https://github.com/aio-libs/janus/commit/2e30d8a0f3c77c383a39da9b5c233a5c93a049fb""><code>2e30d8a</code></a> Bump coverage from 6.1.2 to 6.2 (<a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/382"">#382</a>)</li>; <li><a href=""https://github.com/aio-libs/janus/commit/7b72d8577517422dd0719da1c6a0fb33d8a10e23""><code>7b72d85</code></a> Bump to 0.7</li>; <li>Additional commits viewable in <a href=""https://github.com/aio-libs/janus/compare/v0.6.0...v1.0.0"">compare view</a></li>; </ul>; </details>; <br />. Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11466:4041,Update,Update,4041,https://hail.is,https://github.com/hail-is/hail/pull/11466,1,['Update'],['Update']
Deployability," green</li>; <li><a href=""https://github.com/aio-libs/janus/commit/ec94b35b2ae095dcb97827f1369c0cd31b7e8e5e""><code>ec94b35</code></a> Fix CI again</li>; <li><a href=""https://github.com/aio-libs/janus/commit/2303208c2f972e38445e7ecec54fda0f3203f566""><code>2303208</code></a> Fix CI</li>; <li><a href=""https://github.com/aio-libs/janus/commit/dff507895bf8d77efea2c4cc1d8b04a8a2986a0b""><code>dff5078</code></a> Bump to 1.0.0</li>; <li><a href=""https://github.com/aio-libs/janus/commit/3421545f3954b7ef6d90e02b7653a7ab685f3e78""><code>3421545</code></a> Bump mypy from 0.910 to 0.920 (<a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/384"">#384</a>)</li>; <li><a href=""https://github.com/aio-libs/janus/commit/56b2d1d8dbd10cce28302a4e1c4224ce219c6246""><code>56b2d1d</code></a> Bump black from 21.11b1 to 21.12b0 (<a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/383"">#383</a>)</li>; <li><a href=""https://github.com/aio-libs/janus/commit/883e82bea0af1d12a68e92148b75b3344b31227a""><code>883e82b</code></a> Update README.rst</li>; <li><a href=""https://github.com/aio-libs/janus/commit/2e30d8a0f3c77c383a39da9b5c233a5c93a049fb""><code>2e30d8a</code></a> Bump coverage from 6.1.2 to 6.2 (<a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/382"">#382</a>)</li>; <li>See full diff in <a href=""https://github.com/aio-libs/janus/compare/v0.7.0...v1.0.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=janus&package-manager=pip&previous-version=0.7.0&new-version=1.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12436:2079,Update,Update,2079,https://hail.is,https://github.com/hail-is/hail/pull/12436,1,['Update'],['Update']
Deployability," hail/python/hailtop/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5663682](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5663682) | `cryptography:` <br> `40.0.2 -> 41.0.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJmZWM3ZmQ2Ny0xZmE0LTRlNzEtODQ4Ni1hMDk5YThmYWM3NzgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImZlYzdmZDY3LTFmYTQtNGU3MS04NDg2LWEwOTlhOGZhYzc3OCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13138:1309,upgrade,upgraded,1309,https://hail.is,https://github.com/hail-is/hail/pull/13138,1,['upgrade'],['upgraded']
Deployability," hail/python/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **658/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.3 | HTTP Request Smuggling <br/>[SNYK-PYTHON-AIOHTTP-5798483](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-5798483) | `aiohttp:` <br> `3.8.4 -> 3.8.5` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlMjk5ZmU1Ni0wNGI1LTQ3MzEtYmUzYS03M2ZmYzgxZTZjYjgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImUyOTlmZTU2LTA0YjUtNDczMS1iZTNhLTczZmZjODFlNmNiOCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13283:1401,upgrade,upgraded,1401,https://hail.is,https://github.com/hail-is/hail/pull/13283,1,['upgrade'],['upgraded']
Deployability," have been changed for v1beta1. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108139"">kubernetes/kubernetes#108139</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>) [SIG Auth and Testing]</li>; <li>Fix OpenAPI serialization of the x-kubernetes-validations field (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108030"">kubernetes/kubernetes#108030</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>) [SIG API Machinery]</li>; <li>A new field <code>omitManagedFields</code> has been added to both <code>audit.Policy</code> and <code>audit.PolicyRule</code>; so cluster operators can opt in to omit managed fields of the request and response bodies from; being written to the API audit log. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/94986"">kubernetes/kubernetes#94986</a>, <a href=""https://github.com/tkashem""><code>@​tkashem</code></a>) [SIG API Machinery, Auth, Cloud Provider and Testing]</li>; <li>A small regression in Service updates was fixed. The circumstances are so unlikely that probably nobody would ever hit it. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104601"">kubernetes/kubernetes#104601</a>, <a href=""https://github.com/thockin""><code>@​thockin</code></a>)</li>; <li>Added a feature gate <code>StatefulSetAutoDeletePVC</code>, which allows PVCs automatically created for StatefulSet pods to be automatically deleted. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/99728"">kubernetes/kubernetes#99728</a>, <a href=""https://github.com/mattcary""><code>@​mattcary</code></a>)</li>; <li>Client-go impersonation config can specify a UID to pass impersonated uid information through in requests. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104483"">kubernetes/kubernetes#104483</a>, <a href=""https://github.com/margocrawf""><code>@​margocrawf</code></a>)</li>; <li>Create ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:2182,update,updates,2182,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['update'],['updates']
Deployability," help debugging of JVM crashing in production until the JVM logs are shown on a per-worker page. 2. JVMEntryway is now a real gradle project. I need to compile against log4j, and I didn't want to do that by hand with `javac`. Ignore gradlew, gradlew.bat, and gradle/wrapper, they're programmatically generated by gradle. 3. Add logging to JVMEntryway. JVMEntryway now logs its arguments into the QoB job log. I also log exceptions from the main thread or the cancel thread into the job log. We also flush the logs after the main thread completes, the cancel thread completes, and when the try-catch exits. This should ensure that regardless of what goes wrong (even if both threads fail to start) we at least see the arguments that the JVMEntryway received. 4. Use log4j2 programmatic reconfiguration after every job. This restores log4j2 to well enough working order that, *if you do not try to reconfigure it using log4j1 programmatic configuration*, logs will work. All old versions of Hail use log4j1 programmatic configuration. As a result, **all old versions of Hail will still have no logs**. However, new versions of Hail will log correctly even if an old version of Hail used the JVM before it. 5. `QoBAppender`. This is how we always should have done logging. A custom appender which we can flush and then redirect to a new file at our whim. I followed the log4j2 best practices for creating a new appender. All these annotations, factory methods, and managers are The Right Way, for better or worse. If we ever ban old versions of Hail from the cluster, then we can also eliminate the log4j2 reconfiguration. New versions of Hail work fine without any runtime log configuration (thanks to `QoBAppender`). I would like to eliminate reconfiguration because log4j2 reconfiguration leaves around oprhaned appenders and appender managers. Maybe I'm implementing the Appender or Appender Manager interfaces wrong, but I've read over that code a bunch of times and I cannot sort out what I am mis",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941:1358,configurat,configuration,1358,https://hail.is,https://github.com/hail-is/hail/pull/12941,1,['configurat'],['configuration']
Deployability," href=""https://github-redirect.dependabot.com/axios/axios/issues/3825"">#3825</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/e091491127893a476b0223ab72f788c3b30fc082""><code>e091491</code></a> Update README.md (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3936"">#3936</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/b42fbad57b093bb7214991161c5355bd46b864d0""><code>b42fbad</code></a> Removed un-needed bracket</li>; <li><a href=""https://github.com/axios/axios/commit/520c8dccdef92cccbe51ea7cd96ad464c6401914""><code>520c8dc</code></a> Updating CI status badge (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3953"">#3953</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/axios/axios/compare/v0.21.1...v0.21.2"">compare view</a></li>; </ul>; </details>; <details>; <summary>Maintainer changes</summary>; <p>This version was pushed to npm by <a href=""https://www.npmjs.com/~jasonsaayman"">jasonsaayman</a>, a new releaser for axios since your current version.</p>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=axios&package-manager=npm_and_yarn&previous-version=0.21.1&new-version=0.21.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11080:13676,release,releaser,13676,https://hail.is,https://github.com/hail-is/hail/pull/11080,2,['release'],['releaser']
Deployability," href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2837"">cbeust/testng#2837</a></li>; <li>Support getting dependencies info for a test by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2839"">cbeust/testng#2839</a></li>; <li>Honour regex in dependsOnMethods by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2838"">cbeust/testng#2838</a></li>; <li>Ensure All tests run all the time by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2842"">cbeust/testng#2842</a></li>; <li>Deprecate support for running Spock Tests by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2846"">cbeust/testng#2846</a></li>; <li>Streamline dependsOnMethods for configurations by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2845"">cbeust/testng#2845</a></li>; <li>Ensure ITestContext available for JUnit4 tests by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2848"">cbeust/testng#2848</a></li>; <li>Deprecate support for running JUnit tests by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2849"">cbeust/testng#2849</a></li>; </ul>; <h2>New Contributors</h2>; <ul>; <li><a href=""https://github.com/gruenich""><code>@​gruenich</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2781"">cbeust/testng#2781</a></li>; <li><a href=""https://github.com/anatolyuzhakov""><code>@​anatolyuzhakov</code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:7376,configurat,configurations,7376,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['configurat'],['configurations']
Deployability," href=""https://github-redirect.dependabot.com/psf/black/issues/2966"">#2966</a>)</li>; <li>On Python 3.11 and newer, use the standard library's <code>tomllib</code> instead of <code>tomli</code>; (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2903"">#2903</a>)</li>; <li><code>black-primer</code>, the deprecated internal devtool, has been removed and copied to a; <a href=""https://github.com/cooperlees/black-primer"">separate repository</a> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2924"">#2924</a>)</li>; </ul>; <h3>Parser</h3>; <ul>; <li>Black can now parse starred expressions in the target of <code>for</code> and <code>async for</code>; statements, e.g <code>for item in *items_1, *items_2: pass</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2879"">#2879</a>).</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/psf/black/commit/ae2c0758c9e61a385df9700dc9c231bf54887041""><code>ae2c075</code></a> Prepare release 22.3.0 (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2968"">#2968</a>)</li>; <li><a href=""https://github.com/psf/black/commit/e9681a40dcb3d38b56b301d811bb1c55201fd97e""><code>e9681a4</code></a> Fix _unicodefun patch code for Click 8.1.0 (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2966"">#2966</a>)</li>; <li><a href=""https://github.com/psf/black/commit/ac7402cbf6a0deb5c74e9abcffc5bd7b1148fda5""><code>ac7402c</code></a> Bump sphinx from 4.4.0 to 4.5.0 in /docs (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2959"">GH-2959</a>)</li>; <li><a href=""https://github.com/psf/black/commit/f239d227c003c52126239e1b9a37c36c2b2b8305""><code>f239d22</code></a> Enforce no formatting changes for PRs via CI (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2951"">GH-2951</a>)</li>; <li><a href=""https://github.com/psf/black/commit/bd1e98034907463f5d86f4d87e89202dc6c34dd4""><code>bd1e980</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11696:6526,release,release,6526,https://hail.is,https://github.com/hail-is/hail/pull/11696,1,['release'],['release']
Deployability," href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9636"">#9636</a>: The <code>pythonpath</code> plugin was renamed to <code>python_path</code>. This avoids a conflict with the <code>pytest-pythonpath</code> plugin.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9642"">#9642</a>: Fix running tests by id with <code>::</code> in the parametrize portion.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9643"">#9643</a>: Delay issuing a <code>~pytest.PytestWarning</code>{.interpreted-text role=&quot;class&quot;} about diamond inheritance involving <code>~pytest.Item</code>{.interpreted-text role=&quot;class&quot;} and; <code>~pytest.Collector</code>{.interpreted-text role=&quot;class&quot;} so it can be filtered using <code>standard warning filters &lt;warnings&gt;</code>{.interpreted-text role=&quot;ref&quot;}.</li>; </ul>; <h2>7.0.0</h2>; <h1>pytest 7.0.0 (2022-02-03)</h1>; <p>(<strong>Please see the full set of changes for this release also in the 7.0.0rc1 notes below</strong>)</p>; <h2>Deprecations</h2>; <ul>; <li>; <p><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9488"">#9488</a>: If custom subclasses of nodes like <code>pytest.Item</code>{.interpreted-text role=&quot;class&quot;} override the; <code>__init__</code> method, they should take <code>**kwargs</code>. See; <code>uncooperative-constructors-deprecated</code>{.interpreted-text role=&quot;ref&quot;} for details.</p>; <p>Note that a deprection warning is only emitted when there is a conflict in the; arguments pytest expected to pass. This deprecation was already part of pytest; 7.0.0rc1 but wasn't documented.</p>; </li>; </ul>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9355"">#9355</a>: Fixed error message prints function decorators when using assert in Python 3.8 and above.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:1844,release,release,1844,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['release'],['release']
Deployability," href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10498"">#10498</a>: gettext: TypeError is raised when sorting warning messages if a node; has no line number. Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10493"">#10493</a>: HTML Theme: :rst:dir:<code>topic</code> directive is rendered incorrectly with; Docutils 0.18. Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10495"">#10495</a>: IndexError is raised for a :rst:role:<code>kbd</code> role having a separator.; Patch by Adam Turner.</li>; </ul>; <h1>Release 5.0.0 (released May 30, 2022)</h1>; <h2>Dependencies</h2>; <p>5.0.0 b1</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10164"">#10164</a>: Support <code>Docutils 0.18</code>_. Patch by Adam Turner.</li>; </ul>; <p>.. _Docutils 0.18: <a href=""https://docutils.sourceforge.io/RELEASE-NOTES.html#release-0-18-2021-10-26"">https://docutils.sourceforge.io/RELEASE-NOTES.html#release-0-18-2021-10-26</a></p>; <h2>Incompatible changes</h2>; <p>5.0.0 b1</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/907d27dc6506c542c11a7dd16b560eb4be7da5fc""><code>907d27d</code></a> Bump to 5.0.2 final</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/ed6970311349e54ceebe24ede255378fcd9d94e5""><code>ed69703</code></a> Update CHANGES for PR <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10535"">#10535</a></li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/377d8668b5c93cc224fec46f2f3c2920b25107ca""><code>377d866</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10535"">#10535</a> from AA-Turner/css-nav-contents</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/709602437df850d55",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11925:3656,release,release-,3656,https://hail.is,https://github.com/hail-is/hail/pull/11925,1,['release'],['release-']
Deployability," href=""https://github.com/KleinSamuel""><code>@​KleinSamuel</code></a> made their first contribution in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1664"">samtools/htsjdk#1664</a></li>; <li><a href=""https://github.com/gileshall""><code>@​gileshall</code></a> made their first contribution in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1674"">samtools/htsjdk#1674</a></li>; <li><a href=""https://github.com/omicsorama""><code>@​omicsorama</code></a> made their first contribution in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1635"">samtools/htsjdk#1635</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/samtools/htsjdk/compare/3.0.5...4.0.0"">https://github.com/samtools/htsjdk/compare/3.0.5...4.0.0</a></p>; <h2>3.0.5</h2>; <h2>The last gasp of Java 8</h2>; <p>This release includes very minor new features as well as a bug fix. It is mostly notable because it is intended to be the last release supporting Java 8. Future releases will target Java 17.</p>; <h2>What's Changed</h2>; <ul>; <li>Update commons-compress to close vulnerabilities by <a href=""https://github.com/lbergelson""><code>@​lbergelson</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1639"">samtools/htsjdk#1639</a></li>; <li>Removing scala test infrastructure by <a href=""https://github.com/lbergelson""><code>@​lbergelson</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1640"">samtools/htsjdk#1640</a></li>; <li>Adding Cigar.fromCigarString() by <a href=""https://github.com/lbergelson""><code>@​lbergelson</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1647"">samtools/htsjdk#1647</a></li>; <li>Updating gradle to 7.6 by <a href=""https://github.com/lbergelson""><code>@​lbergelson</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1650"">samtools/htsjdk#1650</a></li>; <li>Expose the ability to encode a Genotoype into a GT field by <a href=""https://github.com/lbergelson""><co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13576:4177,release,releases,4177,https://hail.is,https://github.com/hail-is/hail/pull/13576,1,['release'],['releases']
Deployability," href=""https://github.com/boto/boto3/commit/1de404aff4ecb1c5560b4e023f0614d8149622ed""><code>1de404a</code></a> fix typo: 'are specified the' should be 'are specified in the' (<a href=""https://github-redirect.dependabot.com/boto/boto3/issues/3499"">#3499</a>)</li>; <li><a href=""https://github.com/boto/boto3/commit/47f20744b3c57223da5e14e185120586f8212af8""><code>47f2074</code></a> Merge branch 'release-1.26.13'</li>; <li><a href=""https://github.com/boto/boto3/commit/3d4081ccb7c350538ba3d6a5073c59575a812eb0""><code>3d4081c</code></a> Merge branch 'release-1.26.13' into develop</li>; <li>Additional commits viewable in <a href=""https://github.com/boto/boto3/compare/1.26.7...1.26.15"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=boto3&package-manager=pip&previous-version=1.26.7&new-version=1.26.15)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12498:9609,update,updates,9609,https://hail.is,https://github.com/hail-is/hail/pull/12498,1,['update'],['updates']
Deployability," href=""https://github.com/dateutil/dateutil/commit/ee85831cc25d34ff597cfb3f2d90ce5904dbc561""><code>ee85831</code></a> Build releases with Python 3.9</li>; <li><a href=""https://github.com/dateutil/dateutil/commit/6b337ea412d399fb48771c544b1a6880763b46c6""><code>6b337ea</code></a> Automate cutting new releases</li>; <li><a href=""https://github.com/dateutil/dateutil/commit/9c2ad8f981ece1bdb3d52527f1cb39523b11d862""><code>9c2ad8f</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/1056"">#1056</a> from ffe4/issue_1029</li>; <li>Additional commits viewable in <a href=""https://github.com/dateutil/dateutil/compare/2.8.1...2.8.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=python-dateutil&package-manager=pip&previous-version=2.8.1&new-version=2.8.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11518:11458,update,updates,11458,https://hail.is,https://github.com/hail-is/hail/pull/11518,1,['update'],['updates']
Deployability," href=""https://github.com/elastic/elasticsearch-hadoop/commit/8327792db7605353c6d4d43e85c8ad7cb31f2e51""><code>8327792</code></a> Bump to version 8.4.1</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/4d2e4b12b83f84521ce36634a3eeb0904137c89b""><code>4d2e4b1</code></a> [DOCS] Add 8.4.0 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1989"">#1989</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1990"">#1990</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/71f288bf1b473d0ed34b9dd4284bf33aa98a0ccf""><code>71f288b</code></a> [DOCS] Add 8.3.3 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1983"">#1983</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1985"">#1985</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/712679f88772fb15184ad7c87dea220a87803f44""><code>712679f</code></a> Upgrade to Gradle 7.5 (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1980"">#1980</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/a4d14077a58ba3272469d48500ce007c725f1c73""><code>a4d1407</code></a> [DOCS] Added 8.3.2 RNs (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1978"">#1978</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/elastic/elasticsearch-hadoop/compare/v7.17.1...v8.4.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.elasticsearch:elasticsearch-spark-20_2.12&package-manager=gradle&previous-version=7.17.1&new-version=8.4.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12358:6870,Upgrade,Upgrade,6870,https://hail.is,https://github.com/hail-is/hail/pull/12358,1,['Upgrade'],['Upgrade']
Deployability," href=""https://github.com/elastic/elasticsearch-hadoop/commit/8327792db7605353c6d4d43e85c8ad7cb31f2e51""><code>8327792</code></a> Bump to version 8.4.1</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/4d2e4b12b83f84521ce36634a3eeb0904137c89b""><code>4d2e4b1</code></a> [DOCS] Add 8.4.0 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1989"">#1989</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1990"">#1990</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/71f288bf1b473d0ed34b9dd4284bf33aa98a0ccf""><code>71f288b</code></a> [DOCS] Add 8.3.3 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1983"">#1983</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1985"">#1985</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/712679f88772fb15184ad7c87dea220a87803f44""><code>712679f</code></a> Upgrade to Gradle 7.5 (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1980"">#1980</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/a4d14077a58ba3272469d48500ce007c725f1c73""><code>a4d1407</code></a> [DOCS] Added 8.3.2 RNs (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1978"">#1978</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/elastic/elasticsearch-hadoop/compare/v8.0.0...v8.4.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.elasticsearch:elasticsearch-spark-30_2.12&package-manager=gradle&previous-version=8.0.0&new-version=8.4.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any confl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:6869,Upgrade,Upgrade,6869,https://hail.is,https://github.com/hail-is/hail/pull/12319,1,['Upgrade'],['Upgrade']
Deployability," href=""https://github.com/methane""><code>@​methane</code></a> in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1142"">PyMySQL/PyMySQL#1142</a></li>; <li>chore(deps): update dessant/lock-threads action to v5 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1141"">PyMySQL/PyMySQL#1141</a></li>; <li>doc: use rtd theme by <a href=""https://github.com/methane""><code>@​methane</code></a> in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1143"">PyMySQL/PyMySQL#1143</a></li>; <li>use Ruff as formatter by <a href=""https://github.com/methane""><code>@​methane</code></a> in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1144"">PyMySQL/PyMySQL#1144</a></li>; <li>chore(deps): update dependency sphinx-rtd-theme to v2 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1147"">PyMySQL/PyMySQL#1147</a></li>; <li>chore(deps): update actions/setup-python action to v5 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1152"">PyMySQL/PyMySQL#1152</a></li>; <li>chore(deps): update github/codeql-action action to v3 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1154"">PyMySQL/PyMySQL#1154</a></li>; <li>chore(deps): update codecov/codecov-action action to v4 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1158"">PyMySQL/PyMySQL#1158</a></li>; <li>Support error packet without sqlstate by <a href=""https://github.com/methane""><code>@​methane</code></a> in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1160"">PyMySQL/PyMySQL#1160</a></li>; <li>test json - mariadb without JSON type by <a href=""https://github.com/grooverdan""><code>@​grooverdan</code></a> in <a href=""h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14556:2653,update,update,2653,https://hail.is,https://github.com/hail-is/hail/pull/14556,1,['update'],['update']
Deployability," href=""https://github.com/pallets/jinja/releases"">jinja2's releases</a>.</em></p>; <blockquote>; <h2>3.1.3</h2>; <p>This is a fix release for the 3.1.x feature branch.</p>; <ul>; <li>Fix for <a href=""https://github.com/pallets/jinja/security/advisories/GHSA-h5c8-rqwp-cp95"">GHSA-h5c8-rqwp-cp95</a>. You are affected if you are using <code>xmlattr</code> and passing user input as attribute keys.</li>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-3"">https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-3</a></li>; <li>Milestone: <a href=""https://github.com/pallets/jinja/milestone/15?closed=1"">https://github.com/pallets/jinja/milestone/15?closed=1</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/blob/main/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 3.1.3</h2>; <p>Released 2024-01-10</p>; <ul>; <li>Fix compiler error when checking if required blocks in parent templates are; empty. :pr:<code>1858</code></li>; <li><code>xmlattr</code> filter does not allow keys with spaces. GHSA-h5c8-rqwp-cp95</li>; <li>Make error messages stemming from invalid nesting of <code>{% trans %}</code> blocks; more helpful. :pr:<code>1918</code></li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/jinja/commit/d9de4bb215fd1cc8092a410fb834c7c4060b1fc1""><code>d9de4bb</code></a> release version 3.1.3</li>; <li><a href=""https://github.com/pallets/jinja/commit/50124e16561f17f6c1ec85a692f6551418971cdc""><code>50124e1</code></a> skip test pypi</li>; <li><a href=""https://github.com/pallets/jinja/commit/9ea7222ef3f184480be0d0884e30ccfb4172b17b""><code>9ea7222</code></a> use trusted publishing</li>; <li><a href=""https://github.com/pallets/jinja/commit/da703f7aae36b1e88baaa20de334d7ff6378fdde""><code>da703f7</code></a> use trusted publishing</li>; <li><a href=""https",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14144:1085,Release,Released,1085,https://hail.is,https://github.com/hail-is/hail/pull/14144,3,['Release'],['Released']
Deployability," href=""https://github.com/prometheus/client_python/commit/da15e4a4d671b8aea0e60fc859d5df8102be3897""><code>da15e4a</code></a> Change to imports to fix go-to-declaration in editors (<a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/747"">#747</a>)</li>; <li><a href=""https://github.com/prometheus/client_python/commit/3ef865e1cccae66f63ae764762a700c5775a5190""><code>3ef865e</code></a> Allow to add labels inside a context manager (<a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/730"">#730</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/prometheus/client_python/compare/v0.11.0...v0.13.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=prometheus-client&package-manager=pip&previous-version=0.11.0&new-version=0.13.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11515:5662,update,updates,5662,https://hail.is,https://github.com/hail-is/hail/pull/11515,1,['update'],['updates']
Deployability," href=""https://github.com/python-pillow/Pillow/commit/8c1dc819fd91471825da01976ac0e0bc8789590f""><code>8c1dc81</code></a> Update CHANGES.rst [ci skip]</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/e37b25087d39bd54495380a9898c8c7a2a4698d1""><code>e37b250</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7244"">#7244</a> from radarhere/imagefont_max_string_length</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/d398fedb9d5af22316c715d2066176d15031d439""><code>d398fed</code></a> Added underscores for readability</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/1fe1bb49c452b0318cad12ea9d97c3bef188e9a7""><code>1fe1bb4</code></a> Added ImageFont.MAX_STRING_LENGTH</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/7c945f5131cf8596084b32af582f90a43b090540""><code>7c945f5</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7243"">#7243</a> from radarhere/releasenotes</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/0fb69fa821155c1b213f3f3488d3057b6ba7c154""><code>0fb69fa</code></a> Added release notes for <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7123"">#7123</a></li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/b7f1af77fd6ec9468438e25f72b003c16a9e6661""><code>b7f1af7</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7230"">#7230</a> from nulano/add-pyproject.toml</li>; <li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/9.5.0...10.0.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=9.5.0&new-version=10.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:14059,release,releasenotes,14059,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['release'],['releasenotes']
Deployability," href=""https://github.com/sass/libsass/releases/tag/3.6.5"">https://github.com/sass/libsass/releases/tag/3.6.5</a></p>; <h2>Version 0.20.1</h2>; <p>Released on August 27, 2020.</p>; <ul>; <li>(no changes, re-releasing to test build automation)</li>; </ul>; <h2>Version 0.20.0</h2>; <p>Released on May 1, 2020.</p>; <ul>; <li>Produce abi3 wheels on macos / linux [:issue:<code>307</code> by Anthony Sottile]</li>; <li>Follow up the libsass upstream: 3.6.4 --- See the release notes of LibSass; 3.6.4__. [:issue:<code>313</code> by Anthony Sottile]</li>; </ul>; <p>__ <a href=""https://github.com/sass/libsass/releases/tag/3.6.4"">https://github.com/sass/libsass/releases/tag/3.6.4</a></p>; <h2>Version 0.19.4</h2>; <p>Released on November 3, 2019.</p>; <ul>; <li>Follow up the libsass upstream: 3.6.3 --- See the release notes of LibSass; 3.6.3__. [:issue:<code>304</code> by Anthony Sottile]</li>; </ul>; <p>__ <a href=""https://github.com/sass/libsass/releases/tag/3.6.3"">https://github.com/sass/libsass/releases/tag/3.6.3</a></p>; <h2>Version 0.19.3</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/sass/libsass-python/commit/13c0d60e244c694dec88d8ac8370d7aae6dce4d0""><code>13c0d60</code></a> 0.21.0</li>; <li><a href=""https://github.com/sass/libsass-python/commit/5c94c2a72dab34367758e229c487de50f1430283""><code>5c94c2a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/344"">#344</a> from sass/3_6_5</li>; <li><a href=""https://github.com/sass/libsass-python/commit/ad69f6e023a6d8fdde4428b7a497b60fb5515215""><code>ad69f6e</code></a> update libsass to 3.6.5</li>; <li><a href=""https://github.com/sass/libsass-python/commit/38735e2fdc30ecb21f6eebb253c1b7a9a45dc757""><code>38735e2</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/343"">#343</a> from sass/pre-commit-ci-update-config<",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11508:3207,release,releases,3207,https://hail.is,https://github.com/hail-is/hail/pull/11508,1,['release'],['releases']
Deployability," href=""https://redirect.github.com/Textualize/rich/issues/3057"">Textualize/rich#3057</a></li>; </ul>; <h2>[13.5.0] - 2023-07-29</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs not expanding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting <code>__eq__</code> <a href=""https://redirect.github.com/Textualize/rich/issues/2875"">Textualize/rich#2875</a></li>; <li>Fix rich.pretty.install breakage in iPython <a href=""https://redirect.github.com/Textualize/rich/issues/3013"">Textualize/rich#3013</a></li>; </ul>; <h3>Added</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/Textualize/rich/commit/fd981823644ccf50d685ac9c0cfe8e1e56c9dd35""><code>fd98182</code></a> Merge pull request <a href=""https://redirect.github.com/Textualize/rich/issues/3202"">#3202</a> from Textualize/bump1370</li>; <li><a href=""https://github.com/Textualize/rich/commit/1f04c9c2f7bfbeaf42f40d8013ac6eba38bfffda""><code>1f04c9c</code></a> new version</li>; <li><a href=""https://github.com/Textualize/rich/commit/59b1aca63bf9ec69ada93af960d8b2a7bd920477""><code>59b1aca</code></a> Fix double-width characters disappearing when wrapping (<a href=""https://redirect.github.com/Textualize/rich/issues/3180"">#3180</a>)</li>; <li><a href=""https://github.com/Textualize/rich/commit/b32e42bda00c6275d4e18dcbe298268094359549""><code>b32e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14012:4765,install,install,4765,https://hail.is,https://github.com/hail-is/hail/pull/14012,2,['install'],['install']
Deployability," href=""https://redirect.github.com/Textualize/rich/issues/3057"">Textualize/rich#3057</a></li>; </ul>; <h2>[13.5.0] - 2023-07-29</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs not expanding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting <code>__eq__</code> <a href=""https://redirect.github.com/Textualize/rich/issues/2875"">Textualize/rich#2875</a></li>; <li>Fix rich.pretty.install breakage in iPython <a href=""https://redirect.github.com/Textualize/rich/issues/3013"">Textualize/rich#3013</a></li>; </ul>; <h3>Added</h3>; <ul>; <li>Added Text.extend_style method.</li>; <li>Added Span.extend method.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Text.tab_size now defaults to <code>None</code> to indicate that Console.tab_size should be used.</li>; </ul>; <h2>[13.4.2] - 2023-06-12</h2>; <h3>Changed</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/Textualize/rich/commit/e9f75c9912ed25b9777bc0257853370951220b17""><code>e9f75c9</code></a> Merge branch 'py312'</li>; <li><a href=""https://github.com/Textualize/rich/commit/35b64f1237f1c64326329b4668b116809a7fc596""><code>35b64f1</code></a> Merge pull request <a href=""https://redirect.github.com/Textualize/rich/issues/3139"">#3139</a> from Textualize/py312</li>; <li><a href=""https://github.com/Textualize/rich/commit/c8ff546416b086c92d1ab9a1a0b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13758:4492,install,install,4492,https://hail.is,https://github.com/hail-is/hail/pull/13758,2,['install'],['install']
Deployability," href=""https://redirect.github.com/Textualize/rich/issues/3057"">Textualize/rich#3057</a></li>; </ul>; <h2>[13.5.0] - 2023-07-29</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs not expanding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting <code>__eq__</code> <a href=""https://redirect.github.com/Textualize/rich/issues/2875"">Textualize/rich#2875</a></li>; <li>Fix rich.pretty.install breakage in iPython <a href=""https://redirect.github.com/Textualize/rich/issues/3013"">Textualize/rich#3013</a></li>; </ul>; <h3>Added</h3>; <ul>; <li>Added Text.extend_style method.</li>; <li>Added Span.extend method.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Text.tab_size now defaults to <code>None</code> to indicate that Console.tab_size should be used.</li>; </ul>; <h2>[13.4.2] - 2023-06-12</h2>; <h3>Changed</h3>; <ul>; <li>Relaxed markdown-it-py dependency</li>; </ul>; <h2>[13.4.1] - 2023-05-31</h2>; <h3>Fixed</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/Textualize/rich/commit/ec91917deb47b43188312e0e3f03bbab7e4e2e7e""><code>ec91917</code></a> changelog</li>; <li><a href=""https://github.com/Textualize/rich/commit/5360fe6fe4f582e5a5bec591cf7433ed85e6863d""><code>5360fe6</code></a> version bump</li>; <li><a href=""https://github.com/Textualize/rich/commit/e0d3aee1eccd424c98d05b94910f9d5ddb821",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13651:4422,install,install,4422,https://hail.is,https://github.com/hail-is/hail/pull/13651,2,['install'],['install']
Deployability," href=""https://redirect.github.com/Textualize/rich/issues/3057"">Textualize/rich#3057</a></li>; </ul>; <h2>[13.5.0] - 2023-07-29</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs not expanding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting <code>__eq__</code> <a href=""https://redirect.github.com/Textualize/rich/issues/2875"">Textualize/rich#2875</a></li>; <li>Fix rich.pretty.install breakage in iPython <a href=""https://redirect.github.com/Textualize/rich/issues/3013"">Textualize/rich#3013</a></li>; </ul>; <h3>Added</h3>; <ul>; <li>Added Text.extend_style method.</li>; <li>Added Span.extend method.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Text.tab_size now defaults to <code>None</code> to indicate that Console.tab_size should be used.</li>; </ul>; <h2>[13.4.2] - 2023-06-12</h2>; <h3>Changed</h3>; <ul>; <li>Relaxed markdown-it-py dependency</li>; </ul>; <h2>[13.4.1] - 2023-05-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed typing extensions import in markdown <a href=""https://redirect.github.com/Textualize/rich/issues/2979"">Textualize/rich#2979</a></li>; </ul>; <h2>[13.4.0] - 2023-05-31</h2>; <h3>Added</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/Textualize/rich/commit/720800e6930d85ad027b1e9bd0cbb96b5e994ce3""><code>720800e</code></a> fix tab size issue</li>; <li><a href=""https:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13380:3826,install,install,3826,https://hail.is,https://github.com/hail-is/hail/pull/13380,2,['install'],['install']
Deployability," identify invalid rules rather than hang parsing them,; and to correctly parse <code>/</code> within converter arguments. :pr:<code>2489</code></li>; <li>Update subpackage imports in :mod:<code>werkzeug.routing</code> to use the; <code>import as</code> syntax for explicitly re-exporting public attributes.; :pr:<code>2493</code></li>; <li>Parsing of some invalid header characters is more robust. :pr:<code>2494</code></li>; <li>When starting the development server, a warning not to use it in a; production deployment is always shown. :issue:<code>2480</code></li>; <li><code>LocalProxy.__wrapped__</code> is always set to the wrapped object when; the proxy is unbound, fixing an issue in doctest that would cause it; to fail. :issue:<code>2485</code></li>; <li>Address one <code>ResourceWarning</code> related to the socket used by; <code>run_simple</code>. :issue:<code>2421</code></li>; </ul>; <h2>Version 2.2.1</h2>; <p>Released 2022-07-27</p>; <ul>; <li>Fix router so that <code>/path/</code> will match a rule <code>/path</code> if strict; slashes mode is disabled for the rule. :issue:<code>2467</code></li>; <li>Fix router so that partial part matches are not allowed; i.e. <code>/2df</code> does not match <code>/&lt;int&gt;</code>. :pr:<code>2470</code></li>; <li>Fix router static part weighting, so that simpler routes are matched; before more complex ones. :issue:<code>2471</code></li>; <li>Restore <code>ValidationError</code> to be importable from; <code>werkzeug.routing</code>. :issue:<code>2465</code></li>; </ul>; <h2>Version 2.2.0</h2>; <p>Released 2022-07-23</p>; <ul>; <li>Deprecated <code>get_script_name</code>, <code>get_query_string</code>,; <code>peek_path_info</code>, <code>pop_path_info</code>, and; <code>extract_path_info</code>. :pr:<code>2461</code></li>; <li>Remove previously deprecated code. :pr:<code>2461</code></li>; <li>Add MarkupSafe as a dependency and use it to escape values when; rendering HTML. :issue:<code>2419</code></li>; </ul>; <!-- raw HTML omit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12119:3338,Release,Released,3338,https://hail.is,https://github.com/hail-is/hail/pull/12119,1,['Release'],['Released']
Deployability," image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Created 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Created container; Normal Started 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq ; NAME READY STATUS RESTARTS AGE; notebook-worker-9szt8 0/1 Running 0 49s. Started container; Warning Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension install --user --py jupyter_spark && \; jupyter contrib nbextension install --user && \; jupyter nbextension enable --user --py jupyter_spark && \; jupyter nbextension enable --user --py widgetsnbextension && \; jupyter nbextension enable --user collapsible_headings/main && \; jupyter nbextension enable --user move_selected_cells/main. COPY ./resources/ /home/jovyan; ```. And the actual worker creation in notebook.py. ```py; def start_pod(jupyter_token, image, labels={}):; print(""IMAGE IN START IS"", image); pod_id = uuid.uuid4().hex; service_spec = kube.client.V1ServiceSpec(; selector={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id},; ports=[kube.client.V1ServicePort(port=80, target_port=8888)]",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:1706,update,update,1706,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218,2,"['install', 'update']","['install', 'update']"
Deployability," in LSP <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15262"">#15262</a> (<a href=""https://github.com/trungleduc""><code>@​trungleduc</code></a>)</li>; <li>Fix outputarea package from not detecting updates <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15642"">#15642</a> (<a href=""https://github.com/MFA-X-AI""><code>@​MFA-X-AI</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Backport PR <a href=""https://redirect.github.com/jupyterlab/jupyterlab/issues/15524"">#15524</a>: Fix visual tests <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15578"">#15578</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Documentation improvements</h3>; <ul>; <li>Remove Python 3.0, Notebook 5 mentions from contributor docs <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15710"">#15710</a> (<a href=""https://github.com/JasonWeill""><code>@​JasonWeill</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab/graphs/contributors?from=2024-01-19&amp;to=2024-01-30&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3AFoSuCloud+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​FoSuCloud</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Agithub-actions+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aj264415+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​j264415</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3AJasonWeill+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​JasonWeill</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajupyterlab-bot+updated%",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14218:2424,release,release,2424,https://hail.is,https://github.com/hail-is/hail/pull/14218,2,['release'],['release']
Deployability," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:13089,configurat,configurations,13089,https://hail.is,https://github.com/hail-is/hail/pull/8561,7,"['Deploy', 'configurat', 'deploy']","['Deploy', 'configurations', 'deploy']"
Deployability," in addition to disallowing spaces. Regardless of any validation done by Jinja, user input should never be used as keys to this filter, or must be separately validated first. GHSA-h75v-3vvj-5mfj</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/blob/main/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 3.1.4</h2>; <p>Released 2024-05-05</p>; <ul>; <li>The <code>xmlattr</code> filter does not allow keys with <code>/</code> solidus, <code>&gt;</code>; greater-than sign, or <code>=</code> equals sign, in addition to disallowing spaces.; Regardless of any validation done by Jinja, user input should never be used; as keys to this filter, or must be separately validated first.; :ghsa:<code>h75v-3vvj-5mfj</code></li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/jinja/commit/dd4a8b5466d8790540c181590b14db4d4d889d57""><code>dd4a8b5</code></a> release version 3.1.4</li>; <li><a href=""https://github.com/pallets/jinja/commit/0668239dc6b44ef38e7a6c9f91f312fd4ca581cb""><code>0668239</code></a> Merge pull request from GHSA-h75v-3vvj-5mfj</li>; <li><a href=""https://github.com/pallets/jinja/commit/d655030770081e2dfe46f90e27620472a502289d""><code>d655030</code></a> disallow invalid characters in keys to xmlattr filter</li>; <li><a href=""https://github.com/pallets/jinja/commit/a7863ba9d3521f1450f821119c50d19d7ecea329""><code>a7863ba</code></a> add ghsa links</li>; <li><a href=""https://github.com/pallets/jinja/commit/b5c98e78c2ee7d2bf0aa06d29ed9bf7082de9cf4""><code>b5c98e7</code></a> start version 3.1.4</li>; <li><a href=""https://github.com/pallets/jinja/commit/da3a9f0b804199845fcb76f2e08748bdaeba93ee""><code>da3a9f0</code></a> update project files (<a href=""https://redirect.github.com/pallets/jinja/issues/1968"">#1968</a>)</li>; <li><a href=""https://github.com/pallets/jinja/commit/0ee5eb41d1a2d7d9a05a02",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14526:1877,release,release,1877,https://hail.is,https://github.com/hail-is/hail/pull/14526,3,['release'],['release']
Deployability," in create-release.yml</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/21fbee5e83a3e9d34e589d06c66d928f3a67923c""><code>21fbee5</code></a> Fix OIDC token payload</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/1a403e4f9ad4d8d7a3e4c7c34f55d9a45ed600bd""><code>1a403e4</code></a> Add informational log messaging</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/258b0ea9fcdd05221ebead5e5c162a7ad37c4412""><code>258b0ea</code></a> Revert &quot;Switch to using <code>github.request</code>&quot;</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/f9c89e5d8431155caa7be57d923f96004a2dd4bd""><code>f9c89e5</code></a> Switch to using <code>github.request</code></li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/52c7f66ce172f723e8227896fe02165d288cb28f""><code>52c7f66</code></a> Use the correct token minting URL for TestPyPI</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/6079f28faa2a60d00f62b02786f23cd489019cdb""><code>6079f28</code></a> Install twine in PyPI publish workflow</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/3d43b9efdb49706cad6947f0c4d877d603781fe6""><code>3d43b9e</code></a> Fix github-script syntax in create-release.yml</li>; <li>Additional commits viewable in <a href=""https://github.com/sphinx-doc/sphinx/compare/v6.2.1...v7.1.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinx&package-manager=pip&previous-version=6.2.1&new-version=7.1.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and opt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13295:6372,Install,Install,6372,https://hail.is,https://github.com/hail-is/hail/pull/13295,1,['Install'],['Install']
Deployability," in create-release.yml</li>; <li>Additional commits viewable in <a href=""https://github.com/sphinx-doc/sphinx/compare/v6.2.1...v7.1.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinx&package-manager=pip&previous-version=6.2.1&new-version=7.1.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13295:8184,upgrade,upgrade,8184,https://hail.is,https://github.com/hail-is/hail/pull/13295,3,['upgrade'],['upgrade']
Deployability," in for loops that reference <code>self</code> if the binary operation that; started the for loop uses a <code>self</code> that is encapsulated in tuples or lists.</p>; <p>Ref <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1360"">PyCQA/astroid#1360</a>; Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/4826"">#4826</a></p>; </li>; <li>; <p>Output better error message if unsupported file formats are used with <code>pyreverse</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5950"">#5950</a></p>; </li>; <li>; <p>Fix pyreverse diagrams type hinting for classmethods and staticmethods.</p>; </li>; <li>; <p>Fix pyreverse diagrams type hinting for methods returning None.</p>; </li>; <li>; <p>Fix matching <code>--notes</code> options that end in a non-word character.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5840"">#5840</a></p>; </li>; <li>; <p>Updated the position of messages for class and function defintions to no longer cover; the complete definition. Only the <code>def</code> or <code>class</code> + the name of the class/function; are covered.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5466"">#5466</a></p>; </li>; <li>; <p><code>using-f-string-in-unsupported-version</code> and <code>using-final-decorator-in-unsupported-version</code> msgids; were renamed from <code>W1601</code> and <code>W1602</code> to <code>W2601</code> and <code>W2602</code>. Disabling using these msgids will break.; This is done in order to restore consistency with the already existing msgids for <code>apply-builtin</code> and; <code>basestring-builtin</code> from the now deleted python 3K+ checker. There is now a check that we're not using; existing msgids or symbols from deleted checkers.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5729"">#5729</a></p>; </li>; <li>; <p>The line numbering for messag",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11674:1449,Update,Updated,1449,https://hail.is,https://github.com/hail-is/hail/pull/11674,1,['Update'],['Updated']
Deployability," in the terraform does not exist in hail-vdc. If you approve of the approach I can add it in manually. . ## Terraform changes; This adds a new CI terraform module that adds a CI bucket, sets some permissions for the CI service account and adds some K8s secrets like github tokens and the zulip config. This allows the terraform deployment to optionally include resources needed for CI. This was the best way I could think to introduce this infra with the least changes, but it's not what I want in the long term. Right now we have one monolithic root module that includes all the resources necessary to run batch, with the option for tacking on CI. I would rather extract most of our root module into a `batch` module (and while we're break down the innards into modules like vdc, db, etc.) and have the root module be something that can be easily pieced together from the library of modules. This would be a decently big refactor and more importantly would require existing deployments to manually overhaul their terraform state, so it's something I want to do carefully but also sooner is better than later. Given how terraform state is indexed, I believe more modularity will be easier to manage in the long term. ## CI changes; This adds the following features to CI; - Watched branches can be marked as `mergeable`. `mergeable=true` should be the default behavior and `false` prevents CI from merging a PR on GitHub. This allows multiple CI's to run tests in different environments without stepping on each others' toes. This *does not*, however, consider statuses from multiple CIs when making the decision to merge a PR. That is currently based on the build status, and later should be changed to consider the collection of statuses on GitHub.; - Custom Deploy Steps: This is a collection of build.yaml steps that CI should run on `deploy`. This allows a deployment to only deploy a subset of the infrastructure to default, though it will run the entire test suite. Based on the semantics, `req",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11053:1186,deploy,deployments,1186,https://hail.is,https://github.com/hail-is/hail/pull/11053,1,['deploy'],['deployments']
Deployability," instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.10.0 - 2024-03-27</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14427:1390,Update,Update,1390,https://hail.is,https://github.com/hail-is/hail/pull/14427,1,['Update'],['Update']
Deployability," is special here vs the rest of the standard library? What assumption >are you working under?. Yes. The C++11 final standard introduced some new constraints on the definition of std::string; and std::list (essentially outlawing a copy-on-write implementation of std::string, and requiring; that std::list::size() must be O(1)). Consequently libstdc++ had to be redesigned with incompatible implementations of ; std::string and std::list to become C++11-compliant. Systems using the earlier libstdc++; (which also typically have g++-4.8.x/4.9x) have the old-ABI not-fully-compliant std::string; (and std::list, but I've gone 20 years without ever using that). Later versions of libstdc++ have *both* flavors of std::string, but use namespaces to allow them; to coexist (but not to be interchangeable, so interfaces between old-ABI and new-ABI are; problematic). https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html. ""In the GCC 5.1 release libstdc++ introduced a new library ABI that includes new implementations of std::string and std::list. These changes were necessary to conform to the 2011 C++ standard which forbids Copy-On-Write strings and requires lists to keep track of their size. In order to maintain backwards compatibility for existing code linked to libstdc++ the library's soname has not changed and the old implementations are still supported in parallel with the new ones. This is achieved by defining the new implementations in an inline namespace so they have different names for linkage purposes, e.g. the new version of std::list<int> is actually defined as std::__cxx11::list<int>. Because the symbols for the new implementations have different names the definitions for both versions can be present in the same library."". OSX doesn't have this ABI-compatibility issue because for several years it has been using; libc++ as the default library, and libc++ is a post-C++11 rewrite-from-scratch implementation; of the required standard-library functionality. My und",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424797612:978,release,release,978,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424797612,1,['release'],['release']
Deployability," is the culmination of <code>6</code> months of hard work. It contains; many new features, numerous bug-fixes, improved test coverage and better; documentation. There have been a number of deprecations and API changes; in this release, which are documented below. All users are encouraged to; upgrade to this release, as there are a large number of bug-fixes and; optimizations. Before upgrading, we recommend that users check that; their own code does not use deprecated SciPy functionality (to do so,; run your code with <code>python -Wd</code> and check for <code>DeprecationWarning</code> s).; Our development attention will now shift to bug-fix releases on the; 1.8.x branch, and on adding new features on the master branch.</p>; <p>This release requires Python <code>3.8+</code> and <code>NumPy 1.17.3</code> or greater.</p>; <p>For running on PyPy, PyPy3 <code>6.0+</code> is required.</p>; <h1>Highlights of this release</h1>; <ul>; <li>A sparse array API has been added for early testing and feedback; this; work is ongoing, and users should expect minor API refinements over; the next few releases.</li>; <li>The sparse SVD library PROPACK is now vendored with SciPy, and an interface; is exposed via <code>scipy.sparse.svds</code> with <code>solver='PROPACK'</code>. It is currently; default-off due to potential issues on Windows that we aim to; resolve in the next release, but can be optionally enabled at runtime for; friendly testing with an environment variable setting of <code>USE_PROPACK=1</code>.</li>; <li>A new <code>scipy.stats.sampling</code> submodule that leverages the <code>UNU.RAN</code> C; library to sample from arbitrary univariate non-uniform continuous and; discrete distributions</li>; <li>All namespaces that were private but happened to miss underscores in; their names have been deprecated.</li>; </ul>; <h1>New features</h1>; <h1><code>scipy.fft</code> improvements</h1>; <p>Added an <code>orthogonalize=None</code> parameter to the real transforms in <code>sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11538:1244,release,release,1244,https://hail.is,https://github.com/hail-is/hail/pull/11538,2,['release'],"['release', 'releases']"
Deployability," is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: Java heap space; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:10834,install,install,10834,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,3,['install'],['install']
Deployability," metric attributions in AWS Personalize</li>; <li>api-change:<code>polly</code>: [<code>botocore</code>] Add two new neural voices - Ola (pl-PL) and Hala (ar-AE).</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/cc2984fc4fe2a399404a81711eb9ece3fb8d6eb7""><code>cc2984f</code></a> Merge branch 'release-1.26.15'</li>; <li><a href=""https://github.com/boto/boto3/commit/0cb8b0e0b79f9b73494172bafcf25dba43205a59""><code>0cb8b0e</code></a> Bumping version to 1.26.15</li>; <li><a href=""https://github.com/boto/boto3/commit/b786787f24c9b3f3276c4770038d811064f600ac""><code>b786787</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/96da06d3dc41d4d04503929a7b5894d82c71c08f""><code>96da06d</code></a> Merge branch 'release-1.26.14'</li>; <li><a href=""https://github.com/boto/boto3/commit/61de529b5f9a7bdcc8c76debb472a7f934d048e6""><code>61de529</code></a> Merge branch 'release-1.26.14' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/c92111ed1a7346642060fd7f6dfedcb3770a9650""><code>c92111e</code></a> Bumping version to 1.26.14</li>; <li><a href=""https://github.com/boto/boto3/commit/53c38e2c6849af7c0c47a4d21ce41bfbad7d80cb""><code>53c38e2</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/1de404aff4ecb1c5560b4e023f0614d8149622ed""><code>1de404a</code></a> fix typo: 'are specified the' should be 'are specified in the' (<a href=""https://github-redirect.dependabot.com/boto/boto3/issues/3499"">#3499</a>)</li>; <li><a href=""https://github.com/boto/boto3/commit/47f20744b3c57223da5e14e185120586f8212af8""><code>47f2074</code></a> Merge branch 'release-1.26.13'</li>; <li><a href=""https://github.com/boto/boto3/commit/3d4081ccb7c350538ba3d6a5073c59575a812eb0""><code>3d4081c</code></a> Merge branch 'release-1.26.13' into develop</li>; <li>Additional com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12498:8239,release,release-,8239,https://hail.is,https://github.com/hail-is/hail/pull/12498,1,['release'],['release-']
Deployability," more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJiN2QwMTZlZS0zODA0LTQwMjItOWE0Yi01MzExNjZhNjBjMWQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImI3ZDAxNmVlLTM4MDQtNDAyMi05YTRiLTUzMTE2NmE2MGMxZCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""b7d016ee-3804-4022-9a4b-531166a60c1d"",""prPublicId"":""b7d016ee-3804-4022-9a4b-531166a60c1d"",""dependencies"":[{""name"":""cryptography"",""from"":""41.0.3"",""to"":""41.0.4""}],""packageManager"":""pip"",""projectPublicId"":""701495b8-b53d-48af-82fe-1a6c57aa56cb"",""projectUrl"":""https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CRYPTOGRAPHY-5914629""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[611],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13698:2362,upgrade,upgrade,2362,https://hail.is,https://github.com/hail-is/hail/pull/13698,6,"['patch', 'update', 'upgrade']","['patch', 'patches-to-fix-vulnerabilities', 'updated-fix-title', 'upgrade']"
Deployability," moving them out of the SciPy repository, hosting them externally and</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/scipy/scipy/commit/dde50595862a4f9cede24b5d1c86935c30f1f88a""><code>dde5059</code></a> REL: 1.10.0 final [wheel build]</li>; <li><a href=""https://github.com/scipy/scipy/commit/7856f281b016c585b82d03723c4494bcdbdcd4a5""><code>7856f28</code></a> Merge pull request <a href=""https://redirect.github.com/scipy/scipy/issues/17696"">#17696</a> from tylerjereddy/treddy_110_final_prep</li>; <li><a href=""https://github.com/scipy/scipy/commit/205b6243c6d075d05695e7ac6d007e0f03bfbf42""><code>205b624</code></a> DOC: add missing author</li>; <li><a href=""https://github.com/scipy/scipy/commit/1ab9f1b10145f0a974d5531700e72d1fb4229b76""><code>1ab9f1b</code></a> DOC: update 1.10.0 relnotes</li>; <li><a href=""https://github.com/scipy/scipy/commit/ac2f45fbe1e39a8f52c1ea2e68764009f02973c0""><code>ac2f45f</code></a> MAINT: integrate._qmc_quad: mark as private with preceding underscore</li>; <li><a href=""https://github.com/scipy/scipy/commit/3e0ae1a21f51ebee3a77733c42700d87a0c35d7d""><code>3e0ae1a</code></a> REV: integrate.qmc_quad: delay release to SciPy 1.11.0</li>; <li><a href=""https://github.com/scipy/scipy/commit/34cdf05c86548de1c4ca1b2798cdc23885af807b""><code>34cdf05</code></a> MAINT: FFT pybind11 fixups</li>; <li><a href=""https://github.com/scipy/scipy/commit/843500aabde17aaf1eec65c589d50bd12ee35039""><code>843500a</code></a> Merge pull request <a href=""https://redirect.github.com/scipy/scipy/issues/17689"">#17689</a> from mdhaber/gh17686</li>; <li><a href=""https://github.com/scipy/scipy/commit/089924b61012a106ffa4f58939b0180124051a0b""><code>089924b</code></a> REL: integrate.qmc_quad: remove from release notes</li>; <li><a href=""https://github.com/scipy/scipy/commit/3e47110f10e3267d228e9da84174f3cee325e7c3""><code>3e47110</code></a> REL: 1.10.0rc3 unreleased</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13227:3793,integrat,integrate,3793,https://hail.is,https://github.com/hail-is/hail/pull/13227,1,['integrat'],['integrate']
Deployability," mysql tables in use 27, locked 27; LOCK WAIT 47 lock struct(s), heap size 8312, 215 row lock(s), undo log entries 211; MySQL thread id 682, OS thread handle 140330866251520, query id 4746389 10.32.3.39 dgoldste-batch-user executing; INSERT INTO aggregated_job_group_resources_v3 (batch_id, job_group_id, resource_id, token, `usage`); SELECT attempt_resources.batch_id,; job_group_self_and_ancestors.ancestor_id,; attempt_resources.deduped_resource_id,; NAME_CONST('rand_token',189),; NAME_CONST('msec_diff_rollup',1671) * quantity; FROM attempt_resources; LEFT JOIN jobs ON attempt_resources.batch_id = jobs.batch_id AND attempt_resources.job_id = jobs.job_id; LEFT JOIN job_group_self_and_ancestors ON jobs.batch_id = job_group_self_and_ancestors.batch_id AND jobs.job_group_id = job_group_self_and_ancestors.job_group_id; WHERE attempt_resources.batch_id = NEW.batch_id AND attempt_resources.job_id = NEW.job_id AND attempt_resources.attempt_id = NEW.attempt_id; FOR UPDATE; ON DUPLICATE KEY UPDATE `usage` = aggregated_job_group_resources_v3.`usage` + NAME_CONST('msec_diff_rollup',1671) * quanti. *** (2) HOLDS THE LOCK(S):; RECORD LOCKS space id 376 page no 8 n bits 408 index PRIMARY of table `dgoldste-batch`.`aggregated_billing_project_user_resources_v3` trx id 2486477 lock_mode X locks rec but not gap; Record lock, heap no 228 PHYSICAL RECORD: n_fields 7; compact format; info bits 0; 0: len 4; hex 74657374; asc test;;; 1: len 8; hex 64676f6c64737465; asc dgoldste;;; 2: len 4; hex 80000009; asc ;;; 3: len 4; hex 80000034; asc 4;;; 4: len 6; hex 00000025f0cd; asc % ;;; 5: len 7; hex 810000021b01cd; asc ;;; 6: len 8; hex 80000000001b09e0; asc ;;. Record lock, heap no 249 PHYSICAL RECORD: n_fields 7; compact format; info bits 0; 0: len 4; hex 74657374; asc test;;; 1: len 8; hex 64676f6c64737465; asc dgoldste;;; 2: len 4; hex 80000009; asc ;;; 3: len 4; hex 800000ad; asc ;;; 4: len 6; hex 00000025f0cd; asc % ;;; 5: len 7; hex 010000008c050d; asc ;;; 6: len 8; hex 8000000000471350;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14380:3753,UPDATE,UPDATE,3753,https://hail.is,https://github.com/hail-is/hail/issues/14380,2,['UPDATE'],['UPDATE']
Deployability," namespace's database-server-config; The current approach in main does a little trick. Since the current `createDatabase` step uses the `database-server-config` from default to generate admin/user sql configs, the CI pipeline creates a dummy database `test-database-instance` to create a `sql-test-instance-admin-config` that inherits the credentials from the production `database-server-config`, and then copies that within the test namespace to `database-server-config`. In this change, since we are creating the server ourselves, we can just replace these with a step that creates a `database-server-config` from scratch, and then uses that for the DB pod. Overall making these changes really gave me the heebie jeebies that the test and dev namespaces have all these credentials to the CloudSQL server. I'm glad this gets rid of that. ### Accessing the database server; We use the DB pod's service DNS name as the `host` so inside Kubernetes this Just Works. The one caveat is the CI pipeline in which we run migrations in batch jobs. Those jobs need a way to reach the DB pod. I achieve this with a NodePort and then use the job's K8s credentials to resolve the node and port that the DB is on. The code I've added to do this resolution feels a bit janky, wouldn't mind some feedback on that. In terms of security, if a user job was able to somehow resolve the address of a test db, they would still not have the credentials to access it, and this is currently also the case with the production database. Nevertheless, this does raise an action item that we should only allow traffic to the k8s and DB subnets for `network=private` jobs, but I think we should make that a separate PR. ### Database creation; In order to test this properly in a dev deploy, I needed to make some changes to `create_database.py`. In main, dev deploys can't create databases. I think they should be able to, and those operations should just be idempotent. When allowing dev deploys to create databases, I hit the `A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13030:1155,pipeline,pipeline,1155,https://hail.is,https://github.com/hail-is/hail/pull/13030,1,['pipeline'],['pipeline']
Deployability," not require calling &quot;exec&quot;).; Added a way to mimic functools.wraps-generated decorators.; Ported the Continuous Integration from Travis to GitHub.</p>; <h2>4.4.2 (2020-02-29)</h2>; <p>Sylvan Mosberger (<a href=""https://github.com/Infinisil"">https://github.com/Infinisil</a>) contributed a patch to; some doctests that were breaking on NixOS.; John Vandenberg (<a href=""https://github.com/jayvdb"">https://github.com/jayvdb</a>) made a case for removing the usage</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/micheles/decorator/commits/5.1.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=decorator&package-manager=pip&previous-version=4.4.0&new-version=5.1.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11799:3025,update,updates,3025,https://hail.is,https://github.com/hail-is/hail/pull/11799,1,['update'],['updates']
Deployability," offset if not at char boundary</li>; <li>Additional commits viewable in <a href=""https://github.com/ijl/orjson/compare/3.6.4...3.6.7"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=orjson&package-manager=pip&previous-version=3.6.4&new-version=3.6.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11572:5755,upgrade,upgrade,5755,https://hail.is,https://github.com/hail-is/hail/pull/11572,3,['upgrade'],['upgrade']
Deployability," on light</li>; <li><a href=""https://github.com/Textualize/rich/commit/a972ca05522577de2f98eb7c957deead9c87b38f""><code>a972ca0</code></a> changelog</li>; <li><a href=""https://github.com/Textualize/rich/commit/bef0e50b63cf7294ae6c27bf8a79cbe3592599a0""><code>bef0e50</code></a> Merge pull request <a href=""https://redirect.github.com/Textualize/rich/issues/3130"">#3130</a> from Textualize/fix-table-inline-styles</li>; <li><a href=""https://github.com/Textualize/rich/commit/e30b822ecc264c5c4f984a023124d31d8052de49""><code>e30b822</code></a> Fix markdown table rendering issue.</li>; <li>Additional commits viewable in <a href=""https://github.com/Textualize/rich/compare/v12.6.0...v13.5.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=rich&package-manager=pip&previous-version=12.6.0&new-version=13.5.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13651:7195,update,updates,7195,https://hail.is,https://github.com/hail-is/hail/pull/13651,2,['update'],['updates']
Deployability," only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ijl/orjson/commit/11c7de8e5846fa65449aa1f6ffc05c5a1090df03""><code>11c7de8</code></a> 3.10.0</li>; <li><a href=""https://github.com/ijl/orjson/commit/1fc3ed80c24864607be709d29e0d5f47fc507626""><code>1fc3ed8</code></a> Support numpy.float16</li>; <li><a href=""https://github.com/ijl/orjson/commit/56c1a03216426c54dfbe9a4b6c3f70013c65a1f8""><code>56c1a03</code></a> cargo update, build misc</li>; <li><a href=""https://github.com/ijl/orjson/commit/a348f59f0b55d92a1364523560f52f5b3cf9c12a""><code>a348f59</code></a> 3.9.15</li>; <li><a href=""https://github.com/ijl/orjson/commit/b0e4d2c06ce06c6e63981bf0276e4b7c74e5845e""><code>b0e4d2c</code></a> yyjson 0eca326, recursion limit</li>; <li><a href=""https://github.com/ijl/orjson/commit/5067eadc84cf516e4eb33bcb09ad756bb59dc42e""><code>5067ead</code></a> impl_escape_unchecked() byte exact read</li>; <li><a href=""https://github.com/ijl/orjson/commit/e04ea735b087742b6cee738aa295d8b835c3a195""><code>e04ea73</code></a> cargo update, build misc</li>; <li><a href=""https://github.com/ijl/orjson/commit/ba8c701292e4720b4e10210b266be5666d098fb6""><code>ba8c701</code></a> 3.9.14</li>; <li><a href=""https://github.com/ijl/orjson/commit/a2f7b7bfa4987c102892793ab7c7483fcb8050a0""><code>a2f7b7b</code></a> impl_format_simd!() lift create from loop, rotate left</li>; <li><a href=""https://github.com/ijl/orjson/commit/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14427:3655,update,update,3655,https://hail.is,https://github.com/hail-is/hail/pull/14427,1,['update'],['update']
Deployability," or: setup.py --help [cmd1 cmd2 ...]; or: setup.py --help-commands; or: setup.py cmd --help; ; error: invalid command 'bdist_wheel'; ----------------------------------------; ERROR: Failed building wheel for pypandoc; ERROR: Failed to build one or more wheels; Traceback (most recent call last):; File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/site-packages/setuptools/installer.py"", line 128, in fetch_build_egg; subprocess.check_call(cmd); File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/subprocess.py"", line 363, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/Users/spascal/.pyenv/versions/3.7.9/bin/python3.7', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/tmpspi2awj3', '--quiet', 'pypandoc']' returned non-zero exit status 1.; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/setup.py"", line 224, in <module>; 'Programming Language :: Python :: Implementation :: PyPy']; File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/site-packages/setuptools/__init__.py"", line 143, in setup; _install_setup_requires(attrs); File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/site-packages/setuptools/__init__.py"", line 138, in _install_setup_requires; dist.fetch_build_eggs(dist.setup_requires); File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/site-packages/setuptools/dist.py"", line 698, in fetch_build_eggs; replace_conflicting=True,; File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 783, in resolve; replace_conflicting=replace_conflicting; File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 1066, in best_match; return self.obtain(req, i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9742:3227,install,install-,3227,https://hail.is,https://github.com/hail-is/hail/issues/9742,1,['install'],['install-']
Deployability," org.graalvm.buildtools:native-maven-plugin to v0.9.17 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1759"">#1759</a>) (<a href=""https://github.com/googleapis/java-storage/commit/7e3175a56a06dac0aa0841f221a486bb69b5c9bf"">7e3175a</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.13.1...v2.14.0"">2.14.0</a> (2022-10-26)</h2>; <h3>Google Cloud Storage gRPC API Preview</h3>; <p>The first release of <code>google-cloud-storage</code> with support for a subset of the Google Cloud Storage gRPC API which is in private preview. The most common operations have all been implemented and are available for experimentation.</p>; <p>Given not all public api surface of <code>google-cloud-storage</code> classes are supported for gRPC a new annotation <code>@TransportCompatibility</code> has been added to various classes, methods and fields/enum values to signal where that thing can be expected to work. As we implement more of the operations these annotations will be updated.</p>; <p>All new gRPC related APIs are annotated with <code>@BetaApi</code> to denote they are in preview and the possibility of breaking change is present. At this time, opting to use any of the gRPC transport mode means you are okay with the possibility of a breaking change happening. When the APIs are out of preview, we will remove the <code>@BetaApi</code> annotation to signal they are now considered stable and will not break outside a major version.</p>; <p><strong><em>NOTICE</em></strong>: Using the gRPC transport is exclusive. Any operations which have not yet been implemented for gRPC will result in a runtime error. For those operations which are not yet implemented, please continue to use the existing HTTP transport.</p>; <p>Special thanks (in alphabetical order) to <a href=""https://github.com/BenWhitehead""><code>@​BenWhitehead</code></a>, <a href=""https://github.com/frankyn""><code>@​frankyn</code></a>, <a href=""https://github.com/jesselove",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12456:8116,update,updated,8116,https://hail.is,https://github.com/hail-is/hail/pull/12456,2,['update'],['updated']
Deployability," overhaul of our Kubernetes load balancers / service discovery. This involves moving off of NGINX onto Envoy, but more importantly involves better control of what namespaces and services are active in our cluster at a given point in time. TL;DR Switching from NGINX to Envoy with CI acting as the ""control plane"" for our internal networking allows us to more easily dynamically configure our Kubernetes networking and achieve proper connection pooling/load-balancing over TLS, which translates to less resource consumption and lower request latencies. ## Motivation; This is primarily a performance-motivated change, and one largely based on our (ab)use of NGINX in order to work with our dynamically-generated Kubernetes test namespaces. Currently, we configure NGINX by creating server blocks that dynamically resolve and dispatch requests based on matching regular expressions on the host and path headers. This is in large part due that at gateway deploy time we do not statically know all of the namespaces and namespace-service combinations that will exist in the cluster in the future. This is true for `default`, but not test namespaces, and NGINX will refuse to start with statically-configured clusters that it cannot reach. Making the server blocks make the routing decisions dynamically circumvents this limitation. However, this prevents usage of NGINX [upstream](http://nginx.org/en/docs/http/ngx_http_upstream_module.html) blocks that provide connection pooling, at least in the community edition, and as a result the gateways will create and terminate a TCP connection per http request. This likely causes minor delays on the front-end through gateway, but this hampers performance greatly in job scheduling. The batch driver is forced to establish a new TCP connection and do an SSL handshake with the internal-gateway multiple times per job, which is expensive and slow. We currently have to dedicate a 2-core NGINX sidecar for the batch-driver just to terminate TLS with internal-ga",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:973,deploy,deploy,973,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['deploy'],['deploy']
Deployability," own `config` or `url`. `Dataset.from_name_and_json()` now calls `DatasetVersion.get_region()` method to retrieve the dataset from the bucket in the selected region if `custom_config` is `False`. ; - The `DatasetVersion.get_region()` method takes the dataset `name`, a list of `DatasetVersion` objects, and a `region`, and returns a list of the versions that are available for that region. This method calls the instance method `in_region()` to check if the dataset is available in the requested region.; - If `in_region()` determines the desired region is not available for some dataset that otherwise is available in another region, it will raise a warning. If user still tries to call `db.annotate_rows_db()` using a dataset unavailable in their region, then it will get caught by the `_check_availability` instance method in the `DB` class and raise a `ValueError`.; - Started to add documentation to the classes and methods, still a work in progress. Changes to datasets and datasets API site:; - Added the `ldsc_baselineLD_annotations`, `ldsc_baselineLD_ldscores`, and `ldsc_baseline_ldscores` datasets to the `annotation_db.json` configuration file. Now accessible via `load_dataset()` and `db.annotate_rows_db()` (for the annotations at least).; - New `.rst` files in `hail/python/hail/docs/datasets` have been generated to reflect the available datasets in the config file, and `hail/python/hail/docs/datasets.rst` has been updated with the new files as well. Future updates:; - In next PR can add the functionality to automatically determine the users region.; - Also considering modifying the `load_datasets()` function a bit to only require one `version` parameter, to be consistent with the way the version strings are formatted in `annotation_db.json`, and to avoid having to check if the version and reference genome are available separately. Something like this:. ```; mt_1kg = hl.experimental.load_dataset(name='1000_Genomes_autosomes',; version='phase_3-GRCh37', ; region='us'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9496:2287,configurat,configuration,2287,https://hail.is,https://github.com/hail-is/hail/pull/9496,3,"['configurat', 'update']","['configuration', 'updated', 'updates']"
Deployability," permissions for the CI service account and adds some K8s secrets like github tokens and the zulip config. This allows the terraform deployment to optionally include resources needed for CI. This was the best way I could think to introduce this infra with the least changes, but it's not what I want in the long term. Right now we have one monolithic root module that includes all the resources necessary to run batch, with the option for tacking on CI. I would rather extract most of our root module into a `batch` module (and while we're break down the innards into modules like vdc, db, etc.) and have the root module be something that can be easily pieced together from the library of modules. This would be a decently big refactor and more importantly would require existing deployments to manually overhaul their terraform state, so it's something I want to do carefully but also sooner is better than later. Given how terraform state is indexed, I believe more modularity will be easier to manage in the long term. ## CI changes; This adds the following features to CI; - Watched branches can be marked as `mergeable`. `mergeable=true` should be the default behavior and `false` prevents CI from merging a PR on GitHub. This allows multiple CI's to run tests in different environments without stepping on each others' toes. This *does not*, however, consider statuses from multiple CIs when making the decision to merge a PR. That is currently based on the build status, and later should be changed to consider the collection of statuses on GitHub.; - Custom Deploy Steps: This is a collection of build.yaml steps that CI should run on `deploy`. This allows a deployment to only deploy a subset of the infrastructure to default, though it will run the entire test suite. Based on the semantics, `requested_step_names`, the empty list will deploy everything.; - Custom github context: This is just so CIs running in different clouds/environments don't step on each other in the GitHub statuses.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11053:1972,Deploy,Deploy,1972,https://hail.is,https://github.com/hail-is/hail/pull/11053,5,"['Deploy', 'deploy']","['Deploy', 'deploy', 'deployment']"
Deployability," pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; # k get pod batch-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466:4342,pipeline,pipeline,4342,https://hail.is,https://github.com/hail-is/hail/issues/6466,2,['pipeline'],"['pipeline', 'pipeline-']"
Deployability," pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which internal namespaces (dev and PR) are currently active. This requires relatively small changes to CI to track active namespaces but overall is a pretty small change. Note that this does not introduce a dependency on CI to support production traffic, only development traffic.; 3. Deployments that run more than 1 replica (but really can be all of them) are run behind Headless Services, which expose the underlying pod IPs so Envoy can handle load-balancing instead of kube-proxy. This allows Envoy to make smart load-balancing decisions and correctly enforce rate-limiting when using connection pools. The namespace tracking in CI in Point 2 is possible before we make any changes to our networking, so that comes first in #12093. Point 3 is taken care of in #12094, and the rest of Point 2 and Poi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:4412,deploy,deploy,4412,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['deploy'],['deploy']
Deployability," ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:5424,deploy,deploy,5424,https://hail.is,https://github.com/hail-is/hail/pull/8513,3,['deploy'],"['deploy', 'deployed']"
Deployability," print in kernelspecapp <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/923"">#923</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2023-01-26&amp;to=2023-01-30&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2023-01-26..2023-01-30&amp;type=Issues""><code>@​blink1073</code></a></p>; <h2>v8.0.1</h2>; <h2>8.0.1</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v8.0.0...dc6113c360e05122430b8e130374e9f4e4b701d7"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Fix json_output in kernelspec app <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/921"">#921</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2023-01-26&amp;to=2023-01-26&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2023-01-26..2023-01-26&amp;type=Issues""><code>@​blink1073</code></a></p>; <h2>v8.0.0</h2>; <h2>8.0.0</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.3.5...760a7835d8b20a9daea3737759b1751d5e55dad8"">Full Changelog</a>)</p>; <p>This release is primarily focused on improving <code>asyncio</code> support, while aiming to have minimal API changes.</p>; <h3>Enhancements made</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_client/blob/main/CHANGELOG.md"">jupyter-client's changelog</a>.</em></p>; <blockquote>; <h2>8.0.2</h2>; <p>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12656:1981,release,release,1981,https://hail.is,https://github.com/hail-is/hail/pull/12656,1,['release'],['release']
Deployability," pull requests: <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1322"">#1322</a>, <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1323"">#1323</a>, <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1324"">#1324</a>; Closes <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1282"">#1282</a>; Ref <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1103"">#1103</a></p>; </li>; <li>; <p>Fixed crash with recursion error for inference of class attributes that referenced; the class itself.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5408"">PyCQA/pylint#5408</a></p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/PyCQA/astroid/commit/07c0f60ffc1017d0a9a2bb605a5c645781a8c088""><code>07c0f60</code></a> Bump astroid to 2.10.0, update changelog</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/e6dc5ef0f8c2d28bc9d2ffa226fbb5e4e58d88f3""><code>e6dc5ef</code></a> Fix some typoes in the Changelog</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/b6d17107f2e02df4ce5080536bb783a25273b33f""><code>b6d1710</code></a> Changed NodeNG.tolineno to use end_lineno when it is available (<a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1351"">#1351</a>)</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/0acb961d7375131c3d1e7a3580f974b6e8c5ef94""><code>0acb961</code></a> Refactor: Stop adding arbitrary attributes to module obj when building (<a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1215"">#1215</a>)</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/62aa3bb63c3ca0cda19a1bb294a6b052c2346189""><code>62aa3bb</code></a> Restore custom distutils handling for resolving paths to submodules. (<a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1386"">#1386</a>)</li>; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11463:3369,update,update,3369,https://hail.is,https://github.com/hail-is/hail/pull/11463,2,['update'],['update']
Deployability," pybind11 fixups</li>; <li><a href=""https://github.com/scipy/scipy/commit/843500aabde17aaf1eec65c589d50bd12ee35039""><code>843500a</code></a> Merge pull request <a href=""https://redirect.github.com/scipy/scipy/issues/17689"">#17689</a> from mdhaber/gh17686</li>; <li><a href=""https://github.com/scipy/scipy/commit/089924b61012a106ffa4f58939b0180124051a0b""><code>089924b</code></a> REL: integrate.qmc_quad: remove from release notes</li>; <li><a href=""https://github.com/scipy/scipy/commit/3e47110f10e3267d228e9da84174f3cee325e7c3""><code>3e47110</code></a> REL: 1.10.0rc3 unreleased</li>; <li>Additional commits viewable in <a href=""https://github.com/scipy/scipy/compare/v1.9.3...v1.10.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=scipy&package-manager=pip&previous-version=1.9.3&new-version=1.10.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13227:5188,update,updates,5188,https://hail.is,https://github.com/hail-is/hail/pull/13227,1,['update'],['updates']
Deployability," pycares, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **451/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.3 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI0Y2E5NmE2ZC02MjMxLTQ1YTctYmQyOS1kYTA0ZmZhNTliYzQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjRjYTk2YTZkLTYyMzEtNDVhNy1iZDI5LWRhMDRmZmE1OWJjNCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14238:1653,upgrade,upgraded,1653,https://hail.is,https://github.com/hail-is/hail/pull/14238,1,['upgrade'],['upgraded']
Deployability," raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/88f9647a223c77a29153683b49bc29852ed80721""><code>88f9647</code></a> Bump to 4.4.0 final</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/fc428ad324ef38402f1e93b38c61cd6348980ed2""><code>fc428ad</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9822"">#9822</a> from jakobandersen/intersphinx_role</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/5d595ec0c4294f45f3138c4c581b84c39cae5e29""><code>5d595ec</code></a> intersphinx role, simplify role_name check</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/6ee0ecbe40ab8a3251538409cf35ffcc04765bfa""><code>6ee0ecb</code></a> intersphinx role, simplify role name matching</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/3bf8bcd6e151a78b0dd003a3e76ff4c65566b6e6""><code>3bf8bcd</code></a> intersphinx role, update docs</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/c11b109d591a74f87de071ec4782ac3ab782ea38""><code>c11b109</code></a> intersphinx role: :external+inv:<strong>: instead of :external:inv+</strong>:</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/9589a2bc0531598cdd69f260f2f2c2dbc5852d6e""><code>9589a2b</code></a> intersphinx role, remove redundant method</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/941db550f02d76ee2b93300584ac85dc599d21e6""><code>941db55</code></a> intersphinx role, fix flake8 warnings</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/9a3f2b85421948c98647b10106c1bbb5ff1b0628""><code>9a3f2b8</code></a> intersphinx role, CHANGES</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/540d76035cc6bbf7ee18d0eb9bf63e4c3651d1f9""><code>540d760</code></a> intersphinx role, documentation</li>; <li>Additional commits viewable in <a href=""https://github.com/sphinx-doc/sphinx/c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11522:6222,update,update,6222,https://hail.is,https://github.com/hail-is/hail/pull/11522,2,['update'],['update']
Deployability," region is set</li>; <li>bugfix:s3: [<code>botocore</code>] fixes missing x-amz-content-sha256 header for s3 object lambda</li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] Adding support for Amazon AppFlow to transfer the data to Amazon Redshift databases through Amazon Redshift Data API service. This feature will support the Redshift destination connector on both public and private accessible Amazon Redshift Clusters and Amazon Redshift Serverless.</li>; <li>api-change:<code>kinesisanalyticsv2</code>: [<code>botocore</code>] Support for Apache Flink 1.15 in Kinesis Data Analytics.</li>; </ul>; <h1>1.26.14</h1>; <ul>; <li>api-change:<code>route53</code>: [<code>botocore</code>] Amazon Route 53 now supports the Asia Pacific (Hyderabad) Region (ap-south-2) for latency records, geoproximity records, and private DNS for Amazon VPCs in that region.</li>; </ul>; <h1>1.26.13</h1>; <ul>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow provides a new API called UpdateConnectorRegistration to update a custom connector that customers have previously registered. With this API, customers no longer need to unregister and then register a connector to make an update.</li>; <li>api-change:<code>auditmanager</code>: [<code>botocore</code>] This release introduces a new feature for Audit Manager: Evidence finder. You can now use evidence finder to quickly query your evidence, and add the matching evidence results to an assessment report.</li>; <li>api-change:<code>chime-sdk-voice</code>: [<code>botocore</code>] Amazon Chime Voice Connector, Voice Connector Group and PSTN Audio Service APIs are now available in the Amazon Chime SDK Voice namespace. See <a href=""https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html"">https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html</a> for more details.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits<",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12507:5682,Update,UpdateConnectorRegistration,5682,https://hail.is,https://github.com/hail-is/hail/pull/12507,2,"['Update', 'update']","['UpdateConnectorRegistration', 'update']"
Deployability," region is set</li>; <li>bugfix:s3: [<code>botocore</code>] fixes missing x-amz-content-sha256 header for s3 object lambda</li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] Adding support for Amazon AppFlow to transfer the data to Amazon Redshift databases through Amazon Redshift Data API service. This feature will support the Redshift destination connector on both public and private accessible Amazon Redshift Clusters and Amazon Redshift Serverless.</li>; <li>api-change:<code>kinesisanalyticsv2</code>: [<code>botocore</code>] Support for Apache Flink 1.15 in Kinesis Data Analytics.</li>; </ul>; <h1>1.26.14</h1>; <ul>; <li>api-change:<code>route53</code>: [<code>botocore</code>] Amazon Route 53 now supports the Asia Pacific (Hyderabad) Region (ap-south-2) for latency records, geoproximity records, and private DNS for Amazon VPCs in that region.</li>; </ul>; <h1>1.26.13</h1>; <ul>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow provides a new API called UpdateConnectorRegistration to update a custom connector that customers have previously registered. With this API, customers no longer need to unregister and then register a connector to make an update.</li>; <li>api-change:<code>auditmanager</code>: [<code>botocore</code>] This release introduces a new feature for Audit Manager: Evidence finder. You can now use evidence finder to quickly query your evidence, and add the matching evidence results to an assessment report.</li>; <li>api-change:<code>chime-sdk-voice</code>: [<code>botocore</code>] Amazon Chime Voice Connector, Voice Connector Group and PSTN Audio Service APIs are now available in the Amazon Chime SDK Voice namespace. See <a href=""https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html"">https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html</a> for more details.</li>; <li>api-change:<code>cloudfront</code>: [<code>botocore</code>] CloudFront API support for staging distributions ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12498:1368,Update,UpdateConnectorRegistration,1368,https://hail.is,https://github.com/hail-is/hail/pull/12498,4,"['Update', 'update']","['UpdateConnectorRegistration', 'update']"
Deployability," release to v4</li>; <li>Additional commits viewable in <a href=""https://github.com/jaraco/keyrings.alt/compare/v3.5.2...v4.2.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=keyrings-alt&package-manager=pip&previous-version=3.5.2&new-version=4.2.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12448:4850,upgrade,upgrade,4850,https://hail.is,https://github.com/hail-is/hail/pull/12448,3,['upgrade'],['upgrade']
Deployability," reopen if this occurs again. Posting the job configurations here before I delete the jobs. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/batch-worker"",""us-docker.pkg.dev/hail-vdc/hail/benchmark"",""us-docker.pkg.dev/hail-vdc/hail/blog_nginx"",""us-docker.pkg.dev/hail-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-docker.pkg.dev/hail-vdc/hail/grafana"",""us-docker.pkg.dev/hail-vdc/hail/hail-base"",""us-docker.pkg.dev/hail-vdc/hail/hail-build"",""us-docker.pkg.dev/hail-vdc/hail/hail-buildkit"",""us-docker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python38"",""us-docker.pkg.dev/hail-vdc/hail/hail-ubuntu"",""us-docker.pkg.dev/hail-vdc/hail/memory"",""us-docker.pkg.dev/hail-vdc/hail/monitoring"",""us-docker.pkg.dev/hail-vdc/hail/notebook"",""us-docker.pkg.dev/hail-vdc/hail/notebook_nginx"",""us-docker.pkg.dev/hail-vdc/hail/prometheus"",""us-docker.pkg.dev/hail-vdc/hail/service-base"",""us-docker.pkg.dev/hail-vdc/hail/service-java-run-base"",""us-docker.pkg.dev/hail-vdc/hail/test-ci"",""us-docker.pkg.dev/hail-vdc/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95""],""grace"":""48h"",""recursive"":true,""tag_filter_all"":""cache-pr-.*""}; ```. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545:1147,install,installed-,1147,https://hail.is,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545,1,['install'],['installed-']
Deployability," requires frozenlist, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **556/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.4 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-JINJA2-6150717](https://snyk.io/vuln/SNYK-PYTHON-JINJA2-6150717) | `jinja2:` <br> `3.1.2 -> 3.1.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlOTc1OTMyYy1kNmNhLTQ0NTUtYmU4ZC04NzY1ZGY0MTZjMWMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImU5NzU5MzJjLWQ2Y2EtNDQ1NS1iZThkLTg3NjVkZjQxNmMxYyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14141:1951,upgrade,upgraded,1951,https://hail.is,https://github.com/hail-is/hail/pull/14141,1,['upgrade'],['upgraded']
Deployability," requires frozenlist, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **556/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.4 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-JINJA2-6150717](https://snyk.io/vuln/SNYK-PYTHON-JINJA2-6150717) | `jinja2:` <br> `3.1.2 -> 3.1.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlYTQ5ODFkZC02M2FmLTQ4YzYtYTIwMC05NjkyZjg2ZTlhNjIiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImVhNDk4MWRkLTYzYWYtNDhjNi1hMjAwLTk2OTJmODZlOWE2MiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/b7c31419-ec34-40f1-8bc6-ad8303fb329b?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/b7c31419-ec34-40f1-8bc6-ad8303fb329b?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14140:1507,upgrade,upgraded,1507,https://hail.is,https://github.com/hail-is/hail/pull/14140,1,['upgrade'],['upgraded']
Deployability," routes are matched; before more complex ones. :issue:<code>2471</code></li>; <li>Restore <code>ValidationError</code> to be importable from; <code>werkzeug.routing</code>. :issue:<code>2465</code></li>; </ul>; <h2>Version 2.2.0</h2>; <p>Released 2022-07-23</p>; <ul>; <li>Deprecated <code>get_script_name</code>, <code>get_query_string</code>,; <code>peek_path_info</code>, <code>pop_path_info</code>, and; <code>extract_path_info</code>. :pr:<code>2461</code></li>; <li>Remove previously deprecated code. :pr:<code>2461</code></li>; <li>Add MarkupSafe as a dependency and use it to escape values when; rendering HTML. :issue:<code>2419</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/werkzeug/commit/15fcb87d36f4ed45b127692d2d739266b918503c""><code>15fcb87</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/werkzeug/issues/2499"">#2499</a> from pallets/release-2.2.2</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/87b082a02373aa2feba5750f5768efd6013f701d""><code>87b082a</code></a> release version 2.2.2</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/110b4cdbc1c86125065e56c8d64d0c560883b42b""><code>110b4cd</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/werkzeug/issues/2498"">#2498</a> from pallets/socket-warning</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/b484e5497d12a11766544d79d320d5953d3a753d""><code>b484e54</code></a> handle unclosed socket resource warning</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/bebf46d336782e70510f00e3a08db1e1453ce68a""><code>bebf46d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/werkzeug/issues/2497"">#2497</a> from pallets/local-wrapped</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/82d3fba4e7955e00833eff123fd11564fc96cdae""><code>82d3fb",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12119:4761,release,release-,4761,https://hail.is,https://github.com/hail-is/hail/pull/12119,1,['release'],['release-']
Deployability," severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Use of a Broken or Risky Cryptographic Algorithm <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6149518](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6149518) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Uncontrolled Resource Consumption (&#x27;Resource Exhaustion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6157248](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6157248) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIxZDhjNDI0MS1hOTllLTQwZDktOTM5Yy0zZWMzM2NkNTI0ZjkiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjFkOGM0MjQxLWE5OWUtNDBkOS05MzljLTNlYzMzY2Q1MjRmOSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14230:2117,upgrade,upgraded,2117,https://hail.is,https://github.com/hail-is/hail/pull/14230,1,['upgrade'],['upgraded']
Deployability," speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1356,install,install,1356,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,1,['install'],['install']
Deployability," stable JupyterLab 4.0 in the releaser hook <a href=""https://redirect.github.com/jupyter/notebook/pull/7183"">#7183</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; <li>Update publish-release workflow for PyPI trusted publisher <a href=""https://redirect.github.com/jupyter/notebook/pull/7176"">#7176</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/notebook/graphs/contributors?from=2023-10-17&amp;to=2024-01-19&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Abrichet+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​brichet</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ad5423197+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​d5423197</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Agithub-actions+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ajtpio+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Akrassowski+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ameeseeksmachine+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​meeseeksmachine</code></a></p>; <!-- raw HTML omitted -->; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/jupyter/notebook/commit/80e992e9f4cfa6cc1fcf4d84cebe09d53e769790""><code>80e992e</code></a> Publish 7.0.7</li>; <li><a href=""https://github.com/jupyter/notebook/commit/089c78c48fd00b2b0d2f33e4463eb42018e86803""><code>089c78c</code></a> Update to JupyterLab 4.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14182:5169,update,updated,5169,https://hail.is,https://github.com/hail-is/hail/pull/14182,1,['update'],['updated']
Deployability," stable JupyterLab 4.0 in the releaser hook <a href=""https://redirect.github.com/jupyter/notebook/pull/7183"">#7183</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; <li>Update publish-release workflow for PyPI trusted publisher <a href=""https://redirect.github.com/jupyter/notebook/pull/7176"">#7176</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/notebook/graphs/contributors?from=2023-10-17&amp;to=2024-01-19&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Abrichet+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​brichet</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ad5423197+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​d5423197</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Agithub-actions+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ajtpio+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Akrassowski+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ameeseeksmachine+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​meeseeksmachine</code></a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/notebook/blob/@jupyter-notebook/tree@7.0.7/CHANGELOG.md"">notebook's changelog</a>.</em></p>; <blockquote>; <h2>7.0.7</h2>; <p>(<a href=""https://github.com/jupyter/notebook/compare/@jupyter-notebook/application-extension@7.0.6...089c78c48fd00b2b0d2f33e4463eb42018e8",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14182:2305,update,updated,2305,https://hail.is,https://github.com/hail-is/hail/pull/14182,1,['update'],['updated']
Deployability," still uses the HTML4 writer.</li>; <li>Support for Sphinx versions &lt; 5.0 was removed.</li>; <li>In addition, our supported dependencies will match the dependencies from our; lowest supported Sphinx release, version 5.0: Python &gt;= 3.6 and docutils &gt; 0.14 and &lt; 0.19</li>; </ul>; <p>.. _release-1.3.0:</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/7c9b1b5d391f6d7fae72274393eb25d1df96e546""><code>7c9b1b5</code></a> Release 2.0 final (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1544"">#1544</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/c1044107602faf9be43e4358bc4f8b6abff9b420""><code>c104410</code></a> Bump for next potential release, 2.0.0rc5 (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1539"">#1539</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/53ca116ef64123735e5e445258b8b103ad31a26e""><code>53ca116</code></a> Release 2.0.0rc4 (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1538"">#1538</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/4498e97b462688bac2ff3615ac1da1b867b21842""><code>4498e97</code></a> Fix AttributeError when one of <code>css_files</code> is a string (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1537"">#1537</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/84aea9b0ba78b7f07df4b624e5200ed4c842794c""><code>84aea9b</code></a> Increment for next potential release (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1536"">#1536</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/febde39c7cd5f56fe19979adeee05e1e16eadfe2""><code>febde39</code></a> Release 2.0.0rc3 (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1535"">#1535</a>)</li>; <li><a href=""https://g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14502:1662,Release,Release,1662,https://hail.is,https://github.com/hail-is/hail/pull/14502,1,['Release'],['Release']
Deployability," submit bug reports, patches, or suggestions for improvement, see the; Apache Commons Codec website:</p>; <p><a href=""https://commons.apache.org/proper/commons-codec/"">https://commons.apache.org/proper/commons-codec/</a></p>; <p>Download page: <a href=""https://commons.apache.org/proper/commons-codec/download_codec.cgi"">https://commons.apache.org/proper/commons-codec/download_codec.cgi</a></p>; <hr />; <pre><code> Apache Commons Codec 1.14 RELEASE NOTES; December 30 2019; </code></pre>; <p>The Apache Commons Codec package contains simple encoder and decoders for; various formats such as Base64 and Hexadecimal. In addition to these; widely used encoders and decoders, the codec package also maintains a</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/apache/commons-codec/commit/c89d2af770f05457fbefa5fb4713c888bf177fb2""><code>c89d2af</code></a> Prepare for 1.15 release</li>; <li><a href=""https://github.com/apache/commons-codec/commit/ba81ed5dc06661d931a4bb8f7abaa51ee5300396""><code>ba81ed5</code></a> Use gav=true for the maven central redirect</li>; <li><a href=""https://github.com/apache/commons-codec/commit/cb629f03516e21ba7daeb4dd9a7b5fe3c76fc841""><code>cb629f0</code></a> Update maven central badge</li>; <li><a href=""https://github.com/apache/commons-codec/commit/b8090b34914ef456a1262292b554c7a5e1e623e8""><code>b8090b3</code></a> Fix coverage badge to use the 'master' branch not the default 'trunk'</li>; <li><a href=""https://github.com/apache/commons-codec/commit/fa0562e71e2661768c4d2e324ff978aa2af1be08""><code>fa0562e</code></a> Test Context toString has debugging info</li>; <li><a href=""https://github.com/apache/commons-codec/commit/f39003c953df152ff737474d2d2f27b611963a1c""><code>f39003c</code></a> Test isInAlphabet</li>; <li><a href=""https://github.com/apache/commons-codec/commit/9ac33a12a500bbc3ea40685aac61c95169443957""><code>9ac33a1</code></a> Test all const",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12385:2970,release,release,2970,https://hail.is,https://github.com/hail-is/hail/pull/12385,1,['release'],['release']
Deployability," support for Sphinx&gt;=4.0.0</li>; <li>Added: tests for Windows and macOS</li>; <li>Changed: switch to KaTeX 0.13.11</li>; <li>Changed: switched CI tests from Travis to Github Actions</li>; <li>Changed: running sphinx will now fail in pre-render mode; if KaTeX fails</li>; <li>Removed: support for Python 2.7, 3.4, 3.5</li>; </ul>; <p>Version 0.7.2 (2021-04-28)</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/ce89a95b3b330a19ad4562b87aacc69ddb6742f2""><code>ce89a95</code></a> Release 0.8.6</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/c230f938a2d3b5913004d004613f67b69ebaf526""><code>c230f93</code></a> Allow sphinx&gt;=4.0.0 in setup.cfg (<a href=""https://github-redirect.dependabot.com/hagenw/sphinxcontrib-katex/issues/64"">#64</a>)</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/c6456022c32c540ffc5db6c684d8f8bf70a966f3""><code>c645602</code></a> Release 0.8.5</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/598efcf53499f2d0a58cccc1942120dee07de3f1""><code>598efcf</code></a> Remove unintentional whitespace in prerender mode (<a href=""https://github-redirect.dependabot.com/hagenw/sphinxcontrib-katex/issues/62"">#62</a>)</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/da43ec0d98471d0cf02292cc03c05ed526777bf0""><code>da43ec0</code></a> Release 0.8.4</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/358887325d6d9cbae839fd77bebb9de0fae3b474""><code>3588873</code></a> Increase top padding of equations (<a href=""https://github-redirect.dependabot.com/hagenw/sphinxcontrib-katex/issues/61"">#61</a>)</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/5ef309a8c9c7f9f12c3c17693adef49f593eb5de""><code>5ef309a</code></a> TST: re-enable link checks (<a href=""https://github-redirect.dependabot.com/hagenw/sphinxcont",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11458:4089,Release,Release,4089,https://hail.is,https://github.com/hail-is/hail/pull/11458,2,['Release'],['Release']
Deployability," tag marking the end of support for that branch. We encourage everyone to upgrade, and to use a tool such as <a href=""https://pypi.org/project/pip-tools/"">pip-tools</a> to pin all dependencies and control upgrades.</p>; <ul>; <li>Changes: <a href=""https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-0"">https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-0</a></li>; <li>Milestone: <a href=""https://github.com/pallets/click/milestone/9?closed=1"">https://github.com/pallets/click/milestone/9?closed=1</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/click/blob/main/CHANGES.rst"">click's changelog</a>.</em></p>; <blockquote>; <h2>Version 8.1.1</h2>; <p>Released 2022-03-30</p>; <ul>; <li>Fix an issue with decorator typing that caused type checking to; report that a command was not callable. :issue:<code>2227</code></li>; </ul>; <h2>Version 8.1.0</h2>; <p>Released 2022-03-28</p>; <ul>; <li>; <p>Drop support for Python 3.6. :pr:<code>2129</code></p>; </li>; <li>; <p>Remove previously deprecated code. :pr:<code>2130</code></p>; <ul>; <li><code>Group.resultcallback</code> is renamed to <code>result_callback</code>.</li>; <li><code>autocompletion</code> parameter to <code>Command</code> is renamed to; <code>shell_complete</code>.</li>; <li><code>get_terminal_size</code> is removed, use; <code>shutil.get_terminal_size</code> instead.</li>; <li><code>get_os_args</code> is removed, use <code>sys.argv[1:]</code> instead.</li>; </ul>; </li>; <li>; <p>Rely on :pep:<code>538</code> and :pep:<code>540</code> to handle selecting UTF-8 encoding; instead of ASCII. Click's locale encoding detection is removed.; :issue:<code>2198</code></p>; </li>; <li>; <p>Single options boolean flags with <code>show_default=True</code> only show; the default if it is <code>True</code>. :issue:<code>1971</code></p>; </li>; <li>; <p>The <code>command</code> and <code>group</code> decorators can be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11721:1873,Release,Released,1873,https://hail.is,https://github.com/hail-is/hail/pull/11721,1,['Release'],['Released']
Deployability," take locks. 2. The makefile conforms to the customary use-whatever-is-on-$PATH, with the slight wrinkle that; the full pathnames of the commands used will be visible in the build log - so if someone; picks up something weird we'll at least see it. 3. There is a cache of NativeModule objects, so that we won't do enormous numbers of; calls to dlopen/dlclose. This may help in shuffle code, which creates a new PackDecoder; for each RV. 4. The hash function on (options, source) is now beefed up to cope with having only a; few distinct values of options; and is modified with the output of ""$(CXX) --version"",; so that when you upgrade your compiler, you won't get hits on modules compiled with the old; compiler. 5. build.gradle has a new target ""nativeLibPrebuilt"", for updating the prebuilt/lib/linux-x86-64; or prebuilt/lib/darwin. 6. The committed prebuilt libraries are built thus:. darwin - On my MacOS laptop, with the default (clang-based) compiler, -march=sandybridge; From my reading, I believe this should be compatible withall MacBook Pro's; released since 2011, and all versions of MacOS since 10.9 (the first to use; libc++ rather than libstdc++ as the default C++ library) - we're now at 10.13,; with 10.14 arriving some time in the fall. linux-x86-64 - Built on my home desktop running Ubuntu-16.04 LTS, and g++-5.0.4, with; -fabi-version=9. In theory this should work with all systems based on g++5.x and; later. I made some effort to move std::string out of the interfaces between prebuilt; and dynamic code, which gives it some chance of working on systems based on; g++-4.x, but haven't tested that. I'm planning to fire up VM's either in cloud or under VirtualBox, to test this against Ubuntu-14.04,; Ubuntu-18.04, and the latest stable RHEL, which should cover most of the bases. In the interest of getting this committed, I have not made changes relating to logging and; error messages. The DLL's are still in the jar, and I think it has to stay that way because; all nodes ne",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863:1229,release,released,1229,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863,1,['release'],['released']
Deployability," the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI5NmE4NGVhMS1hYzgxLTQxYmEtOGYzNC02MGU1ZTdhYzNjZTMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6Ijk2YTg0ZWExLWFjODEtNDFiYS04ZjM0LTYwZTVlN2FjM2NlMyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""96a84ea1-ac81-41ba-8f34-60e5e7ac3ce3"",""prPublicId"":""96a84ea1-ac81-41ba-8f34-60e5e7ac3ce3"",""dependencies"":[{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-SETUPTOOLS-3180412""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[509],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lessons/redos/javascript/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12896:2498,upgrade,upgrade,2498,https://hail.is,https://github.com/hail-is/hail/pull/12896,6,"['patch', 'update', 'upgrade']","['patch', 'patches-to-fix-vulnerabilities', 'updated-fix-title', 'upgrade']"
Deployability," the first release of v2. An alpha version for users to have a preview of the new mistune.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/lepture/mistune/blob/master/docs/changes.rst"">mistune's changelog</a>.</em></p>; <blockquote>; <h2>Changelog</h2>; <p>Here is the full history of mistune v2.</p>; <p>Version 2.0.4</p>; <pre><code>; Released on Jul 15, 2022; <ul>; <li>Fix <code>url</code> plugin in <code>&amp;lt;a&amp;gt;</code> tag</li>; <li>Fix <code>*</code> formatting</li>; </ul>; <p>Version 2.0.3; </code></pre></p>; <p>Released on Jun 27, 2022</p>; <ul>; <li>Fix <code>table</code> plugin</li>; <li>Security fix for CVE-2022-34749</li>; </ul>; <p>Version 2.0.2</p>; <pre><code>; Released on Jan 14, 2022; <p>Fix <code>escape_url</code></p>; <p>Version 2.0.1; </code></pre></p>; <p>Released on Dec 30, 2021</p>; <p>XSS fix for image link syntax.</p>; <p>Version 2.0.0</p>; <pre><code>; Released on Dec 5, 2021; <p>This is the first non-alpha release of mistune v2.</p>; <p>Version 2.0.0rc1; </code></pre></p>; <p>Released on Feb 16, 2021</p>; <p>Version 2.0.0a6</p>; <pre><code>; &lt;/tr&gt;&lt;/table&gt; ; </code></pre>; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/lepture/mistune/commit/3f422f1e84edae0f39756c45be453ecde534b755""><code>3f422f1</code></a> Version bump 2.0.3</li>; <li><a href=""https://github.com/lepture/mistune/commit/a6d43215132fe4f3d93f8d7e90ba83b16a0838b2""><code>a6d4321</code></a> Fix asteris emphasis regex CVE-2022-34749</li>; <li><a href=""https://github.com/lepture/mistune/commit/5638e460459cb59ceb20e4ce4716c802d4d73c53""><code>5638e46</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/lepture/mistune/issues/307"">#307</a> from jieter/patch-1</li>; <li><a href=""https://github.com/lepture/mistune/commit/0eba47196a81453bafe1f2492748a87475063dff""><code>0eba471</code></a> Fix t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12064:1653,Release,Released,1653,https://hail.is,https://github.com/hail-is/hail/pull/12064,2,"['Release', 'release']","['Released', 'release']"
Deployability," the first release of v2. An alpha version for users to have a preview of the new mistune.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/lepture/mistune/blob/master/docs/changes.rst"">mistune's changelog</a>.</em></p>; <blockquote>; <h2>Changelog</h2>; <p>Here is the full history of mistune v2.</p>; <p>Version 2.0.4</p>; <pre><code>; Released on Jul 15, 2022; <ul>; <li>Fix <code>url</code> plugin in <code>&amp;lt;a&amp;gt;</code> tag</li>; <li>Fix <code>*</code> formatting</li>; </ul>; <p>Version 2.0.3; </code></pre></p>; <p>Released on Jun 27, 2022</p>; <ul>; <li>Fix <code>table</code> plugin</li>; <li>Security fix for CVE-2022-34749</li>; </ul>; <p>Version 2.0.2</p>; <pre><code>; Released on Jan 14, 2022; <p>Fix <code>escape_url</code></p>; <p>Version 2.0.1; </code></pre></p>; <p>Released on Dec 30, 2021</p>; <p>XSS fix for image link syntax.</p>; <p>Version 2.0.0</p>; <pre><code>; Released on Dec 5, 2021; <p>This is the first non-alpha release of mistune v2.</p>; <p>Version 2.0.0rc1; </code></pre></p>; <p>Released on Feb 16, 2021</p>; <p>Version 2.0.0a6</p>; <pre><code>; &lt;/tr&gt;&lt;/table&gt; ; </code></pre>; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/lepture/mistune/commit/b92a5febd4da3d7097a3d2b8d7cac6f5d57ea20c""><code>b92a5fe</code></a> Version bump 2.0.4</li>; <li><a href=""https://github.com/lepture/mistune/commit/98a1c0afc51d4be719cb17401a35e62f46206915""><code>98a1c0a</code></a> Fix url plugin render, <a href=""https://github-redirect.dependabot.com/lepture/mistune/issues/308"">#308</a></li>; <li><a href=""https://github.com/lepture/mistune/commit/979d6d3bfc7d6159f38deb8e751611e4205033f6""><code>979d6d3</code></a> Fix * parsing, <a href=""https://github-redirect.dependabot.com/lepture/mistune/issues/312"">#312</a></li>; <li><a href=""https://github.com/lepture/mistune/commit/f857f048ebb2f6f2bb7ab97dcb7a15",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12066:1653,Release,Released,1653,https://hail.is,https://github.com/hail-is/hail/pull/12066,4,"['Release', 'release']","['Released', 'release']"
Deployability," the jars and wheel in release mode:; ```bash; HAIL_RELEASE_MODE=1 make -C hail wheel; ```. 2. Dry-run the upload-artifacts target and inspect output; ```bash; cloud_base is set to ""gs://hail-common/hailctl/dataproc/0.2.129"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/init_notebook.py python/hailtop/hailctl/dataproc/reso",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:1068,deploy,deploy,1068,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,1,['deploy'],['deploy']
Deployability," the non-central chi-squared distribution has a closed form implementation (indeed, Hail implements this CDF: `hl.pchisqtail`), the generalized chi-squared distribution does not have a closed form. There are at least four distinct algorithms for evaluating the CDF. To my knowledge, the oldest one is by Robert Davies:. Davies, Robert. ""The distribution of a linear combination of chi-squared; random variables."" Applied Statistics 29 323-333. 1980. The [original publication](http://www.robertnz.net/pdf/lc_chisq.pdf) includes a Fortran implementation in the publication. Davies' [website](http://www.robertnz.net/QF.htm) also includes a C version. Hail includes a copy of the C version as `davies.cpp`. I suspect this code contains undefined behavior. Moreover, it is not supported on Apple M1 machines because we don't ship binaries for that platform. It seemed to me that the simplest solution is to port this algorithm to Scala. This PR is that port. I tested against the 39 test cases provided Davies with the source code. I also added some doctests based on the CDF plots from Wikipedia. The same 39 test cases are tested in Scala and in Python. I am open to suggestions for the name. `pgenchisq` seems to strike a balance between clarity and brevity. I believe this is the first CDF which can fail to converge. I included some relevant debugging information. I think we should standardize on a schema, but I need more examples before I am certain of the right standard. I am open to critique of `GeneralizedChiSquaredDistribution.scala` but I will strongly argue against significant refactoring. I worry that we will subtly break this algorithm. I directly reached out to Robert Davies to clarify the licensing of this algorithm. It appears to have been released at least under both GPL2 and MIT by unaffiliated third parties (who, really, have no right to apply a license to it). Do not remove WIP until I resolve this. With this PR in place, `hl.skat` can be implemented entirely in Python.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12605:2660,release,released,2660,https://hail.is,https://github.com/hail-is/hail/pull/12605,1,['release'],['released']
Deployability," the requirements on [scipy](https://github.com/scipy/scipy) to permit the latest version.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/scipy/scipy/releases"">scipy's releases</a>.</em></p>; <blockquote>; <h1>SciPy 1.9.2 Release Notes</h1>; <p>SciPy <code>1.9.2</code> is a bug-fix release with no new features; compared to <code>1.9.1</code>. It also provides wheels for Python <code>3.11</code>; on several platforms.</p>; <h1>Authors</h1>; <ul>; <li>Hood Chatham (1)</li>; <li>Thomas J. Fan (1)</li>; <li>Ralf Gommers (22)</li>; <li>Matt Haberland (5)</li>; <li>Julien Jerphanion (1)</li>; <li>Loïc Estève (1)</li>; <li>Nicholas McKibben (2)</li>; <li>Naoto Mizuno (1)</li>; <li>Andrew Nelson (3)</li>; <li>Tyler Reddy (28)</li>; <li>Pamphile Roy (1)</li>; <li>Ewout ter Hoeven (2)</li>; <li>Warren Weckesser (1)</li>; <li>Meekail Zain (1) +</li>; </ul>; <p>A total of 14 people contributed to this release.; People with a &quot;+&quot; by their names contributed a patch for the first time.; This list of names is automatically generated, and may not be fully complete.</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/scipy/scipy/commit/656076ca6b490f587e9bd9c4cd10cb259a687c5b""><code>656076c</code></a> MAINT: wheel push 1.9.2 [wheel build]</li>; <li><a href=""https://github.com/scipy/scipy/commit/ad0d0f907010fbc8b66cdbe8ce0af2683881a309""><code>ad0d0f9</code></a> REL: set 1.9.2 released [wheel build]</li>; <li><a href=""https://github.com/scipy/scipy/commit/d9ad9801323653a2015b4d3e80d6d3ea93b6c021""><code>d9ad980</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17150"">#17150</a> from tylerjereddy/treddy_scipy_192_more_backports</li>; <li><a href=""https://github.com/scipy/scipy/commit/6b098c25223e224ff44101f86bbc86efecffe1d9""><code>6b098c2</code></a> TST: optimize.milp: remove problematic timeout/iteration test</li>; <li><a href=""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12352:1024,patch,patch,1024,https://hail.is,https://github.com/hail-is/hail/pull/12352,1,['patch'],['patch']
Deployability," the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2981,update,update,2981,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,1,['update'],['update']
Deployability," time</li>; <li>Additional commits viewable in <a href=""https://github.com/cbeust/testng/compare/testng-6.8.21...7.7.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.testng:testng&package-manager=gradle&previous-version=6.8.21&new-version=7.7.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:18038,upgrade,upgrade,18038,https://hail.is,https://github.com/hail-is/hail/pull/12665,3,['upgrade'],['upgrade']
Deployability," to ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/init_notebook.py python/hailtop/hailctl/dataproc/resources/vep-GRCh37.sh python/hailtop/hailctl/dataproc/resources/vep-GRCh38.sh build/deploy/dist/hail-0.2.129-py3-none-any.whl gs://hail-common/hailctl/dataproc/0.2.129; gcloud storage objects update -r gs://hail-common/hailctl/dataproc/0.2.129 --add-acl-grant=entity=AllUsers,role=READER; gcloud storage objects update ""gs://hail-common/hailctl/dataproc/0.2.129/*"" --temporary-hold; ```. Note the following:; - mill is not invoked; - dep",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:1466,deploy,deploy,1466,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,3,['deploy'],['deploy']
Deployability," to 1.4.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pandas-dev/pandas/releases"">pandas's releases</a>.</em></p>; <blockquote>; <h2>Pandas 1.4.1</h2>; <p>This is the first patch release in the 1.4.x series and includes some regression fixes and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.1/whatsnew/v1.4.1.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.4.0</h2>; <p>This release includes some new features, bug fixes, and performance improvements. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.0/whatsnew/v1.4.0.html"">full whatsnew</a> for a list of all the changes. pandas 1.4.0 supports Python 3.8 and higher.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install -c conda-forge pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.4.0rc0</h2>; <p>We are pleased to announce a release candidate for pandas 1.4.0. If all goes well, we'll release pandas 1.4.0 in about two weeks.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4/whatsnew/v1.4.0.html"">whatsnew</a> for a list of all the changes. pandas 1.4.0 supports Python 3.8 and higher.</p>; <p>The release will be available on conda-forge and PyPI.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11539:1066,upgrade,upgrade,1066,https://hail.is,https://github.com/hail-is/hail/pull/11539,1,['upgrade'],['upgrade']
Deployability," to CSS template files</li>; <li>Additional commits viewable in <a href=""https://github.com/sphinx-doc/sphinx/compare/v3.5.4...v5.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinx&package-manager=pip&previous-version=3.5.4&new-version=5.0.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11925:7733,upgrade,upgrade,7733,https://hail.is,https://github.com/hail-is/hail/pull/11925,3,['upgrade'],['upgrade']
Deployability," to a fixed version:; - hail/python/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **591/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.28.2 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJiNzM2NzI0Yi1hY2RiLTRiOTUtYWQwMy1hYWI3MjkyZGNlYzQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImI3MzY3MjRiLWFjZGItNGI5NS1hZDAzLWFhYjcyOTJkY2VjNCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13116:1286,upgrade,upgraded,1286,https://hail.is,https://github.com/hail-is/hail/pull/13116,1,['upgrade'],['upgraded']
Deployability," to a fixed version:; - hail/python/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **591/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.28.2 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlNDQxZTBmNS1jZDQ4LTQzZDUtYTdkMy1kMTM4YzQ2ZTc2NTgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImU0NDFlMGY1LWNkNDgtNDNkNS1hN2QzLWQxMzhjNDZlNzY1OCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13158:1378,upgrade,upgraded,1378,https://hail.is,https://github.com/hail-is/hail/pull/13158,1,['upgrade'],['upgraded']
Deployability," to latest models</li>; <li><a href=""https://github.com/boto/botocore/commit/7b4b3bbb13a5d59097e6d5f178de58e280fdb553""><code>7b4b3bb</code></a> Resolve endpoint with default partition when no region is set (<a href=""https://github-redirect.dependabot.com/boto/botocore/issues/2818"">#2818</a>)</li>; <li><a href=""https://github.com/boto/botocore/commit/cc3f1c22f55ba50ca792eb73e7a6f721abdcc5ee""><code>cc3f1c2</code></a> Fix: S3 Object Lambda requests miss x-amz-content-sha256 headers (<a href=""https://github-redirect.dependabot.com/boto/botocore/issues/2819"">#2819</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/boto/botocore/compare/1.29.13...1.29.16"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=botocore&package-manager=pip&previous-version=1.29.13&new-version=1.29.16)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12503:3890,update,updates,3890,https://hail.is,https://github.com/hail-is/hail/pull/12503,1,['update'],['updates']
Deployability," updates.</li>; <li>api-change:<code>amplifyuibuilder</code>: [<code>botocore</code>] We are adding the ability to configure workflows and actions for components.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/67b84e02c185294c54a8e49510d4cb962e89cee2""><code>67b84e0</code></a> Merge branch 'release-1.21.13'</li>; <li><a href=""https://github.com/boto/boto3/commit/99acd545b20fe30ffa2f589a674c5a7ad74c266b""><code>99acd54</code></a> Bumping version to 1.21.13</li>; <li><a href=""https://github.com/boto/boto3/commit/83a8f662655bada44d442df7f33cb20d71ead257""><code>83a8f66</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/261b0f2ffe079b6940d683657fcad358195f882e""><code>261b0f2</code></a> Merge branch 'release-1.21.12'</li>; <li><a href=""https://github.com/boto/boto3/commit/a972b1bed4caacf0c97f1056cabdfe4b5ccc2681""><code>a972b1b</code></a> Merge branch 'release-1.21.12' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/44f4f5ef0b66d1a508685b62388f1e4a7d60dace""><code>44f4f5e</code></a> Bumping version to 1.21.12</li>; <li><a href=""https://github.com/boto/boto3/commit/bb003d02bd7afefede0ab4678abaea99fe1662ce""><code>bb003d0</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/ad9a92e8fe3d5bc90b2980cdb6839c713b56fbda""><code>ad9a92e</code></a> Merge branch 'release-1.21.11'</li>; <li><a href=""https://github.com/boto/boto3/commit/42b3e0d3c1d02acaa4c0c4127e9523d2c389b675""><code>42b3e0d</code></a> Merge branch 'release-1.21.11' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/bc87db2261257cf3437824d07080fc061847f21c""><code>bc87db2</code></a> Bumping version to 1.21.11</li>; <li>Additional commits viewable in <a href=""https://github.com/boto/boto3/compare/1.17.54...1.21.13"">compare view</a></li>; </ul>; </detai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11504:6327,release,release-,6327,https://hail.is,https://github.com/hail-is/hail/pull/11504,1,['release'],['release-']
Deployability," updating an existing named query.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds support for new AMI property 'lastLaunchedTime'</li>; <li>api-change:<code>servicecatalog-appregistry</code>: [<code>botocore</code>] AppRegistry is deprecating Application and Attribute-Group Name update feature. In this release, we are marking the name attributes for Update APIs as deprecated to give a heads up to our customers.</li>; </ul>; <h1>1.21.8</h1>; <ul>; <li>api-change:<code>elasticache</code>: [<code>botocore</code>] Doc only update for ElastiCache</li>; <li>api-change:<code>panorama</code>: [<code>botocore</code>] Added NTP server configuration parameter to ProvisionDevice operation. Added alternate software fields to DescribeDevice response</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/261b0f2ffe079b6940d683657fcad358195f882e""><code>261b0f2</code></a> Merge branch 'release-1.21.12'</li>; <li><a href=""https://github.com/boto/boto3/commit/44f4f5ef0b66d1a508685b62388f1e4a7d60dace""><code>44f4f5e</code></a> Bumping version to 1.21.12</li>; <li><a href=""https://github.com/boto/boto3/commit/bb003d02bd7afefede0ab4678abaea99fe1662ce""><code>bb003d0</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/ad9a92e8fe3d5bc90b2980cdb6839c713b56fbda""><code>ad9a92e</code></a> Merge branch 'release-1.21.11'</li>; <li><a href=""https://github.com/boto/boto3/commit/42b3e0d3c1d02acaa4c0c4127e9523d2c389b675""><code>42b3e0d</code></a> Merge branch 'release-1.21.11' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/bc87db2261257cf3437824d07080fc061847f21c""><code>bc87db2</code></a> Bumping version to 1.21.11</li>; <li><a href=""https://github.com/boto/boto3/commit/b439b9d69375efd28132ddfc4361568eb86c949c""><code>b439b9d</code></a> Add changelog entries from botocor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11486:5138,release,release-,5138,https://hail.is,https://github.com/hail-is/hail/pull/11486,1,['release'],['release-']
Deployability," url, **kwargs); usr/local/lib/python3.9/dist-packages/hailtop/aiocloud/common/session.py:105: in request; return await retry_transient_errors(self._request_with_valid_authn, method, url, **kwargs); usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:780: in retry_transient_errors; return await retry_transient_errors_with_debug_string('', 0, f, *args, **kwargs); usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:796: in retry_transient_errors_with_debug_string; return await f(*args, **kwargs); usr/local/lib/python3.9/dist-packages/hailtop/aiocloud/common/session.py:117: in _request_with_valid_authn; return await self._http_session.request(method, url, **kwargs); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. async def request_and_raise_for_status():; json_data = kwargs.pop('json', None); if json_data is not None:; if kwargs.get('data') is not None:; raise ValueError('data and json parameters cannot be used at the same time'); kwargs['data'] = aiohttp.BytesPayload(; value=orjson.dumps(json_data),; # https://github.com/ijl/orjson#serialize; #; # ""The output is a bytes object containing UTF-8""; encoding=""utf-8"",; content_type=""application/json"",; ); resp = await self.client_session._request(method, url, **kwargs); if raise_for_status:; if resp.status >= 400:; # reason should always be not None for a started response; assert resp.reason is not None; body = (await resp.read()).decode(); await resp.release(); > raise ClientResponseError(; resp.request_info,; resp.history,; status=resp.status,; message=resp.reason,; headers=resp.headers,; body=body,; ); E hailtop.httpx.ClientResponseError: 400, message='Bad Request', url=URL('http://internal.hail/pr-14351-default-yojxd4mck4io/batch/api/v1alpha/batches/321/update-fast') body=""400: error while inserting job group 1 into batch 321: (1213, 'Deadlock found when trying to get lock; try restarting transaction')""; ```. ### Version. 0.2.128. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14413:3403,release,release,3403,https://hail.is,https://github.com/hail-is/hail/issues/14413,2,"['release', 'update']","['release', 'update-fast']"
Deployability," use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); /hail/test/BRCA1.raw_indel.vcf MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2; >>> ; ```. ----------------; When I executed the command in local mode , there seems to hava some result:; ```; [root@tele-1 ~]# python; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; >>> hc = HailContext();; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/opt/NfsDir/UserDir/wanghn/BRCA1.raw_indel.vcf').write('/opt/NfsDir/UserDir/wanghn/BRCA1.raw_indel_1.vds'); hail: info: No multiallelics detected.; hail: info: Coerced unsorted dataset; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; ```; ------------------------; How can I check if my spark configuration meet the requirement of the hail?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506:2769,configurat,configuration,2769,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506,1,['configurat'],['configuration']
Deployability, varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-d,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:3317,deploy,deploy-,3317,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['deploy'],['deploy-']
Deployability, varname in '$arguments'; + '[' -z e ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; + for varname in '$arguments'; + '[' -z f ']'; + echo WHEEL_FOR_AZURE=f; WHEEL_FOR_AZURE=f; + for varname in '$arguments'; + '[' -z g ']'; + echo WEBSITE_TAR=g; WEBSITE_TAR=g; + exit 1; make: *** [release] Error 1; ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE= make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE= \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERS,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:14047,release,release,14047,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability," version bump</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/31c8dacdc727673e9099f1ac86019714cdccec67""><code>31c8dac</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7675"">#7675</a> from python-pillow/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/40a3f91af2c78870676a13629b5902bab4ab4cf0""><code>40a3f91</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7674"">#7674</a> from nulano/url-example</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/cb41b0cc78eeefbd9ed2ce8c10f8d6d4c405a706""><code>cb41b0c</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/de62b25ed318f1604aa4ccd6f942a04c6b2c8b59""><code>de62b25</code></a> fix image url in &quot;Reading from URL&quot; example</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/7c526a6c6bdc7cb947f0aee1d1ee17c266ff6c61""><code>7c526a6</code></a> Update CHANGES.rst [ci skip]</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/d93a5ad70bf94dbb63bdbfb19491a02976574d6d""><code>d93a5ad</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7553"">#7553</a> from bgilbert/jpeg-rgb</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/aed764fe8404926472499208a39e5bf90d861b2a""><code>aed764f</code></a> Update CHANGES.rst [ci skip]</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/f8df5303fa9daf40cf8bfe232403cb40389d8f8f""><code>f8df530</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7672"">#7672</a> from nulano/imagefont-negative-crop</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/24e9485e6bb733a1a816f228dc75fd0086a93e19""><code>24e9485</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7671"">#7671</a> from rada",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:14113,Update,Update,14113,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['Update'],['Update']
Deployability," versions at all. Alternatively, have you ever considered distributing Hail through conda-forge or bioconda, where you could specify a JRE that should be installed with it and automatically linked?. Is there a better channel than Github Issues for feature requests? I realize this is not a bug report, and if you want to just close it and say ""nope"" that's fine, but I've seen a good number of first-time hail users get a bad impression because of this. . ### Ramble about other nitpicks below. I don't want to spam this repo with issues, but I also noticed while poking around at hail:; 1. It seems to use the default Java GC, which is now G1 in Java 9+. Performance on newer Javas would likely improve with `-XX:+UseParallelGC` in java opts; 2. The Hail jar includes module-info.class from azure storage, this broke my first attempt to use `jdeps` to see what modules it needs. Specifically, `hail-all-spark.jar` says it exports:; ```; backend % jar --describe-module --file=hail-all-spark.jar; releases: 9. com.azure.storage.blob@12.22.0 jar:file:///Users/alex/src/hail/hail/build/deploy/hail/backend/hail-all-spark.jar!/module-info.class; exports com.azure.storage.blob; exports com.azure.storage.blob.models; exports com.azure.storage.blob.options; exports com.azure.storage.blob.sas; exports com.azure.storage.blob.specialized; requires com.azure.storage.common transitive; requires com.azure.storage.internal.avro; requires com.fasterxml.jackson.dataformat.xml; requires java.base mandated; qualified exports com.azure.storage.blob.implementation to com.azure.storage.blob.batch com.azure.storage.blob.cryptography com.azure.storage.file.datalake; qualified exports com.azure.storage.blob.implementation.models to com.azure.storage.blob.batch com.azure.storage.blob.cryptography; qualified exports com.azure.storage.blob.implementation.util to com.azure.storage.blob.batch com.azure.storage.blob.changefeed com.azure.storage.blob.cryptography com.azure.storage.blob.nio com.azure.storage.file.d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14433:3880,release,releases,3880,https://hail.is,https://github.com/hail-is/hail/issues/14433,1,['release'],['releases']
Deployability," via &quot;argsCanBeInterpretedByShell&quot;</li>; <li><a href=""https://github.com/microsoft/debugpy/commit/6b276e339cd850c5f8c93ff4bdbd305dd963d7bb""><code>6b276e3</code></a> Step in/step over support for IPython. Fixes <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/869"">#869</a></li>; <li><a href=""https://github.com/microsoft/debugpy/commit/a294092d9c6d8459126ecb8f537b6012fb7e7d28""><code>a294092</code></a> Properly stop at line 1 in frame eval mode. Fixes <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/995"">#995</a></li>; <li>Additional commits viewable in <a href=""https://github.com/microsoft/debugpy/compare/v1.6.0...v1.6.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=debugpy&package-manager=pip&previous-version=1.6.0&new-version=1.6.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12103:5702,update,updates,5702,https://hail.is,https://github.com/hail-is/hail/pull/12103,2,['update'],['updates']
Deployability," view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=protobuf&package-manager=pip&previous-version=3.20.1&new-version=3.20.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12223:4320,upgrade,upgrade,4320,https://hail.is,https://github.com/hail-is/hail/pull/12223,9,['upgrade'],['upgrade']
Deployability," view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=requests&package-manager=pip&previous-version=2.28.2&new-version=2.31.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13091:8749,upgrade,upgrade,8749,https://hail.is,https://github.com/hail-is/hail/pull/13091,18,['upgrade'],['upgrade']
Deployability," view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=rich&package-manager=pip&previous-version=12.6.0&new-version=13.6.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). You can trigger a rebase of this PR by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>. > **Note**; > Automatic rebases have been disabled on this pull request as it has been open for over 30 days.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13758:8310,upgrade,upgrade,8310,https://hail.is,https://github.com/hail-is/hail/pull/13758,6,['upgrade'],['upgrade']
Deployability," view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=rich&package-manager=pip&previous-version=12.6.0&new-version=13.7.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). You can trigger a rebase of this PR by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>. > **Note**; > Automatic rebases have been disabled on this pull request as it has been open for over 30 days.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14012:9029,upgrade,upgrade,9029,https://hail.is,https://github.com/hail-is/hail/pull/14012,6,['upgrade'],['upgrade']
Deployability," view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=rich&package-manager=pip&previous-version=12.6.0&new-version=13.7.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). You can trigger a rebase of this PR by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>. > **Note**; > Automatic rebases have been disabled on this pull request as it has been open for over 30 days.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14376:8090,upgrade,upgrade,8090,https://hail.is,https://github.com/hail-is/hail/pull/14376,6,['upgrade'],['upgrade']
Deployability," was ready at about 19:05:33 worker time because that's when we see the; image cleanup loop start running. The worker times out and kills itself before; the driver initiates another scheduling loop. This PR ensures that aiohttp is; already initialized and that the only work we need to do before we can; accept a job is parse the JSON response for our token. ```; 2022-03-02 19:06:21,325	ACCEPTABLE_QUERY_JAR_URL_PREFIX hail-az://haildevtest/test/iy40biv5rl1j/jars; 2022-03-02 19:06:30,168	JVM-0: trying to open socket; 2022-03-02 19:06:30,169	JVM-1: trying to open socket; 2022-03-02 19:06:30,169	JVM-2: trying to open socket; 2022-03-02 19:06:30,169	JVM-3: trying to open socket; 2022-03-02 19:06:30,170	JVM-4: trying to open socket; 2022-03-02 19:06:30,170	JVM-5: trying to open socket; 2022-03-02 19:06:30,170	JVM-6: trying to open socket; 2022-03-02 19:06:30,171	JVM-7: trying to open socket; 2022-03-02 19:06:30,171	JVM-8: trying to open socket; 2022-03-02 19:06:30,171	JVM-9: trying to open socket; 2022-03-02 19:06:30,172	JVM-10: trying to open socket; 2022-03-02 19:06:30,172	JVM-11: trying to open socket; 2022-03-02 19:06:30,173	JVM-12: trying to open socket; 2022-03-02 19:06:30,173	JVM-13: trying to open socket; 2022-03-02 19:06:30,173	JVM-14: trying to open socket; 2022-03-02 19:06:30,173	JVM-15: trying to open socket; 2022-03-02 19:06:30,203	BATCH_LOGS_ROOT hail-az://haildevtest/test/batch/logs/4piCT5NOAh9FghF7VjKqZh/batch; 2022-03-02 19:06:30,204	EXAMPLE BATCH_JOB_LOGS_PATH hail-az://haildevtest/test/batch/logs/4piCT5NOAh9FghF7VjKqZh/batch/1/1/abc123/main/log; 2022-03-02 19:06:30,204	No environment configuration found.; 2022-03-02 19:06:30,205	ManagedIdentityCredential will use IMDS; 2022-03-02 19:06:30,419	JVM-0: trying to open socket; 2022-03-02 19:06:30,420	JVM-1: trying to open socket; 2022-03-02 19:06:30,421	JVM-2: trying to open socket; 2022-03-02 19:06:30,421	JVM-3: trying to open socket; 2022-03-02 19:06:30,422	JVM-4: trying to open socket; 2022-03-02 19:06:30,4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:28066,configurat,configuration,28066,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['configurat'],['configuration']
Deployability," while the Cython code preserved it as; <code>//some-path</code>. Now, both do the latter.; <code>[#5498](https://github.com/aio-libs/aiohttp/issues/5498) &lt;https://github.com/aio-libs/aiohttp/issues/5498&gt;</code>_</p>; </li>; </ul>; <hr />; <h1>3.7.3 (2020-11-18)</h1>; <h2>Features</h2>; <ul>; <li>Use Brotli instead of brotlipy; <code>[#3803](https://github.com/aio-libs/aiohttp/issues/3803) &lt;https://github.com/aio-libs/aiohttp/issues/3803&gt;</code>_</li>; <li>Made exceptions pickleable. Also changed the repr of some exceptions.; <code>[#4077](https://github.com/aio-libs/aiohttp/issues/4077) &lt;https://github.com/aio-libs/aiohttp/issues/4077&gt;</code>_</li>; </ul>; <h2>Bugfixes</h2>; <ul>; <li>Raise a ClientResponseError instead of an AssertionError for a blank; HTTP Reason Phrase.; <code>[#3532](https://github.com/aio-libs/aiohttp/issues/3532) &lt;https://github.com/aio-libs/aiohttp/issues/3532&gt;</code>_</li>; <li>Fix <code>web_middlewares.normalize_path_middleware</code> behavior for patch without slash.; <code>[#3669](https://github.com/aio-libs/aiohttp/issues/3669) &lt;https://github.com/aio-libs/aiohttp/issues/3669&gt;</code>_</li>; <li>Fix overshadowing of overlapped sub-applications prefixes.; <code>[#3701](https://github.com/aio-libs/aiohttp/issues/3701) &lt;https://github.com/aio-libs/aiohttp/issues/3701&gt;</code>_</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/0a26acc1de9e1b0244456b7881ec16ba8bb64fc3""><code>0a26acc</code></a> Bump aiohttp to v3.7.4 for a security release</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/021c416c18392a111225bc7326063dc4a99a5138""><code>021c416</code></a> Merge branch 'ghsa-v6wp-4m6f-gcjg' into master</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/4ed7c25b537f71c6245bb74d6b20e5867db243ab""><code>4ed7c25</code></a> Bump chardet from 3.0.4 to 4.0.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10115:5964,patch,patch,5964,https://hail.is,https://github.com/hail-is/hail/pull/10115,1,['patch'],['patch']
Deployability," who are comfortable with the idea of a counter but not with `group_by`. I added a new dataset for doctests and I realized a couple things:; - doctest_write_data.py is not deterministic; - if I add/change one dataset, my commit explodes with changes to all the datasets (see above); - doctest_write_data.py has to be run by *me*, it's not run by CI. I also noticed that when you specify no `row_key` to `import_matrix_table` you get a row key called `row_id`, which is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had to address all the test failures introduced since the last time `doct",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6856:1049,install,installation,1049,https://hail.is,https://github.com/hail-is/hail/pull/6856,2,['install'],"['install', 'installation']"
Deployability," with child resources, LZ4 data compression, custom record sizes, and unsetting volume quotas and reservations.</li>; <li>api-change:<code>fis</code>: [<code>botocore</code>] This release adds logging support for AWS Fault Injection Simulator experiments. Experiment templates can now be configured to send experiment activity logs to Amazon CloudWatch Logs or to an S3 bucket.</li>; <li>api-change:<code>route53-recovery-cluster</code>: [<code>botocore</code>] This release adds a new API option to enable overriding safety rules to allow routing control state updates.</li>; <li>api-change:<code>amplifyuibuilder</code>: [<code>botocore</code>] We are adding the ability to configure workflows and actions for components.</li>; <li>api-change:<code>athena</code>: [<code>botocore</code>] This release adds support for updating an existing named query.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds support for new AMI property 'lastLaunchedTime'</li>; <li>api-change:<code>servicecatalog-appregistry</code>: [<code>botocore</code>] AppRegistry is deprecating Application and Attribute-Group Name update feature. In this release, we are marking the name attributes for Update APIs as deprecated to give a heads up to our customers.</li>; </ul>; <h1>1.21.8</h1>; <ul>; <li>api-change:<code>elasticache</code>: [<code>botocore</code>] Doc only update for ElastiCache</li>; <li>api-change:<code>panorama</code>: [<code>botocore</code>] Added NTP server configuration parameter to ProvisionDevice operation. Added alternate software fields to DescribeDevice response</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/261b0f2ffe079b6940d683657fcad358195f882e""><code>261b0f2</code></a> Merge branch 'release-1.21.12'</li>; <li><a href=""https://github.com/boto/boto3/commit/44f4f5ef0b66d1a508685b62388f1e4a7d60dace""><code>44f4f5e</code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11486:4194,release,release,4194,https://hail.is,https://github.com/hail-is/hail/pull/11486,2,"['release', 'update']","['release', 'update']"
Deployability," | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-WHEEL-3180413](https://snyk.io/vuln/SNYK-PYTHON-WHEEL-3180413) | `wheel:` <br> `0.30.0 -> 0.38.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI2NTQzMzZlYi02MmRmLTQ0ODAtOTFkOS0xZDg4N2FmNmQwMTUiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjY1NDMzNmViLTYyZGYtNDQ4MC05MWQ5LTFkODg3YWY2ZDAxNSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:10743,upgrade,upgraded,10743,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['upgrade'],['upgraded']
Deployability," | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-WHEEL-3180413](https://snyk.io/vuln/SNYK-PYTHON-WHEEL-3180413) | `wheel:` <br> `0.30.0 -> 0.38.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3YmFjNzAzOC00ZmQzLTQ3YmItOGUwMy0yNjRmYTUxNDRlNGQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjdiYWM3MDM4LTRmZDMtNDdiYi04ZTAzLTI2NGZhNTE0NGU0ZCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14108:9727,upgrade,upgraded,9727,https://hail.is,https://github.com/hail-is/hail/pull/14108,1,['upgrade'],['upgraded']
Deployability," | amazon-ebs: --exclude 'docs/' \; 883 | amazon-ebs: --exclude 'dist/' \; 884 | amazon-ebs: --exclude 'test/' \; 885 | amazon-ebs: --exclude '*.log' \; 886 | amazon-ebs: python/ build/deploy/; 887 | amazon-ebs: # Clear the bdist build cache before building the wheel; 888 | amazon-ebs: cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; 889 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-ebs: Downloading aiohttp_session-2.7.0-py3-none-any.whl (14 kB); 899 | amazon-ebs: Collecting asyncinit<0.3,>=0.2.4; 900 | amazon-ebs: Downloading asyncinit-0.2.4-py3-none-any.whl (2.8 kB); 901 | amazon-ebs: Collecting avro<1.12,>=1.10; 902 | amazon-ebs: Downloading avro-1.11.1.tar.gz (84 kB); 903 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.2/84.2 kB 22.0 MB/s eta 0:00:00; 904 | amazon-ebs: Installing build dependencies: started; 905 | amazon-ebs: Installing build dependencies: finished with status 'done'; 906 | amazon-ebs: Getting requirements to build wheel: started; 907 | amazon-ebs: Getting requirements to bu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:1629,install,install,1629,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['install'],['install']
Deployability,"!-- raw HTML omitted -->; <h2>2.25.4</h2>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab_server/compare/v2.25.3...15e796699f04e06db9ed23a689d454feae36ffbd"">Full Changelog</a>)</p>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Use updated releaser workflows <a href=""https://redirect.github.com/jupyterlab/jupyterlab_server/pull/442"">#442</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Use json5 typings <a href=""https://redirect.github.com/jupyterlab/jupyterlab_server/pull/441"">#441</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Enforce pytest 7 <a href=""https://redirect.github.com/jupyterlab/jupyterlab_server/pull/439"">#439</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Fix test util typings <a href=""https://redirect.github.com/jupyterlab/jupyterlab_server/pull/437"">#437</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab_server/graphs/contributors?from=2024-02-14&amp;to=2024-03-11&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab_server+involves%3Ablink1073+updated%3A2024-02-14..2024-03-11&amp;type=Issues""><code>@​blink1073</code></a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/jupyterlab/jupyterlab_server/commit/cca5a50acfedfb45a843acf733ddbb410634451e""><code>cca5a50</code></a> Publish 2.26.0</li>; <li><a href=""https://github.com/jupyterlab/jupyterlab_server/commit/2cc9672e751943f5c51af9d4174f0b4d986e74a0""><code>2cc9672</code></a> Ignore pageconfig file if JSON is zero-length (<a href=""https://redirect.github.com/jupyterlab/jupyterlab_server/issues/444"">#444</a>)</li>; <li><a href=""https://github.com/jupyterlab/jupyterlab_server/commit/8d036f520c8546b8c1c5208b32e097df",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14483:6140,release,release,6140,https://hail.is,https://github.com/hail-is/hail/pull/14483,1,['release'],['release']
Deployability,"![Finally.](https://media.giphy.com/media/yIsbuPCEOgNHO/giphy.gif). - update endpoints to handle the ""zen"" that GitHub sends when a web hook is created. - update `make run-local` and friends for the new IP of the `dk-test` micro instance. - remove the unused `refresh_statuses` (this was intended to recover build state from github's commit statuses, but the commit status description is limited to like 120 characters, so I gave up on this a while ago, but never removed the code). - `.strip()` the GitHub token in case there are newlines. - print the SHA being deployed in the log statement. - add `hail-ci-build.sh` to CI, which just invokes `make test-in-cluster`(which in turn runs `test-in-cluster.sh`. - `test-in-cluster.sh` copies the secrets for testing to the expected locations and exposes the pod in which it is running with an internal service, recent changes to `site` [redirect sub URLs of ci.test.is to services named using this scheme](https://github.com/hail-is/hail/blob/master/site/hail.nginx.conf#L38-L41). GitHub uses these URLs to send updates to the CI under test about the watched repositories. - `test-locally.sh` now installs `../batch` into the currently running `pip` before testing (NB: if you edit batch and run the tests without committing the changes you've made to batch, this will pass tests but fail when pushed to a PR!). - `test-locally.sh` activates the `hail-ci` conda environment itself because it was not being propagated from the `Makefile`. I don't know why, but this is a simple fix. - `test-locally.sh` starts the ci after the repository is created. CI will print error messages if a watched repository doesn't exist. - `test/test-ci.py` now uses access tokens for all interaction with GitHub, previously it relied on the latent privileges that I and Cotton had in our environments. - `test/test-ci.py` uses a temporary, but not automatically deleted, directory when the environment variable `IN_CLUSTER` is set to `true` (to which it is set by `test-in-c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4474:70,update,update,70,https://hail.is,https://github.com/hail-is/hail/pull/4474,3,"['deploy', 'update']","['deployed', 'update']"
Deployability,"![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **561/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.5 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **519/1000** <br/> **Why?** Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI5ZjJhMGZlMy1kYmVkLTQ2YzAtYmQyMC0yMjM3NzFiYzE0OTciLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjlmMmEwZmUzLWRiZWQtNDZjMC1iZDIwLTIyMzc3MWJjMTQ5NyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:10313,upgrade,upgraded,10313,https://hail.is,https://github.com/hail-is/hail/pull/14327,1,['upgrade'],['upgraded']
Deployability,""" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""4d1e728e-269c-49a2-a2d0-bf1c04966e29"",""prPublicId"":""4d1e728e-269c-49a2-a2d0-bf1c04966e29"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494,539],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14026:5457,patch,patch,5457,https://hail.is,https://github.com/hail-is/hail/pull/14026,2,"['patch', 'upgrade']","['patch', 'upgrade']"
Deployability,""",""from"":""0.8.4"",""to"":""2.0.3""},{""name"":""nbconvert"",""from"":""5.6.1"",""to"":""6.3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13835:9523,patch,patch,9523,https://hail.is,https://github.com/hail-is/hail/pull/13835,2,"['patch', 'upgrade']","['patch', 'upgrade']"
Deployability,""",""from"":""0.8.4"",""to"":""2.0.3""},{""name"":""nbconvert"",""from"":""5.6.1"",""to"":""6.3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13866:9549,patch,patch,9549,https://hail.is,https://github.com/hail-is/hail/pull/13866,2,"['patch', 'upgrade']","['patch', 'upgrade']"
Deployability,""":""13e2c460-fe06-4099-adab-e1f8fa931de0"",""prPublicId"":""13e2c460-fe06-4099-adab-e1f8fa931de0"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""42.0.2""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""SNYK-PYTHON-CRYPTOGRAPHY-5663682"",""SNYK-PYTHON-CRYPTOGRAPHY-5777683"",""SNYK-PYTHON-CRYPTOGRAPHY-5813745"",""SNYK-PYTHON-CRYPTOGRAPHY-5813746"",""SNYK-PYTHON-CRYPTOGRAPHY-5813750"",""SNYK-PYTHON-CRYPTOGRAPHY-5914629"",""SNYK-PYTHON-CRYPTOGRAPHY-6036192"",""SNYK-PYTHON-CRYPTOGRAPHY-6050294"",""SNYK-PYTHON-CRYPTOGRAPHY-6092044"",""SNYK-PYTHON-CRYPTOGRAPHY-6126975"",""SNYK-PYTHON-CRYPTOGRAPHY-6210214"",""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[554,704,509,454,616,584,479,509,509,509,509,589,509,691,399,479,399,539,479,479,616,616,489,519],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Use After Free](https://learn.snyk.io/lesson/use-after-free/?loc&#x3D;fix-pr); 🦉 [Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;)](https://learn.snyk.io/lesson/type-confusion/?loc&#x3D;fix-pr); 🦉 [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14329:11931,patch,patch,11931,https://hail.is,https://github.com/hail-is/hail/pull/14329,2,"['patch', 'upgrade']","['patch', 'upgrade']"
Deployability,""":""9f2a0fe3-dbed-46c0-bd20-223771bc1497"",""prPublicId"":""9f2a0fe3-dbed-46c0-bd20-223771bc1497"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""42.0.2""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""SNYK-PYTHON-CRYPTOGRAPHY-5663682"",""SNYK-PYTHON-CRYPTOGRAPHY-5777683"",""SNYK-PYTHON-CRYPTOGRAPHY-5813745"",""SNYK-PYTHON-CRYPTOGRAPHY-5813746"",""SNYK-PYTHON-CRYPTOGRAPHY-5813750"",""SNYK-PYTHON-CRYPTOGRAPHY-5914629"",""SNYK-PYTHON-CRYPTOGRAPHY-6036192"",""SNYK-PYTHON-CRYPTOGRAPHY-6050294"",""SNYK-PYTHON-CRYPTOGRAPHY-6092044"",""SNYK-PYTHON-CRYPTOGRAPHY-6126975"",""SNYK-PYTHON-CRYPTOGRAPHY-6210214"",""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[554,704,509,454,616,584,479,509,509,509,509,589,509,691,399,479,399,539,479,479,616,616,561,519],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Use After Free](https://learn.snyk.io/lesson/use-after-free/?loc&#x3D;fix-pr); 🦉 [Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;)](https://learn.snyk.io/lesson/type-confusion/?loc&#x3D;fix-pr); 🦉 [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:11943,patch,patch,11943,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,"['patch', 'upgrade']","['patch', 'upgrade']"
Deployability,""">#11762</a> [component: docs] Replace slash with backslash for PS commands</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11767"">#11767</a> [component: bokehjs] Upgrade jquery-ui to resolve security concerns</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11781"">#11781</a> [component: examples] fix transform jitter example</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11786"">#11786</a> bokeh 2.4.2 backports</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11790"">#11790</a> [component: build] Bryanv/pin sphinx 42</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11797"">#11797</a> Add OS to bokeh info</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11805"">#11805</a> More 3.0 -&gt; 2.4.2 backports</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11810"">#11810</a> [component: docs] Update docs for new issue forms</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11824"">#11824</a> Updates for release</li>; </ul>; </li>; </ul>; <h2>2021-10-13 2.4.1:</h2>; <ul>; <li>; <p>bugfixes:</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11119"">#11119</a> [component: bokehjs] [BUG] varea_stack() and vline_stack() fails to update correctly when new source data is different length</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11625"">#11625</a> [component: tests] [BUG] Codebase test failures in Windows</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11627"">#11627</a> [BUG] mypy tests fail in Windows</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11629"">#11629</a> [BUG] Hover tool takes long time to render</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11633"">#11633</a> [component: bokehjs] [BUG] RangesUpdate ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11540:2797,Update,Update,2797,https://hail.is,https://github.com/hail-is/hail/pull/11540,1,['Update'],['Update']
Deployability,""">#7187</a>; [radarhere]</p>; </li>; <li>; <p>Fixed saving TIFF multiframe images with LONG8 tag types <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7078"">#7078</a>; [radarhere]</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python-pillow/Pillow/commit/6e28ed1f36d0eb74053af54e1eddc9c29db698cd""><code>6e28ed1</code></a> 10.0.0 version bump</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/c827f3b30f50bf04fd65daeeba6bbfd56fc7b50e""><code>c827f3b</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7246"">#7246</a> from radarhere/deallocate</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/39a3b1d83edcf826c3864e26bedff5b4e4dd331b""><code>39a3b1d</code></a> Fixed deallocating mask images</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/8c1dc819fd91471825da01976ac0e0bc8789590f""><code>8c1dc81</code></a> Update CHANGES.rst [ci skip]</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/e37b25087d39bd54495380a9898c8c7a2a4698d1""><code>e37b250</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7244"">#7244</a> from radarhere/imagefont_max_string_length</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/d398fedb9d5af22316c715d2066176d15031d439""><code>d398fed</code></a> Added underscores for readability</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/1fe1bb49c452b0318cad12ea9d97c3bef188e9a7""><code>1fe1bb4</code></a> Added ImageFont.MAX_STRING_LENGTH</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/7c945f5131cf8596084b32af582f90a43b090540""><code>7c945f5</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7243"">#7243</a> from radarhere/releasenotes</li>; <li><a href=""https://github.com/python-pillow/Pillow",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:13172,Update,Update,13172,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['Update'],['Update']
Deployability,"""><code>2ef893a</code></a> Merge <a href=""https://github.com/jaraco/skeleton"">https://github.com/jaraco/skeleton</a></li>; <li><a href=""https://github.com/python/importlib_metadata/commit/97e0293b8bf317b54f49c25add7d44830f9180fe""><code>97e0293</code></a> In _read_egg_info_reqs, when requires.txt exists but is empty, return an empt...</li>; <li>Additional commits viewable in <a href=""https://github.com/python/importlib_metadata/compare/0.1...v4.11.3"">compare view</a></li>; </ul>; </details>; <br />. Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11596:5964,upgrade,upgrade,5964,https://hail.is,https://github.com/hail-is/hail/pull/11596,3,['upgrade'],['upgrade']
Deployability,"""><code>69527bc</code></a> bookworm is py311 now (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8200"">#8200</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/111deefb659b8d73c56d3ce89458f2df973d60e4""><code>111deef</code></a> backport main branch CI to 39.0.x (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8153"">#8153</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/338a65a7df74e189f6b5d1d3a6315ffa911b21c2""><code>338a65a</code></a> 39.0.0 version bump (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/7954"">#7954</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/84a3cd7abb16f594d8c315e8aedb4be02583bf6a""><code>84a3cd7</code></a> automatically download and upload circleci wheels (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/7949"">#7949</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/525c0b3d5d89eab7f953be5de5d2b75da1c816f8""><code>525c0b3</code></a> Type annotate release.py (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/7951"">#7951</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/46d2a94d1b574abf5b9e88f84fa7400a138c4edb""><code>46d2a94</code></a> Use the latest 3.10 release when wheel building (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/7953"">#7953</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/f150dc15582c05b1b94cf08ed3b1fbc9c4f52267""><code>f150dc1</code></a> fix CI to work with ubuntu 22.04 (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/7950"">#7950</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/8867724b2b6db528d2900414ef86c122a1f5602a""><code>8867724</code></a> fix README for python3 (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/7947"">#7947</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/pyca/crypto",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12668:5108,release,release,5108,https://hail.is,https://github.com/hail-is/hail/pull/12668,4,['release'],['release']
Deployability,"""><code>@​asottile</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2063"">#2063</a> issue by <a href=""https://github.com/a666""><code>@​a666</code></a>.</li>; </ul>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pre-commit/pre-commit/blob/master/CHANGELOG.md"">pre-commit's changelog</a>.</em></p>; <blockquote>; <h1>2.17.0 - 2022-01-18</h1>; <h3>Features</h3>; <ul>; <li>add warnings for regexes containing <code>[\\/]</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2151"">#2151</a> issue by <a href=""https://github.com/sanjioh""><code>@​sanjioh</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2154"">#2154</a> PR by <a href=""https://github.com/kuviokelluja""><code>@​kuviokelluja</code></a>.</li>; </ul>; </li>; <li>upgrade supported ruby versions.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2205"">#2205</a> PR by <a href=""https://github.com/jalessio""><code>@​jalessio</code></a>.</li>; </ul>; </li>; <li>allow <code>language: conda</code> to use <code>mamba</code> or <code>micromamba</code> via; <code>PRE_COMMIT_USE_MAMBA=1</code> or <code>PRE_COMMIT_USE_MICROMAMBA=1</code> respectively.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2204"">#2204</a> issue by <a href=""https://github.com/janjagusch""><code>@​janjagusch</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2207"">#2207</a> PR by <a href=""https://github.com/xhochy""><code>@​xhochy</code></a>.</li>; </ul>; </li>; <li>display <code>git --version</code> in error report.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2210"">#2210</a> PR by <a href=""https://github.com/asottile""><co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11460:7240,upgrade,upgrade,7240,https://hail.is,https://github.com/hail-is/hail/pull/11460,2,['upgrade'],['upgrade']
Deployability,""">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pyasn1-modules&package-manager=pip&previous-version=0.3.0&new-version=0.4.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14472:3710,upgrade,upgrade,3710,https://hail.is,https://github.com/hail-is/hail/pull/14472,3,['upgrade'],['upgrade']
Deployability,"""deploy"" is a really overloaded term when it comes to CI and release much more accurately reflects that this step does.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13931:1,deploy,deploy,1,https://hail.is,https://github.com/hail-is/hail/pull/13931,2,"['deploy', 'release']","['deploy', 'release']"
Deployability,"""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIyN2MzNWY4NC0yNDIyLTRmNzUtYWMxYy1mODQxOGJmNzRlMzciLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjI3YzM1Zjg0LTI0MjItNGY3NS1hYzFjLWY4NDE4YmY3NGUzNyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""27c35f84-2422-4f75-ac1c-f8418bf74e37"",""prPublicId"":""27c35f84-2422-4f75-ac1c-f8418bf74e37"",""dependencies"":[{""name"":""cryptography"",""from"":""41.0.7"",""to"":""42.0.2""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CRYPTOGRAPHY-6149518"",""SNYK-PYTHON-CRYPTOGRAPHY-6157248"",""SNYK-PYTHON-CRYPTOGRAPHY-6210214""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[509,581,451],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Use of a Broken or Risky Cryptographic Algorithm](https://learn.snyk.io/lesson/insecure-hash/?loc&#x3D;fix-pr); 🦉 [Uncontrolled Resource Consumption (&#x27;Resource Exhaustion&#x27;)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [NULL Pointer Dereference](https://learn.snyk.io/lesson/null-dereference/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14234:4000,patch,patch,4000,https://hail.is,https://github.com/hail-is/hail/pull/14234,3,"['patch', 'update', 'upgrade']","['patch', 'updated-fix-title', 'upgrade']"
Deployability,"""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1708"">#1708</a>) (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1769"">#1769</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/e0efa85c3cc7a0a092ab96a13f121b2d0e553c38""><code>e0efa85</code></a> test(deps): update dependency com.google.cloud:google-cloud-pubsub to v1.120....</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/feb9f06d7031915ce50a609f99a4d885e2b21f34""><code>feb9f06</code></a> test(deps): update dependency com.google.api.grpc:proto-google-cloud-pubsub-v...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/b05ee42b24bb8b18b7cfcfd921a6a4f70d930ad2""><code>b05ee42</code></a> test(deps): update testbench version to v0.32.0 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1768"">#1768</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/8ea8131d17eba29859518da7199bbd03019d0644""><code>8ea8131</code></a> chore: update google-auth to 2.14.1 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1703"">#1703</a>) (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1767"">#1767</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/googleapis/java-storage/compare/v1.106.0...v2.15.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.google.cloud:google-cloud-storage&package-manager=gradle&previous-version=1.106.0&new-version=2.15.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12529:13697,update,update,13697,https://hail.is,https://github.com/hail-is/hail/pull/12529,1,['update'],['update']
Deployability,"""https://github-redirect.dependabot.com/lepture/mistune/issues/307"">#307</a> from jieter/patch-1</li>; <li><a href=""https://github.com/lepture/mistune/commit/0eba47196a81453bafe1f2492748a87475063dff""><code>0eba471</code></a> Fix typo in guide.rst</li>; <li><a href=""https://github.com/lepture/mistune/commit/61e9337884e20f9f8fdc0b7788d319afdd259729""><code>61e9337</code></a> Fix table plugin</li>; <li><a href=""https://github.com/lepture/mistune/commit/76dec68c4514c2612ef9263b49c6ec7f4d77bd14""><code>76dec68</code></a> Add documentation for renderer heading when TOC enabled</li>; <li>Additional commits viewable in <a href=""https://github.com/lepture/mistune/compare/v0.8.4...v2.0.4"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=mistune&package-manager=pip&previous-version=0.8.4&new-version=2.0.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12066:4242,update,updates,4242,https://hail.is,https://github.com/hail-is/hail/pull/12066,2,['update'],['updates']
Deployability,"""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2301"">#2301</a> from jeff-m-sullivan/rscript-path</li>; <li><a href=""https://github.com/pre-commit/pre-commit/commit/d65016042b67d09139876e1903e839a168dfa7c3""><code>d650160</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2252"">#2252</a> from daschuer/worktree_fix</li>; <li><a href=""https://github.com/pre-commit/pre-commit/commit/fd0177ae3ae5f94b36aafb54ab496f76fcead7b9""><code>fd0177a</code></a> implement default_install_hook_types</li>; <li>Additional commits viewable in <a href=""https://github.com/pre-commit/pre-commit/compare/v2.17.0...v2.18.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pre-commit&package-manager=pip&previous-version=2.17.0&new-version=2.18.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11731:13161,update,updates,13161,https://hail.is,https://github.com/hail-is/hail/pull/11731,1,['update'],['updates']
Deployability,"""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9819"">#9819</a>)</li>; </ul>; <h1>Ruby</h1>; <ul>; <li>Disable the aarch64 build on macOS until it can be fixed. (<a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9816"">#9816</a>)</li>; </ul>; <h1>Other</h1>; <ul>; <li>Fix versioning issues in 3.20.0</li>; </ul>; <h2>Protocol Buffers v3.20.1-rc1</h2>; <p>#PHP</p>; <ul>; <li>Fix building packaged PHP extension (<a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9727"">#9727</a>)</li>; </ul>; <p>#Other</p>; <ul>; <li>Fix versioning issues in 3.20.0</li>; </ul>; <h2>Protocol Buffers v3.20.0</h2>; <p>2022-03-25 version 3.20.0 (C++/Java/Python/PHP/Objective-C/C#/Ruby/JavaScript)</p>; <h1>Ruby</h1>; <ul>; <li>Dropped Ruby 2.3 and 2.4 support for CI and releases. (<a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9311"">#9311</a>)</li>; <li>Added Ruby 3.1 support for CI and releases (<a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9566"">#9566</a>).</li>; <li>Message.decode/encode: Add recursion_limit option (<a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9218"">#9218</a>/<a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9486"">#9486</a>)</li>; <li>Allocate with xrealloc()/xfree() so message allocation is visible to the; Ruby GC. In certain tests this leads to much lower memory usage due to more; frequent GC runs (<a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9586"">#9586</a>).</li>; <li>Fix conversion of singleton classes in Ruby (<a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9342"">#9342</a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/protocolbuffers/protobuf/commits"">c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12563:2917,release,releases,2917,https://hail.is,https://github.com/hail-is/hail/pull/12563,1,['release'],['releases']
Deployability,"""https://github.com/Azure/azure-sdk-for-python/commit/5b6ff12ffbb3cc440bc73b4e714ec260ab0f03ac""><code>5b6ff12</code></a> [Storage] Update <code>rename_directory</code> lease param name (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/23411"">#23411</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/4513dbda0f23964a7690eee938f9fdaf91cf33b8""><code>4513dbd</code></a> [Storage] Fix duplicate type signatures in async (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/23375"">#23375</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/1ed6c301cff55f240cc968eb2482a58a688ef352""><code>1ed6c30</code></a> [pipeline] update existing README.md to drop py2 (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/23332"">#23332</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/f7136780995ccd504770976549d603b7e6187ced""><code>f713678</code></a> [Storage] Remove batch delete files feature for GA release (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/23320"">#23320</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/8479758cd6b1daf4360cf3de19bea70bcb6895dd""><code>8479758</code></a> [Storage] Address comments from API Review for March release (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/23294"">#23294</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/eeb2f3b8c7c115337f9d71166aca91db732c931e""><code>eeb2f3b</code></a> [Storage] Add missing SAS permissions to Storage packages (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/23179"">#23179</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/038b35f890d2363edc1254ac2ee61918b2b84b66""><code>038b35f</code></a> [Storage] Fix bug with <code>ignore_read_only</code> in <code>start_copy_from_url()</code> (<",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11610:3973,release,release,3973,https://hail.is,https://github.com/hail-is/hail/pull/11610,2,['release'],['release']
Deployability,"""https://github.com/Azure/azure-sdk-for-python/commit/dc7c5a16d39df8a8d4b838a7240e58f64fc824f2""><code>dc7c5a1</code></a> [Storage] API View Feedback For STG84 GA (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/25085"">#25085</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/9f66f6bce7d777b34c03dc9a633148acd0c4f238""><code>9f66f6b</code></a> [Storage] Revert removing aiohttp dependency for storage.blob.aio (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/25084"">#25084</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/e40d3e1d985cee13a2e0d070fb8e04958905f468""><code>e40d3e1</code></a> [storage.blob] Remove aiohttp as dependency for storage.blob.aio (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/24965"">#24965</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/7915719f211cc1217dfca6f3973a2b1f04c2e3f5""><code>7915719</code></a> [Storage] Prepare for STG83 GA release (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/25040"">#25040</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/155eb8b69b3cd2f8ef992cf1436bf2751769ac42""><code>155eb8b</code></a> [Storage] Add <code>progress_hook</code> to file-share upload/download (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/24997"">#24997</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/66dd3bef2d6e531e83cefd65be4cbbf41fcf2531""><code>66dd3be</code></a> [Storage] Fix more flaky lease tests (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/25011"">#25011</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/030141734a239fa6fb1aa7a8c43d322c82753510""><code>0301417</code></a> [Storage] Add argument to perf tests to use client-side encryption (<a href=""https://github-redirect.dependabot.co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12109:3431,release,release,3431,https://hail.is,https://github.com/hail-is/hail/pull/12109,1,['release'],['release']
Deployability,"""https://github.com/jupyter/jupyter_client/releases"">jupyter-client's releases</a>.</em></p>; <blockquote>; <h2>v8.0.2</h2>; <h2>8.0.2</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v8.0.1...717d36edcd9ce595f727d8b5a27e270c2a6e2c46"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Add papermill downstream check and fix kernel client replies <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/925"">#925</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Adopt more ruff rules <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/924"">#924</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Prefer print in kernelspecapp <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/923"">#923</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2023-01-26&amp;to=2023-01-30&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2023-01-26..2023-01-30&amp;type=Issues""><code>@​blink1073</code></a></p>; <h2>v8.0.1</h2>; <h2>8.0.1</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v8.0.0...dc6113c360e05122430b8e130374e9f4e4b701d7"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Fix json_output in kernelspec app <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/921"">#921</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2023-01-26&amp;to=2023-01-26&amp;type=c"">GitHub contributors page for this release</a>)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12656:1182,release,release,1182,https://hail.is,https://github.com/hail-is/hail/pull/12656,1,['release'],['release']
Deployability,"""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Agithub-actions+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ajtpio+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Akrassowski+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ameeseeksmachine+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​meeseeksmachine</code></a></p>; <!-- raw HTML omitted -->; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/jupyter/notebook/commit/80e992e9f4cfa6cc1fcf4d84cebe09d53e769790""><code>80e992e</code></a> Publish 7.0.7</li>; <li><a href=""https://github.com/jupyter/notebook/commit/089c78c48fd00b2b0d2f33e4463eb42018e86803""><code>089c78c</code></a> Update to JupyterLab 4.0.11 (<a href=""https://redirect.github.com/jupyter/notebook/issues/7215"">#7215</a>)</li>; <li><a href=""https://github.com/jupyter/notebook/commit/109ba7578886283ad7be54d189904132bd7bb6f0""><code>109ba75</code></a> Backport PR <a href=""https://redirect.github.com/jupyter/notebook/issues/7176"">#7176</a>: Update publish-release workflow for PyPI trusted publisher...</li>; <li><a href=""https://github.com/jupyter/notebook/commit/d252423198e3bce218fd4c370a706f373dcb4c78""><code>d252423</code></a> Update ruff config and typing (<a href=""https://redirect.github.com/jupyter/notebook/issues/7145"">#7145</a>) (<a href=""https://redirect.github.com/jupyter/notebook/issues/7186"">#7186</a>)</li>; <li><a href=""https://github.com/jupyter/notebook/commit/d2ef92f0b385b7ecd11cbf0f3af181ee8e494623""><code>d2ef92f</code></a> Backport PR <a href=""https://redirect.github.com/jupyter/notebook/issues/7142"">#7142</a>: Clean up lint handling (<a href=""https:/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14182:6124,Update,Update,6124,https://hail.is,https://github.com/hail-is/hail/pull/14182,1,['Update'],['Update']
Deployability,"""https://github.com/sphinx-doc/sphinx/releases"">sphinx's releases</a>.</em></p>; <blockquote>; <h2>Sphinx 7.1.0</h2>; <p>Changelog: <a href=""https://www.sphinx-doc.org/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v7.0.1</h2>; <p>Changelog: <a href=""https://www.sphinx-doc.org/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v7.0.0</h2>; <p>Changelog: <a href=""https://www.sphinx-doc.org/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v7.0.0rc1</h2>; <p>Changelog: <a href=""https://www.sphinx-doc.org/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/sphinx-doc/sphinx/blob/master/CHANGES"">sphinx's changelog</a>.</em></p>; <blockquote>; <h1>Release 7.1.0 (released Jul 24, 2023)</h1>; <h2>Incompatible changes</h2>; <ul>; <li>; <p>Releases are no longer signed, given the <code>change in PyPI policy</code>_.</p>; <p>.. _change in PyPI policy: <a href=""https://blog.pypi.org/posts/2023-05-23-removing-pgp/"">https://blog.pypi.org/posts/2023-05-23-removing-pgp/</a></p>; </li>; </ul>; <h2>Deprecated</h2>; <ul>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11412"">#11412</a>: Emit warnings on using a deprecated Python-specific index entry type; (namely, <code>module</code>, <code>keyword</code>, <code>operator</code>, <code>object</code>, <code>exception</code>,; <code>statement</code>, and <code>builtin</code>) in the :rst:dir:<code>index</code> directive, and; set the removal version to Sphinx 9. Patch by Adam Turner.</li>; </ul>; <h2>Features added</h2>; <ul>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11415"">#11415</a>: Add a checksum to JavaScript and CSS asset URIs included within; generated HTML, using the CRC32 algorithm.</li>; <li>:meth:<code>~sphinx.application.Sphinx",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13295:1074,release,released,1074,https://hail.is,https://github.com/hail-is/hail/pull/13295,2,"['Release', 'release']","['Releases', 'released']"
Deployability,"""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[554,704,624,531,604,589,726,434,589,449,696,589,479,519,509,711,701,586,586,384,494,539,589],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14108:11795,patch,patch,11795,https://hail.is,https://github.com/hail-is/hail/pull/14108,2,"['patch', 'upgrade']","['patch', 'upgrade']"
Deployability,"""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:862"", ""message"": ""batch 9 cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,958"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,967"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,969"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:501"", ""message"": ""job (9, 1, 'main') changed state: Ready -> Cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,974"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""_delete_pvc:251"", ""message"": ""deleting persistent volume claim batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,976"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,977"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6617:2728,update,update,2728,https://hail.is,https://github.com/hail-is/hail/issues/6617,1,['update'],['update']
Deployability,"""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JINJA2-6150717"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PROMPTTOOLKIT-6141120"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494,539,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Cross-site Scripting (XSS)](https://learn.snyk.io/lesson/xss/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14257:10831,patch,patch,10831,https://hail.is,https://github.com/hail-is/hail/pull/14257,2,"['patch', 'upgrade']","['patch', 'upgrade']"
Deployability,"""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""40.5.0"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.32.2"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JINJA2-6150717"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PROMPTTOOLKIT-6141120"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494,539,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Cross-site Scripting (XSS)](https://learn.snyk.io/lesson/xss/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14364:11184,patch,patch,11184,https://hail.is,https://github.com/hail-is/hail/pull/14364,2,"['patch', 'upgrade']","['patch', 'upgrade']"
Deployability,# Batch Inter-Job File Dependencies. The important addition of this PR is a `copy_service_account_name` field on batch jobs that permits the client to authorize with some GCP credentials stored in a k8s secret. The copy pods and the main pod only share the `/io` folder (k8s forbids mounting a volume as `/`). This interface is pretty onerous. Pipelines will be updated to use this interface. Pipelines is becoming the easy-to-use interface to batch.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5574:344,Pipeline,Pipelines,344,https://hail.is,https://github.com/hail-is/hail/pull/5574,3,"['Pipeline', 'update']","['Pipelines', 'updated']"
Deployability,"# Changes; - include the GCP profiler utility in a new service-base only docker requirements; - include necessary `build-essentials` (in particular, `gcc`) in the service-base image; - enable profiler utility for batch driver when HAIL_SHOULD_PROFILE is set, which is set on dev deploy or deploy. # Profiler Information; We have [enough quota](https://console.cloud.google.com/iam-admin/quotas?_ga=2.207444499.1054557266.1580858577-1098760465.1578424762&project=hail-vdc&folder&organizationId=548622027621&service=cloudprofiler.googleapis.com) to handle a substantial number of parallel profiled batch-driver instances. I don't think we have enough to handle a deluge of PRs, so I disabled profiling of PRs. Stackdriver Profiling is currently free, but we should keep an eye on it. When it becomes a paid product, we may reconsider these settings. # Example. I ran a pipeline with 1000 `/bin/true` jobs and then went to the [profiler page](; https://console.cloud.google.com/profiler;timespan=10m;end=2020-02-03T20:50:00.000Z/batch-driver/CPU?project=hail-vdc).; <img width=""1274"" alt=""Screen Shot 2020-02-03 at 3 55 55 PM"" src=""https://user-images.githubusercontent.com/106194/73690123-acbb7e80-469d-11ea-83e6-c3499c3e3c87.png"">. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8030:279,deploy,deploy,279,https://hail.is,https://github.com/hail-is/hail/pull/8030,3,"['deploy', 'pipeline']","['deploy', 'pipeline']"
Deployability,# EDIT: See end of thread for new approach. The issue is that `use OpenBLAS` puts `libopenblas.so` on `LD_LIBRARY_PATH`. `netlib-java` links against `libblas.so` which is expected to contain the CBLAS symbols. It appears that Debian-style distributions use `update-alternatives` to symlink `libblas.so` to a library of the user's choice. Broad's UGER cluster does not provide such `update-alternatives` functionality. There exists two fixes:; - create a symlink to `libopenblas.so` named `libblas.so` and put it on the LD_LIBRARY_PATH; - use `LD_PRELOAD` to forcibly load `libopenblas.so`. The two solutions look like:. 1.; ```; mkdir ~/lib; ln -s /broad/software/free/Linux/redhat_7_x86_64/pkgs/openblas_0.2.20/lib/libopenblas.so ~/lib/libblas.so.3; export LD_LIBARRY_PATH=~/lib:$LD_LIBRARY_PATH; ```. 2.; ```; export LD_PRELOAD=/broad/software/free/Linux/redhat_7_x86_64/pkgs/openblas_0.2.20/lib/libopenblas.so; ```. Clearly neither of these are ideal. I recommend users place the lines from option 1 in an rc file.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5559#issuecomment-472953523:258,update,update-alternatives,258,https://hail.is,https://github.com/hail-is/hail/issues/5559#issuecomment-472953523,2,['update'],['update-alternatives']
Deployability,"# Overview. Query on Batch fundamentally eliminates Spark in favor of Hail Batch. Each Hail pipeline starts a; single job batch which acts as the ""driver"" or ""compiler"". The driver executes new batches whenever; it encounters a parallelizeAndComputeWithIndex. You can try this branch out in your developer; namespace by executing this:. ```; hailctl dev config set default_namespace default; hailctl dev deploy --branch danking/hail:nqs-copy --steps delete_batch_tables,deploy_batch; make -C query ipython; ```. The last command compiles and uploads a JAR to GCS. These JARs are deleted in 24 hours. If you have; already uploaded a JAR for theee current SHA in the last 24 hours, you can use `make -C query; ipython-no-deps` to start a new session without uploading a new JAR. You can interact at ipython like this:. ```; In [1]: import hail as hl; ...: hl.utils.range_table(10).collect(); Initializing Hail with default parameters...; /Users/dking/projects/hail/hail/python/hail/utils/java.py:54: UserWarning: When using the query service backend, use `await Env._async_hc()'; warnings.warn('When using the query service backend, use `await Env._async_hc()\''); Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.81-edcde5c1b324; LOGGING: writing to /Users/dking/projects/hail/query/hail-20220113-1844-0.2.81-edcde5c1b324.log; Out[1]: ; [Struct(idx=0),; Struct(idx=1),; Struct(idx=2),; Struct(idx=3),; Struct(idx=4),; Struct(idx=5),; Struct(idx=6),; Struct(idx=7),; Struct(idx=8),; Struct(idx=9)]; ```. The very first time you execute this comand, it will run four batches to generate the four default; reference genomes and download those to your local machine. Those four reference genomes are cached; per-SHA on your local machine, so future Hail pipelines will have lower latency. If you have the ""standing working"" enabled, you should expect a latency of ~8s for the above job. I; think we can approximately halve this by re-using classloaders. Ask me more ab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194:92,pipeline,pipeline,92,https://hail.is,https://github.com/hail-is/hail/pull/11194,2,"['deploy', 'pipeline']","['deploy', 'pipeline']"
Deployability,"# logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApprox(h2)'). ##### Reality check that sigma and sdPost are close; sigma; sdPost; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1222,integrat,integrate,1222,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538,1,['integrat'],['integrate']
Deployability,"## Bug fixes to enable Azure deployment. Most of these bugs were discovered in deploying the MySQL server from scratch, specifically deploying version 8.0. ; Some were encountered when we hit certificate issues in trying to run the `./bootstrap.sh deploy_unmanaged` step multiple times within 24hrs. Documentation was clarified in order to resolve this issue. - build.yaml; - Step one fails on rerun since the /repo directory exists, -p to fix; - ci/create_database.py; - In MySQL 8 a new error was introduced [4006](https://dev.mysql.com/doc/mysql-errors/8.0/en/server-error-reference.html#error_er_cannot_user_referenced_as_definer); - This error gets triggered on the CREATE USER IF NOT EXISTS commands for both user and admin if the user was previously created and set a a definer on any events/triggers.; - Really this statement should be a no-op given that the user exists, but for some reason the error triggers anyway.; - To get around this I added a manual check if the user/admin exists and if they do simply skip the create user command. This fixes the bug and allows the MySQL db deploy to finish properly. - dev-docs/letsencrypt.md; - Debugging was confusing since the revoke command addressed ids we were unable to find.; - After extensive searching I added to the documentation how to find your existing cert IDs if you need to revoke them. - infra/azure/README.md; - Added clarity to the Azure deployment documentation. - infra/azure/bootstrap.sh; - Added the passing of additional flag arguments to terraform; - In our case the passing of the `-upgrade` flag to the terraform init step was required in order to continue. - infra/azure/main.tf, infra/azure/modules/batch/main.tf, infra/azure/modules/batch/variables.tf infra/azure/variables.tf; - Add additional argument for the az_storage_account.; - The name must be globally unique in Azure, so the original argument failed on our deployment since it shared the name with the Hail team's Azure deployment",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12065:29,deploy,deployment,29,https://hail.is,https://github.com/hail-is/hail/pull/12065,8,"['deploy', 'upgrade']","['deploy', 'deploying', 'deployment', 'upgrade']"
Deployability,"## Change Description. Config and records relating to the appsec deployment instance. Necessary for future maintenance and management of the appsec instance. ## Security Assessment. - This change has no security impact. ### Impact Description. No impact because this relates only to the parallel appsec instance, not the main production instance. (Reviewers: please confirm the security impact before approving)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14726:65,deploy,deployment,65,https://hail.is,https://github.com/hail-is/hail/pull/14726,1,['deploy'],['deployment']
Deployability,"## Change Description. Replacing pyright with pyright[nodejs] in /dev/requirements.txt, allowing for installation of necessary node.js binaries to run pyright when one installs dev requirements. As per https://pypi.org/project/pyright/, pyright needs node to run, but pip installing pyright doesn't also install node. Pyright acknowledges this, and they have a separate pip installation command that will also give you the necessary node binaries to run pyright. We now specify that proper pyright+node install in our requirements.txt. ## Security Assessment. Delete all except the correct answer:; - This change has a low security impact; - [ ] Required: The impact has been assessed and approved by appsec. ### Impact Description. Updating old pyright installation in /dev/requirements.txt, no impact on security-sensitive systems. (Reviewers: please confirm the security impact before approving)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14741:101,install,installation,101,https://hail.is,https://github.com/hail-is/hail/pull/14741,7,['install'],"['install', 'installation', 'installing', 'installs']"
Deployability,## Change Description. This PR updates the tutorial instructions for running Hail on Dataproc to clarify that the Google Cloud SDK must be installed and configured before Hail can be run. I made this change to prevent future users from also needing to Google these setup instructions. ## Security Assessment. - This change has no security impact. ### Impact Description. This change only updates documentation and has no immediate end user impact. The updated documentation does not leak any proprietary / confidential / sensitive information about Hail or any related systems.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14737:31,update,updates,31,https://hail.is,https://github.com/hail-is/hail/pull/14737,4,"['install', 'update']","['installed', 'updated', 'updates']"
Deployability,"## Change Description. Updates new developer template to include the need for a Google project role. ## Security Assessment. - This change has no security impact. ### Impact Description. None, documentation change only. (Reviewers: please confirm the security impact before approving)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14733:23,Update,Updates,23,https://hail.is,https://github.com/hail-is/hail/pull/14733,1,['Update'],['Updates']
Deployability,"## Change Description. When going through the [documentation for installing Hail on OSX](https://hail.is/docs/0.2/install/macosx.html), I noticed that the syntax for installing Java via Homebrew was out of date. This PR updates the documentation to use the latest syntax for Homebrew. It also updates the command to install version 11 of Temurin instead of version 8. ## Security Assessment. - This change has no security impact. ### Impact Description. This change updates documentation only and has no immediate end user impact. This change updates the recommended version of Temurin to a newer version (11 vs. 8). It is reasonable to assume that the newer version is at least as secure as previous versions. So, there also should be no negative security impact on future users of this documentation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14729:65,install,installing,65,https://hail.is,https://github.com/hail-is/hail/pull/14729,8,"['install', 'update']","['install', 'installing', 'updates']"
Deployability,"## Mill setup. There is a mill wrapper script `millw` checked into the repo in the `hail/` subdirectory. You can either invoke it directly, as we did with `gradlew`, or copy it somewhere on your path and rename it `mill`. This way you can just run `mill ...`, and it will identify the correct version of mill to use for the project you're in and invoke that. For more details on the installation options, see the github for the wrapper script [here](https://github.com/lefou/millw). To verify that it's working, and download the actual mill jar, run `./millw --version` (or `mill --version` if you put it on your path) from the `hail/` subdirectory. It should report version `0.11.6`. ## Mill from the command line. * `mill clean` - delete all output files (in `hail/hail/out`). Or only delete the output of one target, e.g. `mill clean test.compile`; * `mill compile` - compiles the root module (not including tests); * `mill test.compile` - compiles tests (and, transitively, the rest of the root module); * `mill test.test`, or for short `mill test` - run all tests. You can pass options to the test runner (TestNG currently), e.g.* * `mill test -methods is.hail.expr.ir.CallFunctionsSuite.constructors` to run one test, or `mill test -threadcount 4 -parallel classes` to use 4 threads and parallelize over test classes; * `mill test.testOnly is.hail.expr.ir.CallFunctionsSuite` - run all tests in one or more specified classes. You can use `*` to match anything, e.g. `mill test.testOnly ""*.CallFunctionsSuite""`, or `mill test.testOnly ""is.hail.expr.ir.*""`. You can pass options to the test runner (TestNG currently) after a `--`, e.g. `mill test.testOnly ""is.hail.expr.ir.*"" -- -parallel classes`; * `mill __.testCached` - once the codebase is more modularized, will run tests on only modules whose dependencies have changed since the last test run; * `mill reformat` - runs scalafmt on all sources in the root module (currently that's all scala sources, but hopefully not for long). `mill __.ref",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147:383,install,installation,383,https://hail.is,https://github.com/hail-is/hail/pull/14147,1,['install'],['installation']
Deployability,"### Background. In the world of low-level container runtimes there exists the term ""bundle"", which basically means the pair of a root filesystem and a `config.json` file containing all of the other necessary information to run the container. If you invoke `crun` or `runc` with `--bundle /path/to/bundle`, the runtime assumes the following:. - The configuration file for the container is located at `/path/to/bundle/config.json`; - That `config.json` contains a field [`root.path`](https://github.com/opencontainers/runtime-spec/blob/main/config.md#root) that specifies the location of the root filesystem, most commonly as a path relative to `/path/to/bundle`. `crun` offers a way to explicitly reference the location of `config.json` through its `--config` flag. This seems fairly innocuous, but specifying a custom `--config` path can have some unfortunate unintended consequences because it invalidates the assumption in the specification that the configuration resides at `/path/to/bundle/config.json`. Specifically, it breaks [Hooks](https://github.com/opencontainers/runtime-spec/blob/main/config.md#posix-platform-hooks). When a hook is run, the runtime (crun) feeds it the [container state](https://github.com/opencontainers/runtime-spec/blob/main/runtime.md#state), a JSON of information about the container including the `bundle` path. Any hook that attempts to load the `config.json`, like for example, the `nvidia-container-runtime-hook`, will crash. ### Change. This change stops using the `--config` flag for crun and instead does the following to create a well-formed bundle:. - Instead of the bundle being the merged directory of the container overlay, it is the container's scratch directory; - `root.path` is adjusted inside of `config.json` to now point to the merged directory of the container overlay. I've opted to use an absolute path here because why use a relative path.; - Move `config.json` into the container scratch directory so that it is inside the root of the bundle d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13438:348,configurat,configuration,348,https://hail.is,https://github.com/hail-is/hail/pull/13438,2,['configurat'],['configuration']
Deployability,"### Before; <img width=""299"" alt=""Screen Shot 2022-10-06 at 13 32 52"" src=""https://user-images.githubusercontent.com/84595986/194380378-d99890b4-e7ec-4144-84fe-9b8db28aeec9.png"">. ### After; <img width=""301"" alt=""Screen Shot 2022-10-06 at 13 32 31"" src=""https://user-images.githubusercontent.com/84595986/194380470-4d839129-b0d1-4422-b8d3-d350ed3b10f4.png"">. Ignore the missing `[-]` icon; this is a local build of the docs, and I suspect I'm missing a font that it expects me to have installed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12282:485,install,installed,485,https://hail.is,https://github.com/hail-is/hail/pull/12282,1,['install'],['installed']
Deployability,### Change Description. Corrections and updates to the gcp deploy instructions following a fresh deployment in a brand new project. ### Security Assessment. This change has no security impact. No impact because this does not impact the production system. (Reviewers: please confirm the security impact before approving),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14722:40,update,updates,40,https://hail.is,https://github.com/hail-is/hail/pull/14722,3,"['deploy', 'update']","['deploy', 'deployment', 'updates']"
Deployability,"### Change Description. In [09437c5](https://github.com/hail-is/hail/commit/09437c531b47c9af2faa196817c6edeaba17fced), I moved `pytest.ini` up a directory but didn't update the artefacts built in ci.; As a consequence, test stages like `test_unchecked_allocator` have not run since. ### Security Assessment. This change has no security impact",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14721:166,update,update,166,https://hail.is,https://github.com/hail-is/hail/pull/14721,1,['update'],['update']
Deployability,"### Change Description. Jobs within our CI pipeline often invoke `mill` indirectly through `Makefile` prerequisites.; By staging mill's build area to and from `/derived/{release/debug}/hail/out`, mill will not rebuild artefacts from previous steps.; Exposing `MILLOPTS` in `hail/Makefile` allows us to build in CI without a compilation server. ; Using a compilation server may have been why we experienced intermittent failures between building the jar and copying to its final destination.; Note that the `--no-server` option must be the first argument to `millw`. ### Security Assessment; - [x] This change has no security impact. Description of the security impact and necessary mitigations:. Only derived files from the mill + python build process are staged and unstaged.; No secrets or otherwise sensitive information are contained therein. (Reviewers: please confirm the security impact before approving)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14709:43,pipeline,pipeline,43,https://hail.is,https://github.com/hail-is/hail/pull/14709,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,### Change Description. Make the PR template a little easier to work with. ### Security Assessment. - This change has no security impact. Description of the security impact and necessary mitigations:. No impact because this is not code being deployed to production. (Reviewers: please confirm the security impact before approving),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14723:242,deploy,deployed,242,https://hail.is,https://github.com/hail-is/hail/pull/14723,1,['deploy'],['deployed']
Deployability,"### Change Description. The changes in #14708 caused our vep images to be rebuilt, breaking them. The solution here is the one that we should have done from the beginning, install dependencies through the system package manager. In the interest of my sanity and effort, I only added the libraries that were being problematic from CPAN to the list of system installed ones. ### Security Assessment; - [x] This change has no security impact",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14713:172,install,install,172,https://hail.is,https://github.com/hail-is/hail/pull/14713,2,['install'],"['install', 'installed']"
Deployability,"### Create a User. ```; sudo adduser --system teamcity --ingroup www-data; sudo groupadd teamcity; sudo adduser teamcity teamcity; ```. (for some reason it didn't create the `teamcity` group on the first line). ### Move the Binaries. I moved the TeamCity Server and Agent binaries into `/home/teamcity`:. ```; ubuntu@ip-172-31-8-190:/home/teamcity$ ll; total 32; drwxr-xr-x 8 teamcity www-data 4096 Sep 7 18:55 ./; drwxr-xr-x 4 root root 4096 Sep 7 18:16 ../; drwxrwxr-x 6 teamcity teamcity 4096 Sep 7 18:33 .BuildServer/; drwxr-xr-x 5 teamcity www-data 4096 Sep 7 18:55 .gradle/; drwxr-xr-x 13 teamcity teamcity 4096 Aug 22 19:22 TeamCity/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent1/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent2/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent3/; ```. ### Update the `init.d` scripts. #### `/etc/init.d/teamcity`. ```; #!/bin/sh; ### BEGIN INIT INFO; # Provides: teamcity ; # Required-Start: $local_fs $network; # Required-Stop: $local_fs; # Default-Start: 2 3 4 5; # Default-Stop: 0 1 6; # Short-Description: teamcity ; # Description: teamcity build server; ### END INIT INFO; # /etc/init.d/teamcity - startup script for teamcity; export TEAMCITY_DATA_PATH=""/home/teamcity/.BuildServer""; export TEAMCITY_SERVER_OPTS=-Djava.awt.headless=true # Configure TeamCity for use on a headless OS. case $1 in; start); start-stop-daemon --start -c teamcity --exec /home/teamcity/TeamCity/bin/teamcity-server.sh start; ;;. stop); start-stop-daemon --start -c teamcity --exec /home/teamcity/TeamCity/bin/teamcity-server.sh stop; ;;. esac. exit 0; ```. #### `/etc/init.d/teamcityAgents`. ```; #!/bin/bash; ### BEGIN INIT INFO; # Provides: teamcityAgents ; # Required-Start: $local_fs $network; # Required-Stop: $local_fs; # Default-Start: 2 3 4 5; # Default-Stop: 0 1 6; # Short-Description: teamcityAgents ; # Description: TeamCity build agents ; ### END INIT INFO. USER=""teamcity""; AGENTS=(TeamCityAgent1 TeamCityAgent2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/675#issuecomment-245383790:849,Update,Update,849,https://hail.is,https://github.com/hail-is/hail/issues/675#issuecomment-245383790,1,['Update'],['Update']
Deployability,"### Description. In this pull request, I add a function to perform a Cochran-Mantel-Haenszel statistical test for association. This pull request closes #13481. ### Testing. I add unit tests. Since I have not used R before (the [associated GitHub issue](https://github.com/hail-is/hail/issues/13481) suggests using R to create test cases), I created the unit tests from examples that I found on the internet. I linked these sources in the code for the unit tests. I built the documentation locally and inspected it to confirm that it matches my expectations. I am having trouble testing the docstring examples locally. When I run `make -C hail doctest-query`, the tests error due to a checksum exception. ### Discussion. ~I have not added an example to the documentation that uses a matrix table yet. (This is an acceptance criteria in #13481.) I wanted to get some advice about the best way to do this. I think ideally, the example would have a binary phenotype, an allele to test for association, and some stratifying variable. I tried to search through the existing code to find suitable example matrix tables in the docstrings, but I didn't find anything promising. I would appreciate help here.~. Update: thanks to @patrick-schultz's recommendation, I have added an example using a matrix table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255:1201,Update,Update,1201,https://hail.is,https://github.com/hail-is/hail/pull/14255,1,['Update'],['Update']
Deployability,"### Description. In this pull request, I update the docstring for the `Table.head` method to more clearly illustrate what the method does. ### Testing. I ran `make -C hail doctest-query` and confirmed that the test for `hail.table.Table.head` passed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14241:41,update,update,41,https://hail.is,https://github.com/hail-is/hail/pull/14241,1,['update'],['update']
Deployability,"### Description. In this pull request, I update the docstrings for the `sample`, `head`, and `semi_join` methods of the `Table` class with clearer examples that demonstrate the functionality of each method. I also update the unit test for the `sample` method so that it is deterministic and stronger. ### Testing. I ran `make -C hail doctest-query hail/table.py` to confirm that the docstring tests are passing for the methods that I updated.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14242:41,update,update,41,https://hail.is,https://github.com/hail-is/hail/pull/14242,3,['update'],"['update', 'updated']"
Deployability,"### Security Assessment. - [x] This change has no security impact. Description of the security impact and necessary mitigations:; This change doesn't change anything that we actually run, we just split the `make_pr_for` step of the release script into its own file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14715:232,release,release,232,https://hail.is,https://github.com/hail-is/hail/pull/14715,1,['release'],['release']
Deployability,"### Summary of Changes. - make the vectorized IBS tests depend on libsmidpp, I have no idea how this wasn't failing builds before. - fix `generate-build-info.sh` to actually set `BRANCH` instead of setting the `DATE` to the current git branch. Also remove unnecessary parameter passing to `echo_build_properties`. - remove `doctest` as a separate build step (it's now part of python tests). - fix `DOCS_STATUS` in `hail-ci-build.sh`. - remove some unused files: `list_pypi_versions.py`, `publish-to-pypi.sh` (which duplicates, wrongly, `python/deploy.sh`. - simplify `python/deploy.sh`, by managing copied files in make, we no longer clean up files, but the JAR is only 30 MB anyway. - small change to `conftest.py` triggered a bad diff: I added a `try`/`finally` block to ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130:544,deploy,deploy,544,https://hail.is,https://github.com/hail-is/hail/pull/5130,2,['deploy'],['deploy']
Deployability,"### What happened?. # Executive Summary. We will pin to `orjson<=3.9.11` until https://github.com/ijl/orjson/pull/457 merges and addresses the root cause of these segfaults. This issue is resolved when orjson merges orjson#457, releases a new version, and we upgrade to it. # Details. Tests that use the py4j_backend and thus rely on orjson to (de)serialize data have been intermittently segfaulting:; ```; [2024-02-08 22:36:47] test/hail/matrixtable/test_file_formats.py::test_backward_compatability_ht[/io/resources/backward_compatability/1.6.0/table/6.ht/] Fatal Python error: Segmentation fault. Thread 0x00007fa51d817640 (most recent call first):; File ""/usr/lib/python3.9/selectors.py"", line 416 in select; File ""/usr/lib/python3.9/socketserver.py"", line 232 in serve_forever; File ""/usr/lib/python3.9/threading.py"", line 917 in run; File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner; File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap. Thread 0x00007fa5273ff640 (most recent call first):; File ""/usr/local/lib/python3.9/dist-packages/py4j/clientserver.py"", line 58 in run; File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner; File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap. Current thread 0x00007fa52bd6b000 (most recent call first):; File ""/usr/local/lib/python3.9/dist-packages/hail/backend/py4j_backend.py"", line 217 in _rpc; File ""/usr/local/lib/python3.9/dist-packages/hail/backend/backend.py"", line 212 in table_type; ...; ```. Line 217 only does one thing: call `orjson.dumps`. https://github.com/hail-is/hail/blob/b3df76360f931f54688bb03bf5774643c0b8205e/hail/python/hail/backend/py4j_backend.py#L216-L218. Indeed, `orjson` has had [this issue since 3.9.12](https://github.com/ijl/orjson/issues/452) and we just recently updated orjson from 3.9.10 to 3.9.12:. ```; commit d2615543476bde5d01061499c92f26124b85caf3; Author: Dan King <daniel.zidan.king@gmail.com>; Date: Fri Feb 2 14:21:47 2024 -0500. [dependencies] mass upd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14299:228,release,releases,228,https://hail.is,https://github.com/hail-is/hail/issues/14299,2,"['release', 'upgrade']","['releases', 'upgrade']"
Deployability,"### What happened?. # Problem. A user submitted a Hail Batch batch which read from a bucket whose storage class was ""Archive"". This produced a large unexpected spend because Archive class objects cost 0.05 USD per GB whereas Standard class objects are free. # Solution. Hail Batch and Hail Query should collect the set of buckets which were used as reads, imports, input files, or temporary intermediates and assert that the bucket default storage class is the standard class / hot tier. In Google this is called the [Standard storage class](https://cloud.google.com/storage/docs/storage-classes#standard). In Azure, this is called the [Hot tier](https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview?tabs=azure-portal). A user should be able to explicitly override this setting with an allowlist. The allowlist should be initialized using Hail's [standard configuration system](https://github.com/hail-is/hail/blob/48d7b5cfbf9a2231d72dd0a1a682da28422fde4b/hail/python/hailtop/config/user_config.py#L42). In Hail Query, this allowlist should be a setting on `hl.init`. In Hail Batch, it should be a setting on `ServiceBackend`. ### Version. 0.2.115. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13003:879,configurat,configuration,879,https://hail.is,https://github.com/hail-is/hail/issues/13003,1,['configurat'],['configuration']
Deployability,### What happened?. 1. Make a copy of https://docs.google.com/document/d/1deV-i3_oMGBwUreDUKEhovdLawBdf0feshcP1h11wR0/edit; 2. Fill it out; 3. Ping #hail-on-terra in Broad Institute slack to determine next steps for publicly rolling out Batch in Azure-Terra. ### Version. 0.2.126. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13953:225,rolling,rolling,225,https://hail.is,https://github.com/hail-is/hail/issues/13953,1,['rolling'],['rolling']
Deployability,"### What happened?. > Laura Gauthier: I'm struggling with some DRAGEN data that probably doesn't quite meet the VCF spec. I got the import working, but once I go to split multi-allelics, one of the annotations seems to be the wrong length because I get an array index out of bounds exception. Is there anyway to get more info on the variant that's causing the problem? VCFtool validator found a bunch of issues with FORMAT annotations and I've turned them all into count=1 strings, but there must be something else.; > ...; > Tim Poterba (he/him): yeah, the answer is that this isn't a parse failure, it's a failure of the split_multi_hts method to support haploid sex chromosome calls; > Tim Poterba (he/him): the right plan is to support sex chromosomes The Right Way™ and update all of Hail to infer, track, and use appropriate ploidy but that's not at all what the system looks like right now. ### Version. 0.2.117. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13149:775,update,update,775,https://hail.is,https://github.com/hail-is/hail/issues/13149,1,['update'],['update']
Deployability,"### What happened?. A test case of ours (see populationgenomics/production-pipelines#424) newly abends with Hail 0.2.120. In essence, the relevant code does. ```python; combiner = hl.vds.new_combiner(; gvcf_paths=[…a bunch of gvcf_paths…],; output_path=str(vds_path),; […etc…]; ) ; combiner.run(). vds = hl.vds.read_vds(str(vds_path)); vds = hl.vds.split_multi(vds, filter_changed_loci=True); ```. This raises an exception within `split_multi()`:. ```; AttributeError: 'StringExpression' object has no attribute 'is_non_ref'; ```. As can be seen from the full log, this exception occurs when _hail/python/hail/experimental/sparse_mt/sparse_split_multi.py_'s `transform_entries()` looks at the LPGT entry and tries to use a `CallExpression` method on it. Tracing the vds through the combiner code, we can see that the returned data structure lists the LPGT entry as `str` rather than `call` as in Hail 0.2.119:. ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; ----------------------------------------; Entry fields:; 'LA': array<int32>; 'LGT': call; 'LAD': array<int32>; 'LPGT': str # BROKEN, should be ""call""; 'LPL': array<int32>; 'RGQ': int32; 'gvcf_info': struct {; […etc…]; ```. Hence the problem appears to be in the combining and I suspect may have been caused by PR #13206. Any hints on where this field may have been reverted to a `StringExpression` in the new combiner code? I can try to debug this further, or is this enough to go on for those familiar with this code?. ### Version. 0.2.120 (worked as expected in 0.2.119 and prior). ### Relevant log output. ```shell; cpg_workflows/large_cohort/dense_subset.py:24: in run; vds = hl.vds.split_multi(vds, filter_changed_loci=True); <decorator-gen-1858>:2: in split_multi; ???; /opt/hostedtoolcache/Python/3.10.12/x64/lib/python3.10/site-pa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13337:75,pipeline,pipelines,75,https://hail.is,https://github.com/hail-is/hail/issues/13337,1,['pipeline'],['pipelines']
Deployability,"### What happened?. A user running RHEL 9 [reported an error](https://discuss.hail.is/t/hail-fails-after-installing-it-on-a-single-computer/3653) importing movie lens data when running 0.2.126 but succeeded on 0.2.120. This error did not reproduce on MacOS. We should verify which version of hail this error is introduced and whether the hail installation is fully broken or for some reason is just movie lens/a subset of functionality. ### Version. 0.2.126. ### Relevant log output. ```shell; 2023-11-20 18:25:51.813 Hail: WARN: This Hail JAR was compiled for Spark 3.3.0, running with Spark 3.3.3.; Compatibility is not guaranteed.; 2023-11-20 18:25:53.340 Hail: INFO: SparkUI: http://xxxxx:4040; 2023-11-20 18:25:54.037 Hail: INFO: Running Hail version 0.2.126-ee77707f4fab; 2023-11-20 18:27:48.120 Hail: INFO: downloading MovieLens-100k data ...; Source: https://files.grouplens.org/datasets/movielens/ml-100k.zip; 2023-11-20 18:27:50.320 Hail: INFO: importing users table and writing to data/users.ht ...; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14049:105,install,installing-it-on-a-single-computer,105,https://hail.is,https://github.com/hail-is/hail/issues/14049,2,['install'],"['installation', 'installing-it-on-a-single-computer']"
Deployability,"### What happened?. After #13440 lands, we should be guaranteed that `ubuntu:22.04` exists inside the internal docker registry. Then, we need to update the `docker_root_image` field in the `global-config` to use `ubuntu:22.04` instead of `ubuntu:20.04`. Doing so requires the following:. 1. A PR that updates the terraform in `infra/azure` and `infra/gcp` to use `ubuntu:22.04` in the global-config. The azure terraform should be applied against `haildev`.; 2. Manually editing the `global-config` in `hail-vdc` to switch `docker_root_image` to `ubuntu:22.04`.; 3. A follow up PR once the global-configs are updated that removes `ubuntu:20.04` from `docker/third-party/images.txt`. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13545:145,update,update,145,https://hail.is,https://github.com/hail-is/hail/issues/13545,3,['update'],"['update', 'updated', 'updates']"
Deployability,"### What happened?. After I ran the ""make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0"", I get the following error. > Configure project :; WARNING: Hail primarily tested with Spark 3.3.2, use other versions at your own risk. > Task :compileScala; [Error] /gpfs/fs1/home/jl/Hail2/hail/hail/src/main/scala/is/hail/HailContext.scala:127:21: value implOpMulMatrix_DMD_DVD_eq_DVD is not a member of object breeze.linalg.DenseMatrix; one error found. > Task :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; > Run with --info option to get more log output.; > Run with --scan to get full insights. BUILD FAILED in 4m 52s; 2 actionable tasks: 2 executed; make: *** [build/libs/hail-all-spark.jar] Error 1. ### Version. Hail 0.2.13. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14235:42,install,install-on-cluster,42,https://hail.is,https://github.com/hail-is/hail/issues/14235,1,['install'],['install-on-cluster']
Deployability,"### What happened?. Although it is not possible to avoid all cross-region access (and thus costs), there are some obvious preventable misuses. For example, the following pipeline should error:. ```; b = hb.Batch(regions=['us-east1']); x = b.read_input('gs://bucket-in-central1/'); b.new_job(f'cat {x}'); b.run(); ```; But the following pipeline should not error:; ```; b = hb.Batch(regions=['us-east1']); x = b.read_input('gs://bucket-in-central1/'); j = b.new_job(f'cat {x}'); j.regions(['us-central1']); ```; The following should error because the job *could* be in us-east1:; ```; b = hb.Batch(regions=['us-east1', 'us-central1']); x = b.read_input('gs://bucket-in-central1/'); b.new_job(f'cat {x}'); b.run(); ```; The following should error:; ```; b = hb.Batch(regions=['us-east1']) # remote_tmpdir is set in config file as a us-centra1 bucket; j = b.new_job(f'echo hi > {j.f}'); j2 = b.new_job(f'cat {j.f}'); b.run(); ```. ### Version. 0.2.119. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13232:170,pipeline,pipeline,170,https://hail.is,https://github.com/hail-is/hail/issues/13232,2,['pipeline'],['pipeline']
Deployability,"### What happened?. At time of writing, building hail with `SPARK_VERSION=3.4.0` errors with the following message:. ```; hail/src/main/scala/is/hail/HailContext.scala:119:21: value implOpMulMatrix_DMD_DVD_eq_DVD is not a member of object breeze.linalg.DenseMatrix; ```. This is due to a major version upgrade and breaking change in the Breeze library on which spark and hail depend. The exact error is a rename and refactor. The method `DenseMatrix.implOpMulMatrix_DMD_DVD_eq_DVD` is now `HasOps.impl_OpMulMatrix_DMD_DVD_eq_DVD`. Notice the method name change and the fact that `HasOps` does not exist in the version of Breeze (1.x) that is used in Spark 3.3. Hail should build with Spark 3.4, but since we only officially support one version of Spark (whichever Dataproc currently is running), it would be reasonable to wait to fully upgrade to Spark 3.4 when [Dataproc 2.2](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.2) is GA instead of trying to do something hacky to support both versions of Breeze. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13971:302,upgrade,upgrade,302,https://hail.is,https://github.com/hail-is/hail/issues/13971,3,"['release', 'upgrade']","['release-', 'upgrade']"
Deployability,"### What happened?. Ben submitted a pipeline where the first 85% of jobs run in us-central1 while the last 15% run in us-east1. The autoscaler only looks at the head of the job queue and then sorts the result set to figure out the regions to spin up instances in. The scheduler looks at the entire job queue and then sorts the result set to figure out the regions to spin up instances in. The sort order placed us-east1 before us-central1. Concretely, the autoscaler is spinning up instances in us-central1 only while the scheduler is trying to schedule jobs in us-east1. See also: https://github.com/hail-is/hail/pull/13268. ### Version. 0.2.118. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13269:36,pipeline,pipeline,36,https://hail.is,https://github.com/hail-is/hail/issues/13269,1,['pipeline'],['pipeline']
Deployability,### What happened?. Ben tried to submit a batch with 0.2.126 and it kept failing with errors due to not able to enter into a task. A quick google search showed this could be a bad nest_asyncio interaction. I recommended downgrading to 0.2.120 before this possibly related PR (#13614) went in and that unblocked Ben. I could not replicate this behavior on 3.9 or 3.11 on my laptop with 0.2.126. This error occurred on a fresh install for Ben for 3.9 and 3.11. https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/RuntimeError.3A.20Cannot.20enter.20into.20task/near/404939884. ### Version. 0.2.126. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14051:425,install,install,425,https://hail.is,https://github.com/hail-is/hail/issues/14051,1,['install'],['install']
Deployability,"### What happened?. Cal, Lindo, Leo, report that Hail’s matrix multiplication fails on certain pipelines for no obvious reason. I think this is only in QoB/lowered code. . - https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/.60pc_relate.60.20crash.20with.200.2E2.2E110 (same looking issue but this was fixed https://github.com/hail-is/hail/pull/12797); - https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/ld.20prune.20error; - https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/pc_relate.20crash.200.2E2.2E122-be9d88a80695. ### Version. 0.2.122. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688:95,pipeline,pipelines,95,https://hail.is,https://github.com/hail-is/hail/issues/13688,1,['pipeline'],['pipelines']
Deployability,"### What happened?. Consider adding support for GPUs. This requires changes to the frontend, the driver, and the worker. In particular, getting the worker configuration correct is tricky. We could develop faster if we had a fully local version of Batch development. For example, consider starting a linux VM which had separate containers running: auth, batch, batch-driver, mysql, and a single worker. Each container could live update changes to the source code. Such a system should also work on OS X, although the networking may be more complicated due to the Linux VM. It might be easier to just expect developers to have a separately managed Linux VM on their Mac?. ### Version. 0.2.122. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13630:155,configurat,configuration,155,https://hail.is,https://github.com/hail-is/hail/issues/13630,2,"['configurat', 'update']","['configuration', 'update']"
Deployability,"### What happened?. Consider this simple pipeline that reads a matrix table. ```python3; import hail as hl; hl.init(master='local[1]'); hl.read_matrix_table('/Users/dking/projects/hail-data/foo.mt')._force_count_rows(); ```. <details>; <summary>The matrix table's type.</summary>. ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; ----------------------------------------; Entry fields:; 'LA': array<int32>; 'LGT': call; 'LAD': array<int32>; 'LPGT': call; 'LPL': array<int32>; 'RGQ': int32; 'gvcf_info': struct {; AC: array<int32>, ; AF: array<float64>, ; AN: int32, ; AS_BaseQRankSum: array<float64>, ; AS_FS: array<float64>, ; AS_InbreedingCoeff: array<float64>, ; AS_MQ: array<float64>, ; AS_MQRankSum: array<float64>, ; AS_QD: array<float64>, ; AS_QUALapprox: array<int32>, ; AS_RAW_BaseQRankSum: str, ; AS_RAW_MQ: array<float64>, ; AS_RAW_MQRankSum: array<tuple (; float64, ; int32; )>, ; AS_RAW_ReadPosRankSum: array<tuple (; float64, ; int32; )>, ; AS_ReadPosRankSum: array<float64>, ; AS_SB_TABLE: array<array<int32>>, ; AS_SOR: array<float64>, ; AS_VarDP: array<int32>, ; BaseQRankSum: float64, ; ExcessHet: float64, ; FS: float64, ; InbreedingCoeff: float64, ; MQ: float64, ; MQRankSum: float64, ; MQ_DP: int32, ; QD: float64, ; QUALapprox: int32, ; RAW_GT_COUNT: array<int32>, ; RAW_MQandDP: array<int32>, ; ReadPosRankSum: float64, ; SOR: float64, ; VarDP: int32; }; 'DP': int32; 'GQ': int32; 'MIN_DP': int32; 'PID': str; 'PS': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; ```; </details>. <details>; <summary>The matrix table entries RVD's EType [1]</summary>. ```; +EBaseStruct{; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:; +EArray[EBaseStruct{; LA:EArray[EInt3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13792:41,pipeline,pipeline,41,https://hail.is,https://github.com/hail-is/hail/issues/13792,1,['pipeline'],['pipeline']
Deployability,"### What happened?. Currently, almost all of our tests are integration tests which require:; 1. Compiling Scala code.; 2. Building a JAR (takes ~30 seconds on my MBP); 3. Running pytest (can take as long as 20 seconds). All of this is a lot slower than iterating with a live running Scala process. We should have tests of various parts of the compiler operating at the IR level. For example, MatrixIR to TableIR lowering should have plenty of in Scala IR-level tests. Likewise for TableIR to CDAIR. The optimizer/simplifier should also have tests at each level which assert certain kinds of code is sufficiently cleaned up by the optimizer. ### Version. 0.2.122. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13638:59,integrat,integration,59,https://hail.is,https://github.com/hail-is/hail/issues/13638,1,['integrat'],['integration']
Deployability,"### What happened?. Currently, in order to change the rate limit in `internal-gateway`, one has to manually edit `envoy.py` and redeploy CI. This is non-standard, time intensive and can be accidentally reverted if CI merges a new commit to `main`. CI already regularly updates the envoy configuration `internal-gateway` uses to account for services in new namespaces, so making the rate limit configurable should be a simple CRUD task that would greatly ease operation of batch under high load. One gotcha to keep in mind is that while we run CI as a control plane for our ""dynamic cluster topology"", it should still be possible to manually deploy `internal-gateway` in a standalone Batch cluster (see `internal-gateway/Makefile`), so `envoy.py` should still be runnable as a standalone script. ### Version. 0.2.128. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14399:269,update,updates,269,https://hail.is,https://github.com/hail-is/hail/issues/14399,3,"['configurat', 'deploy', 'update']","['configuration', 'deploy', 'updates']"
Deployability,"### What happened?. Deploy that failed: https://ci.hail.is/batches/7990161; Image in question: `us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-o6zrkvemh79g`; Logs: https://cloudlogging.app.goo.gl/xhHT47c1UnMHMd8eA. Unclear why a tag prefixed with `deploy-` was deleted, that would seem to not meet any of our policies. ### Version. 0.2.122. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603:20,Deploy,Deploy,20,https://hail.is,https://github.com/hail-is/hail/issues/13603,3,"['Deploy', 'deploy']","['Deploy', 'deploy', 'deploy-']"
Deployability,"### What happened?. Follow up on #13445 - I almost succeed to install hail on AWS but still have some environment issue:. * I am trying to install Hail v0.2.124; * on AWS EMR v6.9.1 (latest version with Spark 3.3.0 suggested on hail doc); * I upgrade to python 3.9.18; ```sh; $ python --version; Python 3.9.18; ```; I activate java 11.0.20.1; ```sh; $ java -version; openjdk version ""11.0.20.1"" 2023-08-22 LTS; OpenJDK Runtime Environment Corretto-11.0.20.9.1 (build 11.0.20.1+9-LTS); OpenJDK 64-Bit Server VM Corretto-11.0.20.9.1 (build 11.0.20.1+9-LTS, mixed mode); ```; * I clone hail; ```sh; $ cd /tmp; $ git clone --branch 0.2.124 --depth 1 https://github.com/broadinstitute/hail.git; ```; * I build hail; ```sh; $ cd hail/hail/; $ make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.0; [...]; Successfully installed hail-0.2.124; hailctl config set query/backend spark; ```; * At this point Hail seems correcly installed; ```sh; $ pip show hail; Name: hail; Version: 0.2.124; Summary: Scalable library for exploring and analyzing genomic data.; Home-page: https://hail.is; Author: Hail Team; Author-email: hail@broadinstitute.org; License: UNKNOWN; Location: /home/hadoop/.local/lib/python3.9/site-packages; ```; * For sake of configuration I create a symlink of the hail backend; ```sh; sudo ln -sf /home/hadoop/.local/lib/python3.9/site-packages/hail/backend /opt/hail/backend; ```; * Confident of the. installation I try to run spark shell; ```sh; $ spark-shell; [...]; Exception in thread ""main"" java.lang.NoSuchMethodError: 'scala.reflect.internal.settings.MutableSettings ; ```. I am out of idea on how to solve the current situation. ; Thanks. ### Version. 0.2.124. ### Relevant log output. ```shell; $ spark-shell; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837:62,install,install,62,https://hail.is,https://github.com/hail-is/hail/issues/13837,6,"['install', 'upgrade']","['install', 'install-on-cluster', 'installed', 'upgrade']"
Deployability,"### What happened?. For example, https://github.com/hail-is/hail/issues/12905 will add a Batch-based AS-VQSR pipeline. Our VDS generation SOP should include a step wherein we use that new pipeline to generate an AS-VQSR table. DSP has developed a new variant filtration model (I think it's called VETS?) which we should consider using instead. ### Version. 0.2.122. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13660:109,pipeline,pipeline,109,https://hail.is,https://github.com/hail-is/hail/issues/13660,2,['pipeline'],['pipeline']
Deployability,"### What happened?. GRh38 has patch versions. https://www.ncbi.nlm.nih.gov/grc/help/patches/ We need to understand how often our input data changes patch versions. We need to understand the ramifications of putting a patch version into the Locus type (what happens when you try to combine datasets with different patch versions?). Importantly, the patch versions appear to change the reference base. If variants are aligned to a patched reference, then we really ought to return the patched reference base when requested. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14107:30,patch,patch,30,https://hail.is,https://github.com/hail-is/hail/issues/14107,8,['patch'],"['patch', 'patched', 'patches']"
Deployability,"### What happened?. GVS team reports this. Send updates / further inquiry to Rori. ![image](https://github.com/hail-is/hail/assets/106194/b8b3ead8-7659-4cf1-b764-872cff031403). `wdl_vds` is the result of combining GVS Avro files. - This script is the top-level https://github.com/broadinstitute/gatk/blob/ah_var_store/scripts/variantstore/wdl/extract/hail_gvs_import.py; - This script imports from GVS, I suspect something in here is the issue https://github.com/broadinstitute/gatk/blob/ah_var_store/scripts/variantstore/wdl/extract/import_gvs.py. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13823:48,update,updates,48,https://hail.is,https://github.com/hail-is/hail/issues/13823,1,['update'],['updates']
Deployability,"### What happened?. Hail propagates nicely explained error messages from java to python when an exception is thrown in the user's pipeline. However, the hail python front end does not handle a situation where the java backend disappears entirely, which can happen in the case of an OOM killer killing the JVM. The result is an error as seen below. In such a scenario, the python front end should add a useful message suggesting that the backend is not reachable and might have run out of memory. ### Version. 0.2.130. ### Relevant log output. ```shell; File ~/Library/Python/3.9/lib/python/site-packages/hail/table.py:2814, in Table.collect(self, _localize, _timed); 2812 e = construct_expr(rows_ir, hl.tarray(t.row.dtype)); 2813 if _localize:; → 2814 return Env.backend().execute(e._ir, timed=_timed); 2815 else:; 2816 return e. File ~/Library/Python/3.9/lib/python/site-packages/hail/backend/backend.py:188, in Backend.execute(self, ir, timed); 186 payload = ExecutePayload(self._render_ir(ir), ‘{“name”:“StreamBufferSpec”}’, timed); 187 try:; → 188 result, timings = self._rpc(ActionTag.EXECUTE, payload); 189 except FatalError as e:; 190 raise e.maybe_user_error(ir) from None. File ~/Library/Python/3.9/lib/python/site-packages/hail/backend/py4j_backend.py:218, in Py4JBackend._rpc(self, action, payload); 216 path = action_routes[action]; 217 port = self._backend_server_port; → 218 resp = self._requests_session.post(f’http://localhost:{port}{path}', data=data); 219 if resp.status_code >= 400:; 220 error_json = orjson.loads(resp.content). File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:637, in Session.post(self, url, data, json, **kwargs); 626 def post(self, url, data=None, json=None, **kwargs):; 627 r""""“Sends a POST request. Returns :class:Response object.; 628; 629 :param url: URL for the new :class:Request object.; (…); 634 :rtype: requests.Response; 635 “””; → 637 return self.request(“POST”, url, data=data, json=json, **kwargs). File ~/Library/Python/3.9/l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14557:130,pipeline,pipeline,130,https://hail.is,https://github.com/hail-is/hail/issues/14557,1,['pipeline'],['pipeline']
Deployability,"### What happened?. Hana Snow is the engineer for SEQR. Previously, SEQR used elastic search as its datastore. Unfortunately, elastic search was very expensive because, to get reasonable performance, SEQR indexed nearly every field. The ES index was huge and the VM resources necessary to run an ES instance on that index were expensive (like 1000s USD per month). I've been supporting Hana as much as I can, but she needs someone who can be more dedicated and responsive than me. She uses a k8s cluster. She has a SEQR frontend deployment. She also has a Hail deployment (statefulset maybe?). The Hail pod has an SSD mounted read-only. That SSD has all the SEQR data in Hail Table form. There are many tables with annotations (variant metadata, like ""probability this variant is damaging"" or ""likely causes this to happen to the protein""). There are also ""per-family"" tables which contain all the sequences within a single family. Many queries are directly against a particular family. Those tables are small and quick to read. There's also one giant table containing all the sequences from all the families. That table is large and expensive to read. A lot of our engineering work has been around making sure queries against that table are fast. Tim, at one point, had enough of her system locally that he could experiment with running queries on his laptop against his SSD. He hacked on the queries themselves and on Hail itself until the bandwidth was fast enough that the queries should complete fast enough on the full dataset. Fast enough varies but generally a couple tens of seconds is OK. The work here is to pair with Hana to diagnose performance issues and make changes until the queries are acceptably fast. The first thing I would do is update her to the latest Hail (with the array decoder improvement as well as the memory overhead stuff on which Daniel is working). Then, with Hana's help, test the timing of some queries. If the queries are still too slow, your options are:; 1. Chec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882:529,deploy,deployment,529,https://hail.is,https://github.com/hail-is/hail/issues/13882,2,['deploy'],['deployment']
Deployability,"### What happened?. Hello,. I installed hail into an empty, new Python 3.12.2 virtual environment, and was not able to import it. I see a failure like this:. ```; (venv) (py312) alex@rpi400:~/hail $ python; Python 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:38:53) [GCC 12.3.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/alex/hail/venv/lib/python3.12/site-packages/hail/__init__.py"", line 2, in <module>; import pkg_resources; ModuleNotFoundError: No module named 'pkg_resources'; ```. It looks like in Python 3.12, the bundled setuptools was removed and new virtual environments will not have setuptools in them, it needs to be specifically installed through pip: https://github.com/python/cpython/issues/95299. This could be fixed either by adding `setuptools` to hail's requirements so that it will be installed when users install hail, or hail could remove usage of setuptools & its associated modules (`pkg_resources`) at runtime, as some other projects have done: https://github.com/TDAmeritrade/stumpy/issues/950. At a glance, the cleanest thing to do here may be to move off of the deprecated `pkg_resources` and to the recommended `importlib` if it has what you need: https://setuptools.pypa.io/en/latest/pkg_resources.html. I also have to admit that I discovered this while playing around with hail on a Raspberry Pi 4, so it is possible that something else broken caused this failure, but I believe I understand what's happening. Here's my full `pip freeze` for reference:. ```; (venv) (py312) alex@rpi400:~/hail $ pip freeze; aiodns==2.0.0; aiohttp==3.9.3; aiosignal==1.3.1; attrs==23.2.0; avro==1.11.3; azure-common==1.1.28; azure-core==1.30.1; azure-identity==1.15.0; azure-mgmt-core==1.4.0; azure-mgmt-storage==20.1.0; azure-storage-blob==12.19.1; bokeh==3.4.0; boto3==1.34.73; botocore==1.34.73; cachetools==5.3.3; certifi==2024.2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14428:30,install,installed,30,https://hail.is,https://github.com/hail-is/hail/issues/14428,2,['install'],['installed']
Deployability,"### What happened?. Hello,. It looks like Hail has a hard coded check to only run on Java 8 and 11, despite Spark supporting Java 17 for a couple years now, including on spark 3.3.x, which is the currently used release for `pip install hail`: https://spark.apache.org/releases/spark-release-3-3-0.html#build. **Would it be possible to add Java 17 support**, or possibly even remove the Java version check in general so that it can track what underlying Spark does without additional updates? . There are a bunch of benefits of moving to Java 17, including:; 1. https://kstefanj.github.io/2021/11/24/gc-progress-8-17.html - Significant garbage collector improvements that will likely improve throughput and reduce costs; 2. https://vmnotescom.wordpress.com/2021/09/14/java-17-whats-new-removed-and-preview-in-jdk-17/ - Better Apple Silicon support. I know that darwin-aarch64 has been backported to 8 and 11, but 17 is faster on that platform.; 3. https://spark.apache.org/releases/spark-release-3-5-0.html#removals-behavior-changes-and-deprecations - The next release of Spark will require Java 17 as a minimum version, and making the change now is easier than making more changes all at once in the future.; . > The following features will be removed in the next Spark major release; > ; > Support for Java 8 and Java 11, and the minimal supported Java version will be Java 17; > Support for Scala 2.12, and the minimal supported Scala version will be 2.13. Also, requiring specifically Java 8 or 11 has led to some friction for students and researchers who are first evaluating hail. In the past few weeks, I've talked to a lot of students and researchers who wanted to evaluate hail, followed the documentation to install Azul Java 8 but already had an existing Java install and did not update their PATH or JAVA_HOME. Most of their existing Java versions were 17, as 17 is the current default on most Linux distros and a common one to have been installed via Brew in the past few years on Mac. Alt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14433:211,release,release,211,https://hail.is,https://github.com/hail-is/hail/issues/14433,7,"['install', 'release', 'update']","['install', 'release', 'release-', 'releases', 'updates']"
Deployability,"### What happened?. Hello,; I have some gwas results that I have converted into a pandas dataframe. Typically with dataframes I pickle my outputs for speed and easily maintaining data types. Within All of Us we have separate analysis environments whether we're using hail or not. The environment that doesn't have hail is much cheaper for simple analyses and does not have pyspark installed. You can see in the error below when I try to reread the pickled dataframe, I get an error that it can't find the pyspark from within a hail module. If I write the dataframe as a csv, read it back in, and then pickle it then the error goes away. This suggests to me that the dataframe created by hail maintains reference to hail objects and pandas is attempting to recreate these objects when unpickling. I suspect this is not intentional. ```python; # Hail environment; vat_simplified_file = os.path.join(bucket, 'vat.ht'); gwas = hl.read_table(gwas_results_file_no_sex_chr); vat = hl.read_table(vat_simplified_file); gwas = gwas.filter(gwas.p_value <= 1e-4); combined = gwas.join(vat, how='left'); combined_pandas = combined.to_pandas(). gwas_pandas_file = os.path.join(bucket, 'gwas_results.pkl'); combined_pandas.to_pickle(gwas_pandas_file); ```. ```python; # Non hail environment without pyspark; combined_pandas = pd.read_pickle(gwas_pandas_file). ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); /opt/conda/lib/python3.7/site-packages/pandas/io/pickle.py in read_pickle(filepath_or_buffer, compression, storage_options); 216 # expected ""IO[bytes]""; --> 217 return pickle.load(handles.handle) # type: ignore[arg-type]; 218 except excs_to_catch:. /opt/conda/lib/python3.7/site-packages/hail/__init__.py in <module>; 32 # E402 module level import not at top of file; ---> 33 from .table import Table, GroupedTable, asc, desc # noqa: E402; 34 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402. /opt/conda",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14004:381,install,installed,381,https://hail.is,https://github.com/hail-is/hail/issues/14004,1,['install'],['installed']
Deployability,"### What happened?. Here's a tail of the log showing the rapidly increasing RAM use. I'm working on a simple replicable pipeline now. Does not depend on the use of `filter_changed_loci`. ```; 2023-09-11 16:22:59.815 : INFO: RegionPool: REPORT_THRESHOLD: 1.0G allocated (662.3M blocks / 363.4M chunks), regions.size = 3, 0 current java objects, thread 24: Thread-3; 2023-09-11 16:23:01.488 : INFO: executing D-Array [table_scan_prefix_sums_singlestage] with 1 tasks, contexts size = 430.00 B, globals size = 2.52 MiB; 2023-09-11 16:23:01.540 : INFO: RegionPool: initialized for thread 115: Executor task launch worker for task 0.0 in stage 37.0 (TID 442); 2023-09-11 16:23:01.567 : INFO: RegionPool: REPORT_THRESHOLD: 2.2M allocated (64.0K blocks / 2.1M chunks), regions.size = 1, 0 current java objects, thread 115: Executor task launch worker for task 0.0 in stage 37.0 (TID 442); 2023-09-11 16:23:01.572 : INFO: RegionPool: REPORT_THRESHOLD: 4.2M allocated (64.0K blocks / 4.1M chunks), regions.size = 1, 0 current java objects, thread 115: Executor task launch worker for task 0.0 in stage 37.0 (TID 442); 2023-09-11 16:23:01.573 : INFO: RegionPool: REPORT_THRESHOLD: 4.3M allocated (64.0K blocks / 4.2M chunks), regions.size = 1, 0 current java objects, thread 115: Executor task launch worker for task 0.0 in stage 37.0 (TID 442); 2023-09-11 16:23:01.573 : INFO: RegionPool: REPORT_THRESHOLD: 4.3M allocated (128.0K blocks / 4.2M chunks), regions.size = 2, 0 current java objects, thread 115: Executor task launch worker for task 0.0 in stage 37.0 (TID 442); 2023-09-11 16:23:01.573 : INFO: RegionPool: REPORT_THRESHOLD: 12.3M allocated (192.0K blocks / 12.1M chunks), regions.size = 3, 0 current java objects, thread 115: Executor task launch worker for task 0.0 in stage 37.0 (TID 442); 2023-09-11 16:23:01.579 : INFO: RegionPool: REPORT_THRESHOLD: 12.4M allocated (192.0K blocks / 12.2M chunks), regions.size = 3, 0 current java objects, thread 115: Executor task launch worker for task 0.0 in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13606:120,pipeline,pipeline,120,https://hail.is,https://github.com/hail-is/hail/issues/13606,1,['pipeline'],['pipeline']
Deployability,"### What happened?. Hi,; I am on a macOS Ventura and I have successfully installed hail (v 0.2.109) on a conda env. Everything seems to run properly, except that I don't get any plots. Bokeh was installed in the env, v1.4.0., pysark =3.13 and scala=2.11.8 are some relevant packages that may contribute to this issue. When starting Hail, this is the output I get:. 2023-02-20 11:07:38.798 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.1.3; SparkUI available at http://amaru-2.local:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.109-b71b065e4bb6; LOGGING: writing to /Users/alanmejiamaza/hail-20230220-1107-0.2.109-b71b065e4bb6.log. It seems to be that the issue comes from the spark version? which is the correct spark version for a conda env on a mac? I have followed the tutorials and seemed to work fine except for the plots. I don't have any output when invoking commands for plots. Can anyone tell me the specific versions needed to run all Hail properties?. Thanks. ### Version. 0.2.109. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12717:73,install,installed,73,https://hail.is,https://github.com/hail-is/hail/issues/12717,2,['install'],['installed']
Deployability,"### What happened?. I am trying to install Hail v0.2.120 on AWS EMR 6.9.0. Versions:; - Python 3.8.16; - Java 1.8.0; - Spark 3.3.0. After updating Python to 3.8 and cloning hail repo, I compile hail using the command below. ```sh; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.0; ```. Here I get an error. ```sh ; + pip-compile --quiet python/requirements.txt python/pinned-requirements.txt --output-file=/tmp/tmp.aWUFJ1BMnP; ../check_pip_requirements.sh: line 13: pip-compile: command not found; ```. While I do have pip-compile installed. ```sh ; pip-compile --help; Usage: pip-compile [OPTIONS] [SRC_FILES]... Compiles requirements.txt from requirements.in, pyproject.toml, setup.cfg,; or setup.py specs. Options:; ```. Note that `make clean` did not solve the issue. see logs attached. ### Version. 0.2.120. ### Relevant log output. ```shell; BUILD SUCCESSFUL in 2m 46s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; /usr/lib64/python3.8/distutils/dist.py:274: UserWarning: Unknown distribution option: 'long_description_content_type'; warnings.warn(msg); installing to build/bdist.linux-x86_64/wheel; creating build/bdist.linux-x86_64/wheel/hail-0.2.120.dist-info/WHEEL; creating 'dist/hail-0.2.120-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it; adding 'hail/__init__.py'; adding 'hail/builtin_references.py'; adding 'hail/conftest.py'; adding 'hail/context.py'; adding 'hail/hai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13445:35,install,install,35,https://hail.is,https://github.com/hail-is/hail/issues/13445,3,['install'],"['install', 'install-on-cluster', 'installed']"
Deployability,"### What happened?. I installed from PyPI and obtained hail 0.2.132. I made sure I used a completely clean environment with nothing in it (using pixi). . When I did . ```; import hail; ```. I got this error:. ```; >>> import hail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hail/__init__.py"", line 40, in <module>; from hail.utils import (; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hail/utils/__init__.py"", line 4, in <module>; from .hadoop_utils import (; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hail/utils/hadoop_utils.py"", line 7, in <module>; from hail.fs.hadoop_fs import HadoopFS; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hail/fs/hadoop_fs.py"", line 8, in <module>; from hailtop.fs.fs import FS; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hailtop/fs/__init__.py"", line 1, in <module>; from .fs_utils import (; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hailtop/fs/fs_utils.py"", line 4, in <module>; from hailtop.aiocloud.aiogoogle import GCSRequesterPaysConfiguration; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hailtop/aiocloud/aiogoogle/__init__.py"", line 1, in <module>; from .client import (; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hailtop/aiocloud/aiogoogle/client/__init__.py"", line 8, in <module>; from .storage_client import (; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hailtop/aiocloud/aiogoogle/client/storage_client.py"", line 14, in <module>; from hailtop.aiotools import FeedableAsyncIterable, WriteBuffer; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hailtop/aiotools/__init__.py"", line 1, in <module>; from .fs import (; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14630:22,install,installed,22,https://hail.is,https://github.com/hail-is/hail/issues/14630,1,['install'],['installed']
Deployability,"### What happened?. I wrote a bed2hailmatrix workflow and ran it on Terra platform to convert from plink bed format to hail matrix format. . https://github.com/shengqh/warp/blob/develop/pipelines/vumc_biostatistics/genotype/VUMCBed2HailMatrix.wdl. code is pretty simple:. ```; import hail as hl. hl.init(spark_conf={""spark.driver.memory"": ""~{memory_gb}g""}). #contig_recoding is hard coded for human only; dsplink = hl.import_plink(bed=""~{source_bed}"",; bim=""~{source_bim}"",; fam=""~{source_fam}"",; reference_genome=""~{reference_genome}"",; contig_recoding={; '1': 'chr1',; '2': 'chr2',; '3': 'chr3',; '4': 'chr4',; '5': 'chr5',; '6': 'chr6',; '7': 'chr7',; '8': 'chr8',; '9': 'chr9',; '10': 'chr10',; '11': 'chr11',; '12': 'chr12',; '13': 'chr13',; '14': 'chr14',; '15': 'chr15',; '16': 'chr16',; '17': 'chr17',; '18': 'chr18',; '19': 'chr19',; '20': 'chr20',; '21': 'chr21',; '22': 'chr22',; 'X': 'chrX',; 'Y': 'chrY',; 'MT': 'chrM'}). dsplink.write(""~{target_prefix}"", overwrite=True); ```. When I tested it on the chr12 with 34523 samples and 18377527 variants from one of my dataset in Terra (100 g was allocated for this task), it failed with error message:. ```; java.lang.NegativeArraySizeException: null; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.resize(IdentityObjectIntMap.java:542); at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:306); at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:300); at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:162); at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:307); at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:300); at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:162); at com.esotericsoftware.kryo.util.MapReferenceResolver.addWrittenObject(MapReferenceResolver.java:41); at com.esotericsoftware.kryo.Kryo.w",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:186,pipeline,pipelines,186,https://hail.is,https://github.com/hail-is/hail/issues/14168,1,['pipeline'],['pipelines']
Deployability,### What happened?. I've been waiting to delete temporary files used for swapping the VEP reference files to ones that are indexed. These shouldn't be needed any longer. I wanted to wait until the new indexed VEP files have gotten more use and we have confirmation there are no issues in case we need to quickly swap anything back to the original configuration. The files to be deleted are:. ```; gs://hail-qob-vep-grch38-us-central1/95_GRCh38_indexed.tar # this is just a backup for the new indexed dataproc files. This file is in all of the dataproc VEP buckets already; gs://hail-qob-vep-grch38-us-central1/homo_sapiens_backup/ # these are the original unindexed reference files; gs://hail-qob-vep-grch38-us-central1-test/ # this was just used for staging. should be okay to just delete; ```. ### Version. 0.2.128. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14414:347,configurat,configuration,347,https://hail.is,https://github.com/hail-is/hail/issues/14414,1,['configurat'],['configuration']
Deployability,"### What happened?. In #14675 I replaced `END` with `LEN` in VDS. In doing so, I made sure that both fields were present so as to not break people's existing pipelines. I added a hidden `_drop_end` flag to `read_vds` in order to be able to (mostly in the combiner) not have the `END` field present. This lead to a strange code pattern:. https://github.com/chrisvittal/hail/blob/f39364c177e0b009589826b2c6b3cd36c3ec359d/hail/python/hail/vds/variant_dataset.py#L44-L46. When running the final VDS+VDS merge in [`test_combiner_run`](https://github.com/chrisvittal/hail/blob/f39364c177e0b009589826b2c6b3cd36c3ec359d/hail/python/test/hail/vds/test_combiner.py#L178-L222) on the local backend, this failed with a memory error (in debug mode):. ```; RuntimeException: invalid memory access: 140a68008/00000001: not in 140a58008/00010000; ```. Applying this patch fixed `test_combiner_run`:; ```patch; diff --git a/hail/python/hail/vds/variant_dataset.py b/hail/python/hail/vds/variant_dataset.py; index 0f851e7364..01be83a982 100644; --- a/hail/python/hail/vds/variant_dataset.py; +++ b/hail/python/hail/vds/variant_dataset.py; @@ -41,9 +41,14 @@ def read_vds(; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals). - reference_data = VariantDataset._add_len_end(reference_data); + # if LEN is missing, add it, _add_len is a no-op if LEN is already present; + reference_data = VariantDataset._add_len(reference_data); if _drop_end:; - reference_data = reference_data.drop('END'); + if 'END' in reference_data.entry:; + reference_data = reference_data.drop('END'); + else: # if END is missing, add it, _add_end is a no-op if END is already present; + reference_data = VariantDataset._add_end(reference_data); +; vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14705:158,pipeline,pipelines,158,https://hail.is,https://github.com/hail-is/hail/issues/14705,3,"['patch', 'pipeline']","['patch', 'pipelines']"
Deployability,"### What happened?. In particular, edits to HTML, CSS, SASS, and JavaScript should update the UI in real-time. This development-mode UI should support fetching data from either from a live backend (in any namespace) or should have an easy way for the developer to intercede (by programing) and return mock data. ### Version. 0.2.122. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13629:83,update,update,83,https://hail.is,https://github.com/hail-is/hail/issues/13629,1,['update'],['update']
Deployability,"### What happened?. In the past, we liberally incorporated partly tested, but powerful, functionality into hail.experimental or into the public modules with an `_` prefix. This has two negative consequences:; 1. Users discover this functionality and, despite the warnings or lack of documentation, begin to rely on it. For example, the variants team relies on `hail.experimental.full_outer_join_mt`. Since we're good people, we don't pull the rug out from under folks. However, in practice, some items in `hail.experimental` will eventually become de-facto public methods.; 2. Really great functionality is never released to the public! See, for example, `hl.agg._reservoir_sample`. Completing this issue means:; 1. Gathering a list of all the experimental and underscore methods.; 2. Assessing the effort to properly test.; 3. Assessing the value of the method to our users.; 4. For high-value methods, tickets are created to properly test them and make them public. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13480:613,release,released,613,https://hail.is,https://github.com/hail-is/hail/issues/13480,1,['release'],['released']
Deployability,"### What happened?. It seems like the event loop policy has changed again. ### Version. 0.2.126. ### Relevant log output. ```shell; (base) dking@wm28c-761 hail % HAIL_QUERY_BACKEND=service ipython; Python 3.10.9 (main, Jan 11 2023, 09:18:18) [Clang 14.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.18.1 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl. In [2]: hl.init(); /Users/dking/miniconda3/lib/python3.10/site-packages/hail/context.py:350: UserWarning: The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a future release.; warnings.warn(; ---------------------------------------------------------------------------; RuntimeError Traceback (most recent call last); File ~/miniconda3/lib/python3.10/site-packages/hailtop/hail_event_loop.py:12, in hail_event_loop(); 11 try:; ---> 12 asyncio.get_running_loop(); 13 nest_asyncio.apply(). RuntimeError: no running event loop. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last); Cell In[2], line 1; ----> 1 hl.init(). File <decorator-gen-1760>:2, in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, backend, driver_cores, driver_memory, worker_cores, worker_memory, gcs_requester_pays_configuration, regions, gcs_bucket_allow_list). File ~/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py:587, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 584 @decorator; 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_). File ~/miniconda3/lib/python3.10/site-packages/hail/context.py:357, in init(sc, app_name, maste",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14099:631,release,release,631,https://hail.is,https://github.com/hail-is/hail/issues/14099,1,['release'],['release']
Deployability,"### What happened?. It seems that the local filesystem can, infrequently, stall when executing `rmtree`. Note that the error about the directory being non-empty is because we have a bug in `rm_dir`: we try to remove the directory even if the children tasks failed. It oddly seems to have happened on both a deploy batch and a PR batch:; - PR: https://ci.hail.is/batches/7706444/jobs/170; - deploy: https://ci.hail.is/batches/7707793/jobs/172. ```; [2023-08-02 05:33:14] test/hail/utils/test_hl_hadoop_and_hail_fs.py::test_hadoop_methods_3[local] PASSED; +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_1 (139802083059456) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_0 (139802091452160) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-0_0 (139802205742848) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:307,deploy,deploy,307,https://hail.is,https://github.com/hail-is/hail/issues/13361,2,['deploy'],['deploy']
Deployability,"### What happened?. JVMJobs exist to provide a warm JVM to Hail jobs; however, in practice, there are two issues:; 1. A JVM warmed for one JAR (i.e. version) of Hail has limited benefit for a different JAR. Only shared classes like those in `java.util` could have been JITed.; 2. As the number of non-JVM running jobs grows, the likelihood that a JVMJob lands on a worker with a warm JVM decreases. Suppose instead that, as a part of the deploy process, we executed a series of Hail pipelines using the LocalBackend and export the JIT cache. We then store *both* the JAR and the JIT cache in GCS. A user job loads both the JAR and the JIT cache and starts a fresh JVM that loads from that JIT cache. Every JVMJob now, by definition, lands on a hot JVM. References; - ""Compile Stashing"" https://docs.azul.com/prime/Compile-Stashing; - ""Tuning JIT Compilations"" https://docs.azul.com/prime/analyzing-tuning-warmup#tuning-jit-compilations; - ""ReadyNow Warm-Up Optimizer"" https://docs.azul.com/prime/analyzing-tuning-warmup#use-readynow-warm-up-optimizer. ### Version. 0.2.122. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13675:438,deploy,deploy,438,https://hail.is,https://github.com/hail-is/hail/issues/13675,2,"['deploy', 'pipeline']","['deploy', 'pipelines']"
Deployability,"### What happened?. Most stored procedures take either a shared or exclusive lock on a relevant row of the `jobs` table near the start of the procedure, but not all. This appears to interact poorly with the `attempts_after_update` trigger as it attempts to take an exclusive lock on rows in the `jobs` table in the below join with the attempt resources tables. It's not clear exactly what the right fix is. It should be simple enough not to join on the jobs table in the `FOR UPDATE`, but we should also evaluate when in our various transactions a lock should be taken on the jobs table and whether it should be an X or S lock. ### Version. 0.2.128. ### Relevant log output. ```shell; ------------------------; LATEST DETECTED DEADLOCK; ------------------------; 2024-02-29 15:07:05 140331971655424; *** (1) TRANSACTION:; TRANSACTION 2486515, ACTIVE 0 sec inserting; mysql tables in use 27, locked 27; LOCK WAIT 16 lock struct(s), heap size 1128, 9 row lock(s), undo log entries 1; MySQL thread id 703, OS thread handle 140330830395136, query id 4745489 10.32.3.39 dgoldste-batch-user executing; INSERT INTO aggregated_billing_project_user_resources_v3 (billing_project, user, resource_id, token, `usage`); SELECT cur_billing_project, cur_user,; attempt_resources.deduped_resource_id,; rand_token,; msec_diff_rollup * quantity; FROM attempt_resources; WHERE attempt_resources.batch_id = NEW.batch_id AND attempt_resources.job_id = NEW.job_id AND attempt_id = NEW.attempt_id; FOR UPDATE; ON DUPLICATE KEY UPDATE `usage` = aggregated_billing_project_user_resources_v3.`usage` + msec_diff_rollup * quantity. *** (1) HOLDS THE LOCK(S):; RECORD LOCKS space id 351 page no 4 n bits 248 index PRIMARY of table `dgoldste-batch`.`instances_free_cores_mcpu` trx id 2486515 lock_mode X locks rec but not gap; Record lock, heap no 176 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d64676f6c647374652d7374616e64617264; asc batch-worker-dgoldste-standard; (total ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14380:476,UPDATE,UPDATE,476,https://hail.is,https://github.com/hail-is/hail/issues/14380,1,['UPDATE'],['UPDATE']
Deployability,"### What happened?. Motivated by: https://hail.zulipchat.com/#narrow/stream/128581-Cloud-support/topic/bucket.20regions.20and.20hail.20batch. New users or users in new environments (such as a new laptop) may have not yet configured their batch/regions. This can lead to unexpected egress if jobs land in a region different from their data. Forcing the user to make an explicit choice might alleviate this issue a bit. On the other hand, a user might make one explicit choice and then forget to modify that choice later when a new project begins with data in a new location. This ticket is considered complete when we've listed a few of the scenarios and evaluate the effect of making region configuration explicit. . See also: https://github.com/hail-is/hail/issues/13232. ### Version. 0.2.119. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13234:691,configurat,configuration,691,https://hail.is,https://github.com/hail-is/hail/issues/13234,1,['configurat'],['configuration']
Deployability,"### What happened?. No hail log file is available. > On 0.2.109: 5k samples and 8 interval lists -- WORKED; > 5k samples and 1 interval list -- WORKED; > On 0.2.120: 2k samples and 1 interval list -- WORKED; > On 0.2.120: 2k samples and 2 interval lists -- WORKED; > On 0.2.120: 2k samples and 4 interval list -- ERROR; > On 0.2.120: 2k samples and 8 interval list -- ERROR (edited); > ; > All of these runs were on driver: 96 CPU/684G RAM; > Workers 4 CPU and 8GB RAM; > Spark configuration allocated 512GB for driver; > ; > I have tried the above in various configurations... Maybe a specific interval list is problematic, but that does not seem to be the case; > ; > The interval lists are the same across runs.; > ; > And lastly, the error is the usual Py4J Error. Usually I address this w/ more driver RAM, but I can't go any higher and this used to work fine in Hail 0.2.109.; > ; > I tried downgrading from 120-->109, but I don't believe that I can in Terra, due to Spark incompatibilities. > filtered_mt is a MatrixTable that has already been split and filtered (to drop irrelevant variants). By the time the [following] code blocks are run, `filtered_mt = hl.read_matrix_table(filtered_mt_url)` has been executed.; > Some more information: The code after this (not shown [in the below code blocks]) does additional filtering. If I skip the step `variant_data.export(f""{variant_stat_file_path_stem}_FULL.tsv"")`, I can complete successfully. The issue is that we need the `*_FULL.tsv` output. So, I believe that this is likely a RAM issue on the driver, but this used to work. ```; variant_mt = generate_variant_stats(filtered_mt, interval_names, interval_table_dict). # Main loop to compute variant stats and save to files. # File path stem to use for saving variant stats over different interval lists; variant_stat_file_path_stem = f""{bucket}/batchE/{workflow_nickname}/variant_stats"". variant_data = variant_mt.cols(); variant_data.describe(); #variant_data.to_pandas().to_csv(f""{variant_st",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:478,configurat,configuration,478,https://hail.is,https://github.com/hail-is/hail/issues/13960,2,['configurat'],"['configuration', 'configurations']"
Deployability,"### What happened?. Notify these threads on completion:; - https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/exporting.20sites.20only.20VCF/near/376801844. Using QoB, reading out of GCS, we encounter corrupted blocks on this simple pipeline. ```; Caused by: com.github.luben.zstd.ZstdException: Corrupted block detected; at com.github.luben.zstd.ZstdDecompressCtx.decompressByteArray(ZstdDecompressCtx.java:157) ~[zstd-jni-1.5.2-1.jar:1.5.2-1]; at com.github.luben.zstd.Zstd.decompressByteArray(Zstd.java:409) ~[zstd-jni-1.5.2-1.jar:1.5.2-1]; at is.hail.io.ZstdInputBlockBuffer.readBlock(InputBuffers.scala:649) ~[gs:__hail-query-ger0g_jars_f00f916faf783b89cc2fc00bfc3e39df5485d8b0.jar.jar:0.0.1-SNAPSHOT]; at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:384) ~[gs:__hail-query-ger0g_jars_f00f916faf783b89cc2fc00bfc3e39df5485d8b0.jar.jar:0.0.1-SNAPSHOT]; at is.hail.io.BlockingInputBuffer.readByte(InputBuffers.scala:402) ~[gs:__hail-query-ger0g_jars_f00f916faf783b89cc2fc00bfc3e39df5485d8b0.jar.jar:0.0.1-SNAPSHOT]; ....; ```. A simplified version of the script:. ```python3; import hail as hl; import gnomad.utils.sparse_mt. tmp_dir = 'gs://bucket/'; vds_file = 'gs://neale-bge/bge-wave-1.vds'; out = 'gs://bucket/foo.vcf.bgz'. hl.init(default_reference = 'GRCh38',; tmp_dir = tmp_dir). vds = hl.vds.read_vds(vds_file); mt = hl.vds.to_dense_mt(vds); t = gnomad.utils.sparse_mt.default_compute_info(mt); t = t.annotate(info=t.info.drop('AS_SB_TABLE')); t = t.annotate(info = t.info.drop(; 'AS_QUALapprox', 'AS_VarDP', 'AS_SOR', 'AC_raw', 'AC', 'AS_SB'; )); t = t.drop('AS_lowqual'). hl.methods.export_vcf(dataset = t, output = out, tabix = True); ```. [batch-7751958-2713-main.log](https://github.com/hail-is/hail/files/12314207/batch-7751958-2713-main.log). ### Version. 0.2.120. ### Relevant log output. ```shell; Traceback (most recent call last):; File ""/Users/rye/Projects/VQSR/formatting-VQSR-vcf.py"", line 102, in <module>; main(args); File ""/Users/rye/Proj",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409:256,pipeline,pipeline,256,https://hail.is,https://github.com/hail-is/hail/issues/13409,1,['pipeline'],['pipeline']
Deployability,"### What happened?. One of our unit tests recently changed from taking around 20 seconds to being aborted by a time out after six hours — see populationgenomics/production-pipelines#352. This change turned out to coincide with the release of hail 0.2.113 and the unit test's `pip` selecting the new release. PR #12780 added a recursive `add_dependents` function to `LocalBackend`, that appears to be used to compute the transitive dependencies of each job. Profiling our unit test indicates that it is spending six hours inside this function with no end in sight. Running the job locally for a few seconds with more logging shows that it is calling `add_dependents` with the same `ancestor` and `child` millions of times. I'm not sure whether it's in an actual infinite loop or “merely” a combinatorial disaster than might terminate after a few months of runtime…. The following change, for example,. ```diff; --- a/hail/python/hailtop/batch/backend.py; +++ b/hail/python/hailtop/batch/backend.py; @@ -268,7 +268,7 @@ class LocalBackend(Backend[None]):; def add_dependents(ancestor, child):; dependent_jobs[ancestor].add(child); for ancestor_parent in ancestor._dependencies:; - add_dependents(ancestor_parent, child); + if child not in dependent_jobs[ancestor_parent]: add_dependents(ancestor_parent, child); ; for j in jobs:; for parent in j._dependencies:; ```. reduces it to calling it only once or twice for each `ancestor`/`child` combination, and returns the unit test to completing in ~20 seconds. I am not familiar enough with the data structure to say if that is a correct fix, but something of this nature appears to be needed to return this transitive dependency computation to a sensible runtime. ### Version. 0.2.113. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12915:172,pipeline,pipelines,172,https://hail.is,https://github.com/hail-is/hail/issues/12915,3,"['pipeline', 'release']","['pipelines', 'release']"
Deployability,"### What happened?. Reported by Ben Weisburd [here](https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/assert.20n_bytes.20.3C.20max_bunch_bytesize). ### Version. 0.2.132. ### Relevant log output. ```shell; File ""/Users/weisburd/code/step-pipeline/step_pipeline/batch.py"", line 300, in run; result = self._run_batch_obj(); File ""/Users/weisburd/code/step-pipeline/step_pipeline/batch.py"", line 368, in _run_batch_obj; result = self._batch.run(; File ""/usr/local/lib/python3.9/site-packages/hailtop/batch/batch.py"", line 716, in run; return async_to_blocking(self._async_run(dry_run, verbose, delete_scratch_on_exit, **backend_kwargs)) # type: ignore; File ""/usr/local/lib/python3.9/site-packages/hailtop/utils/utils.py"", line 186, in async_to_blocking; raise exc; File ""/usr/local/lib/python3.9/site-packages/hailtop/utils/utils.py"", line 181, in async_to_blocking; return loop.run_until_complete(task); File ""/usr/local/lib/python3.9/site-packages/nest_asyncio.py"", line 99, in run_until_complete; return f.result(); File ""/usr/local/Cellar/python@3.9/3.9.18_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/futures.py"", line 201, in result; raise self._exception; File ""/usr/local/Cellar/python@3.9/3.9.18_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py"", line 256, in __step; result = coro.send(None); File ""/usr/local/lib/python3.9/site-packages/hailtop/batch/batch.py"", line 747, in _async_run; run_result = await self._backend._async_run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs) # pylint: disable=assignment-from-no-return; File ""/usr/local/lib/python3.9/site-packages/hailtop/batch/backend.py"", line 901, in _async_run; await async_batch.submit(disable_progress_bar=disable_progress_bar); File ""/usr/local/lib/python3.9/site-packages/hailtop/batch_client/aioclient.py"", line 1234, in submit; start_job_group_id, start_job_id = await self._submit(; File ""/usr/local/lib/python3.9/site-packages/hailtop/batc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14637:261,pipeline,pipeline,261,https://hail.is,https://github.com/hail-is/hail/issues/14637,2,['pipeline'],['pipeline']
Deployability,"### What happened?. Reported by Wenhan: https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/ValueError.3A.20min.28.29.20arg.20is.20an.20empty.20sequence/near/352319161. ```; File ""/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/hailtop/batch/backend.py"", line 791, in _async_run; starting_job_id = min(j._client_job.job_id for j in unsubmitted_jobs); ```; I suspect we do not handle waiting on an empty batch / batch update correctly. We should probably bail out immediately from the wait when there are no jobs on which to wait. ### Version. batch. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12924:454,update,update,454,https://hail.is,https://github.com/hail-is/hail/issues/12924,1,['update'],['update']
Deployability,"### What happened?. See #13489 for context. We want to use terraform to keep track of artifact registry cleanup policies once it is available in Terraform. Relevant links:; https://github.com/hashicorp/terraform-provider-google-beta/commit/bc4aa512356891f78415d5f309bfe47b0697ac11; https://github.com/hashicorp/terraform-provider-google/issues/13824. It's not in 4.79.0 (see [what was added since then](https://github.com/hashicorp/terraform-provider-google-beta/compare/v4.79.0...main)). Releases appear to happen ~once a week, so we should be able to import into terraform in September. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13504:489,Release,Releases,489,https://hail.is,https://github.com/hail-is/hail/issues/13504,1,['Release'],['Releases']
Deployability,"### What happened?. See #14385 for details. The minimum supported TLS version for the `batch` and `CI` ABS storage accounts is 1.0, but since these are internal storage accounts, we should abide by [recommended TLS versions](https://developers.cloudflare.com/ssl/reference/protocols/#decide-which-version-to-use). I would assume that our fairly up-to-date python libs are using 1.3, but we should verify this and then upgrade the minimum TLS version to 1.3. ### Version. 0.2.128. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14392:418,upgrade,upgrade,418,https://hail.is,https://github.com/hail-is/hail/issues/14392,1,['upgrade'],['upgrade']
Deployability,"### What happened?. See [here](https://github.com/hail-is/hail/blob/main/dev-docs/services/batch/design.rst#canceller) for context on why this table exists and how it is used. Records are added or updated in this table whenever jobs are added to the database or after an attempt for a job completes. Records are currently only removed when the records belong to a cancelled job group. If a job group runs to completion, we end up with many rows in the database that no longer serve any purpose, and (if you sum over the `token` column), have 0s for all the job columns. This does not affect correctness, but is a lot of wasted space in the database. This leads to two points that together would save a lot of space in the database (I've not quantified how much but `select count(*)` on this table takes longer than I've been willing to wait. 1. Rows in this table with the same key `(batch_id, update_id, job_group_id, inst_coll)` but different `token` value can be ""compacted"" into one row with key `(batch_id, update_id, job_group_id, inst_coll, 0)` (token 0) where all the other columns are summed. This is most useful for cold rows.; 2. Rows whose `n_*_jobs` and `*_cancellable_cores_mcpu` columns are 0 can be deleted. We already do 1 for the [aggregated billing tables](https://github.com/hail-is/hail/blob/bc665db6993bb46d76e90e1a4ef7a15f661fa22d/batch/batch/driver/main.py#L1566). Use tokens for parallelism on hot rows and then compact records so that records from before the current day always end up only using 1 row. Implementing 1 should be a big win for the size of this table. Following that up with 2 would eliminate what I presume to be the vast majority of data in this table. ### Version. 0.2.132. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14623:197,update,updated,197,https://hail.is,https://github.com/hail-is/hail/issues/14623,1,['update'],['updated']
Deployability,### What happened?. See for example this Azure CI deploy job: https://batch.azure.hail.is/batches/4206544/jobs/18. That's from test_spectra. It doesn't need FASTAs at all. ### Version. 0.2.120. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13416:50,deploy,deploy,50,https://hail.is,https://github.com/hail-is/hail/issues/13416,1,['deploy'],['deploy']
Deployability,"### What happened?. Since we guarantee a job will run at least once, there are two issues that can happen:. 1. A user can write a pipeline in which two jobs race to write the same file, e.g.; ```; j = b.new_job(); j.command('echo hello > {j.out}'); j.write_output(j.out, ""gs://bucket/final-output""); ```; 2. Or, a clever user can avoid this race with some randomness:; ```; j = b.new_job(); j.command('echo hello gsutil cp - gs://bucket/final-output-$RANDOM'); ```. The former is a really common pattern and a bit of a footgun! The latter is rare (I don't know anyone who does it) and hard to work with: how would you know the output file of the *successful* attempt?. Hail should provide some mechanism for a user to get the list of successful attempts and their outputs. One simple option is to include some kind of seeded randomness which the user can access and to return either the seed or all the draws of the successful attempt for each job in `/jobs` or for the one job in `/job/{job_id}`. For example, consider:. ```; j = b.new_job(); j.command('echo hello gsutil cp - gs://bucket/final-output-$(/hail-random-str)'); ```. Where `/hail-random-str` is a binary we mount into the container that randomly generates numbers seeded by `(batch id, job id, attempt id)`. Hail should use the same randomness to ensure that `write_output` is reliable. We might also want a way to automatically remove the output files of the non-successful (e.g. preempted) attempts. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13502:130,pipeline,pipeline,130,https://hail.is,https://github.com/hail-is/hail/issues/13502,1,['pipeline'],['pipeline']
Deployability,"### What happened?. Sometimes when I push a branch to my fork using the `pre-push` hooks, I get errors that some scala files (which I did not touch) violate the pre-push hooks for `trailing-whitespace` and `end-of-file-fixer`. I suspect that this is because of the following:. 1. These hooks are not tested in CI and not everyone necessarily has them installed; 2. Therefore, changes that violate these hooks can make it into `main`; 3. (theory) When I run the `pre-push` hooks, many commits that I did not author are on the history that is being pushed to my fork. The pre-push hooks run on any changes on that history, and fail on those changes from 2.; 4. I am forced to use `--no-verify` or introduce trailing-whitespace fixes into an irrelevant PR. Note that there are two ways to satisfy the title. The easiest thing might to not run the untested hooks on `pre-push`, but preferably we should just test all the pre-commit hooks in CI. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13875:351,install,installed,351,https://hail.is,https://github.com/hail-is/hail/issues/13875,1,['install'],['installed']
Deployability,"### What happened?. Suppose you're working with the [Wheat genome](https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/Other.20genome/near/397467764). The following is seemingly correct code but it doesn't work:; ```python3; import hail as hl. rgwheat = hl.ReferenceGenome('Wheat', ...). hl.init(default_reference=rgwheat); ```; The first problem is that the `@typecheck` on `hl.init`, `hl.init_spark`, etc. only allows a built-in reference genome. . Even if we relax that requirement, we encounter a deeper problem: creating the reference genome initializes Hail. In particular, [we call `Env.backend()`](https://github.com/hail-is/hail/blob/main/hail/python/hail/genetics/reference_genome.py#L117-L118) (which calls `Env.hc()`, which forces initialization) so that we can call `add_reference`. What does initialization mean? Historically, it meant connection to or starting a JVM/Spark process. In QoB/ServiceBackend, initialization just loads configurations, it doesn't really do anything irreversible. Regardless of what it does, we only allow initialization *once*. OK, so, there's two possible routes to fix this problem:; 1. Rewrite `ReferenceGenome.__init__` such that it does not initialize Hail. You have to decide how reference genomes are ultimately communicated to the backend. Do you hang a list of all created reference genomes off of the `ReferenceGenome` class? Do you require explicit registering a la `hl.register_reference`? The latter seems a bit silly. The former seems OK, but you could also ...; 2. Allow modification of the default reference after initialization. The default reference genome is just a field on the HailContext: `_default_ref` which is accessed through `hl.default_reference()`. Just modify `hl.default_reference` to *return* the reference with no arguments and *set* the reference with one argument. Now this works:. ```python3; import hail as hl; rgwheat = hl.ReferenceGenome('Wheat', ...); hl.default_reference(rgwheat); mt = hl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13856:974,configurat,configurations,974,https://hail.is,https://github.com/hail-is/hail/issues/13856,1,['configurat'],['configurations']
Deployability,"### What happened?. The CloudSQL backups of the database are backed up into a multi-regional bucket. These need to be switched to regional. There's a setting in the backups part of the cloud console. We should make sure this doesn't mess up the Terraform configuration. The current database size is 1TB.; <img width=""1171"" alt=""Screenshot 2024-03-22 at 4 57 07 PM"" src=""https://github.com/hail-is/hail/assets/1693348/3bfc6d72-f7ab-4083-a2f5-c7053ae021b8"">. ### Version. 0.2.128. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14422:255,configurat,configuration,255,https://hail.is,https://github.com/hail-is/hail/issues/14422,1,['configurat'],['configuration']
Deployability,### What happened?. The VEP GRCh38 image failed to build due to some CPAN issue when installing the big file package. e.g. https://ci.azure.hail.is/batches/3751379/jobs/29. ### Version. c5af9ba0032e6cfe4640d0df652d763dcbdf1e63. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12946:85,install,installing,85,https://hail.is,https://github.com/hail-is/hail/issues/12946,1,['install'],['installing']
Deployability,"### What happened?. The `pyright` pip package is a shim around the `pyright` npm package and tries to install node + npm pyright at runtime if they're not in the PATH. Not only is it wasteful to reinstall them all the time, but it can be [flaky](https://pypi.org/project/pyright/). This should be a build step that happens in the `hail_linter_image` docker image. A solution could trigger pip pyright to install node + npm pyright at image build time or do away with the python shim entirely. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13722:102,install,install,102,https://hail.is,https://github.com/hail-is/hail/issues/13722,2,['install'],['install']
Deployability,"### What happened?. The accounting term for what we call a resource is a ""SKU"". We need to update our SQL and front end to use this standard terminology so as to simplify interactions with financial professionals. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13791:91,update,update,91,https://hail.is,https://github.com/hail-is/hail/issues/13791,1,['update'],['update']
Deployability,"### What happened?. The deduped_resource_id and resource_id columns should not both exist. Currently, when you add a new resource you have to first insert it (with a null deduped_resource_id) and then update the deduped_resource_id with whatever auto increment value your transaction won. We should collapse these columns (and keep an ""old_resource_id"", if necessary) so that our code base is simple to understand. ### Version. 0.2.122. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13657:201,update,update,201,https://hail.is,https://github.com/hail-is/hail/issues/13657,1,['update'],['update']
Deployability,"### What happened?. The infrastructure necessary to run a Hail Batch deployment (network, buckets, DB, Kubernetes cluster) are managed through Terraform in `infra/gcp` and `infra/azure`. In order to migrate terraform resources, the terraform module need to be given input variables specific to our deployment provided through a `global.tfvars` file. Since this file contains secrets, in GCP we encrypt the file with [SOPS](https://github.com/getsops/sops) and check it into the repo so that any developer with the credentials to our deployment can run the terraform. This is not the case in Azure, so if a developer wants to run the Azure terraform they have to obtain the `global.tfvars` from myself. We should use the same strategy for communicating this file as we do in GCP. ### Version. 0.2.129. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14457:69,deploy,deployment,69,https://hail.is,https://github.com/hail-is/hail/issues/14457,3,['deploy'],['deployment']
Deployability,"### What happened?. This code block should work:. ```python; import hailtop.fs as hfs. with hfs.open('gs://foo/bar.txt') as f:; f.seek(2); print(f.read()); ```. But this fails with an `OSError` because the `ReadableStream`s that we produce from blob storage are not seekable. We normally stream through the whole contents of the blob with one GET, so a seek implementation should probably discern based on buffer size whether to skip bytes in the client's buffer or drop the current stream, update a client-side position, and initiate a new request on the next read. ### Version. 0.2.116. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13052:491,update,update,491,https://hail.is,https://github.com/hail-is/hail/issues/13052,1,['update'],['update']
Deployability,"### What happened?. To understand a running pipeline, I need realtime feedback on the profile, not just the final profile at the end of the job. I think `dump` will generate a flame graph on demand. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14073:44,pipeline,pipeline,44,https://hail.is,https://github.com/hail-is/hail/issues/14073,1,['pipeline'],['pipeline']
Deployability,"### What happened?. Using this image https://hub.docker.com/layers/gneak123/guide_browser/latest/images/sha256-8ca2a921828cd147e86519051a0da68522983b7b49f22e1280c83ab9c56b3129?context=explore, deploy a 1 pod deployment into a fresh namespace and serve it at guide-analysis.hail.is. It need not be in build.yaml nor in the infrastructure (just like Duncan's stuff isn't). We're just hosting it as a service to the lab. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14067:193,deploy,deploy,193,https://hail.is,https://github.com/hail-is/hail/issues/14067,2,['deploy'],"['deploy', 'deployment']"
Deployability,### What happened?. We are leaving tasks alive when workers shut down and we do not know which tasks they are. This issue has two parts:. 1. Fix `dump_all_stacktraces` to actually show all the outstanding tasks. Perhaps `log.debug` isn't generating output b/c of our logging configuration.; 2. Figure out why these tasks are running and prevent them from staying running. Several examples [here](https://cloudlogging.app.goo.gl/aMfqzLB4FBa864WJ7). ### Version. 0.2.124. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13908:275,configurat,configuration,275,https://hail.is,https://github.com/hail-is/hail/issues/13908,1,['configurat'],['configuration']
Deployability,"### What happened?. We do not have a formal deprecation and removal process for Batch features. Users might not realize that they need to upgrade until they try to run a workload that fails, and even then they might think there's just a bug and come to us for help. The only thing we can do to mitigate this is wait a ""long enough"" time that we think everyone has upgraded. We should add functionality to the batch client such that, when it receives a `X-Hail-Deprecated` response header from the batch service, it warns the user that they are using deprecated or removed functionality (with a helpful description) and that they should upgrade. This should promote timely upgrades when we deprecate features and reduce support burden when we remove features. ### Version. 0.2.130. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14485:138,upgrade,upgrade,138,https://hail.is,https://github.com/hail-is/hail/issues/14485,4,['upgrade'],"['upgrade', 'upgraded', 'upgrades']"
Deployability,### What happened?. We don't have code to automatically update network prices. We should either add that code or manually update the prices. ### Version. 0.2.124. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13784:56,update,update,56,https://hail.is,https://github.com/hail-is/hail/issues/13784,2,['update'],['update']
Deployability,"### What happened?. We expect the matrix table abstraction to break down at 2M+ samples. Consider that each row now has 2M entries. Even if all the RAM of a highmem machine was made available to row processing, we only have 6.5kB per sample. This is a fair bit of memory! But many reasonable Hail pipelines will generate enough garbage to overflow this. We must develop a block-partitioned matrix table. Instead of each partition containing the entries for every column and a subset of rows, each partition will contain the entries for a subset of rows and columns. While there are many ways to do this partitioning, a simple and workable first try is a ""grid""-like partitioning. Every matrix table has a set of row-key intervals and column-key intervals which define the partitioning. The bounds of the partitions are given by the cross product of these sets of intervals. The visual conception of the partitioning of this matrix table (with its globals, column margin data, row margin data, and entry data) might look like:. ```; +--+ +-----+--+--++---+------+------+; | | | | | || | | |; +--+ +-----+--+--++---+------+------+. ck1 ck2 ...; +--+ rk1 +-----+--+--++---+------+------+; | | | | | || | | |; | | | | | || | | |; +--+ rk2 +-----+--+--++---+------+------+; | | | | | || | | |; +--+ ... +-----+--+--++---+------+------+; | | | | | || | | |; | | | | | || | | |; | | | | | || | | |; +--+ +-----+--+--++---+------+------+; | | | | | || | | |; | | | | | || | | |; +--+ +-----+--+--++---+------+------+; +--+ +-----+--+--++---+------+------+; | | | | | || | | |; | | | | | || | | |; | | | | | || | | |; +--+ +-----+--+--++---+------+------+; ```. The first row-key interval is `[rk1, rk2)`. The first col-key interval is `[ck1, ck2)`. These intervals are define a ""rectangle"" corresponding to the first partition. . All the partitions in the fourth partition column are empty (perhaps these column keys are absent in this dataset). Likewise, all the partitions in the fifth partition row are emp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13800:297,pipeline,pipelines,297,https://hail.is,https://github.com/hail-is/hail/issues/13800,1,['pipeline'],['pipelines']
Deployability,"### What happened?. We have been burned by rare transient errors in Google Cloud Storage three times now. 1. https://github.com/hail-is/hail/pull/14080; 2. https://github.com/hail-is/hail/issues/13721; 3. https://github.com/hail-is/hail/issues/13937. > Fool me once, shame on you, fool me twice shame on me. Before releases, Hail *must* run tests that read on the order of 10 TiB of data so as to ensure that any changes since the last release do not introduce rare transient bugs or at-scale-only memory issues that our users will discover. There are at least four tests in my mind:; 1. Large-scale linear algebra (e.g. PC-Relate & LD-Prune).; 2. gnomAD style frequency calculations on 1M samples grouped 500 ways.; 3. The VDS Combiner.; 4. `show` on 1M samples. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14082:315,release,releases,315,https://hail.is,https://github.com/hail-is/hail/issues/14082,2,['release'],"['release', 'releases']"
Deployability,"### What happened?. We recieved an email bug report that using hail plotting with Bokeh 3.4 installed generates a lot of deprecation warnings. This is in the All of Us workbench, so they don't have direct control over the installation. We should update hail to work with Bokeh 3.4 if it's not too much work. <img width=""892"" alt=""image001"" src=""https://github.com/user-attachments/assets/2e88ec23-2c98-4b24-8dcd-df491462811d"">. ### Version. 0.2.132. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14706:92,install,installed,92,https://hail.is,https://github.com/hail-is/hail/issues/14706,3,"['install', 'update']","['installation', 'installed', 'update']"
Deployability,"### What happened?. When the QoB client on a user's laptop sends a request to create a QoB job, it sends a `jar_spec` parameter as part of the job spec that is either:; - `git_revision`: the git SHA that the hail was built with. The Batch front end takes this and resolves a URL for the published JAR that was created when that commit was merged to `main`.; - `jar_url`: A blob storage URL that points directly to the JAR to use. The Batch front end ensures that this URL is trusted. The `jar_url` setting is mainly for development and debugging purposes, allowing a dev or user to set a URL to a development JAR instead of using a merged commit. In normal configuration fashion, it is possible to set `jar_url` in `hailctl config`. This is an enormous footgun, as users may forget to unset this configuration and continue using the dev jar *even after they install a different hail wheel*. We must do two things:; 1. Remove the ability to set the jar_url through `hailctl` so as to avoid this footgun. Batch should also fully remove support for `jar_url`s so that any users who might be inadvertently using it are loudly alerted (though I suspect there are few if any such users now).; 2. Remove entirely the ability to specify a JAR other than that which was built along with the installed wheel. The proposed plan is to always send `git_revision` for QoB jobs. In order to enable development JARs, Batch should be augmented to search first for production JARs matching a certain revision, and then if that fails search a specified `dev/` subdirectory for the requested revision. These development JARs should not be cached on workers so as to enable debugging development without constant committing. ### Version. 0.2.130. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14539:657,configurat,configuration,657,https://hail.is,https://github.com/hail-is/hail/issues/14539,4,"['configurat', 'install']","['configuration', 'install', 'installed']"
Deployability,### What happened?. [Open ID Connect](https://auth0.com/docs/authenticate/protocols/openid-connect-protocol) is a standard that allows you to basically exchange a proof of identity from identity provider X for an authorized token at identity provider Y. We should support using GCP credentials + OIDC to copy files to and from AWS and Azure. We should then remove the AWS and Azure keys from our GCP deployment that are used to run inter-cloud copy tests. ### Version. 0.2.122. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13613:400,deploy,deployment,400,https://hail.is,https://github.com/hail-is/hail/issues/13613,1,['deploy'],['deployment']
Deployability,"### What happened?. [SAIGE](https://github.com/weizhouUMICH/SAIGE) and its competitor [REGENIE](https://rgcgithub.github.io/regenie/) are the standard bearers for modern GWAS. Hail should expose SAIGE within the Hail Query language. The interface should roughly match `hl.linear_regression_rows`. A Batch pipeline would serve the needs of Broadies (and, indeed, such a pipeline already exists) but has two downsides:; 1. There is substantial I/O involved in exporting the data from Hail-native formats to SAIGE-compatible formats.; 2. Non-Broadies cannot use this pipeline. Query language support for SAIGE would transform the accessibility of SAIGE by making it usable at scale by anyone with access to Hail, which is basically anyone with a large dataset (e.g. [DNANexus](https://med.stanford.edu/gbsc/projects/vapahcs.html), [AoU RWB](https://support.researchallofus.org/hc/en-us/articles/6090679838100-How-to-Work-with-All-of-Us-Genomic-Data-Hail-Plink-), [MVP](https://med.stanford.edu/gbsc/projects/vapahcs.html), [FinnGen](https://www.medrxiv.org/content/10.1101/2022.03.03.22271360v1.full)). There are two options:; 1. Determine and implement the linear algebraic primitives necessary for SAIGE.; 2. Compile and link directly against SAIGE. Expose these functions, via JNI, to the Hail Query language. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13442:305,pipeline,pipeline,305,https://hail.is,https://github.com/hail-is/hail/issues/13442,3,['pipeline'],['pipeline']
Deployability,"### What happened?. [a user attempted to run `conda install bioconda::hail` or some variation of that](https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/jdk11/near/418930859), and it looks like [the hail package on anaconda.org](https://anaconda.org/bioconda/hail) has [a dependency](https://bioconda.github.io/recipes/hail/README.html) on `openjdk 8.*`, which prevents users from using `openjdk 11.*` with it, and is also a pretty old version from over a year ago. the recipe for the hail package on anaconda.org should be updated to have accurate dependencies and use the current version of hail, or we should ask that it be removed if we recommend only installing hail through `pip`. ### Version. n/a. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14229:52,install,install,52,https://hail.is,https://github.com/hail-is/hail/issues/14229,3,"['install', 'update']","['install', 'installing', 'updated']"
Deployability,### What happened?. ```; + gcloud artifacts repositories set-cleanup-policies hail --project=hail-vdc --location=us --policy=/io/repo/infra/gcp-broad/gcp-ar-cleanup-policy.txt --no-dry-run; ERROR: (gcloud.artifacts.repositories) Invalid choice: 'set-cleanup-policies'.; ```. I think the release needs an updated version of cloud and then this should work fine. ### Version. 0.2.127. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14154:287,release,release,287,https://hail.is,https://github.com/hail-is/hail/issues/14154,2,"['release', 'update']","['release', 'updated']"
Deployability,### What happened?. `deploy` tries to use a local docker image. Needs to use the pushed image instead. ### Version. 0.2.132. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14642:21,deploy,deploy,21,https://hail.is,https://github.com/hail-is/hail/issues/14642,1,['deploy'],['deploy']
Deployability,"### What happened?. `hail-0.2.129-py3-none-any.whl` bundled a version of `hailtop/hailctl/deploy.yaml` that was intended for internal testing only. This file provides configuration variables for `hailctl`. The file in [0.2.129](https://github.com/hail-is/hail/releases/tag/0.2.129) pointed to cloud resources in `gs://hail-30-day/` that cause commands like `hailctl dataproc start` to fail due to one of the following:; - the user does not have access to `gs://hail-30-day`, or; - the resources have been deleted according to the bucket's 30-day lifecycle policy. ### Version. 0.2.129. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14452:90,deploy,deploy,90,https://hail.is,https://github.com/hail-is/hail/issues/14452,3,"['configurat', 'deploy', 'release']","['configuration', 'deploy', 'releases']"
Deployability,"### What happened?. dev namespace scaling is doing a good job of keeping costs down over the weekend and over night, but if devs don't `k delete deployments -n NAMESPACE` and `k delete statefulsets -n NAMESPACE`, these deployments stick around all day every day. It seems, in practice, our db's get an entire 2 core node to themselves. Let's eliminate the scale-up job. We will keep the cronjob that scales down namespaces at the end of each workday. We will provide a scale-up command to either the make targets or `devbin/functions.sh`. `hailctl dev deploy`, of course, would also force a scale up (while also blowing away whatever was there before). ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14020:145,deploy,deployments,145,https://hail.is,https://github.com/hail-is/hail/issues/14020,3,['deploy'],"['deploy', 'deployments']"
Deployability,"### What happened?. e.g. see an attempt to use build_python_image to execute some Hail code in the cloud: https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/Error.20in.20QoB.3A.20unknown.20opcode. We should provide users with a simple and straightforward project structure and mechanism for working with a local Python project, its Python dependencies, including, possibly, Hail. It seems to me that a relatively straightforward way to do this would be to recommend the user create a normal, installable python package (and provide instructions on doing so), and then to provide some Hail Batch client functionality that builds an image based on `hailgenetics/hail` (if Hail is required) or `hailgenetics/python-dill` or a user-provided base image (which must have Python, but we'll ensure dill gets installed). The Dockerfile should look something like:. ```; FROM {base_image}; COPY {users_project_dir} /users_project; RUN pip install /users_project; ```. And then that image can be used as the python_default_image (maybe also the default_image?). ### Version. 0.2.117. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13161:515,install,installable,515,https://hail.is,https://github.com/hail-is/hail/issues/13161,3,['install'],"['install', 'installable', 'installed']"
Deployability,### What happened?. https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/How.20to.20use.20hailctl.20batch.20submit.3F/near/385496357. Solution: I think the issue is we're missing this configuration in `hailctl batch submit` https://typer.tiangolo.com/tutorial/commands/context/#configuring-the-context. ### Version. 0.2.120. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13446:205,configurat,configuration,205,https://hail.is,https://github.com/hail-is/hail/issues/13446,1,['configurat'],['configuration']
Deployability,"### What happened?. https://typesense.org/about/. Typesense is open source, so we can just run it in k8s and have a working search for every dev deploy. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13548:145,deploy,deploy,145,https://hail.is,https://github.com/hail-is/hail/issues/13548,1,['deploy'],['deploy']
Deployability,"### What happened?. while trying to run the following code I get the error mention in the title (Invalid maximum heap size: -Xmx0m). import hail as hl; hl.init(default_reference=""GRCh38""). However I tried to resolve the issue with overloading the default setting with new values for spark configuration (command below), unfortunately the error still exists; hl.init(driver_memory='1024m’). ### Version. latest version used in allOfUs research workbench platform. ### Relevant log output. ```shell; Invalid maximum heap size: -Xmx0m; Error: Could not create the Java Virtual Machine.; Error: A fatal exception has occurred. Program will exit.; ---------------------------------------------------------------------------; RuntimeError Traceback (most recent call last); Cell In[14], line 2; 1 #hl.init(default_reference=""GRCh38""); ----> 2 hl.init(driver_memory='1024m'). File <decorator-gen-1756>:2, in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, backend, driver_cores, driver_memory, worker_cores, worker_memory, gcs_requester_pays_configuration, regions, gcs_bucket_allow_list). File /opt/conda/lib/python3.10/site-packages/hail/typecheck/check.py:587, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 584 @decorator; 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_). File /opt/conda/lib/python3.10/site-packages/hail/context.py:364, in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, backend, driver_cores, driver_memory, worker_cores, worker_memory, gcs_requester_pays_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14489:289,configurat,configuration,289,https://hail.is,https://github.com/hail-is/hail/issues/14489,1,['configurat'],['configuration']
Deployability,"#### Summary. It has happened twice. The failing partition is different in each run. 1. 49340 https://batch.hail.is/batches/8069235/jobs/51280; 2. 25997 https://batch.hail.is/batches/8083195/jobs/27937. The pipeline runs two table collects to get sample information, then converts the matrix table to a table of ndarrays of the value `hl.int(hl.is_defined(mt.GT))`. The entries are getting subsetted, so there is skipping going on. In both cases, we are decoding the entry array when the corrupted block is discovered. In the first case, we are skipping an int (must be RGQ based on the etype and type). In the second case, we are decoding a string (must be FT). Since the error happens on a seemingly arbitrary partition, it seems likely this is related to our transient error handling. Both runs use a version of Hail after we fixed the broken transient error handling in GoogleStorageFS (run 1 used fcaafc533e, run 2 used 0.2.126 / ee77707f4f). ---. #### Path forward. If it *is* a transient error, we need to fix how we handle transient errors. Maybe our position handling logic is wrong? If it is *not* a transient error, maybe our skipping logic is wrong? FT appears immediately after RGQ and we know RGQ is getting skipped. Our implementation of `seek` for the compressed block buffers looks sketchy to me, but we're using PartitionNativeReader which does no seeking. Action items:; 1. Log every transient error.; 2. Log the file name and the offset on failure. ---. #### Debugging information. EType:; ```; +EBaseStruct{; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:+EArray[; EBaseStruct{; GT:EInt32,; GQ:EInt32,; RGQ:EInt32,; FT:EBinary,; AD:EArray[EInt32]}]}; ```; (zipped) Type:; ```; Struct{; locus:Locus(GRCh38),; alleles:Array[String],; filters:Set[String],; info:Struct{; AC:Array[Int32]},; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[; Struct{; GT:Call,; GQ:Int32,; FT:String,; AD:Array[Int32]}]}; ```; Source buffer spec:; ```; {""name"":""LEB128BufferSpec"",""child"":; {",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623:207,pipeline,pipeline,207,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623,1,['pipeline'],['pipeline']
Deployability,"###Hail version; N/A Kubernetes v1 API, cluster version 1.10.11. ### What you did; Attempted to schedule a pod through app.hail.is. Waited ~20 minutes. ### What went wrong (all error messages here, including the full java stack trace); Simply stuck in Not PodScheduled (status.condition contains an entry of {status: False, type: PodScheduled} ). This status is also verified using kubectl get pods -w. Total number of pods did not seem onerous by quantity alone, so this must be an issue of resource utilization by some of these pods. ```sh; NAME READY STATUS RESTARTS AGE; apiserver-8658d59d48-r8p6w 1/1 Running 0 9d; auth-gateway-deployment-7d7cf8846f-l5m9b 1/1 Running 0 14h; batch-deployment-6448f84d9c-gxn2c 1/1 Running 0 1h; dk-test-58dffcd944-9xkkx 1/1 Running 0 11d; frontend-766c875db4-cmpvx 1/1 Running 0 8d; gateway-deployment-78c4dd64f5-tdnnc 1/1 Running 0 1h; hail-ci-deployment-5744fd6964-s29xb 1/1 Running 0 1h; image-fetcher-bkpcc 1/1 Running 0 23m; image-fetcher-gb9rs 1/1 Running 0 26m; image-fetcher-glj5p 1/1 Running 0 25m; image-fetcher-kjd7z 1/1 Running 0 23m; image-fetcher-vhv74 1/1 Running 0 25m; image-fetcher-zppvc 1/1 Running 0 24m; notebook-api-deployment-7bb85bfd-z6mvp 1/1 Running 0 12h; notebook-deployment-8546dbcb7c-zfc4r 1/1 Running 0 1h; notebook-worker-2lt2l 1/1 Running 0 46m; notebook-worker-77nqq 1/1 Running 0 1h; notebook-worker-fljx6 1/1 Running 0 3h; notebook-worker-gm6lz 1/1 Running 0 36m; notebook-worker-kj7bb 1/1 Running 0 3h; notebook-worker-n8dgv 0/1 Pending 0 4m; notebook-worker-pshdf 1/1 Running 0 35m; scorecard-deployment-654f774444-vwpzr 1/1 Running 0 51m; site-deployment-6789bd6c5b-lxbxk 1/1 Running 0 51m; spark-master-6f7678b449-jcbnp 1/1 Running 0 9d; spark-worker-569866dff7-l452k 1/1 Running 0 9d; spark-worker-569866dff7-xzmx4 1/1 Running 0 9d; upload-658d7f8c7d-gvj4h 1/1 Running 0 51m; web-deployment-bc6497cdb-qfc9g 1/1 Running 0 2h; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5269:633,deploy,deployment-,633,https://hail.is,https://github.com/hail-is/hail/issues/5269,9,['deploy'],['deployment-']
Deployability,#10612 Is the only user-visible change to batch since the last release.; > CHANGELOG: Made failed Python Jobs have non-zero exit codes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10677#issuecomment-882662210:63,release,release,63,https://hail.is,https://github.com/hail-is/hail/pull/10677#issuecomment-882662210,1,['release'],['release']
Deployability,#10957 introduced loading GCP-specific configuration from the global-config instead of environment variables. The auth-driver uses this but doesn't have a global-config mounted.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10962:39,configurat,configuration,39,https://hail.is,https://github.com/hail-is/hail/pull/10962,1,['configurat'],['configuration']
Deployability,"#11016 Updated default hail version in Makefile, but build.gradle warns if you're not using 3.1.1. That's silly, everything should be 3.1.2 now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11068:7,Update,Updated,7,https://hail.is,https://github.com/hail-is/hail/pull/11068,1,['Update'],['Updated']
Deployability,"#13008 Started using the ci-utils from the CI pipeline for the database jobs, but we actually can't use it safely for the database cleanup step because we might untag the image before the database cleanup jobs run.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13014:46,pipeline,pipeline,46,https://hail.is,https://github.com/hail-is/hail/pull/13014,1,['pipeline'],['pipeline']
Deployability,#13439 updated tornado,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13470#issuecomment-1692321452:7,update,updated,7,https://hail.is,https://github.com/hail-is/hail/pull/13470#issuecomment-1692321452,1,['update'],['updated']
Deployability,"#14056 added a new optional field to the deploy config, `base_path` which is intended to phase out `default_namespace`. But for backwards compatibility reasons we cannot yet remove `default_namespace`. This should all work fine without breaking any workflows like switching back and forth between namespaces so long as `base_path` is not explicitly set in a developer's deploy config. But `hailctl dev config set <property> <value>` does not just set a single property, it loads the deploy config, sets the property, and then writes the whole deploy config back. If the deploy config does anything with default values, which it now does with `base_path`, this round trip does not work. Another simpler example is that currently in main, the following will make two changes to a deploy config not one:. ```; # deploy config of {'location': 'external', 'domain': 'hail.is', 'default_namespace': 'default'}; HAIL_DOMAIN=foo hailctl dev config set location gce. # deploy config will now read {'location': 'gce', 'domain': 'foo', 'default_namespace': 'default'}; ```. This PR should change `hailctl dev config set` so that the only changes that are made to the deploy config are the single property/value change described in the command.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14169:41,deploy,deploy,41,https://hail.is,https://github.com/hail-is/hail/pull/14169,9,['deploy'],['deploy']
Deployability,#14198 removed the GAR-related parameters to release.sh but they were still passed in in build.yaml so it refused to run.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14307:45,release,release,45,https://hail.is,https://github.com/hail-is/hail/pull/14307,1,['release'],['release']
Deployability,"#14301 Switched image building steps in the CI pipeline over to just using CI's own credentials instead of this special `registry-push-credentials` secret. `registry-push-credentials` is now unused in the codebase and can be removed. Will run this when the PR is approved, then merge.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14320:47,pipeline,pipeline,47,https://hail.is,https://github.com/hail-is/hail/pull/14320,1,['pipeline'],['pipeline']
Deployability,"#14560 updated `to_dense_mt` to take into account reference the existence of reference GT fields. However, it was untested. I take our old `test_to_dense_mt` test, and add a haploid `LGT` field to the reference, and check to make sure that the haploid reference is present in the result.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14608:7,update,updated,7,https://hail.is,https://github.com/hail-is/hail/pull/14608,1,['update'],['updated']
Deployability,"#14609 broke the routing for https://hail.is. The `domains` variable here indicates to `gateway` which domains in incoming requests should be routed to the given service. Since #14609 changed the `service` parameter from `str` to `Service`, it silently broke this branch. This wasn't covered by the envoy config generation tests because we didn't have a `www` service in the test configuration. I've added it so that this branch is covered. Fixes #14616",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14617:380,configurat,configuration,380,https://hail.is,https://github.com/hail-is/hail/pull/14617,1,['configurat'],['configuration']
Deployability,"#26</a> (<a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/ipython/comm/graphs/contributors?from=2024-01-02&amp;to=2024-03-12&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Aipython%2Fcomm+involves%3Ablink1073+updated%3A2024-01-02..2024-03-12&amp;type=Issues""><code>@​blink1073</code></a> | <a href=""https://github.com/search?q=repo%3Aipython%2Fcomm+involves%3Apre-commit-ci+updated%3A2024-01-02..2024-03-12&amp;type=Issues""><code>@​pre-commit-ci</code></a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ipython/comm/blob/main/CHANGELOG.md"">comm's changelog</a>.</em></p>; <blockquote>; <h2>0.2.2</h2>; <p>(<a href=""https://github.com/ipython/comm/compare/v0.2.1...76149e7ee0f331772c964ae86cdb8bafebe6dfa2"">Full Changelog</a>)</p>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Update Release Scripts <a href=""https://redirect.github.com/ipython/comm/pull/27"">#27</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Other merged PRs</h3>; <ul>; <li>chore: update pre-commit hooks <a href=""https://redirect.github.com/ipython/comm/pull/26"">#26</a> (<a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/ipython/comm/graphs/contributors?from=2024-01-02&amp;to=2024-03-12&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Aipython%2Fcomm+involves%3Ablink1073+updated%3A2024-01-02..2024-03-12&amp;type=Issues""><code>@​blink1073</code></a> | <a href=""https://github.com/search?q=repo%3Aipython%2Fcomm+involves%3Apre-commit-ci+updated%3A2024-01-02..2024-03-12&amp;type=Issues""><code>@​pre-commit-ci</code></a></p>; <!-- raw HTML omitted -->;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14492:1784,Update,Update,1784,https://hail.is,https://github.com/hail-is/hail/pull/14492,2,"['Release', 'Update']","['Release', 'Update']"
Deployability,"#313 : changed the Interval created from BED file so [Start, End) in BED file becomes [Start + 1, End] in Hail. See updated docs.; #257 : added annotatesamples list which creates a new boolean annotation based on whether a sample id is in the input file or not; #319 : changed regular expression parsing to handle contig\tstart\tend in addition to config:start-end",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/398:116,update,updated,116,https://hail.is,https://github.com/hail-is/hail/pull/398,1,['update'],['updated']
Deployability,"#4659 teaches CI to recover from a build job gone missing, but; I neglected to teach CI how to recover from a deploy job gone; missing. This follows the same strategy but for deploy jobs. If a deploy job is not found in the list of refreshed jobs; it is simply removed from the deploy_jobs map. The next heal; stage of CI will kick off a new batch job for whatever the; latest undeployed SHA is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4683:110,deploy,deploy,110,https://hail.is,https://github.com/hail-is/hail/pull/4683,3,['deploy'],['deploy']
Deployability,"#6927 Should fix the issue I was seeing with the warnings. The issue was that to the `make` invocations that we were making in the compilation of C++ pipelines, it looked like they were in jobserver mode. But they we didn't preserve the right state to execute them. . I think the real solution is unsetting `MAKEFLAGS` in the environment in `pgradle`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6923#issuecomment-524203427:150,pipeline,pipelines,150,https://hail.is,https://github.com/hail-is/hail/pull/6923#issuecomment-524203427,1,['pipeline'],['pipelines']
Deployability,"#8403 was force merged, hand deploying again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8402#issuecomment-606699423:29,deploy,deploying,29,https://hail.is,https://github.com/hail-is/hail/pull/8402#issuecomment-606699423,1,['deploy'],['deploying']
Deployability,"#908</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/jupyter/jupyter_client/compare/v7.4.8...v8.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jupyter-client&package-manager=pip&previous-version=7.4.8&new-version=8.0.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12656:10247,upgrade,upgrade,10247,https://hail.is,https://github.com/hail-is/hail/pull/12656,3,['upgrade'],['upgrade']
Deployability,#9500 didn't update the `hail/hail` makefile,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9567:13,update,update,13,https://hail.is,https://github.com/hail-is/hail/pull/9567,1,['update'],['update']
Deployability,"#9634 Introduced a large performance regression in the `linear_regression_rows_nd` benchmark (making it about 4x slower). This PR fixes that by doing two things:. 1. Move all the global into one single `annotate_globals` expression, so that CSE can work properly. This required fixing a bug in some ndarray expressions that were not correctly tracking their source tables. To make sure I was only referencing the global versions of this computed things, rather accidentally recomputing, I wrapped the global setup in a function to scope the variables. This improvement was minor, didn't hit the real root of the problem. 2. Much more significantly, and not 100% clear why: `process_y_group` is now a function that returns a python dictionary, instead of a hail struct. I can guess that the allocation required by making a struct was wasteful, but it seems crazy that it was ""make the benchmark 4x slower"" amounts of wasteful. . While this is not user facing yet, would be good to get this in before an eventual 0.2.60 release if we want to avoid benchmarks regressing between versions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9666:1018,release,release,1018,https://hail.is,https://github.com/hail-is/hail/pull/9666,1,['release'],['release']
Deployability,"#PdhEnumObjectItems (<a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1442"">#1442</a>)</li>; <li><a href=""https://github.com/java-native-access/jna/commit/99fcfa822db86b1f2ba5823dbf17efeb3d246ad5""><code>99fcfa8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1444"">#1444</a> from matthiasblaesing/update_libffi</li>; <li><a href=""https://github.com/java-native-access/jna/commit/9e473350a5ad5e04aab8b01e4018f973976e19f8""><code>9e47335</code></a> Update CHANGES.md</li>; <li>Additional commits viewable in <a href=""https://github.com/java-native-access/jna/compare/5.6.0...5.12.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=net.java.dev.jna:jna&package-manager=gradle&previous-version=5.6.0&new-version=5.12.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12438:8171,update,updates,8171,https://hail.is,https://github.com/hail-is/hail/pull/12438,1,['update'],['updates']
Deployability,"#version-2-1-2"">https://flask.palletsprojects.com/en/2.1.x/changes/#version-2-1-2</a></li>; <li>Milestone: <a href=""https://github.com/pallets/flask/milestone/21?closed=1"">https://github.com/pallets/flask/milestone/21?closed=1</a></li>; </ul>; <h2>2.1.1</h2>; <p>This is a fix release for the <a href=""https://github.com/pallets/flask/releases/tag/2.1.0"">2.1.0</a> feature release.</p>; <ul>; <li>Changes: <a href=""https://flask.palletsprojects.com/en/2.1.x/changes/#version-2-1-1"">https://flask.palletsprojects.com/en/2.1.x/changes/#version-2-1-1</a></li>; <li>Milestone: <a href=""https://github.com/pallets/flask/milestone/18?closed=1"">https://github.com/pallets/flask/milestone/18?closed=1</a></li>; </ul>; <h2>2.1.0</h2>; <p>This is a feature release, which includes new features and removes previously deprecated features. The 2.1.x branch is now the supported bugfix branch, the 2.0.x branch will become a tag marking the end of support for that branch. We encourage everyone to upgrade, and to use a tool such as <a href=""https://pypi.org/project/pip-tools/"">pip-tools</a> to pin all dependencies and control upgrades.</p>; <ul>; <li>Changes: <a href=""https://flask.palletsprojects.com/en/2.1.x/changes/#version-2-1-0"">https://flask.palletsprojects.com/en/2.1.x/changes/#version-2-1-0</a></li>; <li>Milestone: <a href=""https://github.com/pallets/flask/milestone/13?closed=1"">https://github.com/pallets/flask/milestone/13?closed=1</a></li>; </ul>; <p>We also encourage upgrading to the latest versions of the other Pallets projects as well.</p>; <ul>; <li>Werkzeug 2.1 changes: <a href=""https://werkzeug.palletsprojects.com/en/2.1.x/changes/#version-2-1-0"">https://werkzeug.palletsprojects.com/en/2.1.x/changes/#version-2-1-0</a></li>; <li>Jinja 3.1 changes: <a href=""https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-1"">https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-1</a></li>; <li>Click 8.1 changes: <a href=""https://click.palletsprojects.com/en/8.1.x/changes/#",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12206:3452,upgrade,upgrade,3452,https://hail.is,https://github.com/hail-is/hail/pull/12206,1,['upgrade'],['upgrade']
Deployability,"#version-8-1-2"">https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-2</a></li>; <li>Milestone: <a href=""https://github.com/pallets/click/milestone/17?closed=1"">https://github.com/pallets/click/milestone/17?closed=1</a></li>; </ul>; <h2>8.1.1</h2>; <p>This is a fix release for the <a href=""https://github.com/pallets/click/releases/tag/8.1.0"">8.1.0</a> feature release.</p>; <ul>; <li>Changes: <a href=""https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-1"">https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-1</a></li>; <li>Milestone: <a href=""https://github.com/pallets/click/milestone/14?closed=1"">https://github.com/pallets/click/milestone/14?closed=1</a></li>; </ul>; <h2>8.1.0</h2>; <p>This is a feature release, which includes new features and removes previously deprecated features. The 8.1.x branch is now the supported bugfix branch, the 8.0.x branch will become a tag marking the end of support for that branch. We encourage everyone to upgrade, and to use a tool such as <a href=""https://pypi.org/project/pip-tools/"">pip-tools</a> to pin all dependencies and control upgrades.</p>; <ul>; <li>Changes: <a href=""https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-0"">https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-0</a></li>; <li>Milestone: <a href=""https://github.com/pallets/click/milestone/9?closed=1"">https://github.com/pallets/click/milestone/9?closed=1</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/click/blob/main/CHANGES.rst"">click's changelog</a>.</em></p>; <blockquote>; <h2>Version 8.1.2</h2>; <p>Released 2022-03-31</p>; <ul>; <li>Fix error message for readable path check that was mixed up with the; executable check. :pr:<code>2236</code></li>; <li>Restore parameter order for <code>Path</code>, placing the <code>executable</code>; parameter at the end. It is recommended to use keyword arguments; instead of positi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11801:1440,upgrade,upgrade,1440,https://hail.is,https://github.com/hail-is/hail/pull/11801,1,['upgrade'],['upgrade']
Deployability,"#version-8-1-2"">https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-2</a></li>; <li>Milestone: <a href=""https://github.com/pallets/click/milestone/17?closed=1"">https://github.com/pallets/click/milestone/17?closed=1</a></li>; </ul>; <h2>8.1.1</h2>; <p>This is a fix release for the <a href=""https://github.com/pallets/click/releases/tag/8.1.0"">8.1.0</a> feature release.</p>; <ul>; <li>Changes: <a href=""https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-1"">https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-1</a></li>; <li>Milestone: <a href=""https://github.com/pallets/click/milestone/14?closed=1"">https://github.com/pallets/click/milestone/14?closed=1</a></li>; </ul>; <h2>8.1.0</h2>; <p>This is a feature release, which includes new features and removes previously deprecated features. The 8.1.x branch is now the supported bugfix branch, the 8.0.x branch will become a tag marking the end of support for that branch. We encourage everyone to upgrade, and to use a tool such as <a href=""https://pypi.org/project/pip-tools/"">pip-tools</a> to pin all dependencies and control upgrades.</p>; <ul>; <li>Changes: <a href=""https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-0"">https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-0</a></li>; <li>Milestone: <a href=""https://github.com/pallets/click/milestone/9?closed=1"">https://github.com/pallets/click/milestone/9?closed=1</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/click/blob/main/CHANGES.rst"">click's changelog</a>.</em></p>; <blockquote>; <h2>Version 8.1.3</h2>; <p>Released 2022-04-28</p>; <ul>; <li>Use verbose form of <code>typing.Callable</code> for <code>@command</code> and; <code>@group</code>. :issue:<code>2255</code></li>; <li>Show error when attempting to create an option with; <code>multiple=True, is_flag=True</code>. Use <code>count</code> instead.; :issue:<code>2246</code",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11808:1906,upgrade,upgrade,1906,https://hail.is,https://github.com/hail-is/hail/pull/11808,1,['upgrade'],['upgrade']
Deployability,"$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). I think this may relate to java version, so I re-configured java,but the error still appear, How can I solve it ?. --------------------My java version and path; [root@***-3 opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (build 1.8.0_91-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode). [root@***-3 opt]# echo $JAVA_HOME; /opt/BioDir/jdk/jdk1.8.0_91. [root@bio-x-3 opt]# echo $PATH; /opt/BioDir/plink_1.9:/opt/BioDir/jdk/jdk1.8.0_91/bin:/opt/BioDir/gradle/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. My OS is CentOS7; [root@***-3 opt]# uname -a; Linux bio-x-3 3.10.0-229.14.1.el7.x86_64",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825:1777,deploy,deploy,1777,https://hail.is,https://github.com/hail-is/hail/issues/825,5,['deploy'],['deploy']
Deployability,$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VER,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:3655,deploy,deploy-,3655,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:9131,deploy,deploy-,9131,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,"% gsutil cp ./src/test/resources/ldprune2.vcf gs://danking/chr2.vcf; Copying file://./src/test/resources/ldprune2.vcf [Content-Type=text/x-vcard]...; / [1 files][ 11.5 KiB/ 11.5 KiB] ; Operation completed over 1 objects/11.5 KiB. ; (base) dking@wm28c-761 hail % ipython ; Python 3.10.9 (main, Jan 11 2023, 09:18:18) [Clang 14.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.16.1 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl; ...: hl.import_vcf('gs://danking/chr*.vcf').count(); Initializing Hail with default parameters...; /Users/dking/miniconda3/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:29: UserWarning: You have specified the GCS requester pays configuration in both your spark-defaults.conf (/Users/dking/miniconda3/lib/python3.10/site-packages/pyspark/conf/spark-defaults.conf) and either an explicit argument or through `hailctl config`. For GCS requester pays configuration, Hail first checks explicit arguments, then `hailctl config`, then spark-defaults.conf.; warnings.warn(; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/Users/dking/miniconda3/lib/python3.10/site-packages/pyspark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.3.3; SparkUI available at http://192.168.1.142:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.125-c4e2880b3279; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13915:1516,configurat,configuration,1516,https://hail.is,https://github.com/hail-is/hail/issues/13915,1,['configurat'],['configuration']
Deployability,"%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​fcollonval</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ag547315+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​g547315</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Agabalafou+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​gabalafou</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3AGabrielaVives+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​GabrielaVives</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Agithub-actions+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aj264415+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​j264415</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajtpio+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajupyterlab-probot+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​jupyterlab-probot</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Akrassowski+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Alumberbot-app+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​lumberbot-app</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ameeseeksmachine+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​meeseeksmachine</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aparmentelat+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​parmentelat</code></a> | <a href=""ht",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14184:11238,update,updated,11238,https://hail.is,https://github.com/hail-is/hail/pull/14184,1,['update'],['updated']
Deployability,"&amp;to=2023-12-29&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aafshin+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​afshin</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Abrichet+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​brichet</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Adavidbrochart+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​davidbrochart</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aecharles+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​echarles</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Afcollonval+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​fcollonval</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ag547315+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​g547315</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Agabalafou+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​gabalafou</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3AGabrielaVives+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​GabrielaVives</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Agithub-actions+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aj264415+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​j264415</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajtpio+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14184:10380,update,updated,10380,https://hail.is,https://github.com/hail-is/hail/pull/14184,1,['update'],['updated']
Deployability,"&amp;type=Issues""><code>@​FoSuCloud</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Agithub-actions+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aj264415+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​j264415</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3AJasonWeill+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​JasonWeill</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajupyterlab-bot+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​jupyterlab-bot</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajupyterlab-probot+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​jupyterlab-probot</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Akrassowski+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Alumberbot-app+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​lumberbot-app</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ameeseeksmachine+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​meeseeksmachine</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Awelcome+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​welcome</code></a></p>; <!-- raw HTML omitted -->; <h2>4.0.11</h2>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab/compare/v4.0.10...0708330843fd087134a239d2ad6005b1d543e246"">Full Changelog</a>)</p>; <h3>Security fixes</h3>; <ul>; <li>Potential authentication and CSRF tokens leak in JupyterLab (<a href=""https://github.com/jupyterlab/jupyterlab/security/advisories/GHSA-44cc-43rp-594",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14218:9407,update,updated,9407,https://hail.is,https://github.com/hail-is/hail/pull/14218,1,['update'],['updated']
Deployability,"&amp;type=Issues""><code>@​FoSuCloud</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Agithub-actions+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aj264415+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​j264415</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3AJasonWeill+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​JasonWeill</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajupyterlab-bot+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​jupyterlab-bot</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajupyterlab-probot+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​jupyterlab-probot</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Akrassowski+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Alumberbot-app+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​lumberbot-app</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ameeseeksmachine+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​meeseeksmachine</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Awelcome+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​welcome</code></a></p>; <h2>v4.0.11</h2>; <h2>4.0.11</h2>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab/compare/v4.0.10...0708330843fd087134a239d2ad6005b1d543e246"">Full Changelog</a>)</p>; <h3>Security fixes</h3>; <ul>; <li>Potential authentication and CSRF tokens leak in JupyterLab (<a href=""https://github.com/jupyterlab/jupyterlab/security/advisories/GHSA-44cc-43rp-5947"">GHSA-4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14218:3757,update,updated,3757,https://hail.is,https://github.com/hail-is/hail/pull/14218,1,['update'],['updated']
Deployability,"': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```. And finally, this k8s refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 2, 'output') with pod batch-11-job-2-ad1587""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 3, 'output') with pod batch-11-job-3-d826dd""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1221"", ""message"": ""restarting ready and running jobs with pods not seen in k8s""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1225"", ""message"": ""restarting job (9, 1, 'main')""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod None""}; {""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6617:15241,update,update,15241,https://hail.is,https://github.com/hail-is/hail/issues/6617,1,['update'],['update']
Deployability,']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE=origin make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE=origin \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSI,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:9867,deploy,deploy-,9867,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,"'t be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (What Jupyter hub does.). > I thought it would take less time to get a subdirectory working than figure out how to add a new domain and a cert and deal with DNS. Fair. I added a wildcard *.staging.hail.is for staging, I'll do the same thing for Hail. Then you don't need to change the DNS to add a domain, and I'll write ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:1317,update,updates-in-kubernetes-,1317,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659,1,['update'],['updates-in-kubernetes-']
Deployability,"'t fail if a value is an invalid; pattern. :issue:<code>2195</code></p>; </li>; <li>; <p>It's possible to pass a list of <code>params</code> to <code>@command</code>. Any; params defined with decorators are appended to the passed params.; :issue:<code>2131</code>.</p>; </li>; <li>; <p><code>@command</code> decorator is annotated as returning the correct type if; a <code>cls</code> argument is used. :issue:<code>2211</code></p>; </li>; <li>; <p>A <code>Group</code> with <code>invoke_without_command=True</code> and <code>chain=False</code>; will invoke its result callback with the group function's return; value. :issue:<code>2124</code></p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/click/commit/e4aceee8d2bf7fe9461915b0a21c4359ddcb8dc2""><code>e4aceee</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/click/issues/2224"">#2224</a> from pallets/release-8.1.0</li>; <li><a href=""https://github.com/pallets/click/commit/f8d811e5d5644aca8d32eebff196bf7c659ebf45""><code>f8d811e</code></a> release version 8.1.0</li>; <li><a href=""https://github.com/pallets/click/commit/20c88f02788586a80e6d867854c8313eaba5ad6e""><code>20c88f0</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/click/issues/2223"">#2223</a> from pallets/env-var</li>; <li><a href=""https://github.com/pallets/click/commit/8d7f03dac8739afed890af0c0921965786c5e83c""><code>8d7f03d</code></a> treat empty auto_envvar as None</li>; <li><a href=""https://github.com/pallets/click/commit/ef11be6e49e19a055fe7e5a89f0f1f4062c68dba""><code>ef11be6</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/click/issues/2041"">#2041</a> from spanglerco/shell-completion-option-values</li>; <li><a href=""https://github.com/pallets/click/commit/f2e579ab187ca8fdfbe6ce86de08f0e9f62fe4ae""><code>f2e579a</code></a> shel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11706:4828,release,release-,4828,https://hail.is,https://github.com/hail-is/hail/pull/11706,1,['release'],['release-']
Deployability,"'t run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:2272,update,update,2272,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,1,['update'],['update']
Deployability,"(<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1773"">#1773</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/45b2b38c3850da1795a7fbd33e0560b949cb7810""><code>45b2b38</code></a> chore: use gcp-docuploader 0.6.3 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1708"">#1708</a>) (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1769"">#1769</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/e0efa85c3cc7a0a092ab96a13f121b2d0e553c38""><code>e0efa85</code></a> test(deps): update dependency com.google.cloud:google-cloud-pubsub to v1.120....</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/feb9f06d7031915ce50a609f99a4d885e2b21f34""><code>feb9f06</code></a> test(deps): update dependency com.google.api.grpc:proto-google-cloud-pubsub-v...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/b05ee42b24bb8b18b7cfcfd921a6a4f70d930ad2""><code>b05ee42</code></a> test(deps): update testbench version to v0.32.0 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1768"">#1768</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/8ea8131d17eba29859518da7199bbd03019d0644""><code>8ea8131</code></a> chore: update google-auth to 2.14.1 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1703"">#1703</a>) (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1767"">#1767</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/googleapis/java-storage/compare/v1.106.0...v2.15.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.google.cloud:google-cloud-storage&package-manager=gradle&previous-version=1.106.0&new-version=2.15.1)](https://docs.github.com/en/github/managing-security-vulnerabilit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12529:13421,update,update,13421,https://hail.is,https://github.com/hail-is/hail/pull/12529,1,['update'],['update']
Deployability,"(<a href=""https://redirect.github.com/pallets/jinja/issues/1918"">#1918</a>)</li>; <li><a href=""https://github.com/pallets/jinja/commit/19a55db3b411343309f2faaffaedbb089e841895""><code>19a55db</code></a> Make nested-trans-block exceptions nicer</li>; <li><a href=""https://github.com/pallets/jinja/commit/716795349a41d4983a9a4771f7d883c96ea17be7""><code>7167953</code></a> Merge pull request from GHSA-h5c8-rqwp-cp95</li>; <li><a href=""https://github.com/pallets/jinja/commit/7dd3680e6eea0d77fde024763657aa4d884ddb23""><code>7dd3680</code></a> xmlattr filter disallows keys with spaces</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/3.1.2...3.1.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=3.1.2&new-version=3.1.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14144:3600,update,updates,3600,https://hail.is,https://github.com/hail-is/hail/pull/14144,3,['update'],['updates']
Deployability,"(<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ijl/orjson/commit/11c7de8e5846fa65449aa1f6ffc05c5a1090df03""><code>11c7de8</code></a> 3.10.0</li>; <li><a href=""https://github.com/ijl/orjson/commit/1fc3ed80c24864607be709d29e0d5f47fc507626""><code>1fc3ed8</code></a> Support numpy.float16</li>; <li><a href=""https://github.com/ijl/orjson/commit/56c1a03216426c54dfbe9a4b6c3f70013c65a1f8""><code>56c1a03</code></a> cargo update, build misc</li>; <li><a href=""https://github.com/ijl/orjson/commit/a348f59f0b55d92a1364523560f52f5b3cf9c12a""><code>a348f59</code></a> 3.9.15</li>; <li><a href=""https://github.com/ijl/orjson/commit/b0e4d2c06ce06c6e63981bf0276e4b7c74e5845e""><code>b0e4d2c</code></a> yyjson 0eca326, recursion limit</li>; <li><a h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14427:2984,Update,Update,2984,https://hail.is,https://github.com/hail-is/hail/pull/14427,1,['Update'],['Update']
Deployability,"(DOC: Last changes to release notes for 1....</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/e726483d70938f3bff67e95358841a1f6271b149""><code>e726483</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/48619"">#48619</a> on branch 1.5.x (REGR: Loc.setitem with enlargement raises...</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/f83e2fe3327ad85ae2e8c4ba469fe98383243dbf""><code>f83e2fe</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/48623"">#48623</a> on branch 1.5.x (REGR/DOC: Docs left navbar broke) (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/48625"">#48625</a>)</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/4fbb05591979055708162994e96fb4c61cf2a8ab""><code>4fbb055</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/48601"">#48601</a> on branch 1.5.x (CI: Fix matplolib release issues) (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/48617"">#48617</a>)</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/aabf6597f45436e9ada915ac15d3708f9d4948ca""><code>aabf659</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/48587"">#48587</a> on branch 1.5.x (Fix <code>series.str.startswith(tuple)</code>) (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/48593"">#48593</a>)</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/dfc00bfc5d98f8e2c63356e6a415da8ab7a7b436""><code>dfc00bf</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/48397"">#48397</a> on branch 1.5.x (WARN: Remove false positive warning for i...</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/3f91207dca8971c723605243f6bc113c739ba637""><code>3f91207</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/48572""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12292:3898,release,release,3898,https://hail.is,https://github.com/hail-is/hail/pull/12292,1,['release'],['release']
Deployability,"(I mean I don't think we should be in the habit of deploying docs outside the versioned releases, except in special circumstances)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6567#issuecomment-509642724:51,deploy,deploying,51,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509642724,2,"['deploy', 'release']","['deploying', 'releases']"
Deployability,"(If you want to take a look at a live instance, I've deployed to my namespace and you can look at the batch logs here: https://ci.hail.is/batches/634 ). ignore all the log messages that have to do with `/wang/blog/healthcheck/`---since it's logging to persistent storage, it's also printing all the old logs from a few days ago to the batch logs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548103287:53,deploy,deployed,53,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103287,1,['deploy'],['deployed']
Deployability,"(and to be clear, I'm onboard with the idea of a single command which gets you from zero to simple batch pipelines; jury is out on the Artifact Registry. I think making sure that's configured correctly might be better upstreamed to Sam. Normal users might not have permission to enable/disable things like that.).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1662843157:105,pipeline,pipelines,105,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1662843157,1,['pipeline'],['pipelines']
Deployability,"(https://github.com/boto/boto3) from 1.26.6 to 1.26.9.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/boto/boto3/blob/develop/CHANGELOG.rst"">boto3's changelog</a>.</em></p>; <blockquote>; <h1>1.26.9</h1>; <ul>; <li>api-change:<code>customer-profiles</code>: [<code>botocore</code>] This release enhances the SearchProfiles API by providing functionality to search for profiles using multiple keys and logical operators.</li>; <li>api-change:<code>lakeformation</code>: [<code>botocore</code>] This release adds a new parameter &quot;Parameters&quot; in the DataLakeSettings.</li>; <li>api-change:<code>managedblockchain</code>: [<code>botocore</code>] Updating the API docs data type: NetworkEthereumAttributes, and the operations DeleteNode, and CreateNode to also include the supported Goerli network.</li>; <li>api-change:<code>proton</code>: [<code>botocore</code>] Add support for CodeBuild Provisioning</li>; <li>api-change:<code>rds</code>: [<code>botocore</code>] This release adds support for restoring an RDS Multi-AZ DB cluster snapshot to a Single-AZ deployment or a Multi-AZ DB instance deployment.</li>; <li>api-change:<code>workdocs</code>: [<code>botocore</code>] Added 2 new document related operations, DeleteDocumentVersion and RestoreDocumentVersions.</li>; <li>api-change:<code>xray</code>: [<code>botocore</code>] This release enhances GetServiceGraph API to support new type of edge to represent links between SQS and Lambda in event-driven applications.</li>; </ul>; <h1>1.26.8</h1>; <ul>; <li>api-change:<code>glue</code>: [<code>botocore</code>] Added links related to enabling job bookmarks.</li>; <li>api-change:<code>iot</code>: [<code>botocore</code>] This release add new api listRelatedResourcesForAuditFinding and new member type IssuerCertificates for Iot device device defender Audit.</li>; <li>api-change:<code>license-manager</code>: [<code>botocore</code>] AWS License Manager now supports onboarded Management Accounts",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12466:1033,release,release,1033,https://hail.is,https://github.com/hail-is/hail/pull/12466,3,"['deploy', 'release']","['deployment', 'release']"
Deployability,"(https://github.com/ipython/ipython) from 7.34.0 to 8.12.0.; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ipython/ipython/commit/37242ba43be041d87b9f3262a989dacfd7737029""><code>37242ba</code></a> release 8.12.0</li>; <li><a href=""https://github.com/ipython/ipython/commit/2bb46729ded8e58153f10629c3b6fa0d42acb751""><code>2bb4672</code></a> Update whats new for 3.12 (<a href=""https://redirect.github.com/ipython/ipython/issues/14000"">#14000</a>)</li>; <li><a href=""https://github.com/ipython/ipython/commit/4f0b2357354aa35042244f902f9bcea9da65dfd9""><code>4f0b235</code></a> upate whats new for 3.12</li>; <li><a href=""https://github.com/ipython/ipython/commit/d52bf622ce9922106f8bcc1867c0a27a884201a0""><code>d52bf62</code></a> Allow to dispatch getting documentation on objects. (<a href=""https://redirect.github.com/ipython/ipython/issues/13975"">#13975</a>)</li>; <li><a href=""https://github.com/ipython/ipython/commit/3a9419dce7d6ccf7de39be606eec2fc212ef4445""><code>3a9419d</code></a> Update completer documentation (<a href=""https://redirect.github.com/ipython/ipython/issues/13999"">#13999</a>)</li>; <li><a href=""https://github.com/ipython/ipython/commit/a7d8defdecca3cfa54eada171181e0880c8b6b5f""><code>a7d8def</code></a> Expose <code>auto_suggest.resume_hinting</code>, fix resume on backspace (<a href=""https://redirect.github.com/ipython/ipython/issues/13994"">#13994</a>)</li>; <li><a href=""https://github.com/ipython/ipython/commit/4e7b9408a0b35cabbdc973f131257f4e97a3ddcf""><code>4e7b940</code></a> Fix autosuggestions in multi-line mode, vi command mode delay (<a href=""https://redirect.github.com/ipython/ipython/issues/13991"">#13991</a>)</li>; <li><a href=""https://github.com/ipython/ipython/commit/ad452c1d8bb25e742caa152fb301e0e6626b6faa""><code>ad452c1</code></a> Improve API documentation around configuration of embedded IPython (<a href=""https://redirect.github.com/ipython/ipython/issues/13989"">#13989</a>)</li>; <li><a href=""https://github.com/ipython",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12832:1033,Update,Update,1033,https://hail.is,https://github.com/hail-is/hail/pull/12832,2,['Update'],['Update']
Deployability,"(https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6050294](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6050294) | `cryptography:` <br> `41.0.7 -> 42.0.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Information Exposure <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6126975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6126975) | `cryptography:` <br> `41.0.7 -> 42.0.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIxNjVmNDVkMi00ZDM3LTRmNzAtOGU1OC00OGIxOGJhNmVlOTgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjE2NWY0NWQyLTRkMzctNGY3MC04ZTU4LTQ4YjE4YmE2ZWU5OCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14203:2031,upgrade,upgraded,2031,https://hail.is,https://github.com/hail-is/hail/pull/14203,1,['upgrade'],['upgraded']
Deployability,"(https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6050294](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6050294) | `cryptography:` <br> `41.0.7 -> 42.0.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Information Exposure <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6126975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6126975) | `cryptography:` <br> `41.0.7 -> 42.0.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIyMTAxMzNhYS03MjA2LTRmMzQtYTQ2OC1iYjY5YWJmYTUzZjEiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjIxMDEzM2FhLTcyMDYtNGYzNC1hNDY4LWJiNjlhYmZhNTNmMSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14200:1757,upgrade,upgraded,1757,https://hail.is,https://github.com/hail-is/hail/pull/14200,1,['upgrade'],['upgraded']
Deployability,"(possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital. Re-run analyses without the full available code. See if they can replicate (not p-values, but order). Could generate multiple-hypothesis-test corrected aggregate. These users get publication credit as consortia; 3) People donate minor intellectual capital: Re-run analysis with full available code. Report on success. This will catch bugs, and non-deterministic results (for instance, if reported accuracy de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:8348,pipeline,pipelines,8348,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['pipeline'],['pipelines']
Deployability,"(probably partial) list of things we'll have to do manually once this goes in:. - rename account batch2 => batch (and gsa-key, tokens, etc.); - rebuild database; - had deploy (which includes cleaning up old logs and bucket); - batch service account, roles and role bindings",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7664:168,deploy,deploy,168,https://hail.is,https://github.com/hail-is/hail/pull/7664,1,['deploy'],['deploy']
Deployability,(this is your PR updated against hailctl),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6189:17,update,updated,17,https://hail.is,https://github.com/hail-is/hail/pull/6189,1,['update'],['updated']
Deployability,"(truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/bf4edce5d6c967fa1d6a581b3aaab1bd2f5121cd""><code>bf4edce</code></a> Release v3.9.3 (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8102"">#8102</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/5637e8f02a05cf0ae800ad3e175f4bbe0a3e54cc""><code>5637e8f</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8098"">#8098</a>/aca206fc backport][3.9] Fix backwards compatibility with ssl (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8101"">#8101</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/33f49e8ec985e428083cd78bc11cb5fe0dff0e57""><code>33f49e8</code></a> Bump pypa/cibuildwheel from 2.16.2 to 2.16.4 (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8092"">#8092</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/5ff4b3c405ee741a46d6743209cd32259f939313""><code>5ff4b3c</code></a> Update version</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/94462eea445d43bf574ca6321349f67219ce9cb0""><code>94462ee</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/3957"">#3957</a>/79fe2045 backport][3.9] Improve test suite handling of paths, temp ...</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/24a6d64966d99182e95f5d3a29541ef2fec397ad""><code>24a6d64</code></a> Release v3.9.2 (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8082"">#8082</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/9118a5831e8a65b8c839eb7e4ac983e040ff41df""><code>9118a58</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8079"">#8079</a>/1c335944 backport][3.9] Validate static paths (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8080"">#8080</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/435ad46e6c26cbf6ed9a38764e9ba8e7441a0e3b""><code>435ad46</code",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14219:4092,Update,Update,4092,https://hail.is,https://github.com/hail-is/hail/pull/14219,2,['Update'],['Update']
Deployability,(updated just now to what I *think* is more correct behavior),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6917#issuecomment-523633712:1,update,updated,1,https://hail.is,https://github.com/hail-is/hail/pull/6917#issuecomment-523633712,1,['update'],['updated']
Deployability,(vague) sort out python and R integration story,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/21:30,integrat,integration,30,https://hail.is,https://github.com/hail-is/hail/issues/21,1,['integrat'],['integration']
Deployability,(would like to include that in patch notes),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4871#issuecomment-443783236:31,patch,patch,31,https://hail.is,https://github.com/hail-is/hail/pull/4871#issuecomment-443783236,1,['patch'],['patch']
Deployability,") (<a href=""https://github.com/googleapis/python-logging/commit/e0c5fc02160ae87faf4ba5c2b62be86de6b02cf3"">e0c5fc0</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>allow reading logs from non-project paths (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/444"">#444</a>) (<a href=""https://github.com/googleapis/python-logging/commit/97e32b67603553fe350b6327455fc9f80b8aa6ce"">97e32b6</a>)</li>; <li>api consistency between HTTP and Gapic layers (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/375"">#375</a>) (<a href=""https://github.com/googleapis/python-logging/commit/e1506fa9030776353878048ce562c53bf6ccf7bf"">e1506fa</a>)</li>; </ul>; <h3>Miscellaneous Chores</h3>; <ul>; <li>deprecate AppEngineHandler and ContainerEngineHandler (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/310"">#310</a>) (<a href=""https://github.com/googleapis/python-logging/commit/e3cac888d40bf67af11e57b74615b0c3b8e8aa3e"">e3cac88</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>update usage guide for v3.0.0 (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/456"">#456</a>) (<a href=""https://github.com/googleapis/python-logging/commit/8a67b73cdfcb9da545671be6cf59c724360b1544"">8a67b73</a>)</li>; </ul>; <h2><a href=""https://www.github.com/googleapis/python-logging/compare/v2.6.0...v2.7.0"">2.7.0</a> (2021-11-02)</h2>; <h3>Features</h3>; <ul>; <li>add context manager support in client (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/415"">#415</a>) (<a href=""https://www.github.com/googleapis/python-logging/commit/f5af16439807a0954ee78fa91cb69b9493b80176"">f5af164</a>)</li>; <li>added support for iam AuditData proto (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/396"">#396</a>) (<a href=""https://www.github.com/googleapis/python-logging/commit/e3a1eba74dd8b67bcc73a78f784189ef2a9927c2"">e3a1eba</a>)</li>; <li>use struct",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11574:9760,update,update,9760,https://hail.is,https://github.com/hail-is/hail/pull/11574,1,['update'],['update']
Deployability,") (<a href=""https://github.com/googleapis/python-logging/commit/e0c5fc02160ae87faf4ba5c2b62be86de6b02cf3"">e0c5fc0</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>allow reading logs from non-project paths (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/444"">#444</a>) (<a href=""https://github.com/googleapis/python-logging/commit/97e32b67603553fe350b6327455fc9f80b8aa6ce"">97e32b6</a>)</li>; <li>api consistency between HTTP and Gapic layers (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/375"">#375</a>) (<a href=""https://github.com/googleapis/python-logging/commit/e1506fa9030776353878048ce562c53bf6ccf7bf"">e1506fa</a>)</li>; </ul>; <h3>Miscellaneous Chores</h3>; <ul>; <li>deprecate AppEngineHandler and ContainerEngineHandler (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/310"">#310</a>) (<a href=""https://github.com/googleapis/python-logging/commit/e3cac888d40bf67af11e57b74615b0c3b8e8aa3e"">e3cac88</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>update usage guide for v3.0.0 (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/456"">#456</a>) (<a href=""https://github.com/googleapis/python-logging/commit/8a67b73cdfcb9da545671be6cf59c724360b1544"">8a67b73</a>)</li>; </ul>; <h2>v2.7.0</h2>; <h3>Features</h3>; <ul>; <li>add context manager support in client (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/415"">#415</a>) (<a href=""https://www.github.com/googleapis/python-logging/commit/f5af16439807a0954ee78fa91cb69b9493b80176"">f5af164</a>)</li>; <li>added support for iam AuditData proto (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/396"">#396</a>) (<a href=""https://www.github.com/googleapis/python-logging/commit/e3a1eba74dd8b67bcc73a78f784189ef2a9927c2"">e3a1eba</a>)</li>; <li>use structured logging on GCF with python 3.7 (<a href=""https://github-redirect.dependabot.com/googleapis/pyt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11574:4327,update,update,4327,https://hail.is,https://github.com/hail-is/hail/pull/11574,1,['update'],['update']
Deployability,) failed: Connection reset by peer; E 	at ; E ; E ; E ; E Hail version: 0.2.115-330031a5d973; E Error summary: NativeIoException: readAddress(..) failed: Connection reset by peer. /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:477: FatalError; ------------------------------ Captured log call -------------------------------; INFO batch_client.aioclient:aioclient.py:753 created batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 _make_tsm: found 1000 variants after filtering out monomorphic sites.; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 1/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 2/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 3/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 4/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 5/10; INFO ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12980:5429,update,updated,5429,https://hail.is,https://github.com/hail-is/hail/issues/12980,1,['update'],['updated']
Deployability,") has been added, and is; now preferred over usage of <code>scipy.misc</code> for dataset retrieval.</li>; <li>A new <code>scipy.interpolate.make_smoothing_spline</code> function was added. This; function constructs a smoothing cubic spline from noisy data, using the; generalized cross-validation (GCV) criterion to find the tradeoff between; smoothness and proximity to data points.</li>; <li><code>scipy.stats</code> has three new distributions, two new hypothesis tests, three; new sample statistics, a class for greater control over calculations; involving covariance matrices, and many other enhancements.</li>; </ul>; <h1>New features</h1>; <h1><code>scipy.datasets</code> introduction</h1>; <ul>; <li>A new dedicated <code>datasets</code> submodule has been added. The submodules; is meant for datasets that are relevant to other SciPy submodules ands; content (tutorials, examples, tests), as well as contain a curated; set of datasets that are of wider interest. As of this release, all; the datasets from <code>scipy.misc</code> have been added to <code>scipy.datasets</code>; (and deprecated in <code>scipy.misc</code>).</li>; <li>The submodule is based on <a href=""https://www.fatiando.org/pooch/latest/"">Pooch</a>; (a new optional dependency for SciPy), a Python package to simplify fetching; data files. This move will, in a subsequent release, facilitate SciPy; to trim down the sdist/wheel sizes, by decoupling the data files and; moving them out of the SciPy repository, hosting them externally and</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/scipy/scipy/commit/dde50595862a4f9cede24b5d1c86935c30f1f88a""><code>dde5059</code></a> REL: 1.10.0 final [wheel build]</li>; <li><a href=""https://github.com/scipy/scipy/commit/7856f281b016c585b82d03723c4494bcdbdcd4a5""><code>7856f28</code></a> Merge pull request <a href=""https://redirect.github.com/scipy/scipy/issue",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13227:2286,release,release,2286,https://hail.is,https://github.com/hail-is/hail/pull/13227,1,['release'],['release']
Deployability,")</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Do not format <code>__pypackages__</code> directories by default (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2836"">#2836</a>)</li>; <li>Add support for specifying stable version with <code>--required-version</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2832"">#2832</a>).</li>; <li>Avoid crashing when the user has no homedir (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2814"">#2814</a>)</li>; <li>Avoid crashing when md5 is not available (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2905"">#2905</a>)</li>; <li>Fix handling of directory junctions on Windows (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2904"">#2904</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Update pylint config documentation (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2931"">#2931</a>)</li>; </ul>; <h3>Integrations</h3>; <ul>; <li>Move test to disable plugin in Vim/Neovim, which speeds up loading (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2896"">#2896</a>)</li>; </ul>; <h3>Output</h3>; <ul>; <li>In verbose, mode, log when <em>Black</em> is using user-level config (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2861"">#2861</a>)</li>; </ul>; <h3>Packaging</h3>; <ul>; <li>Fix Black to work with Click 8.1.0 (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2966"">#2966</a>)</li>; <li>On Python 3.11 and newer, use the standard library's <code>tomllib</code> instead of <code>tomli</code>; (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2903"">#2903</a>)</li>; <li><code>black-primer</code>, the deprecated internal devtool, has been removed and copied to a; <a href=""https://github.com/cooperlees/black-primer"">separate repository</a> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2924"">#2924</a>)</li>; </ul>; <h3>Parser</h3>; <ul>; <li>Blac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11696:1882,Integrat,Integrations,1882,https://hail.is,https://github.com/hail-is/hail/pull/11696,2,['Integrat'],['Integrations']
Deployability,")</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/437ac47fe332106a07a2d5335bb89619f1bc23f7""><code>437ac47</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7995"">#7995</a>/43a5bc50 backport][3.9] Fix examples of <code>fallback_charset_resolver</code>...</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/034e5e34ee11c6138c773d85123490e691e1b708""><code>034e5e3</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8042"">#8042</a>/4b91b530 backport][3.9] Tightening the runtime type check for ssl (...</li>; <li>Additional commits viewable in <a href=""https://github.com/aio-libs/aiohttp/compare/v3.9.1...v3.9.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=aiohttp&package-manager=pip&previous-version=3.9.1&new-version=3.9.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14212:7064,update,updates,7064,https://hail.is,https://github.com/hail-is/hail/pull/14212,6,['update'],['updates']
Deployability,")</li>; <li>Additional commits viewable in <a href=""https://github.com/hagenw/sphinxcontrib-katex/compare/0.5.1...v0.8.6"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinxcontrib-katex&package-manager=pip&previous-version=0.5.1&new-version=0.8.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11458:7388,upgrade,upgrade,7388,https://hail.is,https://github.com/hail-is/hail/pull/11458,6,['upgrade'],['upgrade']
Deployability,")</li>; <li>Additional commits viewable in <a href=""https://github.com/hagenw/sphinxcontrib-katex/compare/0.5.1...v0.9.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinxcontrib-katex&package-manager=pip&previous-version=0.5.1&new-version=0.9.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12241:7825,upgrade,upgrade,7825,https://hail.is,https://github.com/hail-is/hail/pull/12241,3,['upgrade'],['upgrade']
Deployability,")</li>; <li>Additional commits viewable in <a href=""https://github.com/samtools/htsjdk/compare/2.24.1...3.0.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.github.samtools:htsjdk&package-manager=gradle&previous-version=2.24.1&new-version=3.0.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12229:10385,upgrade,upgrade,10385,https://hail.is,https://github.com/hail-is/hail/pull/12229,3,['upgrade'],['upgrade']
Deployability,")</li>; <li>Additional commits viewable in <a href=""https://github.com/samtools/htsjdk/compare/2.24.1...3.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.github.samtools:htsjdk&package-manager=gradle&previous-version=2.24.1&new-version=3.0.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12310:10210,upgrade,upgrade,10210,https://hail.is,https://github.com/hail-is/hail/pull/12310,3,['upgrade'],['upgrade']
Deployability,"* Correct spelling of ""decommissioning"" in help for `--graceful-decommission-timeout`.; * Add space between ""match"" and ""the"" in help for `--update-hail-version`.; * Add punctuation to help for `--update-hail-version` for consistency with other arguments.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7335:141,update,update-hail-version,141,https://hail.is,https://github.com/hail-is/hail/pull/7335,2,['update'],['update-hail-version']
Deployability,"* Fix a bug in `calculate_new_intervals`, when the default reference does not match; `reference_genome`; * Make positional argument names valid python ids; * Update various argument help; * Add `--log` argument allowing the user to specify the logfile; * If `--overwrite` is not present, check that `out_file` exists",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8070:158,Update,Update,158,https://hail.is,https://github.com/hail-is/hail/pull/8070,1,['Update'],['Update']
Deployability,"* don't require HAIL_WHEEL to install; * version doesn't actually mean version benchmarked, so that's now a param; * Fix pip in install target",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7118:30,install,install,30,https://hail.is,https://github.com/hail-is/hail/pull/7118,2,['install'],['install']
Deployability,"**EDIT: If you're finding this thread in 2021 or later, do not attempt to use a tiny block size. LD prune works best with the default block size.**. update: using tiny block size:; ```; print(""extract pruned set of variants""); pruned_tbl = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 1000000, memory_per_core = 1000, block_size=75); pruned_tbl.write(""pruned_tbl.ht"", overwrite=True); ```; takes 16.1s, about 40x faster. This is still wayyyyy too slow -- the total input data is 14M!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4506#issuecomment-427593611:149,update,update,149,https://hail.is,https://github.com/hail-is/hail/issues/4506#issuecomment-427593611,1,['update'],['update']
Deployability,"**Things still to do:**; 1. Docs for importbgen, importgen, dosage representation, info score in variant qc; 2. Make sure info score is computed properly -- either implement correctly for non-autosomal variants or return None; 3. Add tests for info score (once we finalized how we're computing); 4. Remove null variant in GenotypeBuilder (from import plink block reader code); 5. Decide how to handle fake ref for multiallelics when original genotype call was null (could be because of rounding errors we get same integer value for close doubles such as 0.4035 and 0.4021); 6. Modify variant qc to read parameter about data so info score only calculated for dosage data and likewise for statistics about depth, gq etc.; 7. Handle sex chromosome names in import PLINK properly (do we need to map ""23"" to ""X"", etc.?); 8. Update the readFam function in import plink to utilize functionality Jon wrote already. **Questions:**; 1. I set the default value of --no-compress to true for `importplink`, `importgen`, and `importbgen`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/372:819,Update,Update,819,https://hail.is,https://github.com/hail-is/hail/pull/372,1,['Update'],['Update']
Deployability,"**User-facing Changes:**; - Added preemptible and machine_type to resources configuration in addition to existing worker_type (will get removed at some point in the future).; - machine_type and preemptible are hidden options in hailtop.batch (I put them in for testing purposes); - Can only specify one of worker_type and machine_type; - Preemptible is only valid for machine_type; - If you specify the machine_type, your storage gets rounded up to the nearest Gi with 10 Gi being the minimum (this is because we use a persistent-SSD as the worker data disk and the min for this is 10 Gi. **Billing Changes:**; - The list of possible machine types we currently support is in the globals -- we only support the n1 family. I insert new resource rates into the Resources SQL table to account for the new nonpreemptible resources.; - If you have a job private instance, you are billed for that instance at the time of the instance's creation (not when it's activated). This is done by modifying the trigger for the attempts table to take the minimum attempt start time rather than the maximum. I'm not sure why we needed the maximum to begin with as the attempt start time was always the same. This is probably a place to double check before merging. **Job State Changes:**; - We now support a new state ""Creating"" which represents an instance has been spun up for a job, but it has not been activated yet; - The SQL user resources table and cancellable resources table needed to be changed to add the n_creating_jobs, n_cancellable_creating_jobs, and the n_cancelled_creating_jobs; - Added a new SQL function `mark_job_creating` that is only called by the job private instance creator.; - A lot of the existing SQL functions had to change to account for the fact that an instance in the pending state can have job-specific operations done such as mark_job_complete if the job is cancelled. In addition, the ""Creating"" state is like ""Running"" for some operations in that an attempt has been created and ac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9972:76,configurat,configuration,76,https://hail.is,https://github.com/hail-is/hail/pull/9972,1,['configurat'],['configuration']
Deployability,"**Work in progress**; _worried the merging and rebasing was done poorly due to the message about merge conflicts at the bottom of this PR - would be useful if someone could let me know or walk me through resolving it_. **Additions:**. - adds method for Blanczos SVD, not yet following the exact interface of the current PCA call; - adds test for Blanczos SVD method; - adds multiple jupyter notebooks where this algorithm was implemented; - adds first version of benchmarking script; - update to requirements.txt regarding gcsfs version should probably be moved to separate PR. **Needs:**; - larger benchmarking; - better test; - hail method for Blanczos PCA to use exact interface and return eigenvalues, scores, and optional loadings as if it were the hail PCA method instead of the current non-centered SVD; - fix the norm(A - QQtA) computation - maybe make it blocked; - possibly block the error bound computation; - possibly replace the numpy library calls to SVD and QR decomposition with distributed hail versions, or at least the SVD call at a minimum since it is easier",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9222:486,update,update,486,https://hail.is,https://github.com/hail-is/hail/pull/9222,1,['update'],['update']
Deployability,"*Update*. If it helps, our configuration includes three VMs. This includes a master and two workers with autoscaling enabled. We have tried using n1-himem-8 and n1-himem-64 machines. Both configurations failed with similar errors. The one above is form the n1-himem-64 configuration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12083#issuecomment-1213381420:1,Update,Update,1,https://hail.is,https://github.com/hail-is/hail/issues/12083#issuecomment-1213381420,4,"['Update', 'configurat']","['Update', 'configuration', 'configurations']"
Deployability,*sigh*. ```; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); ```. Azure's `StorageOutputStream.close` method is not idempotent in the version that we use. It has been made idempotent in `12.18.0`. I would be surprised if spark let us upgrade to a version that recent,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12968#issuecomment-1532328901:1719,upgrade,upgrade,1719,https://hail.is,https://github.com/hail-is/hail/pull/12968#issuecomment-1532328901,1,['upgrade'],['upgrade']
Deployability,"+ CI total time speedups</li>; <li>See full diff in <a href=""https://github.com/saghul/pycares/compare/pycares-4.2.2...pycares-4.3.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pycares&package-manager=pip&previous-version=4.2.2&new-version=4.3.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12559:4897,upgrade,upgrade,4897,https://hail.is,https://github.com/hail-is/hail/pull/12559,3,['upgrade'],['upgrade']
Deployability,", [float]{.title-ref}, [int]{.title-ref} and [bool]{.title-ref} were accepted;; now [bytes]{.title-ref}, [complex]{.title-ref}, [re.Pattern]{.title-ref}, [Enum]{.title-ref} and anything with a [__name__]{.title-ref} are also accepted.</p>; </li>; <li>; <p><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9692"">#9692</a>: <code>pytest.approx</code>{.interpreted-text role=&quot;func&quot;} now raises a <code>TypeError</code>{.interpreted-text role=&quot;class&quot;} when given an unordered sequence (such as <code>set</code>{.interpreted-text role=&quot;class&quot;}).</p>; <p>Note that this implies that custom classes which only implement <code>__iter__</code> and <code>__len__</code> are no longer supported as they don't guarantee order.</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest/commit/0ffe9e07422dfec72479a6d056154ec8b9b0dbae""><code>0ffe9e0</code></a> Prepare release version 7.1.1</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/6f2c1ec0358264f10394fd2459a2e2a00b492844""><code>6f2c1ec</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9784"">#9784</a> from pytest-dev/backport-9768-to-7.1.x</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/a65c47a1a40dad1bb5ce0beb83657d492011a425""><code>a65c47a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9783"">#9783</a> from pytest-dev/backport-9780-to-7.1.x</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/30d995ed25e6d76e85da140663e6253fa5b41935""><code>30d995e</code></a> [pre-commit.ci] auto fixes from pre-commit.com hooks</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/10a14d13181fd69dd0eaf48bf5b3d389de896713""><code>10a14d1</code></a> [7.1.x] testing: fix tests when run under <code>-v</code> or <code>-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11619:4301,release,release,4301,https://hail.is,https://github.com/hail-is/hail/pull/11619,1,['release'],['release']
Deployability,", and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:5770,Deploy,Deploy,5770,https://hail.is,https://github.com/hail-is/hail/pull/8513,2,"['Deploy', 'deploy']","['Deploy', 'deployed']"
Deployability,", but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:5551,deploy,deploy,5551,https://hail.is,https://github.com/hail-is/hail/pull/8513,2,['deploy'],['deploy']
Deployability,", it can now also be `DropRowUIDs`, or for `MatrixRead` `DropColUIDs` or `DropRowColUIDs`. That way in the common case of not needing the read to produce uids, we don't need to pollute the printed IR with large types.; * `hl.read_table` gets an option `_create_row_uids`, to allow for testing uids in python, and similarly for `hl.read_matrix_table`; * There are globally fixed default field names `TableReader.uidFieldName`, `MatrixReader.rowUIDFieldName`, and `MatrixReader.colUIDFieldName`. The full type of any `TableReader`/`MatrixReader` must contain these fields. If a consumer doesn't want uids, it just doesn't include them in the requested type. If it wants different field names, it must use a `TableRename`/`MatrixRename` node. This design ensures that the field pruner doesn't need any awareness of uids.; * An exception to this rule is if the written data already contains any of these special fields, in which case they are just read as usual. This ensures that a write/read in the middle of a pipeline can't change uid fields. We're making the assumption that these reserved field names are never used in user data, so if written data contains one of these fields, it must have been created by us, and so has the correct uid semantics. (Note that this was a late change, and I may have missed converting some readers to handle this case.); * The uids fields always come last in the row/col struct. Note that this requires some care when lowering MatrixTable, to make sure the row uid field comes after the entries field.; * `PartitionReader`s, on the other hand, must specify the name of their uid field. If this field is in the requested type, it will always be generated by the reader, even if the field already existed in the written data. It is now the responsibility of the consumer to choose the uid field name so as not to clobber an existing field.; * Added a trait `CountedIterator`, for iterators which keep track of a row index or file offset. The method `getCurIdx` should",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12031:1136,pipeline,pipeline,1136,https://hail.is,https://github.com/hail-is/hail/pull/12031,1,['pipeline'],['pipeline']
Deployability,", two new hypothesis tests, three; new sample statistics, a class for greater control over calculations; involving covariance matrices, and many other enhancements.</li>; </ul>; <h1>New features</h1>; <h1><code>scipy.datasets</code> introduction</h1>; <ul>; <li>A new dedicated <code>datasets</code> submodule has been added. The submodules; is meant for datasets that are relevant to other SciPy submodules ands; content (tutorials, examples, tests), as well as contain a curated; set of datasets that are of wider interest. As of this release, all; the datasets from <code>scipy.misc</code> have been added to <code>scipy.datasets</code>; (and deprecated in <code>scipy.misc</code>).</li>; <li>The submodule is based on <a href=""https://www.fatiando.org/pooch/latest/"">Pooch</a>; (a new optional dependency for SciPy), a Python package to simplify fetching; data files. This move will, in a subsequent release, facilitate SciPy; to trim down the sdist/wheel sizes, by decoupling the data files and; moving them out of the SciPy repository, hosting them externally and</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/scipy/scipy/commit/dde50595862a4f9cede24b5d1c86935c30f1f88a""><code>dde5059</code></a> REL: 1.10.0 final [wheel build]</li>; <li><a href=""https://github.com/scipy/scipy/commit/7856f281b016c585b82d03723c4494bcdbdcd4a5""><code>7856f28</code></a> Merge pull request <a href=""https://redirect.github.com/scipy/scipy/issues/17696"">#17696</a> from tylerjereddy/treddy_110_final_prep</li>; <li><a href=""https://github.com/scipy/scipy/commit/205b6243c6d075d05695e7ac6d007e0f03bfbf42""><code>205b624</code></a> DOC: add missing author</li>; <li><a href=""https://github.com/scipy/scipy/commit/1ab9f1b10145f0a974d5531700e72d1fb4229b76""><code>1ab9f1b</code></a> DOC: update 1.10.0 relnotes</li>; <li><a href=""https://github.com/scipy/scipy/commit/ac2f45fbe1e39a8f52c1ea2e687640",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13227:2653,release,release,2653,https://hail.is,https://github.com/hail-is/hail/pull/13227,1,['release'],['release']
Deployability,",>=0.14, but you have docutils 0.20.1.; sphinx-rtd-theme 1.3.0 has requirement docutils<0.19, but you have docutils 0.20.1.; notebook 6.5.6 has requirement pyzmq<25,>=17, but you have pyzmq 25.1.1.; matplotlib 3.5.3 requires pillow, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | Heap-based Buffer Overflow <br/>[SNYK-PYTHON-PILLOW-5918878](https://snyk.io/vuln/SNYK-PYTHON-PILLOW-5918878) | `pillow:` <br> `9.5.0 -> 10.0.1` <br> | No | Mature . Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIzNTM4YWIwOC03Yzk4LTRjMDUtOTQ0Ny0yMjYwYjliNjhmY2IiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjM1MzhhYjA4LTdjOTgtNGMwNS05NDQ3LTIyNjBiOWI2OGZjYiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13708:1532,upgrade,upgraded,1532,https://hail.is,https://github.com/hail-is/hail/pull/13708,1,['upgrade'],['upgraded']
Deployability,",StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 3, altAlleles), ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false)))) AS variant#8; ```. Attached is a toy test.in.vds that reproduces the problem [test.in.vds.tar.gz](https://github.com/hail-is/hail/files/709524/test.in.vds.tar.gz). Tested on a clean ed544897f04722142b14b8e620614587c8f398a0 built with gradlew installDist.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1260:10792,install,installDist,10792,https://hail.is,https://github.com/hail-is/hail/issues/1260,1,['install'],['installDist']
Deployability,"- Add `terminationGracePeriodSeconds` to query kubernetes deployment; - Implement `app.on_shutdown` signal handler to wait for all asyncio tasks to complete before returning.; - Upgrade `aiohttp == 0.7.3` to address tasks being cancelled before the on_shutdown method is called: https://github.com/aio-libs/aiohttp/issues/3593. ## Testing. - Adding a ""wait `n` seconds"" method that slept for n seconds, and returned the value of an environment variable. This environment variable meant I could track which version of the deployment my script ran against.; - Taking the `deploy.yaml` from the `deploy query` step of the dev deploy, adding the `TEST_VALUE` environment variable with some value and saving it as `new-deploy.yaml`; - Issuing the first wait request (for 50 seconds) (`https://internal.hail.populationgenomics.org.au/$NAMESPACE/query/api/v1alpha/wait?duration=50`); - Issuing the new deploy with:; ```bash; kubectl -n $NAMESPACE apply -f new-deploy.yaml; kubectl -n $NAMESPACE rollout status --timeout=10m deployment query; ```; - When the new pod is created (seen with `kubectl --namespace $NAMESPACE get pod`), issue the second request to the wait method.; - If all goes well, you should have:; - termination logs like those below,; - the first request successfully fulfilled with the response of env value being None (filled by the first pod); - The second request successfully filled, but has the value of the environment value, the one you set in the deploy.yaml (it got scheduled to the new node). Termination logs:. ```; {""severity"": ""INFO"", ""levelname"": ""INFO"", ""asctime"": ""2021-02-24 23:22:40,472"", ""filename"": ""query.py"", ""funcNameAndLine"": ""on_shutdown:253"", ""message"": ""On shutdown request received, with 2 tasks left"", ""hail_log"": 1}; ++ term; ++ kill -TERM 7; + true; + '[' no == yes ']'; + trap - SIGTERM SIGINT; + wait 7; {""severity"": ""INFO"", ""levelname"": ""INFO"", ""asctime"": ""2021-02-24 23:23:26,004"", ""filename"": ""hail_logging.py"", ""funcNameAndLine"": ""log:40"", ""message"": """,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10106:58,deploy,deployment,58,https://hail.is,https://github.com/hail-is/hail/pull/10106,9,"['Upgrade', 'deploy']","['Upgrade', 'deploy', 'deployment']"
Deployability,"- Added a new build step `test_pipeline_docs` that runs doctest for the pipeline module; - Added pipeline docs to the hail/Makefile with a new target `pipeline-docs`. I also changed the `make-docs` target to be `base-docs` and `hail-docs`. `pipeline-docs` and `hail-docs` depend on `base-docs` and `upload-docs` depends on `hail-docs` and `pipeline-docs`; - Fixed a bunch of places in the documentation for clarity and to make the doctests work.; - Note, some examples are skipped because we can't run docker within docker in local mode. We can consider at another time point running all of the examples with the BatchBackend; - There's a bunch of Sphinx stuff I had to do to get the docs to render how I wanted them to with regards to inherited members (it was including all string methods by default even though). It's possible I can clean that up a bit, but I think it's fine for now.; - Added a link to the docs to the batch dropdown menu.; - Docs will appear at hail.is/docs/pipeline for now. Eventually everything will be renamed to batch, but I elected not to do that now.; - I checked the dropdown works correctly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8086:72,pipeline,pipeline,72,https://hail.is,https://github.com/hail-is/hail/pull/8086,6,['pipeline'],"['pipeline', 'pipeline-docs']"
Deployability,"- Added a shared image gallery to terraform; - Added a managed identity `batch-worker` to terraform; - Gave `batch-worker` ""acrpull"" privileges for the resource group; - Added new config variables in config.mk that are specific to Azure; - Added commands to batch/Makefile to create a boot disk image; - Added an Azure-specific startup script that installs Docker and the CLI and then authenticates and pulls the base image. The disk image we create is specialized. This means it has credentials in there after publishing it. I think this is okay and I specifically used the batch-worker managed identity to login for this. I can try and double check this assumption if you think I'm not correct after reading these docs: https://docs.microsoft.com/en-us/cli/azure/vm?view=azure-cli-latest#az_vm_create. > Accept system or user assigned identities separated by spaces. Use '[system]' to refer system assigned identity, or a resource id to refer user assigned identity. Check out help for more examples. I had to give the batch-worker managed identity in the resource group we want permissions to be an identity for a VM in the build-batch-worker-image resource group.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10834:348,install,installs,348,https://hail.is,https://github.com/hail-is/hail/pull/10834,1,['install'],['installs']
Deployability,"- Added idea of two types of outputs: internal and external. Internal outputs are copied between tasks and external outputs are user specified copies of files.; - Added `valid` and `mentioned` which are sets of resources tasks keep track of. `valid` checks whether the resources were properly declared. `mentioned` keeps a record of the resources used in the command for variables that need to be declared.; - Changed when outputs are copied for external outputs. TaskResourceFiles are copied when the task finishes rather than at the end of the pipeline. InputResourceFiles are copied at the beginning of the pipeline as one job.; - `inputs`, `internal_outputs`, and `external_outputs` only contain resource files (not resource groups) which simplified the code quite a bit!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5962:546,pipeline,pipeline,546,https://hail.is,https://github.com/hail-is/hail/pull/5962,2,['pipeline'],['pipeline']
Deployability,"- All subprocess calls are async. The UI is much more responsive now.; - Make refresh (rather than heal) non-reentrant. There was a race condition where we could update the Github state of a PR we were actively deploying. This seemed error prone.; - We need to lock the repos until the build is fully deployed. I now protect pr.heal.; - Set `batch_changed = True` whenever heal needs to be rerun becomes some of its inputs (build_state, source or target sha, collection of PRs) changed. Next up: deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5921:162,update,update,162,https://hail.is,https://github.com/hail-is/hail/pull/5921,4,"['deploy', 'update']","['deploy', 'deployed', 'deploying', 'update']"
Deployability,"- CI needs to account not only for the services that it is deploying but the services that the test-ci might later deploy into that same namespace; - I had earlier applied batch-driver's rate limit to everything and soon found that it is a bit too restrictive w.r.t. many simultaneous client submissions to the batch front end. With nginx we just didn't have rate limits on anything else, which seems less good than setting high rate limits. I'll admit that 200 was incredibly arbitrary (let's call it optimistic) and I'm happy to take more precise suggestions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12402:59,deploy,deploying,59,https://hail.is,https://github.com/hail-is/hail/pull/12402,2,['deploy'],"['deploy', 'deploying']"
Deployability,"- Fix statuses for dev deploys.; - Fix zulip notify for dev deploys (not currently used).; - Fix HTTPFound for dev deploys and PRs. We need to prefix the redirect with the base path; for this current CI instance. In my dev deploy case,; this prepends: `dking/ci` from which the web browser can; correctly redirect to: `https://internal.hail.is/dking/ci/`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8406:23,deploy,deploys,23,https://hail.is,https://github.com/hail-is/hail/pull/8406,4,['deploy'],"['deploy', 'deploys']"
Deployability,"- I left in pvc_size for backwards compatibility. We can rip it out at some point.; - I'm not happy with how the batch_worker_image seems slower now with the gsutil addition. Not sure if there's a better solution here. I tested a lot of this by hand on dev deploy. For example, making sure the flocks were right, the project quotas were right, the cache was working, the garbage collector, and the backwards compatibility with pvc_size was working. Let me know if you think there are other things I should test. If this is too much, I can think about splitting the storage and the cache into two PRs. I just figured since I almost had it all done together to push on that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9074:257,deploy,deploy,257,https://hail.is,https://github.com/hail-is/hail/pull/9074,1,['deploy'],['deploy']
Deployability,"- Makes a user page at `/me` showing your PRs and your most recent (10) dev deploys. I added a `dev_deploy: 1` attribute to dev deploy batches to identify them, as seemed consistent with `test` and `deploy` attributes, though it might make more sense to change these to `scope` in the future? Either way this will only select dev deploys going forward but that seems like a non-issue. Also needed a hail username to gh username mapping, which is annoying, but deploys are hail username and PRs are github username and I didn't see an existing mapping between the two. - Extracts the pr, job, and newly added dev deploy tables into macros for reuse across pages. As a result removed the duplicate code for the job table between PR and batch pages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10037:76,deploy,deploys,76,https://hail.is,https://github.com/hail-is/hail/pull/10037,6,['deploy'],"['deploy', 'deploys']"
Deployability,"- Need to install npm on build server; - Install following node packages: jsdom, jquery, mathjax-node; - Set the Global Environment Variable for where Node packages are: $NODE_PATH",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/800:10,install,install,10,https://hail.is,https://github.com/hail-is/hail/pull/800,2,"['Install', 'install']","['Install', 'install']"
Deployability,- New job only runs after test_batch because I don't want to spin up everything when running this with dev deploy; - Only run in test and dev (not deploy),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9375:107,deploy,deploy,107,https://hail.is,https://github.com/hail-is/hail/pull/9375,2,['deploy'],['deploy']
Deployability,"- No longer build the normal jar and wheel twice. build_hail now only produces the debug jar+test-jar+wheel and the test jar+wheel. - Remove hail.zip in favor of installing from the wheel. For some reason this breaks the python doc tests because presumably spark can't find the hail jar. I don't understand entirely why this is the case. I'm installing from the wheel instead of what `Dockerfile.hail-base` used to do which was copying the jar under `SPARK_HOME`, but I hoped that pip installing the wheel meant I wouldn't have to deal with that.; - Remove other outputs from the build_hail job that weren't used or didn't need to be proxied through this step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13038:162,install,installing,162,https://hail.is,https://github.com/hail-is/hail/pull/13038,3,['install'],['installing']
Deployability,"- On *deploys*, makes sure that whatever is in our third-party images is in our private registry before starting builds like hail-ubuntu that might depend on those images. This means that we can update our ubuntu base image without the australians needing to deploy any images by hand. However, this does not run in PRs because I 1) didn't want to add that kind of latency for PRs and 2) we don't do any kind of namespacing for our images so if we did include this for a PR that ultimately wasn't merged we would have to manually remove the image anyway so why not manually add it if you're going to PR it… I think point 2 is a little weak but I recall this being what we agreed on a couple months back when we discussed this. I'm wondering if we should just eat the minute or so latency at the beginning of PRs to be safe but it also feels like a shame for something that changes so infrequently. . - Again on deploys, upload the hailgenetics/* images to the private registry if they don't already exist there. This way any deployments that aren't hail team's GCP deployment can get these images automatically when they deploy a new SHA instead of uploading them manually. It won't backfill skipped versions, but we decided that was ok. This seems less relevant for testing on PRs as it will get triggered on releases and we can easily dev deploy to rectify the image if this breaks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12818:6,deploy,deploys,6,https://hail.is,https://github.com/hail-is/hail/pull/12818,9,"['deploy', 'release', 'update']","['deploy', 'deployment', 'deployments', 'deploys', 'releases', 'update']"
Deployability,- Only batch for now -- will add for pipeline and ci later; - This should fail until the kubernetes secret is added; - Requires a password `CLOUD_SQL_PASSWORD` to run the tests locally; (not sure what the best way to distribute this is); - Requires downloading the `cloud_sql_proxy` binary to run the tests locally,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5615:37,pipeline,pipeline,37,https://hail.is,https://github.com/hail-is/hail/pull/5615,1,['pipeline'],['pipeline']
Deployability,- Refactored `exportRectangles` on BlockMatrix from a static function with an input and output file to an instance function with an output file that writes the instance.; - Added `BlockMatrixRectanglesWriter` so exporting rectangles happens through the IR.; - Updated tests and deleted parts of tests that dealt with invalid inputs that aren't applicable when exporting from an already loaded BlockMatrix.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5478:260,Update,Updated,260,https://hail.is,https://github.com/hail-is/hail/pull/5478,1,['Update'],['Updated']
Deployability,"- Resolves #1105, #196; - For VDS and KT writes, writes history to `<output_path>/history.txt`; - For all other file types, writes history to `<output>.history.txt`; - Requires new Python dependency: `pip install autopep8`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2060:205,install,install,205,https://hail.is,https://github.com/hail-is/hail/pull/2060,1,['install'],['install']
Deployability,"- This problem was fixed in Scala 2.11. I ran into elsewhere, like RichVector in Utils. I left this comment:. // FIXME AnyVal in Scala 2.11. We're using Scala 2.10 because the Spark/Intel cluster is running an old version of, well, everything. It should be getting upgraded tomorrow. Then hopefully we can switch to 2.11 permanently. For now, I'd just put a similar comment.; - I'd remove testSingletonVariants. testFilterSamples is still good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/83#issuecomment-160692682:265,upgrade,upgraded,265,https://hail.is,https://github.com/hail-is/hail/pull/83#issuecomment-160692682,1,['upgrade'],['upgraded']
Deployability,- Updated key table case class; - All commands are hidden for now:; - `WriteKeyTable`: Write key table to parquet file; - `ReadKeyTable`: Read key table from parquet file and put in env; - `ExportKeyTable`: Write key table to tsp file; - `AddKeyTable`: Creates a new key table from transformations of a VDS. Names of commands are a work in progress...,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1012:2,Update,Updated,2,https://hail.is,https://github.com/hail-is/hail/pull/1012,1,['Update'],['Updated']
Deployability,"- Use LEN for `to_dense_mt`. We still need to calculate a global position for the end of a reference block, but this version is cleaner.; - Update `truncate_reference_blocks`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14681:140,Update,Update,140,https://hail.is,https://github.com/hail-is/hail/pull/14681,1,['Update'],['Update']
Deployability,"- Using the full domain name instead of the shorthand `<service>.<namespace>` isn't strictly necessary but just made me nervous and I opted for the full unambiguous domain for in-cluster services (what if we had a namespace named `com`?? I feel like that would break some things); - I got the configuration wrong on how to tell envoy *not* to worry about certs of internal namespaces (I'd recommend using the `split` view for the diff because otherwise it's pretty hard to read); - For internal namespaces, using the `prefix` parameter for matching a route allowed `/foo/batch` to match a route like `/foo/batch-driver` which is obviously not great. the `path_separated_prefix` parameter actually does what we want; - Fixed the batch tests not to look at the HTTP 1.1 reason phrase and instead look at the response body to determine the error",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12392:293,configurat,configuration,293,https://hail.is,https://github.com/hail-is/hail/pull/12392,1,['configurat'],['configuration']
Deployability,"- [ ] (@tpoterba) caf1e1e673 add fails_service_backend; - [ ] (@tpoterba, @cseed) a979dfba58 [hail] introduce and use mktemp and mktempd; - [ ] (@tpoterba) dcf026b01c [hail] make is.hail.expr.ir.functions threadsafe; - [ ] (@tpoterba) 807f38c20e [hail] fix use of row requiredness in lowerDistributedSort; - [ ] (@catoverdrive) 12df8eb456 [query-service] handle void-typed IRs in query-service; - [ ] (@catoverdrive) 03357ee83d [query-service] make user cache thread-safe; - [ ] (@tpoterba) 6c6734bc71 [query-service] bugfix: preserve globals through a shuffle; - [ ] (@catoverdrive) a3d2572ce7 [shuffler] log ShuffleCodecSpec anytime it is created; - [ ] (@daniel-goldstein) 8949dfec3c [scala-lsm] bugfix: least key may equal greatest key; - [ ] (@daniel-goldstein) 6067bd8e51 [services] discovered new transient error; - [ ] (@daniel-goldstein) c8356d30bb [shuffler] more assertions in ShuffleClient; - [ ] (@daniel-goldstein) 9991da90f0 [shuffler] bugfix: shuffler needs a HailContext to decode loci; - [ ] (@daniel-goldstein) bc0140ab6f [query-service] move hail.jar earlier in Dockerfile; - [ ] (@daniel-goldstein) f96c28174d [query-service] permit pod scaling and remove cpu limit; - [ ] (@catoverdrive) 6ae26339fe [query-service] simplify socket handling; - [ ] (@jigold) f3db30e23f [batch] teach JVMJob where to find the hail configuration files; - [ ] (@daniel-goldstein) b5c6d85554 [query-service] switch to services team approved logging; - [ ] (@tpoterba) 35a306c066 [query-service] query workers need a hail context; - [ ] (@daniel-goldstein, @catoverdrive) 051c89b8e7 [query-service] use a UNIX Domain Socket for Py-Scala communication; - [ ] (@daniel-goldstein, @catoverdrive) ad9ea73d7a [query-service] run tests against query service",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10072:1334,configurat,configuration,1334,https://hail.is,https://github.com/hail-is/hail/pull/10072,1,['configurat'],['configuration']
Deployability,- [ ] Enable pipelines to be able to run in the background for both Local and Batch modes; - [ ] Add dry-run support to BatchBackend; - [ ] Don't serially copy files in BatchBackend; - [ ] Docker within docker needs to work to test the LocalBackend docker mode,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5380:13,pipeline,pipelines,13,https://hail.is,https://github.com/hail-is/hail/issues/5380,1,['pipeline'],['pipelines']
Deployability,"- [ ] test hl.upload_log(); - [ ] local tests: http://flask.pocoo.org/docs/1.0/testing/; - [ ] build should build docker image, test should deploy the image in a test namespace",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4539:140,deploy,deploy,140,https://hail.is,https://github.com/hail-is/hail/issues/4539,1,['deploy'],['deploy']
Deployability,"- [ ] test order of magnitudes: 10k, 1M, 10M, 15M with `true` jobs, both flat and fan-out-fan-in dags.; - [ ] re-run Konrad's job with job count 4x max autoscaling core-count; - [ ] a dry run of Konrad's 12M job batch (perhaps with shorter individual tasks) with 1,600 cores (200 8-core nodes); - [ ] UI display 12M pods; - [ ] observe behavior of pipeline client code when creating a 12M task pipeline. core-hour = 0.01 USD/hour; core-minute ~= 0.00017 USD / hour. 12M seconds ~= 3333 hours; 12M core-seconds = 33.33 USD. Equivalent cost (33 USD) batches:; 12M 1 second jobs; 120k 100 seconds jobs; 12k 1000 second jobs. Max pods: 150,000; Max nodes: 5000; Max cores (8-core): 40,000; Max cores (64-core): 320,000",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6492:348,pipeline,pipeline,348,https://hail.is,https://github.com/hail-is/hail/issues/6492,2,['pipeline'],['pipeline']
Deployability,"- [ ] use make dry-runs w/ git to ensure that CI executes exactly those tests whose dependencies have changed since the last commit. - [ ] banish `archiveZip`. create `whl` files without spark dependencies, install those on the leader node (do we need to specify the jar separately still?). - [ ] download plink, qctool, and R packages in hail/Makefile and make dependencies for `test`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5199:207,install,install,207,https://hail.is,https://github.com/hail-is/hail/issues/5199,1,['install'],['install']
Deployability,"- [x] There should be a test in the tests that verify that the account is operational, so if it disabled, we get an informative error message. (see issue #4533). - [x] There should be documentation/a playbook about how to get Github unstuck when this happens. (see [here](; https://github.com/hail-is/hail/issues/4517#issuecomment-429135514)). - [ ] We should mock Github so we don't rely on it during the tests, which makes me sad because yay integration tests, and what do we do when the Github API changes/breaks/doesn't behave the way we expect?. Assigning to @danking since he's been through the first two, but maybe someone else can handle the third.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517:444,integrat,integration,444,https://hail.is,https://github.com/hail-is/hail/issues/4517,1,['integrat'],['integration']
Deployability,- `CreateNamespaceStep.public` was entirely unused; - `adminServiceAccount` is not used in `build.yaml` so `CreateNamespaceStep.admin_service_account` is always `None` meaning it has no effect.; - The three environment variables that I deleted from the `deployment.yaml` are as far as I can tell entirely unused (they are now grabbed from the global config),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13967:254,deploy,deployment,254,https://hail.is,https://github.com/hail-is/hail/pull/13967,1,['deploy'],['deployment']
Deployability,"- `geom_point` did not correctly include color in the legend when the color was a continuous variable that had been mapped to a discrete one, e.g. an integer that the user specified as the color. - `geom_bar` and `geom_col` suffered the same issue but additionally lacked the ability to name each color group in the legend.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12768:82,continuous,continuous,82,https://hail.is,https://github.com/hail-is/hail/pull/12768,1,['continuous'],['continuous']
Deployability,- `hail/python/dist` is generated by `setup.py` when publishing to PYPI. It is strictly build artifacts and should never be checked in. - `image-fetcher/deployment.yaml` is auto-generated by the `Makefile`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4658:153,deploy,deployment,153,https://hail.is,https://github.com/hail-is/hail/pull/4658,1,['deploy'],['deployment']
Deployability,- `stat -c` doesn't work on Mac; - I have a bloop directory now (related to metals which is a Scala Emacs IDE); - I had a rogue x in the definition of gateway's deployment; - I had missing semicolons in gateway's deployment; - the router resolver *is* TLS enabled; - the internal-gateway is *not* TLS enabled,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8686:161,deploy,deployment,161,https://hail.is,https://github.com/hail-is/hail/pull/8686,2,['deploy'],['deployment']
Deployability,- add priority class infrastructure throughout; - all pod specs have resource requests and limits; - make es tolerate preemptibles; - make monitoring router tolerate preemptibles. Deployed.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7425:180,Deploy,Deployed,180,https://hail.is,https://github.com/hail-is/hail/pull/7425,1,['Deploy'],['Deployed']
Deployability,"- added reference and contig names/lengths to header in export_vcf; - added line in docs; - added a test; - renamed ExportVcfSuite to ExportVCFSuite to match classname and ImportVCFSuite. @jigold is going to update GenomeReference to capture MD5, at which point I'll make a PR to export other metadata.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2438:208,update,update,208,https://hail.is,https://github.com/hail-is/hail/pull/2438,1,['update'],['update']
Deployability,- added sort on Arrays in expr; - extended sort and sortBy to take an optional Boolean parameter for ascending; - modifed behavior to always place null values at the end; - updated HailExpressionLanguage.md; - added tests for sort and sortBy to ExprSuite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/511:173,update,updated,173,https://hail.is,https://github.com/hail-is/hail/pull/511,1,['update'],['updated']
Deployability,"- but don't enable; - add back deployable to watched branches; - add deploy flag to build configuration; - added Code option that represents a version of code (but not quite a sha, because it might do a merge that produces a new sha) and can check itself out. I think this gets us pretty close. Working on ci2 tests now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5938:31,deploy,deployable,31,https://hail.is,https://github.com/hail-is/hail/pull/5938,3,"['configurat', 'deploy']","['configuration', 'deploy', 'deployable']"
Deployability,- change ready cores to a trigger; - added select for update to critical calls in sql; - added activation and deactivation times to instances table; - added removed parameter to instances table; don't delete instances from table that are removed; - schedule in parallel,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7833:54,update,update,54,https://hail.is,https://github.com/hail-is/hail/pull/7833,2,['update'],['update']
Deployability,- changed worker image from batch-worker-7 to batch-worker-8; - upgraded version of ubuntu from ubuntu-minimal-1804-bionic-v20191024 to ubuntu-minimal-1804-bionic-v20200520; - upgraded docker ; - added debug = true mode to docker daemon,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8875:64,upgrade,upgraded,64,https://hail.is,https://github.com/hail-is/hail/pull/8875,2,['upgrade'],['upgraded']
Deployability,"- convert the VEP configuration file to JSON; - let the conf file fully specify the command line, environment and output schema. This is breaking change. I updated the VEP config files on the cloud:. ```; $ gsutil ls gs://hail-common/vep/vep/*.json; gs://hail-common/vep/vep/vep81-gcloud.json; gs://hail-common/vep/vep/vep85-gcloud.json; gs://hail-common/vep/vep/vep92-GRCm38-gcloud.json; ```. But pipelines will have to change to use them. I tested GRCm38, but not the other ones. @konradjk, would you be up for testing the standard usage? (We really need to add automated VEP tests.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3872:18,configurat,configuration,18,https://hail.is,https://github.com/hail-is/hail/pull/3872,3,"['configurat', 'pipeline', 'update']","['configuration', 'pipelines', 'updated']"
Deployability,"- created is.hail.services package; - added DeployConfig, Tokens with the necessary functionality to get BatchClient working; - BatchClient is built on Apache HttpComponents; - Synchronous, thread safe. HttpClient is thread safe, BatchClient should be, too.; - Simple hello, world! test; - Added build step for Java services tests. FYI @jigold this might be a possible model if we ever rework the Python BatchClient. Also, if there are Batch changes going forward this code will also need to updated. The client is incredibly light weight, so that shouldn't be often, similar to the aiogoogle clients I wrote recently. Next up: Query Batch backend!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8779:44,Deploy,DeployConfig,44,https://hail.is,https://github.com/hail-is/hail/pull/8779,2,"['Deploy', 'update']","['DeployConfig', 'updated']"
Deployability,"- first cut C++ value IR compiler; - followed existing design, e.g. cxx.EmitTriplet, but carry physical type,; - added cxx.Compile, compiles a function takes and returns a single non-missing tuple,; - throws CXXUnsupportedOperation if it can't compile,; - integrate with assertEvalsTo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4663:256,integrat,integrate,256,https://hail.is,https://github.com/hail-is/hail/pull/4663,1,['integrat'],['integrate']
Deployability,"- gateway does all the reverse proxy; - site does the polling and serves the website up with nginx; - stop using crontab, just poll in the background; - set up liveness and readiness checks for 0 downtime changes; - ripped out letsencrypt stuff, which I will PR separately; - made certs into a secret. And something that got glommed on:. - put projects into a single file (projects.txt), still a lot to do here but I guess it is an improvement. FYI this is deployed now",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4602:457,deploy,deployed,457,https://hail.is,https://github.com/hail-is/hail/pull/4602,1,['deploy'],['deployed']
Deployability,"- gateway does all the reverse proxy; - site does the polling and serves the website up with nginx; - stop using crontab, just poll in the background; - set up liveness and readiness checks for 0 downtime changes; - ripped out letsencrypt stuff, which I will PR separately; - made certs into a secret. And something that got glommed on:. - put projects into a single file (projects.txt), still a lot to do here but I guess it is an improvement. FYI this is deployed now. fixes #4464; fixes #4463",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4601:457,deploy,deployed,457,https://hail.is,https://github.com/hail-is/hail/pull/4601,1,['deploy'],['deployed']
Deployability,- google.api_core.exceptions.ServiceUnavailable is just 503 in an unnecessary new class; - Teach DeployConfig.socket to use retryTransientErrors.; - Also retry NoRouteToHostException (the host should *eventually* come into existence).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10310:97,Deploy,DeployConfig,97,https://hail.is,https://github.com/hail-is/hail/pull/10310,1,['Deploy'],['DeployConfig']
Deployability,"- introduce job logging methods that automatically include id, state, and pod name; - add `reap_job` state update logic into `new_state`, use `new_state` in `reap_job`; - remove unused function `refresh_parents_and_maybe_create`, which lead to more dead code which was also removed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6972:107,update,update,107,https://hail.is,https://github.com/hail-is/hail/pull/6972,1,['update'],['update']
Deployability,- made sure the batch client is closed after a pipeline finishes; - fixed cpu and memory to take ints and floats; - Split AsyncWorkerPool into AsyncWorkerPool and AsyncThrottledGather; - Raise errors in AsyncThrottledGather (errors ignored in AsyncWorkerPool),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7333:47,pipeline,pipeline,47,https://hail.is,https://github.com/hail-is/hail/pull/7333,1,['pipeline'],['pipeline']
Deployability,"- move environment.yml files out of the packaged directory so they don't get shipped to end users. - add `make test-pip-deploy` which pip deploys to the next available `devN` version (you have to wait a bit before you can `pip install` it, so I didn't include that in the test, but you can do that manually, or a motivated person can write a polling script). - add `build/dev-conda` which ensures that if the dev-environment file changes since you last ran `make build/dev-conda`, your conda environment is updated. - pedantically use the correct conda environment _everywhere_. - use python to determine cpu count instead of fixing it at 2. - add `jq` as an `env-setup.sh` dependency. - add `make build/credentials.json` which `scp`s a new JSON file containing credentials to the local machine, moreover there are two rules for automatically extracting the credentials for PYPI from this JSON file. - use `ENV_VAR`, a make macro, to ensure we rebuild the appropriate targets (but no more) when a relevant environment variable is changed since last build. - added several missing breeze versions, now we can easily test against new spark versions, just run `SPARK_VERSION=4.0.0 make test`. - fold doctests in with regular tests under `test-python` which uses pytest, no more unnecessary copying as well. - fix build-info. - delete two unused python files in hail root. - correct LIBSIMDPP dependency in C makefile. # Not Doing Yet. - incorporate native lib into this Makefile. Instead, if anything changed in src/main/c since we last built, we rebuild. - fix the directory structure to be compliant with pytests recommended structure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5194:120,deploy,deploy,120,https://hail.is,https://github.com/hail-is/hail/pull/5194,4,"['deploy', 'install', 'update']","['deploy', 'deploys', 'install', 'updated']"
Deployability,"- moved k8s accounts, roles and bindings to vdc/k8s-config.yaml (applied to current cluster); - created deploy service account with privileges on default (should lock down to minimal set); - added batch service account option, use to launch deploy job with deploy-svc; - give batch-svc default-deploy binding so it can launch the deploy pod with suitable privileges",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4596:104,deploy,deploy,104,https://hail.is,https://github.com/hail-is/hail/pull/4596,5,['deploy'],"['deploy', 'deploy-svc']"
Deployability,"- new filter_alleles method: takes MT and lambda,; produces the fields needed to update row/entry fields; (old_to_new, new_to_old, old_locus, old_alleles); - subset_entries_hts and downcode_entries_hts are now; filter_alleles_hts, which calls filter_alleles.; - Exposed min_rep expr function in Python; - Added min_rep to AST FunctionRegistry (Scala); - Removed hl.min_rep method to minrep a MT (easy with the; function now); - Deleted FilterAlleles; - Deleted FilterAlleles (Scala); - Deleted minRep (Scala); - Moved MinRepSuite to Python",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3505:81,update,update,81,https://hail.is,https://github.com/hail-is/hail/pull/3505,1,['update'],['update']
Deployability,"- removes everything related to ci1, pr-builder, hail-ci-*, etc.; - adds build.yaml ci2 build configuration; - makes master deployable (will probably fail)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5974:94,configurat,configuration,94,https://hail.is,https://github.com/hail-is/hail/pull/5974,2,"['configurat', 'deploy']","['configuration', 'deployable']"
Deployability,"- retry every deadlock in two deadlock prone SQL operations; - add prometheus metrics for cores; - fix prometheus when you're not in the default namespace; - retry every docker 500 error, it's 500, not our fault, just retry, right?; - create a billing account for the dev deploying user; - rewrite a couple queries to harmonize table locking a bit; - add globals to delete tables script; - fix list_batches, which was broken by the query language changes; - include primary services developers' namespaces in prometheus monitoring. Fixes #7756",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7783:272,deploy,deploying,272,https://hail.is,https://github.com/hail-is/hail/pull/7783,1,['deploy'],['deploying']
Deployability,"- set up tests that evaluate performance tradeoffs; - build interface for controlling compression levels in import / write; - also test parquet LZ4 / snappy on write with Mitja pipeline, with size",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/293:177,pipeline,pipeline,177,https://hail.is,https://github.com/hail-is/hail/issues/293,1,['pipeline'],['pipeline']
Deployability,"- strike references to `devel` from `site/`, replacing with `0.2`. - add a `clean` target to `site/Makefile` and run it before deploy (in case we're in a dirty environment, e.g. dan's computer)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4595:127,deploy,deploy,127,https://hail.is,https://github.com/hail-is/hail/pull/4595,1,['deploy'],['deploy']
Deployability,"- update .dockerignore for subproject structure; - add batch, ci environments to build image; - update build image; - move hail/Makefile to root; - have toplevel build/deploy scripts call subproject ones",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4349:2,update,update,2,https://hail.is,https://github.com/hail-is/hail/pull/4349,3,"['deploy', 'update']","['deploy', 'update']"
Deployability,- updated docs; - added tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1196:2,update,updated,2,https://hail.is,https://github.com/hail-is/hail/pull/1196,1,['update'],['updated']
Deployability,- updated hail 0.2 tutorial; - added test for struct unpacking with annotate_cols/rows,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2785:2,update,updated,2,https://hail.is,https://github.com/hail-is/hail/pull/2785,1,['update'],['updated']
Deployability,"- updated parameter names; - fixed tests and docs. As with burden tests, explode should be used to handle variants with multiple keys",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2544:2,update,updated,2,https://hail.is,https://github.com/hail-is/hail/pull/2544,1,['update'],['updated']
Deployability,"- web_common/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **658/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.3 | HTTP Request Smuggling <br/>[SNYK-PYTHON-AIOHTTP-5798483](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-5798483) | `aiohttp:` <br> `3.8.4 -> 3.8.5` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJjZmU2NDEwYi1jYjQ3LTQ2YzgtOTYwYy1kOWRlY2UxMjI5ZTIiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImNmZTY0MTBiLWNiNDctNDZjOC05NjBjLWQ5ZGVjZTEyMjllMiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/b7c31419-ec34-40f1-8bc6-ad8303fb329b?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/b7c31419-ec34-40f1-8bc6-ad8303fb329b?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13286:1400,upgrade,upgraded,1400,https://hail.is,https://github.com/hail-is/hail/pull/13286,1,['upgrade'],['upgraded']
Deployability,-- Takes 8 minutes per build. Probably because we are running Java over the network. Figure out how to install locally. -- Figure out how to build more than one job at a time. -- Fix reporting on email to only show Failed Tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/295:103,install,install,103,https://hail.is,https://github.com/hail-is/hail/issues/295,1,['install'],['install']
Deployability,----------------------------- Captured log call -------------------------------; INFO batch_client.aioclient:aioclient.py:753 created batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 _make_tsm: found 1000 variants after filtering out monomorphic sites.; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 1/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 2/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 3/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 4/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 5/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 6/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_fa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12980:5701,update,updated,5701,https://hail.is,https://github.com/hail-is/hail/issues/12980,1,['update'],['updated']
Deployability,"--------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>(); 4 vcf=""/project/ukbiobank/imputation/ad.v1/vcf/ukbb.hg38.imputed.chr""+chr+"".dose.vcf.bgz""; 5 mt=""/project/ukbiobank/imputation/ad.v1/mt/ukbb.hg38.imputed.chr""+chr; ----> 6 hl.import_vcf(vcf).write(mt). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548; 549 update_wrapper(wrapper, f). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, _codec_spec); 2111 """"""; 2112; -> 2113 self._jvds.write(output, overwrite, _codec_spec); 2114; 2115 def globals_table(self) -> Table:. /share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job 2 cancelled because SparkContext was shut down. Java stack trace:; org.apache.spark.SparkException: Job 2 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755:2407,install,install,2407,https://hail.is,https://github.com/hail-is/hail/issues/4755,1,['install'],['install']
Deployability,"-----------------------------; None; 2020-02-14 14:35:22 Hail: WARN: export_vcf: ignored the following fields:; 'varid' (row); 'new_locus' (row); 'old_locus' (row); [Stage 0:======================================================>(292 + 1) / 293]2020-02-14 14:38:52 Hail: INFO: Ordering unsorted dataset with netw; 2020-02-14 14:38:52 Hail: WARN: export_vcf found no row field 'info'. Emitting no INFO fields.; [Stage 2:> (0 + 168) / 279][Stage 2:> (14 + 1) / 293]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/liftover/liftover.py"", line 29, in <module>; hl.export_vcf(mt,""/project/ukbiobank/imp/uk.v3.GRCh38/uk.v3.r38.chr""+chr+"".vcf.bgz""); File ""</share/pkg.7/hail/0.2.19/install/python/lib/python3.6/site-packages/decorator.py:decorator-gen-1289>"", line 2, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 585, in wrapper; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 513, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 108, in execute; File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 221, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the mmChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_ion(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrow.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.sp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:2848,install,install,2848,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['install'],['install']
Deployability,"----------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **265/1000** <br/> **Why?** CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6050294](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6050294) | `cryptography:` <br> `41.0.7 -> 42.0.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **402/1000** <br/> **Why?** Proof of Concept exploit, CVSS 5.9 | Information Exposure <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6126975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6126975) | `cryptography:` <br> `41.0.7 -> 42.0.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJkYWIzNjU3Mi1hNTUwLTQwY2EtYThjZi0zN2ZjODljOWI1OGEiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImRhYjM2NTcyLWE1NTAtNDBjYS1hOGNmLTM3ZmM4OWM5YjU4YSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14201:1981,upgrade,upgraded,1981,https://hail.is,https://github.com/hail-is/hail/pull/14201,1,['upgrade'],['upgraded']
Deployability,"-->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pytest-dev/pytest-asyncio/blob/master/CHANGELOG.rst"">pytest-asyncio's changelog</a>.</em></p>; <blockquote>; <h1>0.20.2 (22-11-11)</h1>; <ul>; <li>Fixes an issue with async fixtures that are defined as methods on a test class not being rebound to the actual test instance. <code>[#197](https://github.com/pytest-dev/pytest-asyncio/issues/197) &lt;https://github.com/pytest-dev/pytest-asyncio/issues/197&gt;</code>_</li>; <li>Replaced usage of deprecated <code>@pytest.mark.tryfirst</code> with <code>@pytest.hookimpl(tryfirst=True)</code> <code>[#438](https://github.com/pytest-dev/pytest-asyncio/issues/438) &lt;https://github.com/pytest-dev/pytest-asyncio/pull/438&gt;</code>_</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/07a1416c2fe15d85fc149b3caa35b057de0b3d6e""><code>07a1416</code></a> Prepare release of v0.20.2.</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/dc3ad211d160006b4a30996c0a2a2c29754ef1fc""><code>dc3ad21</code></a> Build(deps): Bump pytest-trio in /dependencies/default (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-asyncio/issues/441"">#441</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/d9faba85890334f0548732d35f1b1d54a850a69f""><code>d9faba8</code></a> Build(deps): Bump mypy from 0.982 to 0.990 in /dependencies/default (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-asyncio/issues/440"">#440</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/fe63e346154b61bbfe767e585b0b3b55fb37463e""><code>fe63e34</code></a> Handle bound fixture methods correctly (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-asyncio/issues/439"">#439</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12453:3575,release,release,3575,https://hail.is,https://github.com/hail-is/hail/pull/12453,1,['release'],['release']
Deployability,"--chown was added in 17.09.0 on September 26, 2017, but GKE has an old version: 17.03.2-ce. GKE release notes don't indicate any updates and they only moved to 17.03 in February of 2018. 🤷‍♀️",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4949:96,release,release,96,https://hail.is,https://github.com/hail-is/hail/pull/4949,2,"['release', 'update']","['release', 'updates']"
Deployability,"-01-19&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajtpio+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajupyterlab-probot+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​jupyterlab-probot</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Akrassowski+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ameeseeksmachine+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​meeseeksmachine</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Amisterfads+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​misterfads</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Awelcome+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​welcome</code></a></p>; <h2>v4.0.10</h2>; <h2>4.0.10</h2>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab/compare/v4.0.9...b9bc3002b1ab89b9a1c4d2a3007c43275d11e0df"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Backport PR <a href=""https://redirect.github.com/jupyterlab/jupyterlab/issues/15386"">#15386</a>: Improve scrolling to heading <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15565"">#15565</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; <li>Workaround focus leaving input box on consecutive submissions <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15479"">#15479</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; <li>Fix search coming back in notebook and editor <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15443"">#15443</a> (<a href=""https://github.com/krassowski""><code>@​",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14184:3019,update,updated,3019,https://hail.is,https://github.com/hail-is/hail/pull/14184,1,['update'],['updated']
Deployability,"-0106a51b-pgxq pulling image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Pulled 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Successfully pulled image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Created 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Created container; Normal Started 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq ; NAME READY STATUS RESTARTS AGE; notebook-worker-9szt8 0/1 Running 0 49s. Started container; Warning Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension install --user --py jupyter_spark && \; jupyter contrib nbextension install --user && \; jupyter nbextension enable --user --py jupyter_spark && \; jupyter nbextension enable --user --py widgetsnbextension && \; jupyter nbextension enable --user collapsible_headings/main && \; jupyter nbextension enable --user move_selected_cells/main. COPY ./resources/ /home/jovyan; ```. And the actual worker creation in notebook.py. ```py; def start_pod(jupyter_token, image, labels={}):; print(""IMAGE IN START IS"", image); pod_id = uuid.uu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:1564,install,installed,1564,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218,1,['install'],['installed']
Deployability,"-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: running PCA using 63110 variants.; [Stage 5:==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/sp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:5819,install,install,5819,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['install'],['install']
Deployability,"-1.26.14'</li>; <li><a href=""https://github.com/boto/boto3/commit/61de529b5f9a7bdcc8c76debb472a7f934d048e6""><code>61de529</code></a> Merge branch 'release-1.26.14' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/c92111ed1a7346642060fd7f6dfedcb3770a9650""><code>c92111e</code></a> Bumping version to 1.26.14</li>; <li><a href=""https://github.com/boto/boto3/commit/53c38e2c6849af7c0c47a4d21ce41bfbad7d80cb""><code>53c38e2</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/1de404aff4ecb1c5560b4e023f0614d8149622ed""><code>1de404a</code></a> fix typo: 'are specified the' should be 'are specified in the' (<a href=""https://github-redirect.dependabot.com/boto/boto3/issues/3499"">#3499</a>)</li>; <li><a href=""https://github.com/boto/boto3/commit/47f20744b3c57223da5e14e185120586f8212af8""><code>47f2074</code></a> Merge branch 'release-1.26.13'</li>; <li><a href=""https://github.com/boto/boto3/commit/3d4081ccb7c350538ba3d6a5073c59575a812eb0""><code>3d4081c</code></a> Merge branch 'release-1.26.13' into develop</li>; <li>Additional commits viewable in <a href=""https://github.com/boto/boto3/compare/1.26.7...1.26.15"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=boto3&package-manager=pip&previous-version=1.26.7&new-version=1.26.15)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12498:9139,release,release-,9139,https://hail.is,https://github.com/hail-is/hail/pull/12498,1,['release'],['release-']
Deployability,"-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldP",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466:4502,pipeline,pipeline,4502,https://hail.is,https://github.com/hail-is/hail/issues/6466,4,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,"-287-742170; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-non-preemptible-pool-0106a51b-qz7f/10.128.0.160; Start Time: Thu, 05 Sep 2019 15:15:42 -0400; Labels: app=batch-job; batch_id=12728; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=287; user=wang; uuid=ca985fd90f9d46968ab9c480af9c931c; Annotations: <none>; Status: Pending; IP: ; Init Containers:; setup:; Container ID: ; Image: google/cloud-sdk:237.0.0-alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/sh; -c; ; set -ex; (gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData)); gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/12728/287/742170/container_logs && exit 1; rm -rf /io/*; set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchm",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7016:1267,pipeline,pipeline,1267,https://hail.is,https://github.com/hail-is/hail/issues/7016,6,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,"-29&amp;type=Issues""><code>@​jupyterlab-probot</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Akrassowski+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Alumberbot-app+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​lumberbot-app</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ameeseeksmachine+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​meeseeksmachine</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aparmentelat+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​parmentelat</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Atonyfast+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​tonyfast</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Awelcome+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​welcome</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3AWh1isper+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​Wh1isper</code></a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/jupyterlab/jupyterlab/commit/882dd81a6d5aa31388177ce5b49a4c2b3fc7f69e""><code>882dd81</code></a> [ci skip] Publish 4.0.11</li>; <li><a href=""https://github.com/jupyterlab/jupyterlab/commit/1ef7a4fa0202ebdf663e1cc0b45c8813a34a0b96""><code>1ef7a4f</code></a> Merge pull request from GHSA-44cc-43rp-5947</li>; <li><a href=""https://github.com/jupyterlab/jupyterlab/commit/0a7510114b56a0c5da8f7d251e69a67aafb87ef2""><code>0a75101</code></a> Fix CI: lint</li>; <li><a href=""https://github.com/jupyterlab/jupyterlab/commit/dda0033cd49449572d077bbecd33b18d8d05f48a""><code>dda0033</code></a> Merge pull request from GHSA-4m77-cmpx-vjc4</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14184:12462,update,updated,12462,https://hail.is,https://github.com/hail-is/hail/pull/14184,1,['update'],['updated']
Deployability,"-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723e4b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-10-19T03:09:40Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.2' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.124' >> src/main/resources/build-info.properties; creating env/HAIL_DEBUG_MODE which does not exist; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; make -C src/main/c prebuilt; make[1]: Entering directory `/mnt/tm",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:3589,deploy,deploy,3589,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['deploy'],['deploy']
Deployability,"-94 Algorithms for certificates and CRLs</li>; <li>Add RFC8696 providing using Pre-Shared Key (PSK) in the CMS</li>; <li>Add RFC5639 providing identifiers for the Brainpool curves in; Elliptic Curve Cryptography</li>; <li>Add RFC5697 providing Other Certificates Extension</li>; <li>Add RFC4683 providing Subject Identification Method (SIM)</li>; <li>Add RFC4476 providing Attribute Certificate Policies Extension</li>; <li>Add RFC5636 providing Traceable Anonymous Certificate</li>; <li>Add RFC5752 providing Multiple Signatures attribute for CMS</li>; <li>Add RFC5275 providing CMS Symmetric Key Management and Distribution</li>; <li>Add RFC8702 providing SHAKE One-way Hash Functions in the CMS</li>; <li>Add RFC8708 providing HSS/LMS Hash-based Signature Algorithm for CMS</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/ffc91a940f531841cb7b25f898afbc247b29bce4""><code>ffc91a9</code></a> Prepare release 0.3.0 (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/9"">#9</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/9c2ad2b8226d285272ebee1180354e4e02408b62""><code>9c2ad2b</code></a> Add note about new maintainers (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/6"">#6</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/10a10e7c4508ac4d858cbe7c8ac9e46575c2bb5c""><code>10a10e7</code></a> Pass tag to workflow call (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/5"">#5</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/e0c7fd6723bd63db4183352d21dfbebd6c2553b1""><code>e0c7fd6</code></a> Prepare v0.3.0.rc1 with new release workflow (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/3"">#3</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/7d8e520aa7d0e71ef7144ce381c8a41464e687dc""><code>7d8e52",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12928:6732,release,release,6732,https://hail.is,https://github.com/hail-is/hail/pull/12928,1,['release'],['release']
Deployability,"-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline; p.run(dry_run=True); ```. ```bash; #!/bin/bash; set -ex. # change cd to tmp directory; cd /tmp//pipeline.jlQrNJZW/. # __TASK__0 read_input; cp gs://hail-jigold/random_file.txt nfVpMp4n. # __TASK__1 read_input; cp gs://hail-jigold/input.bed 33qZtfwg.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim 33qZtfwg.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam 33qZtfwg.fam. # __TASK__4 subset; __RESOURCE_GROUP__0=33qZtfwg; __RESOURCE_GROUP__1=yibUlBkL; __RESOURCE__6=yibUlBkL.fam; __RESOURCE__10=29aBQihd; __RESOURCE__1=33qZtfwg.bed; __RESOURCE__2=33qZtfwg.bim; __RESOURCE__3=33qZtfwg.fam; __RESOURCE_GROUP__2=YXS0tQKi; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}; awk '{ print $1, $2}' ${__RESOURCE__6} | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > ${__RESOURCE__10}; plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}. # __TASK__5 shapeit; __RESOU",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:1992,pipeline,pipeline,1992,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282,1,['pipeline'],['pipeline']
Deployability,"-change:<code>organizations</code>: [<code>botocore</code>] This release introduces delegated administrator for AWS Organizations, a new feature to help you delegate the management of your Organizations policies, enabling you to govern your AWS organization in a decentralized way. You can now allow member accounts to manage Organizations policies.</li>; <li>api-change:<code>rds</code>: [<code>botocore</code>] This release enables new Aurora and RDS feature called Blue/Green Deployments that makes updates to databases safer, simpler and faster.</li>; <li>api-change:<code>textract</code>: [<code>botocore</code>] This release adds support for classifying and splitting lending documents by type, and extracting information by using the Analyze Lending APIs. This release also includes support for summarized information of the processed lending document package, in addition to per document results.</li>; <li>api-change:<code>transcribe</code>: [<code>botocore</code>] This release adds support for 'inputType' for post-call and real-time (streaming) Call Analytics within Amazon Transcribe.</li>; </ul>; <h1>1.26.16</h1>; <ul>; <li>api-change:<code>grafana</code>: [<code>botocore</code>] This release includes support for configuring a Grafana workspace to connect to a datasource within a VPC as well as new APIs for configuring Grafana settings.</li>; <li>api-change:<code>rbin</code>: [<code>botocore</code>] This release adds support for Rule Lock for Recycle Bin, which allows you to lock retention rules so that they can no longer be modified or deleted.</li>; </ul>; <h1>1.26.15</h1>; <ul>; <li>bugfix:Endpoints: [<code>botocore</code>] Resolve endpoint with default partition when no region is set</li>; <li>bugfix:s3: [<code>botocore</code>] fixes missing x-amz-content-sha256 header for s3 object lambda</li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] Adding support for Amazon AppFlow to transfer the data to Amazon Redshift databases through Amazon Redshift Data",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12507:3957,release,release,3957,https://hail.is,https://github.com/hail-is/hail/pull/12507,1,['release'],['release']
Deployability,"-ci-update-config</li>; <li>Additional commits viewable in <a href=""https://github.com/sass/libsass-python/compare/0.19.2...0.21.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=libsass&package-manager=pip&previous-version=0.19.2&new-version=0.21.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11508:7151,upgrade,upgrade,7151,https://hail.is,https://github.com/hail-is/hail/pull/11508,3,['upgrade'],['upgrade']
Deployability,"-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/230"">tox-dev/sphinx-autodoc-typehints#230</a></li>; <li>Support and require nptyping 2.1.1 by <a href=""https://github.com/gaborbernat""><code>@​gaborbernat</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/232"">tox-dev/sphinx-autodoc-typehints#232</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2"">https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2</a></p>; <h2>1.18.1</h2>; <p>No release notes provided.</p>; <h2>1.18.0</h2>; <p>No release notes provided.</p>; <h2>1.17.1</h2>; <p>No release notes provided.</p>; <h2>typehints_use_rtype support and handle TypeError</h2>; <p>No release notes provided.</p>; <h2>1.16.0</h2>; <p>No release notes provided.</p>; <h2>1.15.3</h2>; <p>No release notes provided.</p>; <h2>1.15.2</h2>; <p>No release notes provided.</p>; <h2>1.15.1</h2>; <p>No release notes provided.</p>; <h2>1.15.0</h2>; <p>No release notes provided.</p>; <h2>1.14.1</h2>; <p>No release notes provided.</p>; <h2>Added document_defaults config option</h2>; <p>No release notes provided.</p>; <h2>Fix NewType is inserting a reference as first argument</h2>; <p>No release notes provided.</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/blob/main/CHANGELOG.md"">sphinx-autodoc-typehints's changelog</a>.</em></p>; <blockquote>; <h2>1.18.3</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2.1.2</code></li>; </ul>; <h2>1.18.2</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2.1.1</code></li>; </ul>; <h2>1.18.1</h2>; <ul>; <li>Fix mocked module import not wor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11909:1866,release,release,1866,https://hail.is,https://github.com/hail-is/hail/pull/11909,1,['release'],['release']
Deployability,"-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/230"">tox-dev/sphinx-autodoc-typehints#230</a></li>; <li>Support and require nptyping 2.1.1 by <a href=""https://github.com/gaborbernat""><code>@​gaborbernat</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/232"">tox-dev/sphinx-autodoc-typehints#232</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2"">https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2</a></p>; <h2>1.18.1</h2>; <p>No release notes provided.</p>; <h2>1.18.0</h2>; <p>No release notes provided.</p>; <h2>1.17.1</h2>; <p>No release notes provided.</p>; <h2>typehints_use_rtype support and handle TypeError</h2>; <p>No release notes provided.</p>; <h2>1.16.0</h2>; <p>No release notes provided.</p>; <h2>1.15.3</h2>; <p>No release notes provided.</p>; <h2>1.15.2</h2>; <p>No release notes provided.</p>; <h2>1.15.1</h2>; <p>No release notes provided.</p>; <h2>1.15.0</h2>; <p>No release notes provided.</p>; <h2>1.14.1</h2>; <p>No release notes provided.</p>; <h2>Added document_defaults config option</h2>; <p>No release notes provided.</p>; <h2>Fix NewType is inserting a reference as first argument</h2>; <p>No release notes provided.</p>; <h2>Python 3.10 support and PEP-563, drop 3.6</h2>; <p>No release notes provided.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/blob/main/CHANGELOG.md"">sphinx-autodoc-typehints's changelog</a>.</em></p>; <blockquote>; <h2>1.18.2</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2.1.1</code></li>; </ul>; <h2>1.18.1</h2>; <ul>; <li>Fix mocked module import not working when used as guarded import</li>; </ul>; <h2>1.18.0",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11893:1363,release,release,1363,https://hail.is,https://github.com/hail-is/hail/pull/11893,1,['release'],['release']
Deployability,"-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; en",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:8013,Deploy,Deploy,8013,https://hail.is,https://github.com/hail-is/hail/pull/8561,2,"['Deploy', 'deploy']","['Deploy', 'deploy']"
Deployability,"-cors/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/522d98936f3995480fe3132b55415d74298d6790""><code>522d989</code></a> Release version 3.0.9 (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/67c4b2cc98ae87cf1fa7df4f97fd81b40c79b895""><code>67c4b2c</code></a> Fix request path normalization (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/5c6e05e996f10be1df1f2ad178560e54a2f82f1b""><code>5c6e05e</code></a> docs: Fix simple typo, garaunteed -&gt; guaranteed</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/566aef21accd0a15cf127a41edbe14a40c80728c""><code>566aef2</code></a> Fixed over-indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@​juanmaneo</code></a> and <a href=""https://github.com/jdevera""><code>@​jdevera</code></a></li>; <li>See full diff in <a href=""https://github.com/corydolphin/flask-cors/compare/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10464:2653,Update,Update,2653,https://hail.is,https://github.com/hail-is/hail/pull/10464,1,['Update'],['Update']
Deployability,"-forge channels:</p>; <pre><code>conda install -c conda-forge pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.4.0rc0</h2>; <p>We are pleased to announce a release candidate for pandas 1.4.0. If all goes well, we'll release pandas 1.4.0 in about two weeks.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4/whatsnew/v1.4.0.html"">whatsnew</a> for a list of all the changes. pandas 1.4.0 supports Python 3.8 and higher.</p>; <p>The release will be available on conda-forge and PyPI.</p>; <p>The release can be installed from PyPI</p>; <pre><code>python -m pip install --upgrade --pre pandas==1.4.0rc0; </code></pre>; <p>Or from conda-forge</p>; <pre><code>conda install -c conda-forge/label/pandas_rc pandas==1.4.0rc0; </code></pre>; <p>Please report any issues with the release candidate on the pandas issue tracker.</p>; <h2>Pandas 1.3.5</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pandas-dev/pandas/commit/06d230151e6f18fdb8139d09abf539867a8cd481""><code>06d2301</code></a> RLS: 1.4.1</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/47e3b409deb41f18e30e447579cba3a246db050e""><code>47e3b40</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/45587"">#45587</a>: DOC: append deprecation (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/45942"">#45942</a>)</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/f61dfde6abbe33e143e83c6685e4b3c1c488f92b""><code>f61dfde</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/45936"">#45936</a>: DOC: 1.4.1 release date (<a href=""https://github-redirect.dependabot.com/pan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11539:2353,release,release,2353,https://hail.is,https://github.com/hail-is/hail/pull/11539,1,['release'],['release']
Deployability,"-java/commit/47e36b651e9abc80f8d711cbff69c821539851c2""><code>47e36b6</code></a> Updating CODEOWNERS for Communication Identity &amp; Common Packages (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32222"">#32222</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/c051757b394000308e0a79bcb93da05875892401""><code>c051757</code></a> Increment package versions for cosmos releases (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32209"">#32209</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/Azure/azure-sdk-for-java/compare/v1.2.1...azure-identity_1.7.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.azure:azure-identity&package-manager=gradle&previous-version=1.2.1&new-version=1.7.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12508:4678,update,updates,4678,https://hail.is,https://github.com/hail-is/hail/pull/12508,1,['update'],['updates']
Deployability,"-metadata/issues/46"">#46</a> from pytest-dev/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/025be8999a22ae395b0e2b8ae4e7c9fa2334f874""><code>025be89</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/429840f4de26276560961929f21aab79ed305875""><code>429840f</code></a> Avoid running nightly on forks</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/c1968f39609978ec9c6a4bcf91c37c6164483f04""><code>c1968f3</code></a> Fix nightly</li>; <li>See full diff in <a href=""https://github.com/pytest-dev/pytest-metadata/compare/v2.0.1...v2.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pytest-metadata&package-manager=pip&previous-version=2.0.1&new-version=2.0.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12188:2597,update,updates,2597,https://hail.is,https://github.com/hail-is/hail/pull/12188,1,['update'],['updates']
Deployability,"-redirect.dependabot.com/bokeh/bokeh/issues/11724"">#11724</a> [NO SQUASH] More 3.0 -&gt; 2.4 backports</li>; </ul>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/bokeh/bokeh/commit/ad33147f5762af8830e68144419e31e46a024caf""><code>ad33147</code></a> Deployment updates for release 2.4.2</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/17578f3a7fce22af09cf105c67769890dfdb5705""><code>17578f3</code></a> also update latest=2.4.2</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/e3e182a740b1a88f6b13d83656df296e02616506""><code>e3e182a</code></a> Merge deployment staging branch staging-2.4.2rc1</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/6bcda8a0b0a6ff9c4449abb40082b63c3ea7e3e4""><code>6bcda8a</code></a> Deployment updates for release 2.4.2rc1</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/638d3ee438716feceac319c40fa6b17655457e5d""><code>638d3ee</code></a> Updates for release (<a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11824"">#11824</a>)</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/56eef3cdac7e195f3564110cbc3a8daab863f961""><code>56eef3c</code></a> Merge deployment staging branch staging-2.4.2dev1</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/99d1fc017eab9e3b8e66b8d40fdc5737c8c9f0fe""><code>99d1fc0</code></a> Deployment updates for release 2.4.2dev1</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/23b5134dee481632775b09a2d37d183a646026a8""><code>23b5134</code></a> Provide complete model context for deserialization of instances (<a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11469"">#11469</a>)</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/64fa0759bab7e2cc48663a2359093dc3e0b58df5""><code>64fa075</code></a> Add OS to bokeh info (<a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11797"">#11797</a>)</li>; <li><a href=""https://gi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11540:7498,Update,Updates,7498,https://hail.is,https://github.com/hail-is/hail/pull/11540,2,"['Update', 'release']","['Updates', 'release']"
Deployability,-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-dock,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:4391,deploy,deploy-,4391,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,". (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/206"">#206</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/308f153f91b7942476f2d4ddda3dc8c99933d598""><code>308f153</code></a> ci: add workflow to publish sdist/wheel to PyPI (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/202"">#202</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/961624063383cbcdc78a61b1d18448429a61a489""><code>9616240</code></a> [chore] update changelog</li>; <li>Additional commits viewable in <a href=""https://github.com/tomplus/kubernetes_asyncio/compare/v19.15.1...23.6.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=kubernetes-asyncio&package-manager=pip&previous-version=19.15.1&new-version=23.6.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:15862,update,updates,15862,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['update'],['updates']
Deployability,". (<a href=""https://redirect.github.com/ipython/ipython/issues/13975"">#13975</a>)</li>; <li><a href=""https://github.com/ipython/ipython/commit/3a9419dce7d6ccf7de39be606eec2fc212ef4445""><code>3a9419d</code></a> Update completer documentation (<a href=""https://redirect.github.com/ipython/ipython/issues/13999"">#13999</a>)</li>; <li><a href=""https://github.com/ipython/ipython/commit/a7d8defdecca3cfa54eada171181e0880c8b6b5f""><code>a7d8def</code></a> Expose <code>auto_suggest.resume_hinting</code>, fix resume on backspace (<a href=""https://redirect.github.com/ipython/ipython/issues/13994"">#13994</a>)</li>; <li><a href=""https://github.com/ipython/ipython/commit/4e7b9408a0b35cabbdc973f131257f4e97a3ddcf""><code>4e7b940</code></a> Fix autosuggestions in multi-line mode, vi command mode delay (<a href=""https://redirect.github.com/ipython/ipython/issues/13991"">#13991</a>)</li>; <li><a href=""https://github.com/ipython/ipython/commit/ad452c1d8bb25e742caa152fb301e0e6626b6faa""><code>ad452c1</code></a> Improve API documentation around configuration of embedded IPython (<a href=""https://redirect.github.com/ipython/ipython/issues/13989"">#13989</a>)</li>; <li><a href=""https://github.com/ipython/ipython/commit/92027083ab69186db3f104fe38651086bcf4e760""><code>9202708</code></a> Handle OSError cases where traceback frames occur from built files (<a href=""https://redirect.github.com/ipython/ipython/issues/13964"">#13964</a>)</li>; <li><a href=""https://github.com/ipython/ipython/commit/fc872d6cfab48d861c43c766d583edde73370836""><code>fc872d6</code></a> Allow to dispatch getting documentation on objects</li>; <li>Additional commits viewable in <a href=""https://github.com/ipython/ipython/compare/7.34.0...8.12.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=ipython&package-manager=pip&previous-version=7.34.0&new-version=8.12.0)](https://docs.github.com/en/github/managing-secur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12832:1856,configurat,configuration,1856,https://hail.is,https://github.com/hail-is/hail/pull/12832,2,['configurat'],['configuration']
Deployability,". A new Amazon Macie (macie2) is now available with significant design improvements and additional features.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] Documentation updates for Amazon EC2.</li>; <li>api-change:<code>sts</code>: [<code>botocore</code>] Documentation updates for AWS Security Token Service.</li>; <li>api-change:<code>connect</code>: [<code>botocore</code>] This release updates the *InstanceStorageConfig APIs so they support a new ResourceType: REAL_TIME_CONTACT_ANALYSIS_SEGMENTS. Use this resource type to enable streaming for real-time contact analysis and to associate the Kinesis stream where real-time contact analysis segments will be published.</li>; </ul>; <h1>1.21.12</h1>; <ul>; <li>api-change:<code>greengrassv2</code>: [<code>botocore</code>] Doc only update that clarifies Create Deployment section.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for data repository associations to use root (&quot;/&quot;) as the file system path</li>; <li>api-change:<code>kendra</code>: [<code>botocore</code>] Amazon Kendra now suggests spell corrections for a query. For more information, see <a href=""https://docs.aws.amazon.com/kendra/latest/dg/query-spell-check.html"">https://docs.aws.amazon.com/kendra/latest/dg/query-spell-check.html</a></li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] Launching Amazon AppFlow Marketo as a destination connector SDK.</li>; <li>api-change:<code>timestream-query</code>: [<code>botocore</code>] Documentation only update for SDK and CLI</li>; </ul>; <h1>1.21.11</h1>; <ul>; <li>api-change:<code>gamelift</code>: [<code>botocore</code>] Minor updates to address errors.</li>; <li>api-change:<code>cloudtrail</code>: [<code>botocore</code>] Add bytesScanned field into responses of DescribeQuery and GetQueryResults.</li>; <li>api-change:<code>athena</code>: [<code>botocore</code>] This release adds support for S3 Object Ownership by allowing the S3 bucket owne",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11504:1909,release,release,1909,https://hail.is,https://github.com/hail-is/hail/pull/11504,1,['release'],['release']
Deployability,". Existing customers can enable this feature with UpdateEventSourcesConfig.</li>; <li>api-change:<code>macie</code>: [<code>botocore</code>] Amazon Macie Classic (macie) has been discontinued and is no longer available. A new Amazon Macie (macie2) is now available with significant design improvements and additional features.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] Documentation updates for Amazon EC2.</li>; <li>api-change:<code>sts</code>: [<code>botocore</code>] Documentation updates for AWS Security Token Service.</li>; <li>api-change:<code>connect</code>: [<code>botocore</code>] This release updates the *InstanceStorageConfig APIs so they support a new ResourceType: REAL_TIME_CONTACT_ANALYSIS_SEGMENTS. Use this resource type to enable streaming for real-time contact analysis and to associate the Kinesis stream where real-time contact analysis segments will be published.</li>; </ul>; <h1>1.21.12</h1>; <ul>; <li>api-change:<code>greengrassv2</code>: [<code>botocore</code>] Doc only update that clarifies Create Deployment section.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for data repository associations to use root (&quot;/&quot;) as the file system path</li>; <li>api-change:<code>kendra</code>: [<code>botocore</code>] Amazon Kendra now suggests spell corrections for a query. For more information, see <a href=""https://docs.aws.amazon.com/kendra/latest/dg/query-spell-check.html"">https://docs.aws.amazon.com/kendra/latest/dg/query-spell-check.html</a></li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] Launching Amazon AppFlow Marketo as a destination connector SDK.</li>; <li>api-change:<code>timestream-query</code>: [<code>botocore</code>] Documentation only update for SDK and CLI</li>; </ul>; <h1>1.21.11</h1>; <ul>; <li>api-change:<code>gamelift</code>: [<code>botocore</code>] Minor updates to address errors.</li>; <li>api-change:<code>cloudtrail</code>: [<code>botocore</code>] Add ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11504:1792,update,update,1792,https://hail.is,https://github.com/hail-is/hail/pull/11504,2,"['Deploy', 'update']","['Deployment', 'update']"
Deployability,". However, we intend to only run these steps when; the pip version changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8550:1133,deploy,deploy,1133,https://hail.is,https://github.com/hail-is/hail/pull/8550,1,['deploy'],['deploy']
Deployability,". The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). The major new build step is `create_certs` which creates a certificate, key, and; list of trusted ""principals"" for each ""principal"". ""Principal"" is a computer; security term referring to an authenticatable identity. In our system, the; services are each unique principals and every client (e.g. the test_batch CI; step) is also a principal. A principal's certificate is a unforgeable proof of; their identity. A principal's ""key"", in our system, is actually a public-private; (i.e. asymmetric) key pair which the client and server use to establish a; symmetric key for each new connection. A list of trusted principals is a list of; certificates. Every incoming connection must provide a certificate in the; trusted list or the server will drop the connection. Every service depends on the `create_certs` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:1089,deploy,deployment,1089,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['deploy'],['deployment']
Deployability,". using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/lib",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8137,deploy,deployMode,8137,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['deploy'],['deployMode']
Deployability,". using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1339,deploy,deployMode,1339,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['deploy'],['deployMode']
Deployability,".. and I hand-deployed the router and it looks good. Most of our existing services can't handle being located at internal.hail.is/ns/svc, so I'm going to make a series of changes to fix that, possibly folded into my auth changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6928#issuecomment-524334117:14,deploy,deployed,14,https://hail.is,https://github.com/hail-is/hail/pull/6928#issuecomment-524334117,1,['deploy'],['deployed']
Deployability,"... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/googleapis/python-cloud-core/commit/57cb3ca14f058c326b15478cd4eacea291020a3e""><code>57cb3ca</code></a> chore(main): release 2.3.2 (<a href=""https://github-redirect.dependabot.com/googleapis/python-cloud-core/issues/202"">#202</a>)</li>; <li><a href=""https://github.com/googleapis/python-cloud-core/commit/2dd10a7b737efc6b45cd9ff0d13efeaaa6cbdc88""><code>2dd10a7</code></a> chore(deps): update actions/setup-python action to v4 (<a href=""https://github-redirect.dependabot.com/googleapis/python-cloud-core/issues/197"">#197</a>)</li>; <li><a href=""https://github.com/googleapis/python-cloud-core/commit/3277c60102803deaae17f665b3f2f80087dd1e0f""><code>3277c60</code></a> fix: require python 3.7+ (<a href=""https://github-redirect.dependabot.com/googleapis/python-cloud-core/issues/201"">#201</a>)</li>; <li><a href=""https://github.com/googleapis/python-cloud-core/commit/1bc7354261e2fcb8426d269d0083d2d993fafe66""><code>1bc7354</code></a> chore(main): release 2.3.1 (<a href=""https://github-redirect.dependabot.com/googleapis/python-cloud-core/issues/195"">#195</a>)</li>; <li><a href=""https://github.com/googleapis/python-cloud-core/commit/1b79d69cdb156ae21dddb5ab7579f6c08fd701b8""><code>1b79d69</code></a> docs: fix changelog header to consistent size (<a href=""https://github-redirect.dependabot.com/googleapis/python-cloud-core/issues/194"">#194</a>)</li>; <li><a href=""https://github.com/googleapis/python-cloud-core/commit/66d0e8692123021ae76d5d0802c130ba05cce181""><code>66d0e86</code></a> chore(python): auto approve template changes (<a href=""https://github-redirect.dependabot.com/googleapis/python-cloud-core/issues/193"">#193</a>)</li>; <li><a href=""https://github.com/googleapis/python-cloud-core/commit/a41c3a78fd6611a4e0e0044f25b1af36bcf3ca6b""><code>a41c3a7</code></a> chore: [autoapprove] update readme_gen.py to include autoescape True (<a href=""https://github-redirect.dependabot.com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12175:8030,release,release,8030,https://hail.is,https://github.com/hail-is/hail/pull/12175,1,['release'],['release']
Deployability,".........50%............60%.............70%............80%.............90%............100%. Welcome to Gradle 8.3!. Here are the highlights of this release:; - Faster Java compilation; - Reduced memory usage; - Support for running on Java 20. For more details see https://docs.gradle.org/8.3/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; WARNING: Hail primarily tested with Spark 3.3.0, use other versions at your own risk. > Task :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; WARNING: The wheel package is not available.; WARNING: The wheel package is not available.; installing to build/bdist.linux-x86_64/wheel; creating build/bdist.linux-x86_64/wheel/hail-0.2.124.dist-info/WHEEL; creating 'dist/hail-0.2.124-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it; adding 'hail/__init__.py'; adding 'hail/builtin_references.py'; adding 'hail/conftest.py'; adding 'hail/context.py'; adding 'hail/hail_logging.py'; adding 'hail/hail_pip_version'; adding 'hail/hail_revision'; adding 'hail/hail_version'; adding 'hail/matrixtable.py'; adding 'hail/table.py'; adding 'hail/backend/_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:16532,deploy,deploy,16532,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['deploy'],['deploy']
Deployability,"...once KinshipMatrix is in, with RRM going there. The current export to file formats on GRM should be moved to KinshipMatrix too. And then doc on lmmreg should be updated to reflect there are more options than RRM available.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1633:164,update,updated,164,https://hail.is,https://github.com/hail-is/hail/issues/1633,1,['update'],['updated']
Deployability,"..2023-12-29&amp;type=Issues""><code>@​GabrielaVives</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Agithub-actions+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aj264415+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​j264415</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajtpio+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajupyterlab-probot+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​jupyterlab-probot</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Akrassowski+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Alumberbot-app+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​lumberbot-app</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ameeseeksmachine+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​meeseeksmachine</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aparmentelat+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​parmentelat</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Atonyfast+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​tonyfast</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Awelcome+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​welcome</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3AWh1isper+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​Wh1isper</code></a></p>; </blockquote>; </details>; <de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14184:11766,update,updated,11766,https://hail.is,https://github.com/hail-is/hail/pull/14184,1,['update'],['updated']
Deployability,"..2023-12-29&amp;type=Issues""><code>@​gabalafou</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3AGabrielaVives+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​GabrielaVives</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Agithub-actions+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aj264415+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​j264415</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajtpio+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajupyterlab-probot+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​jupyterlab-probot</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Akrassowski+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Alumberbot-app+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​lumberbot-app</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ameeseeksmachine+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​meeseeksmachine</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aparmentelat+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​parmentelat</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Atonyfast+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​tonyfast</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Awelcome+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​welcome</code></a> | <a href=""https://github.com/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14184:11591,update,updated,11591,https://hail.is,https://github.com/hail-is/hail/pull/14184,1,['update'],['updated']
Deployability,"..2023-12-29&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aj264415+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​j264415</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajtpio+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajupyterlab-probot+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​jupyterlab-probot</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Akrassowski+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Alumberbot-app+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​lumberbot-app</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ameeseeksmachine+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​meeseeksmachine</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aparmentelat+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​parmentelat</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Atonyfast+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​tonyfast</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Awelcome+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​welcome</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3AWh1isper+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​Wh1isper</code></a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/jupyterlab/jupyterlab/commit/882dd81a6d5aa31388177ce5b49a4c2b3fc7f69e""><code>882dd81</code></a> [ci skip]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14184:11946,update,updated,11946,https://hail.is,https://github.com/hail-is/hail/pull/14184,1,['update'],['updated']
Deployability,"..</li>; <li><a href=""https://github.com/apache/spark/commit/be891ad99083564a7bf7f421e00b2cc4759a679f""><code>be891ad</code></a> [SPARK-39551][SQL][3.2] Add AQE invalid plan check</li>; <li><a href=""https://github.com/apache/spark/commit/1c0bd4c15a28d7c6a2dca846a5b8d0eb1d152aae""><code>1c0bd4c</code></a> [SPARK-39656][SQL][3.2] Fix wrong namespace in DescribeNamespaceExec</li>; <li><a href=""https://github.com/apache/spark/commit/3d084fe3217bea9af4c544f10ead8a2e5b97dad4""><code>3d084fe</code></a> [SPARK-39677][SQL][DOCS][3.2] Fix args formatting of the regexp and like func...</li>; <li>Additional commits viewable in <a href=""https://github.com/apache/spark/compare/v3.1.3...v3.2.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pyspark&package-manager=pip&previous-version=3.1.3&new-version=3.2.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12452:2515,update,updates,2515,https://hail.is,https://github.com/hail-is/hail/pull/12452,1,['update'],['updates']
Deployability,"..v2.26.0"">2.26.0</a> (2023-08-03)</h2>; <h3>Features</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/java-storage/blob/main/CHANGELOG.md"">com.google.cloud:google-cloud-storage's changelog</a>.</em></p>; <blockquote>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.26.1...v2.27.0"">2.27.0</a> (2023-09-12)</h2>; <h3>Features</h3>; <ul>; <li>Add new JournalingBlobWriteSessionConfig usable with gRPC transport (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2194"">#2194</a>) (<a href=""https://github.com/googleapis/java-storage/commit/8880d94c3d1a737dd4492cf66a16ba5e08633a70"">8880d94</a>)</li>; <li>Follow-up CLI Improvements (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2184"">#2184</a>) (<a href=""https://github.com/googleapis/java-storage/commit/d9859768081ea6f872097851d3e318b5bad384d9"">d985976</a>)</li>; <li>Initial CLI for SSB integration and Workload 1 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2166"">#2166</a>) (<a href=""https://github.com/googleapis/java-storage/commit/a349735e7fe108e623a330afec0c8cd608ebeef9"">a349735</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>A resumable session without a Range header should be interpreted as 0 length (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2182"">#2182</a>) (<a href=""https://github.com/googleapis/java-storage/commit/53022011d83e6a8515a5ba008fc45fc2dae39cea"">5302201</a>)</li>; <li>Update User-Agent handling for resumable uploads (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2168"">#2168</a>) (<a href=""https://github.com/googleapis/java-storage/commit/665b714f421d3c13b557d0ff71460c328c010856"">665b714</a>)</li>; <li>Update version resolution logic to be more resilient (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2169"">#2169</a>) (<a href=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13624:6978,integrat,integration,6978,https://hail.is,https://github.com/hail-is/hail/pull/13624,1,['integrat'],['integration']
Deployability,".0.0.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/readthedocs/sphinx_rtd_theme/blob/master/docs/changelog.rst"">sphinx-rtd-theme's changelog</a>.</em></p>; <blockquote>; <h1>2.0.0</h1>; <h2>Added</h2>; <ul>; <li>Support for Sphinx versions <code>6.x</code> and <code>7.x</code></li>; <li>Support for docutils <code>&lt;=0.20</code></li>; </ul>; <h2>Deprecations</h2>; <ul>; <li>The HTML4 writer is now officially deprecated. An error will be thrown if your; project configuration still uses the HTML4 writer.</li>; <li>Support for Sphinx versions &lt; 5.0 was removed.</li>; <li>In addition, our supported dependencies will match the dependencies from our; lowest supported Sphinx release, version 5.0: Python &gt;= 3.6 and docutils &gt; 0.14 and &lt; 0.19</li>; </ul>; <p>.. _release-1.3.0:</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/7c9b1b5d391f6d7fae72274393eb25d1df96e546""><code>7c9b1b5</code></a> Release 2.0 final (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1544"">#1544</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/c1044107602faf9be43e4358bc4f8b6abff9b420""><code>c104410</code></a> Bump for next potential release, 2.0.0rc5 (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1539"">#1539</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/53ca116ef64123735e5e445258b8b103ad31a26e""><code>53ca116</code></a> Release 2.0.0rc4 (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1538"">#1538</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/4498e97b462688bac2ff3615ac1da1b867b21842""><code>4498e97</code></a> Fix AttributeError when one of <code>css_files</code> is a string (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1537"">#1537</a>)</li>; <li><a href=""htt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14502:1138,Release,Release,1138,https://hail.is,https://github.com/hail-is/hail/pull/14502,1,['Release'],['Release']
Deployability,".0.; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/python/typeshed/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=types-requests&package-manager=pip&previous-version=2.27.30&new-version=2.28.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11967:1698,upgrade,upgrade,1698,https://hail.is,https://github.com/hail-is/hail/pull/11967,3,['upgrade'],['upgrade']
Deployability,".0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-20_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.6.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/da4f3c3f209aea47d69c4faf90",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12601:1097,Release,Release,1097,https://hail.is,https://github.com/hail-is/hail/pull/12601,1,['Release'],['Release']
Deployability,".1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-20_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.6.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.1.html</a></p>; <h2>Elasticsearch Hadoop 8.6.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12623:1097,Release,Release,1097,https://hail.is,https://github.com/hail-is/hail/pull/12623,1,['Release'],['Release']
Deployability,".12"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jupyterlab&package-manager=pip&previous-version=4.0.9&new-version=4.0.12)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14218:16149,upgrade,upgrade,16149,https://hail.is,https://github.com/hail-is/hail/pull/14218,3,['upgrade'],['upgrade']
Deployability,".13.1, update changelog</li>; <li>Additional commits viewable in <a href=""https://github.com/PyCQA/pylint/compare/v2.12.2...v2.13.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pylint&package-manager=pip&previous-version=2.12.2&new-version=2.13.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11702:6177,upgrade,upgrade,6177,https://hail.is,https://github.com/hail-is/hail/pull/11702,3,['upgrade'],['upgrade']
Deployability,".16.18.; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/python/typeshed/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=types-six&package-manager=pip&previous-version=1.16.15&new-version=1.16.18)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12114:1690,upgrade,upgrade,1690,https://hail.is,https://github.com/hail-is/hail/pull/12114,3,['upgrade'],['upgrade']
Deployability,".16.19.; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/python/typeshed/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=types-six&package-manager=pip&previous-version=1.16.15&new-version=1.16.19)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12131:1690,upgrade,upgrade,1690,https://hail.is,https://github.com/hail-is/hail/pull/12131,3,['upgrade'],['upgrade']
Deployability,".18.2...1.18.3</a></p>; <h2>1.18.2</h2>; <h2>What's Changed</h2>; <ul>; <li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/230"">tox-dev/sphinx-autodoc-typehints#230</a></li>; <li>Support and require nptyping 2.1.1 by <a href=""https://github.com/gaborbernat""><code>@​gaborbernat</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/232"">tox-dev/sphinx-autodoc-typehints#232</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2"">https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2</a></p>; <h2>1.18.1</h2>; <p>No release notes provided.</p>; <h2>1.18.0</h2>; <p>No release notes provided.</p>; <h2>1.17.1</h2>; <p>No release notes provided.</p>; <h2>typehints_use_rtype support and handle TypeError</h2>; <p>No release notes provided.</p>; <h2>1.16.0</h2>; <p>No release notes provided.</p>; <h2>1.15.3</h2>; <p>No release notes provided.</p>; <h2>1.15.2</h2>; <p>No release notes provided.</p>; <h2>1.15.1</h2>; <p>No release notes provided.</p>; <h2>1.15.0</h2>; <p>No release notes provided.</p>; <h2>1.14.1</h2>; <p>No release notes provided.</p>; <h2>Added document_defaults config option</h2>; <p>No release notes provided.</p>; <h2>Fix NewType is inserting a reference as first argument</h2>; <p>No release notes provided.</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/blob/main/CHANGELOG.md"">sphinx-autodoc-typehints's changelog</a>.</em></p>; <blockquote>; <h2>1.18.3</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2.1.2</code></li>; </ul>; <h2>1.18.2</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2.1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11909:1814,release,release,1814,https://hail.is,https://github.com/hail-is/hail/pull/11909,1,['release'],['release']
Deployability,".2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jupyter-core&package-manager=pip&previous-version=5.7.1&new-version=5.7.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14484:5810,upgrade,upgrade,5810,https://hail.is,https://github.com/hail-is/hail/pull/14484,3,['upgrade'],['upgrade']
Deployability,".2024-01-19&amp;type=Issues""><code>@​brichet</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Afcollonval+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​fcollonval</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Agithub-actions+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajtpio+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajupyterlab-probot+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​jupyterlab-probot</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Akrassowski+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ameeseeksmachine+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​meeseeksmachine</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Amisterfads+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​misterfads</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Awelcome+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​welcome</code></a></p>; <h2>v4.0.10</h2>; <h2>4.0.10</h2>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab/compare/v4.0.9...b9bc3002b1ab89b9a1c4d2a3007c43275d11e0df"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Backport PR <a href=""https://redirect.github.com/jupyterlab/jupyterlab/issues/15386"">#15386</a>: Improve scrolling to heading <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15565"">#15565</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; <li>Workaround focus leaving input box on consecutive sub",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14184:2673,update,updated,2673,https://hail.is,https://github.com/hail-is/hail/pull/14184,1,['update'],['updated']
Deployability,".22.0 release.</li>; <li><a href=""https://github.com/numpy/numpy/commit/125304b035effcd82e366e601b102e7347eaa9ba""><code>125304b</code></a> wip</li>; <li><a href=""https://github.com/numpy/numpy/commit/c283859128b1a4b57014581570a23ed7950a24ea""><code>c283859</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20682"">#20682</a> from charris/backport-20416</li>; <li><a href=""https://github.com/numpy/numpy/commit/5399c03d4a069fe81a1616be0184c9749d7271ee""><code>5399c03</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20681"">#20681</a> from charris/backport-20954</li>; <li><a href=""https://github.com/numpy/numpy/commit/f9c45f8ebf31340b1a5a0371bfca25afcfc4794e""><code>f9c45f8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20680"">#20680</a> from charris/backport-20663</li>; <li><a href=""https://github.com/numpy/numpy/commit/794b36f7e1bf2a8c42774ab0db86a74bd32f674b""><code>794b36f</code></a> Update armccompiler.py</li>; <li><a href=""https://github.com/numpy/numpy/commit/d93b14e3d7abaa1d837825e51671f817788e120f""><code>d93b14e</code></a> Update test_public_api.py</li>; <li><a href=""https://github.com/numpy/numpy/commit/7662c0789cc6a70d5ad4d950ee2e95f3afef7df6""><code>7662c07</code></a> Update <strong>init</strong>.py</li>; <li><a href=""https://github.com/numpy/numpy/commit/311ab52488a7d096ac3bc4c2de0fdae17ecd13ef""><code>311ab52</code></a> Update armccompiler.py</li>; <li>Additional commits viewable in <a href=""https://github.com/numpy/numpy/compare/v1.21.6...v1.22.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=numpy&package-manager=pip&previous-version=1.21.6&new-version=1.22.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11939:4426,Update,Update,4426,https://hail.is,https://github.com/hail-is/hail/pull/11939,2,['Update'],['Update']
Deployability,".3-patch</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/c9ee3ca8820531cd709bb8f8a58a736813346861""><code>c9ee3ca</code></a> deps: update dependency org.apache.httpcomponents:httpmime to v4.5.14 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1796"">#1796</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/cf900f4139f30f89e3c0784467ddc12cc00cf81c""><code>cf900f4</code></a> deps: update dependency org.apache.httpcomponents:httpclient to v4.5.14 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1795"">#1795</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/099a6165722464b46d37206af274a637d3f0461a""><code>099a616</code></a> test(deps): update cross product test dependencies (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1792"">#1792</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/3184d65cce1368c2f39ff85a6ed02cf536902244""><code>3184d65</code></a> deps: update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.19...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/7d6742115bcea6b848a289fdf5c4e4bbafc4cf18""><code>7d67421</code></a> build(deps): update dependency com.google.cloud:google-cloud-shared-config to...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/3bf403e94c035e6cf936e062a1ced2b5221b3912""><code>3bf403e</code></a> deps: update dependency org.apache.httpcomponents:httpcore to v4.4.16 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1786"">#1786</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/googleapis/java-storage/compare/v1.106.0...v2.16.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.google.cloud:google-cloud-storage&package-manager=gradle&previous-ver",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12545:12039,update,update,12039,https://hail.is,https://github.com/hail-is/hail/pull/12545,1,['update'],['update']
Deployability,".3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-20_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.4.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html</a></p>; <h2>Elasticsearch Hadoop 8.4.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html</a></p>; <h2>Elasticsearch Hadoop 8.4.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12358:1098,Release,Release,1098,https://hail.is,https://github.com/hail-is/hail/pull/12358,1,['Release'],['Release']
Deployability,".3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-30_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.4.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html</a></p>; <h2>Elasticsearch Hadoop 8.4.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html</a></p>; <h2>Elasticsearch Hadoop 8.4.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:1097,Release,Release,1097,https://hail.is,https://github.com/hail-is/hail/pull/12319,1,['Release'],['Release']
Deployability,".3</h2>; <ul>; <li>Follow up the libsass upstream: 3.6.2 --- See the release notes of LibSass <a href=""https://github.com/sass/libsass/releases/tag/3.6.2"">3.6.2</a>. [#302 by Anthony Sottile]</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/sass/libsass-python/blob/master/docs/changes.rst"">libsass's changelog</a>.</em></p>; <blockquote>; <h1>Changelog</h1>; <h2>Version 0.21.1</h2>; <p>Released on May 20, 2021.</p>; <ul>; <li>Fix build on OpenBSD. [:issue:<code>310</code> by Denis Fondras].</li>; <li>Produce abi3 wheels on windows. [:issue:<code>322</code> by Anthony Sottile]</li>; <li>Make the manpage build reproducible. [:issue:<code>319</code> by Chris Lamb]</li>; <li>Follow up the libsass upstream: 3.6.5 --- See the release notes of LibSass; 3.6.5__. [:issue:<code>344</code> by Anthony Sottile]</li>; </ul>; <p>__ <a href=""https://github.com/sass/libsass/releases/tag/3.6.5"">https://github.com/sass/libsass/releases/tag/3.6.5</a></p>; <h2>Version 0.20.1</h2>; <p>Released on August 27, 2020.</p>; <ul>; <li>(no changes, re-releasing to test build automation)</li>; </ul>; <h2>Version 0.20.0</h2>; <p>Released on May 1, 2020.</p>; <ul>; <li>Produce abi3 wheels on macos / linux [:issue:<code>307</code> by Anthony Sottile]</li>; <li>Follow up the libsass upstream: 3.6.4 --- See the release notes of LibSass; 3.6.4__. [:issue:<code>313</code> by Anthony Sottile]</li>; </ul>; <p>__ <a href=""https://github.com/sass/libsass/releases/tag/3.6.4"">https://github.com/sass/libsass/releases/tag/3.6.4</a></p>; <h2>Version 0.19.4</h2>; <p>Released on November 3, 2019.</p>; <ul>; <li>Follow up the libsass upstream: 3.6.3 --- See the release notes of LibSass; 3.6.3__. [:issue:<code>304</code> by Anthony Sottile]</li>; </ul>; <p>__ <a href=""https://github.com/sass/libsass/releases/tag/3.6.3"">https://github.com/sass/libsass/releases/tag/3.6.3</a></p>; <h2>Version 0.19.3</h2>; <!-- raw HTML omitted -->; </blockqu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11508:2297,release,releases,2297,https://hail.is,https://github.com/hail-is/hail/pull/11508,1,['release'],['releases']
Deployability,".4.1.; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/python/typeshed/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=types-chardet&package-manager=pip&previous-version=5.0.4&new-version=5.0.4.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12537:1694,upgrade,upgrade,1694,https://hail.is,https://github.com/hail-is/hail/pull/12537,3,['upgrade'],['upgrade']
Deployability,".4.2 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1998"">#1998</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1999"">#1999</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/d10ae2a9d38eefc9fefb5cdb16f2fec61c5160b6""><code>d10ae2a</code></a> Bump to version 8.4.2</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/ee6bd94c0a54f6226fa30e0681287cc72cf26b83""><code>ee6bd94</code></a> [DOCS] Added RNs for 8.4.1 (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1996"">#1996</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/8327792db7605353c6d4d43e85c8ad7cb31f2e51""><code>8327792</code></a> Bump to version 8.4.1</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/4d2e4b12b83f84521ce36634a3eeb0904137c89b""><code>4d2e4b1</code></a> [DOCS] Add 8.4.0 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1989"">#1989</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1990"">#1990</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/71f288bf1b473d0ed34b9dd4284bf33aa98a0ccf""><code>71f288b</code></a> [DOCS] Add 8.3.3 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1983"">#1983</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1985"">#1985</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/712679f88772fb15184ad7c87dea220a87803f44""><code>712679f</code></a> Upgrade to Gradle 7.5 (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1980"">#1980</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/a4d14077a58ba3272469d48500ce007c725f1c73""><code>a4d1407</code></a> [DOCS] Added 8.3.2 RNs ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:6134,release,release,6134,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['release'],['release']
Deployability,".5.0 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2032"">#2032</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/4d09926f840998a20d4f45380723d2d77b46544a""><code>4d09926</code></a> [DOCS] Add 8.4.3 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2028"">#2028</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/d498aa068ef552aef9de3325e9d105611e5adbba""><code>d498aa0</code></a> Update index.adoc (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2000"">#2000</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/e90ae30e6977b43eea3e08c8a2d35ba2ef43a2e4""><code>e90ae30</code></a> Bump to version 8.6.0</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/b8fec87ab612f9ca25091e9314fb5be7a697d260""><code>b8fec87</code></a> [DOCS] Add 8.4.2 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1998"">#1998</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/069c07b47da2f9bce703968c56308a58e6855895""><code>069c07b</code></a> Ignoring missing indices if es.index.read.missing.as.empty is true (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1997"">#1997</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/2fcae131824c8221231f5f9331c2a5c677376237""><code>2fcae13</code></a> [DOCS] Added RNs for 8.4.1 (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1995"">#1995</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/elastic/elasticsearch-hadoop/compare/v8.4.3...v8.6.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.elasticsearch:elasticsearch-spar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12601:4159,release,release,4159,https://hail.is,https://github.com/hail-is/hail/pull/12601,1,['release'],['release']
Deployability,".6.; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/python/typeshed/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=types-chardet&package-manager=pip&previous-version=5.0.4.5&new-version=5.0.4.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13034:1698,upgrade,upgrade,1698,https://hail.is,https://github.com/hail-is/hail/pull/13034,3,['upgrade'],['upgrade']
Deployability,".7.1.; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/Azure/msrest-for-python/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=msrest&package-manager=pip&previous-version=0.6.21&new-version=0.7.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11965:1694,upgrade,upgrade,1694,https://hail.is,https://github.com/hail-is/hail/pull/11965,3,['upgrade'],['upgrade']
Deployability,".7.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/Azure/azure-sdk-for-java/releases"">azure-identity's releases</a>.</em></p>; <blockquote>; <h2>azure-identity_1.7.1</h2>; <h2>1.7.1 (2022-11-17)</h2>; <h3>Features Added</h3>; <ul>; <li>Added user-agent header to Identity requests</li>; </ul>; <h2>azure-data-appconfiguration_1.3.9</h2>; <h2>1.3.9 (2022-11-09)</h2>; <h3>Other Changes</h3>; <h4>Dependency Updates</h4>; <ul>; <li>Updated <code>azure-core</code> to <code>1.34.0</code>.</li>; <li>Updated <code>azure-core-http-netty</code> to <code>1.12.7</code>.</li>; </ul>; <h2>azure-communication-chat_1.3.3</h2>; <h2>1.3.3 (2022-11-10)</h2>; <h3>Other Changes</h3>; <h4>Dependency Updates</h4>; <ul>; <li>Upgraded <code>azure-communication-common</code> to 1.2.3</li>; <li>Upgraded <code>azure-core</code> to 1.34.0</li>; </ul>; <h2>azure-data-schemaregistry_1.3.1</h2>; <h2>1.3.1 (2022-11-16)</h2>; <h3>Other Changes</h3>; <h4>Dependency Updates</h4>; <ul>; <li>Update <code>azure-core</code> dependency to <code>1.34.0</code>.</li>; <li>Update <code>azure-core-http-netty</code> dependency to <code>1.12.7</code>.</li>; </ul>; <h2>azure-sdk-bom_1.2.8</h2>; <p>No release notes provided.</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/89123194fe6a4a2f2cdc58535abea9d75d753a79""><code>8912319</code></a> Identity 1.7.1 patch (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32140"">#32140</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/fe0a0dad4771fb7c6105cc412cf9cc8d10722e59""><code>fe0a0da</code></a> Updated versions after patch release. (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32231"">#32231</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/4fc24620f120643d18f85aca1bf3ee53daf7f124""><code>4fc2462</code></a> Increment versions",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12508:1075,Update,Updates,1075,https://hail.is,https://github.com/hail-is/hail/pull/12508,2,['Update'],"['Update', 'Updates']"
Deployability,".8; Start Time: Mon, 24 Jun 2019 23:09:04 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=3bf0b121f62d4cfea15cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-cc8d4 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466:1245,pipeline,pipeline,1245,https://hail.is,https://github.com/hail-is/hail/issues/6466,2,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,".8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-vsk7h (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:17214,pipeline,pipeline,17214,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,2,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,".9.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/hagenw/sphinxcontrib-katex/releases"">sphinxcontrib-katex's releases</a>.</em></p>; <blockquote>; <h2>Release v0.9.0</h2>; <ul>; <li>Added: local KaTeX server; to dramatically speed up pre-rendering</li>; <li>Added: <code>katex.min.js</code> and <code>auto-render.min.js</code>; are now included in the Python package</li>; <li>Added: support for Python 3.10</li>; <li>Changed: use KaTeX 0.16.0</li>; <li>Removed: support for Python 3.6</li>; </ul>; <h2>Release v0.8.6</h2>; <ul>; <li>Fixed: allow to work with Sphinx&gt;=4.0.0</li>; </ul>; <h2>Release v0.8.5</h2>; <ul>; <li>Fixed: remove extra space after inline math when using pre-rendering</li>; </ul>; <h2>Release v0.8.4</h2>; <ul>; <li>Changed: increase top padding of equations by 2px</li>; </ul>; <h2>Release v0.8.3</h2>; <ul>; <li>Fixed: building of documentation on RTD</li>; </ul>; <h2>Release v0.8.2</h2>; <ul>; <li>Fixed: PyPI package version number</li>; </ul>; <h2>Release v0.8.0</h2>; <ul>; <li>Added: support for Python 3.9</li>; <li>Added: support for Sphinx&gt;=4.0.0</li>; <li>Added: tests for Windows and macOS</li>; <li>Changed: switch to KaTeX 0.13.11</li>; <li>Changed: switched CI tests from Travis to Github Actions</li>; <li>Changed: running sphinx will now fail in pre-render mode; if KaTeX fails</li>; <li>Removed: support for Python 2.7, 3.4, 3.5</li>; </ul>; <h2>sphinxcontrib-katex 0.7.2</h2>; <ul>; <li>Fixed: Sphinx&gt;=4.0.0 is not supported at the moment</li>; </ul>; <h2>sphinxcontrib-katex 0.7.1</h2>; <ul>; <li>Fixed: label of fraction example in docs</li>; </ul>; <h2>sphinxcontrib-katex 0.7.0</h2>; <ul>; <li>Added: fraction example to docs</li>; <li>Changed: switch to KaTeX 0.12.0</li>; <li>Changed: add small top and bottom padding to equations</li>; </ul>; <h2>sphinxcontrib-katex 0.6.1</h2>; <p><a href=""https://pypi.org/project/sphinxcontrib-katex/0.6.1/"">https://pypi.org/project/sphinxcontrib-katex/0.6",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12241:1120,Release,Release,1120,https://hail.is,https://github.com/hail-is/hail/pull/12241,1,['Release'],['Release']
Deployability,".9.19...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/7d6742115bcea6b848a289fdf5c4e4bbafc4cf18""><code>7d67421</code></a> build(deps): update dependency com.google.cloud:google-cloud-shared-config to...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/3bf403e94c035e6cf936e062a1ced2b5221b3912""><code>3bf403e</code></a> deps: update dependency org.apache.httpcomponents:httpcore to v4.4.16 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1786"">#1786</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/googleapis/java-storage/compare/v1.106.0...v2.16.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.google.cloud:google-cloud-storage&package-manager=gradle&previous-version=1.106.0&new-version=2.16.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12545:13124,update,updates,13124,https://hail.is,https://github.com/hail-is/hail/pull/12545,1,['update'],['updates']
Deployability,".; </code></pre>; <p>Version: 2023.3.23</p>; <pre><code>Git issue 495: Running time for failing fullmatch increases rapidly with input length; Re-enabled modified repeat guards due to regression in speed caused by excessive backtracking.; </code></pre>; <p>Version: 2023.3.22</p>; <pre><code>Git issue 494: Backtracking failure matching regex `^a?(a?)b?c\1$` against string `abca`; Disabled repeat guards. They keep causing issues, and it's just simpler to rely on timeouts.; </code></pre>; <p>Version: 2022.10.31</p>; <pre><code>Updated text for supported Unicode and Python versions.; </code></pre>; <p>Version: 2022.9.13</p>; <pre><code>Updated to Unicode 15.0.0.; </code></pre>; <p>Version: 2022.9.11</p>; <pre><code>Updated version.; </code></pre>; <p>Version: 2022.8.17</p>; <pre><code>Git issue 477: \v for vertical spacing; <p>Added \p{HorizSpace} (\p{H}) and \p{VertSpace} (\p{V}).; </code></pre></p>; <p>Version: 2022.7.25</p>; <pre><code>Git issue 475: 2022.7.24 improperly released; <p>The file <a href=""https://pypi.org/pypi/regex/2022.7.24/json"">https://pypi.org/pypi/regex/2022.7.24/json</a> was missing references to most of the wheels, so this is a new release in the hope that it was just a glitch in GitHub Actions.; </code></pre></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/mrabarnett/mrab-regex/commit/9f03255c920964c206c0509c14ce9690a00b861d""><code>9f03255</code></a> Removed semicolon after 'else' in 'munge_name'.</li>; <li><a href=""https://github.com/mrabarnett/mrab-regex/commit/797b57a4ac8da86c13e52bf60586cd2432864400""><code>797b57a</code></a> Fixed pyproject.toml and setup.py.</li>; <li><a href=""https://github.com/mrabarnett/mrab-regex/commit/16bcce0d56e84367f61c24b369e23e73a3e9ad9e""><code>16bcce0</code></a> Add changelog.txt.</li>; <li><a href=""https://github.com/mrabarnett/mrab-regex/commit/d235c2c17f2335dd7699f3c29a6ae6db6dbe6dab""><code>d235c2c</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12989:1578,release,released,1578,https://hail.is,https://github.com/hail-is/hail/pull/12989,1,['release'],['released']
Deployability,".; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/boto/boto3/blob/develop/CHANGELOG.rst"">boto3's changelog</a>.</em></p>; <blockquote>; <h1>1.21.12</h1>; <ul>; <li>api-change:<code>greengrassv2</code>: [<code>botocore</code>] Doc only update that clarifies Create Deployment section.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for data repository associations to use root (&quot;/&quot;) as the file system path</li>; <li>api-change:<code>kendra</code>: [<code>botocore</code>] Amazon Kendra now suggests spell corrections for a query. For more information, see <a href=""https://docs.aws.amazon.com/kendra/latest/dg/query-spell-check.html"">https://docs.aws.amazon.com/kendra/latest/dg/query-spell-check.html</a></li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] Launching Amazon AppFlow Marketo as a destination connector SDK.</li>; <li>api-change:<code>timestream-query</code>: [<code>botocore</code>] Documentation only update for SDK and CLI</li>; </ul>; <h1>1.21.11</h1>; <ul>; <li>api-change:<code>gamelift</code>: [<code>botocore</code>] Minor updates to address errors.</li>; <li>api-change:<code>cloudtrail</code>: [<code>botocore</code>] Add bytesScanned field into responses of DescribeQuery and GetQueryResults.</li>; <li>api-change:<code>athena</code>: [<code>botocore</code>] This release adds support for S3 Object Ownership by allowing the S3 bucket owner full control canned ACL to be set when Athena writes query results to S3 buckets.</li>; <li>api-change:<code>keyspaces</code>: [<code>botocore</code>] This release adds support for data definition language (DDL) operations</li>; <li>api-change:<code>ecr</code>: [<code>botocore</code>] This release adds support for tracking images lastRecordedPullTime.</li>; </ul>; <h1>1.21.10</h1>; <ul>; <li>api-change:<code>mediapackage</code>: [<code>botocore</code>] This release adds Hybridcast as an available profile option for Das",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11486:1095,update,update,1095,https://hail.is,https://github.com/hail-is/hail/pull/11486,1,['update'],['update']
Deployability,".; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/protocolbuffers/protobuf/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=protobuf&package-manager=pip&previous-version=3.20.1&new-version=4.21.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12082:1702,upgrade,upgrade,1702,https://hail.is,https://github.com/hail-is/hail/pull/12082,6,['upgrade'],['upgrade']
Deployability,".; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/python/typeshed/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=types-setuptools&package-manager=pip&previous-version=57.4.17&new-version=65.3.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12181:1702,upgrade,upgrade,1702,https://hail.is,https://github.com/hail-is/hail/pull/12181,3,['upgrade'],['upgrade']
Deployability,".; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pandas-dev/pandas/releases"">pandas's releases</a>.</em></p>; <blockquote>; <h2>Pandas 1.5.0</h2>; <p>This release includes some new features, bug fixes, and performance improvements. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.0/whatsnew/v1.5.0.html"">full whatsnew</a> for a list of all the changes. pandas 1.5.0 supports Python 3.8 and higher.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <p><code>conda install -c conda-forge pandas</code></p>; <p>Or via PyPI:</p>; <p><code>python3 -m pip install --upgrade pandas</code></p>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.5.0rc0</h2>; <p>We are pleased to announce a release candidate for pandas 1.5.0. If all goes well, we'll release pandas 1.5.0 in about two weeks.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5/whatsnew/v1.5.0.html"">whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on conda-forge and PyPI.</p>; <p>The release can be installed from PyPI</p>; <pre><code>python -m pip install --upgrade --pre pandas==1.5.0rc0; </code></pre>; <p>Or from conda-forge</p>; <pre><code>conda install -c conda-forge/label/pandas_rc pandas==1.5.0rc0; </code></pre>; <p>Please report any issues with the release candidate on the pandas issue tracker.</p>; <h2>Pandas 1.4.4</h2>; <p>This is a patch release in the 1.4.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.4/whatsnew/v1.4.4.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12292:1076,release,release,1076,https://hail.is,https://github.com/hail-is/hail/pull/12292,1,['release'],['release']
Deployability,".</li>; </ul>; </li>; <li>; <p>Add support for native types in macros. :issue:<code>1510</code></p>; </li>; <li>; <p>The <code>{% trans %}</code> tag can use <code>pgettext</code> and <code>npgettext</code> by; passing a context string as the first token in the tag, like; <code>{% trans &quot;title&quot; %}</code>. :issue:<code>1430</code></p>; </li>; <li>; <p>Update valid identifier characters from Python 3.6 to 3.7.; :pr:<code>1571</code></p>; </li>; <li>; <p>Filters and tests decorated with <code>@async_variant</code> are pickleable.; :pr:<code>1612</code></p>; </li>; <li>; <p>Add <code>items</code> filter. :issue:<code>1561</code></p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/jinja/commit/b08cd4bc64bb980df86ed2876978ae5735572280""><code>b08cd4b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1660"">#1660</a> from pallets/release-3.1.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/1e68ba86177504bb6404288610608b855eab93fa""><code>1e68ba8</code></a> release version 3.1.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/8efee35092404ba67ede8316566be4f430e7b61d""><code>8efee35</code></a> pre-commit updates latest release branch</li>; <li><a href=""https://github.com/pallets/jinja/commit/a24df26d54fa2ccbe9bdaa0bb9419075a00e2699""><code>a24df26</code></a> ignore new mypy finding</li>; <li><a href=""https://github.com/pallets/jinja/commit/9faee281ea75694e28c33e2878879b322359d411""><code>9faee28</code></a> update requirements</li>; <li><a href=""https://github.com/pallets/jinja/commit/b802b5a6ad9deea082c16d9adb6417eda1a184d8""><code>b802b5a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1655"">#1655</a> from dvitek/dvitek/issue1654</li>; <li><a href=""https://github.com/pallets/jinja/commit/746bb95780c17687b27b6d1bf4df1216f0d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12173:4781,release,release-,4781,https://hail.is,https://github.com/hail-is/hail/pull/12173,1,['release'],['release-']
Deployability,.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/669:2690,deploy,deploy,2690,https://hail.is,https://github.com/hail-is/hail/issues/669,6,['deploy'],['deploy']
Deployability,".com/Azure/azure-sdk-for-java/issues/31952"">#31952</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/577e3af11b1d68422840f70a33c62a9e97df1cdb""><code>577e3af</code></a> Cosmos spark3.3 support (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31666"">#31666</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/c558cf20a36c119106da9551b52deae687655d7c""><code>c558cf2</code></a> Prepare Core Libraries for November 2022 Release (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31950"">#31950</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/1907997b1d42ebe739aeff9c028c417c5ca0ecfa""><code>1907997</code></a> add sample for cloning vm to new region (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31912"">#31912</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/3f3889e512e9bff0f542739d793e1d197928c5f5""><code>3f3889e</code></a> dpg/mgmt, update codegen to latest (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31933"">#31933</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/24a912a7b9f5cede6bc6007904a98b7a69c4e349""><code>24a912a</code></a> SyncPollingStrategy Compliment to PollingStrategy (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31923"">#31923</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/46759562feb14a16da80295135ce79556639e460""><code>4675956</code></a> target newest version of proxy tool (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31925"">#31925</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/2c6ab741fa96a78c8b6dd607986cdbe645860747""><code>2c6ab74</code></a> updated CHANGELOG.md (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31922"">#31922</a>)</li>; <li><a href=""https://git",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12460:2323,update,update,2323,https://hail.is,https://github.com/hail-is/hail/pull/12460,1,['update'],['update']
Deployability,".com/GrahamDumpleton/wrapt/commit/f2f1a680113d500f525de78da91ae19235efef16""><code>f2f1a68</code></a> Merge branch 'release/1.14.1'</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/97b72d49a8cda771c6006571486530ca84f3a834""><code>97b72d4</code></a> Update version of cibuildwheel for recent Python versions.</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/337072730beddd653f19c8b1a1157ecbb9d62790""><code>3370727</code></a> Only test Python 3.10 on aarch64 linux due to unreliability of GitHub runners...</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/982ddecf52013ce9bbdf8b48b76ae054844ba31b""><code>982ddec</code></a> Python 3.6 no longer available on aarch64 linux for testing.</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/240fea86df0357f3642db040f912031e4ecdfcb1""><code>240fea8</code></a> Update copyright notice year.</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/9668bbd7c7314d81b7cf8ce4293d04212ae1edee""><code>9668bbd</code></a> Update version in preparation for 1.14.1 release.</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/c86a4d37fa61494957153f76b1d6bbdacfd83205""><code>c86a4d3</code></a> Add classifier for Python 3.11.</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/07239ac21a68ced86860cf3bb52ee0c60faf0915""><code>07239ac</code></a> Document fix for module importers using deprecated APIs.</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/df0e62c2740143cceb6cafea4c306dae1c559ef8""><code>df0e62c</code></a> Deal with module importers that don't implement newer API.</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/72627592324bee1a197925d0e600142bc8719a3e""><code>7262759</code></a> Fix change notes formatting.</li>; <li>Additional commits viewable in <a href=""https://github.com/GrahamDumpleton/wrapt/compare/1.13.3...1.14.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12102:3247,Update,Update,3247,https://hail.is,https://github.com/hail-is/hail/pull/12102,1,['Update'],['Update']
Deployability,".com/Textualize/rich) from 12.6.0 to 13.6.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/Textualize/rich/releases"">rich's releases</a>.</em></p>; <blockquote>; <h2>The Python 3.12 release</h2>; <p>Mostly a meta update in readiness for the release of Python3.12</p>; <h2>[13.6.0] - 2023-09-30</h2>; <h3>Added</h3>; <ul>; <li>Added Python 3.12 to classifiers.</li>; </ul>; <h2>Markdown fixes</h2>; <h2>[13.5.3] - 2023-09-17</h2>; <h3>Fixed</h3>; <ul>; <li>Markdown table rendering issue with inline styles and links <a href=""https://redirect.github.com/Textualize/rich/issues/3115"">Textualize/rich#3115</a></li>; <li>Fix Markdown code blocks on a light background <a href=""https://redirect.github.com/Textualize/rich/issues/3123"">Textualize/rich#3123</a></li>; </ul>; <h2>v13.5.2</h2>; <p>Bugfix</p>; <h2>[13.5.2] - 2023-08-01</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs assertion error</li>; </ul>; <h2>v13.5.1</h2>; <p>Very minor update to URL highlighting</p>; <h2>[13.5.1] - 2023-07-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fix tilde character (<code>~</code>) not included in link regex when printing to console <a href=""https://redirect.github.com/Textualize/rich/issues/3057"">Textualize/rich#3057</a></li>; </ul>; <h2>Mostly cake, one or two puppies</h2>; <p><a href=""https://textual.textualize.io/blog/2023/07/29/pull-requests-are-cake-or-puppies/"">https://textual.textualize.io/blog/2023/07/29/pull-requests-are-cake-or-puppies/</a></p>; <h2>[13.5.0] - 2023-07-29</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs not expanding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13758:1018,update,update,1018,https://hail.is,https://github.com/hail-is/hail/pull/13758,2,['update'],['update']
Deployability,".com/ai/nanoid/issues/335"">#335</a>)</li>; <li><a href=""https://github.com/ai/nanoid/commit/90a446fef3ecaac78e5af2ea01025c4f40182e2b""><code>90a446f</code></a> Update benchmark results</li>; <li><a href=""https://github.com/ai/nanoid/commit/8ba2319b579895cc1f9060b9946a44852f97c509""><code>8ba2319</code></a> bench: add <code>@​napi-rs/uuid</code> v4 (<a href=""https://github-redirect.dependabot.com/ai/nanoid/issues/333"">#333</a>)</li>; <li><a href=""https://github.com/ai/nanoid/commit/f4257780ece488734a65c176e80c2fd8ab6aab8e""><code>f425778</code></a> Release 3.1.32 version</li>; <li>Additional commits viewable in <a href=""https://github.com/ai/nanoid/compare/3.1.23...3.2.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=nanoid&package-manager=npm_and_yarn&previous-version=3.1.23&new-version=3.2.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11284:3643,update,updates,3643,https://hail.is,https://github.com/hail-is/hail/pull/11284,2,['update'],['updates']
Deployability,".com/alculquicondor""><code>@​alculquicondor</code></a>)</li>; <li>Kube-apiserver: <code>--audit-log-version</code> and <code>--audit-webhook-version</code> now only support the default value of <code>audit.k8s.io/v1</code>. The v1alpha1 and v1beta1 audit log versions, deprecated since 1.13, have been removed. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108092"">kubernetes/kubernetes#108092</a>, <a href=""https://github.com/carlory""><code>@​carlory</code></a>)</li>; <li>Kube-apiserver: the <code>metadata.selfLink</code> field can no longer be populated by kube-apiserver; it was deprecated in 1.16 and has not been populated by default since 1.20+. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/107527"">kubernetes/kubernetes#107527</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>)</li>; <li>Kubelet external Credential Provider feature is moved to Beta. Credential Provider Plugin and Credential Provider Config API's updated from v1alpha1 to v1beta1 with no API changes. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108847"">kubernetes/kubernetes#108847</a>, <a href=""https://github.com/adisky""><code>@​adisky</code></a>)</li>; <li>Make STS available replicas optional again. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/109241"">kubernetes/kubernetes#109241</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>)</li>; <li>MaxUnavailable for StatefulSets, allows faster RollingUpdate by taking down more than 1 pod at a time. The number of pods you want to take down during a RollingUpdate is configurable using maxUnavailable parameter. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/82162"">kubernetes/kubernetes#82162</a>, <a href=""https://github.com/krmayankk""><code>@​krmayankk</code></a>)</li>; <li>Non-graceful node shutdown handling is enabled for stateful workload failove",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12196:9225,update,updated,9225,https://hail.is,https://github.com/hail-is/hail/pull/12196,1,['update'],['updated']
Deployability,".com/boto/boto3/commit/83a8f662655bada44d442df7f33cb20d71ead257""><code>83a8f66</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/261b0f2ffe079b6940d683657fcad358195f882e""><code>261b0f2</code></a> Merge branch 'release-1.21.12'</li>; <li><a href=""https://github.com/boto/boto3/commit/a972b1bed4caacf0c97f1056cabdfe4b5ccc2681""><code>a972b1b</code></a> Merge branch 'release-1.21.12' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/44f4f5ef0b66d1a508685b62388f1e4a7d60dace""><code>44f4f5e</code></a> Bumping version to 1.21.12</li>; <li><a href=""https://github.com/boto/boto3/commit/bb003d02bd7afefede0ab4678abaea99fe1662ce""><code>bb003d0</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/ad9a92e8fe3d5bc90b2980cdb6839c713b56fbda""><code>ad9a92e</code></a> Merge branch 'release-1.21.11'</li>; <li><a href=""https://github.com/boto/boto3/commit/42b3e0d3c1d02acaa4c0c4127e9523d2c389b675""><code>42b3e0d</code></a> Merge branch 'release-1.21.11' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/bc87db2261257cf3437824d07080fc061847f21c""><code>bc87db2</code></a> Bumping version to 1.21.11</li>; <li>Additional commits viewable in <a href=""https://github.com/boto/boto3/compare/1.17.54...1.21.13"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=boto3&package-manager=pip&previous-version=1.17.54&new-version=1.21.13)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and option",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11504:6957,release,release-,6957,https://hail.is,https://github.com/hail-is/hail/pull/11504,1,['release'],['release-']
Deployability,".com/boto/boto3/commit/bb003d02bd7afefede0ab4678abaea99fe1662ce""><code>bb003d0</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/ad9a92e8fe3d5bc90b2980cdb6839c713b56fbda""><code>ad9a92e</code></a> Merge branch 'release-1.21.11'</li>; <li><a href=""https://github.com/boto/boto3/commit/42b3e0d3c1d02acaa4c0c4127e9523d2c389b675""><code>42b3e0d</code></a> Merge branch 'release-1.21.11' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/bc87db2261257cf3437824d07080fc061847f21c""><code>bc87db2</code></a> Bumping version to 1.21.11</li>; <li><a href=""https://github.com/boto/boto3/commit/b439b9d69375efd28132ddfc4361568eb86c949c""><code>b439b9d</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/a36efbbbac5941ef19d1cfe4cd418a9aa7c927b9""><code>a36efbb</code></a> Merge branch 'release-1.21.10'</li>; <li><a href=""https://github.com/boto/boto3/commit/833a8e625aee254eb061f0b40addf016426245d2""><code>833a8e6</code></a> Merge branch 'release-1.21.10' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/72d60e1e6174f42e260256d4490048506d170bb8""><code>72d60e1</code></a> Bumping version to 1.21.10</li>; <li>Additional commits viewable in <a href=""https://github.com/boto/boto3/compare/1.17.54...1.21.12"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=boto3&package-manager=pip&previous-version=1.17.54&new-version=1.21.12)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and option",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11486:6385,release,release-,6385,https://hail.is,https://github.com/hail-is/hail/pull/11486,1,['release'],['release-']
Deployability,".com/boto/boto3/commit/fb642196bd5dda0f48636e3eeae5f983835fcef5""><code>fb64219</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/cc2984fc4fe2a399404a81711eb9ece3fb8d6eb7""><code>cc2984f</code></a> Merge branch 'release-1.26.15'</li>; <li><a href=""https://github.com/boto/boto3/commit/9280e856b07feef67b8dc08c3663ffd0b65b8f55""><code>9280e85</code></a> Merge branch 'release-1.26.15' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/0cb8b0e0b79f9b73494172bafcf25dba43205a59""><code>0cb8b0e</code></a> Bumping version to 1.26.15</li>; <li><a href=""https://github.com/boto/boto3/commit/b786787f24c9b3f3276c4770038d811064f600ac""><code>b786787</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/96da06d3dc41d4d04503929a7b5894d82c71c08f""><code>96da06d</code></a> Merge branch 'release-1.26.14'</li>; <li><a href=""https://github.com/boto/boto3/commit/61de529b5f9a7bdcc8c76debb472a7f934d048e6""><code>61de529</code></a> Merge branch 'release-1.26.14' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/c92111ed1a7346642060fd7f6dfedcb3770a9650""><code>c92111e</code></a> Bumping version to 1.26.14</li>; <li>Additional commits viewable in <a href=""https://github.com/boto/boto3/compare/1.26.7...1.26.16"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=boto3&package-manager=pip&previous-version=1.26.7&new-version=1.26.16)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options<",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12502:8088,release,release-,8088,https://hail.is,https://github.com/hail-is/hail/pull/12502,1,['release'],['release-']
Deployability,".com/chardet/chardet/issues/244"">#244</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/49b8341f507bed68f7d3ff7138bb97047a0e04f0""><code>49b8341</code></a> Configure setuptools using the declarative syntax in setup.cfg (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/239"">#239</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/5c73bfcdf819251d1a1d0de672e34480ebafbe1f""><code>5c73bfc</code></a> Run all pre-commit hooks on pull requests (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/236"">#236</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/chardet/chardet/compare/4.0.0...5.0.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=chardet&package-manager=pip&previous-version=4.0.0&new-version=5.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12107:5842,update,updates,5842,https://hail.is,https://github.com/hail-is/hail/pull/12107,1,['update'],['updates']
Deployability,".com/googleapis/java-storage/commit/c8bf3c70cca81ed87a52939fe7da58889c8f55ce"">c8bf3c7</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Document differing behavior of {get,list}{,default}Acl between HTTP and gRPC (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1820"">#1820</a>) (<a href=""https://github.com/googleapis/java-storage/commit/9511b173e84d2b28ab1a1625b16e3e648c3856fb"">9511b17</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.1.1 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1836"">#1836</a>) (<a href=""https://github.com/googleapis/java-storage/commit/3b71fab11ac71039c2a9983821ce02ce25ce311d"">3b71fab</a>)</li>; <li>Update dependency net.jqwik:jqwik to v1.7.2 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1833"">#1833</a>) (<a href=""https://github.com/googleapis/java-storage/commit/83bc261130e89e5994f21e32422054ef6ea2fe8e"">83bc261</a>)</li>; <li>Update dependency org.junit.vintage:junit-vintage-engine to v5.9.2 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1837"">#1837</a>) (<a href=""https://github.com/googleapis/java-storage/commit/5b381845b4f48a691aa3f0cb96599ddefc7e463f"">5b38184</a>)</li>; <li>Update junit-platform.version to v5.9.2 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1838"">#1838</a>) (<a href=""https://github.com/googleapis/java-storage/commit/372521ba80b12e52c74fae5ac766dbe6610ff0b2"">372521b</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.15.1...v2.16.0"">2.16.0</a> (2022-12-06)</h2>; <h3>Features</h3>; <ul>; <li>Add {Compose,Rewrite,StartResumableWrite}Request.object_checksums and Bucket.RetentionPolicy.retention_duration (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1790"">#1790</a>) (<a href=""https://github.com/googleapis/java-storage/com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12598:12380,Update,Update,12380,https://hail.is,https://github.com/hail-is/hail/pull/12598,1,['Update'],['Update']
Deployability,".com/googleapis/java-storage/commit/c8bf3c70cca81ed87a52939fe7da58889c8f55ce"">c8bf3c7</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Document differing behavior of {get,list}{,default}Acl between HTTP and gRPC (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1820"">#1820</a>) (<a href=""https://github.com/googleapis/java-storage/commit/9511b173e84d2b28ab1a1625b16e3e648c3856fb"">9511b17</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.1.1 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1836"">#1836</a>) (<a href=""https://github.com/googleapis/java-storage/commit/3b71fab11ac71039c2a9983821ce02ce25ce311d"">3b71fab</a>)</li>; <li>Update dependency net.jqwik:jqwik to v1.7.2 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1833"">#1833</a>) (<a href=""https://github.com/googleapis/java-storage/commit/83bc261130e89e5994f21e32422054ef6ea2fe8e"">83bc261</a>)</li>; <li>Update dependency org.junit.vintage:junit-vintage-engine to v5.9.2 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1837"">#1837</a>) (<a href=""https://github.com/googleapis/java-storage/commit/5b381845b4f48a691aa3f0cb96599ddefc7e463f"">5b38184</a>)</li>; <li>Update junit-platform.version to v5.9.2 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1838"">#1838</a>) (<a href=""https://github.com/googleapis/java-storage/commit/372521ba80b12e52c74fae5ac766dbe6610ff0b2"">372521b</a>)</li>; </ul>; <h2>v2.16.0</h2>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.15.1...v2.16.0"">2.16.0</a> (2022-12-06)</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/java-storage/blob/main/CHANGELOG.md"">google-cloud-storage's changelog</a>.</em></p>; <blockquote>;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12598:5870,Update,Update,5870,https://hail.is,https://github.com/hail-is/hail/pull/12598,1,['Update'],['Update']
Deployability,".com/googleapis/python-storage/issues/657"">#657</a>)</li>; </ul>; <h3>Features</h3>; <ul>; <li>Remove Python 2 support (<a href=""https://github-redirect.dependabot.com/googleapis/python-storage/issues/657"">#657</a>) (<a href=""https://github.com/googleapis/python-storage/commit/b6116700a4a32d28404c39018138e545f3f7910e"">b611670</a>)</li>; </ul>; <h2><a href=""https://www.github.com/googleapis/python-storage/compare/v1.43.0...v1.44.0"">1.44.0</a> (2022-01-05)</h2>; <h3>Features</h3>; <ul>; <li>add raw_download kwarg to BlobReader (<a href=""https://github-redirect.dependabot.com/googleapis/python-storage/issues/668"">#668</a>) (<a href=""https://www.github.com/googleapis/python-storage/commit/10cdad630739a324ae0b16a3d14a67ca4c8a23c2"">10cdad6</a>)</li>; </ul>; <h3>Documentation</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/googleapis/python-storage/commit/f80f69516b0eaa6ebef6a28d1fd12c9d78f362ce""><code>f80f695</code></a> chore(main): release 2.2.0 (<a href=""https://github-redirect.dependabot.com/googleapis/python-storage/issues/705"">#705</a>)</li>; <li><a href=""https://github.com/googleapis/python-storage/commit/eae1df0f4526eefb21f14eb3f5a319b9395b90c7""><code>eae1df0</code></a> chore(deps): update dependency pytest to v7.1.0 (<a href=""https://github-redirect.dependabot.com/googleapis/python-storage/issues/732"">#732</a>)</li>; <li><a href=""https://github.com/googleapis/python-storage/commit/5d1cfd2050321481a3bc4acbe80537ea666506fa""><code>5d1cfd2</code></a> fix: Fix BlobReader handling of interleaved reads and seeks (<a href=""https://github-redirect.dependabot.com/googleapis/python-storage/issues/721"">#721</a>)</li>; <li><a href=""https://github.com/googleapis/python-storage/commit/e0b3b354d51e4be7c563d7f2f628a7139df842c0""><code>e0b3b35</code></a> fix: retry client side requests timeout (<a href=""https://github-redirect.dependabot.com/googleapis/python-storage/issue",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11578:6318,release,release,6318,https://hail.is,https://github.com/hail-is/hail/pull/11578,1,['release'],['release']
Deployability,".com/ipython/comm/pull/26"">#26</a> (<a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/ipython/comm/graphs/contributors?from=2024-01-02&amp;to=2024-03-12&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Aipython%2Fcomm+involves%3Ablink1073+updated%3A2024-01-02..2024-03-12&amp;type=Issues""><code>@​blink1073</code></a> | <a href=""https://github.com/search?q=repo%3Aipython%2Fcomm+involves%3Apre-commit-ci+updated%3A2024-01-02..2024-03-12&amp;type=Issues""><code>@​pre-commit-ci</code></a></p>; <!-- raw HTML omitted -->; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ipython/comm/commit/d119118d950f2c64f184c37e7e42b4c968701668""><code>d119118</code></a> Publish 0.2.2</li>; <li><a href=""https://github.com/ipython/comm/commit/76149e7ee0f331772c964ae86cdb8bafebe6dfa2""><code>76149e7</code></a> Update Release Scripts (<a href=""https://redirect.github.com/ipython/comm/issues/27"">#27</a>)</li>; <li><a href=""https://github.com/ipython/comm/commit/915898ddeddd0d1c8a1b87c5dcfbe6392fd225b7""><code>915898d</code></a> chore: update pre-commit hooks (<a href=""https://redirect.github.com/ipython/comm/issues/26"">#26</a>)</li>; <li>See full diff in <a href=""https://github.com/ipython/comm/compare/v0.2.1...v0.2.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=comm&package-manager=pip&previous-version=0.2.1&new-version=0.2.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-sta",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14492:3078,Update,Update,3078,https://hail.is,https://github.com/hail-is/hail/pull/14492,2,"['Release', 'Update']","['Release', 'Update']"
Deployability,".com/protocolbuffers/protobuf) from 3.20.1 to 4.21.6.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/protocolbuffers/protobuf/releases"">protobuf's releases</a>.</em></p>; <blockquote>; <h2>Protocol Buffers v3.20.2</h2>; <h1>C++</h1>; <ul>; <li>Reduce memory consumption of MessageSet parsing</li>; <li>This release addresses a <a href=""https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-8gq9-2x98-w8hf"">Security Advisory for C++ and Python users</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/protocolbuffers/protobuf/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=protobuf&package-manager=pip&previous-version=3.20.1&new-version=4.21.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12227:1050,update,updates,1050,https://hail.is,https://github.com/hail-is/hail/pull/12227,2,['update'],['updates']
Deployability,".com/pytest-dev/pytest-asyncio/issues/737"">#737</a></li>; <li>Fix typing errors with recent versions of mypy <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/769"">#769</a></li>; </ul>; <h2>Known issues</h2>; <p>As of v0.23, pytest-asyncio attaches an asyncio event loop to each item of the test suite (i.e. session, packages, modules, classes, functions) and allows tests to be run in those loops when marked accordingly. Pytest-asyncio currently assumes that async fixture scope is correlated with the new event loop scope. This prevents fixtures from being evaluated independently from the event loop scope and breaks some existing test suites (see <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/706"">#706</a>). For example, a test suite may require all fixtures and tests to run in the same event loop, but have async fixtures that are set up and torn down for each module. If you're affected by this issue, please continue using the v0.21 release, until it is resolved.</p>; <h2>pytest-asyncio 0.23.4</h2>; <h1>0.23.4 (2024-01-28)</h1>; <ul>; <li>pytest-asyncio no longer imports additional, unrelated packages during test collection <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/729"">#729</a></li>; <li>Addresses further issues that caused an internal pytest error during test collection</li>; <li>Declares incompatibility with pytest 8 <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/737"">#737</a></li>; </ul>; <h2>pytest-asyncio 0.23.4a2</h2>; <h1>0.23.4 (UNRELEASED)</h1>; <ul>; <li>pytest-asyncio no longer imports additional, unrelated packages during test collection <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/729"">#729</a></li>; <li>Addresses further issues that caused an internal pytest error during test collection</li>; </ul>; <h2>Known issues</h2>; <p>As of v0.23, pytest-asyncio attaches an asyncio event loop to each item of the test suite (i.e. session, packages, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14507:5170,release,release,5170,https://hail.is,https://github.com/hail-is/hail/pull/14507,1,['release'],['release']
Deployability,".com/python-pillow/Pillow/issues/7483"">#7483</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>ImageMath: Inline <code>isinstance</code> check <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7623"">#7623</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>; <li>Update actions/upload-artifact action to v4 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7619"">#7619</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Import plugins relative to the module <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7576"">#7576</a> [<a href=""https://github.com/deliangyang""><code>@​deliangyang</code></a>]</li>; <li>Translate encoder error codes to strings; deprecate <code>ImageFile.raise_oserror()</code> <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7609"">#7609</a> [<a href=""https://github.com/bgilbert""><code>@​bgilbert</code></a>]</li>; <li>Updated readthedocs to latest version of Python <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7611"">#7611</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Support reading BC4U and DX10 BC1 images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/6486"">#6486</a> [<a href=""https://github.com/REDxEYE""><code>@​REDxEYE</code></a>]</li>; <li>Optimize ImageStat.Stat.extrema <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7593"">#7593</a> [<a href=""https://github.com/florath""><code>@​florath</code></a>]</li>; <li>Handle pathlib.Path in FreeTypeFont <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7578"">#7578</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use list comprehensions to create transformed lists <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7597"">#7597</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>; <li>Added suppo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:6801,Update,Updated,6801,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['Update'],['Updated']
Deployability,".com/thibaudcolas/curlylint/pull/77"">#77</a>).</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Add more descriptive error message for missing whitespace between HTML attributes (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/23#issuecomment-700622837"">#23 (comment)</a>, <a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/68"">#68</a>).</li>; <li>Move development dependencies from extras to separate <code>requirements.txt</code> (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/68"">#68</a>).</li>; <li>Declare support for Python 3.9.</li>; <li>Tentatively declare support for Python 3.10 (tested with <code>Python 3.10.0a6+</code>).</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix Python 3.10 deprecation warning by importing Iterable from collections.abc (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/68"">#68</a>).</li>; </ul>; <h2>v0.12.2</h2>; <h2><a href=""https://github.com/thibaudcolas/curlylint/releases/tag/v0.12.2"">v0.12.2</a> 2021-03-06</h2>; <h3>Fixed</h3>; <ul>; <li>The <code>image_alt</code> rule no longer crashes when encountering template conditionals in img attributes (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/57"">#57</a>). Thanks to <a href=""https://github.com/adrien-delhorme""><code>@​adrien-delhorme</code></a>.</li>; </ul>; <h2>v0.12.1</h2>; <h2><a href=""https://github.com/thibaudcolas/curlylint/releases/tag/v0.12.1"">v0.12.1</a> 2021-03-06</h2>; <h3>Fixed</h3>; <ul>; <li>The project’s sdist now includes all needed files to run the test suite (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/49"">#49</a>, <a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/50"">#50</a>). Thanks to <a href=""https://github.com/jayvdb""><code>@​jayvdb</code></a>.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/thibaudco",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11494:1775,release,releases,1775,https://hail.is,https://github.com/hail-is/hail/pull/11494,3,['release'],['releases']
Deployability,".dependabot.com/pandas-dev/pandas/issues/50"">#50</a>...</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/54b40379e3fe6825f676cf02767ee81adb6ffeb5""><code>54b4037</code></a> Backport PR on Branch 1.5.x (REV: revert deprecation of Series.<strong>getitem</strong> sl...</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/71db310a328a0dfa194ef0fe2b95238817b4f419""><code>71db310</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/50396"">#50396</a> on branch 1.5.x (BUG/COMPAT: fix assert_* functions for ne...</li>; <li>Additional commits viewable in <a href=""https://github.com/pandas-dev/pandas/compare/v1.3.5...v1.5.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pandas&package-manager=pip&previous-version=1.3.5&new-version=1.5.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). You can trigger a rebase of this PR by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Depe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12610:5700,update,updates,5700,https://hail.is,https://github.com/hail-is/hail/pull/12610,1,['update'],['updates']
Deployability,".dependabot.com/pre-commit/pre-commit/issues/2293"">#2293</a> PR by <a href=""https://github.com/Holzhaus""><code>@​Holzhaus</code></a>.</li>; </ul>; </li>; <li>Include more information in errors for <code>language_version</code> /; <code>additional_dependencies</code> for languages which do not support them.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2315"">#2315</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; <li>Have autoupdate preferentially pick tags which look like versions when; there are multiple equivalent tags.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2312"">#2312</a> PR by <a href=""https://github.com/mblayman""><code>@​mblayman</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2311"">#2311</a> issue by <a href=""https://github.com/mblayman""><code>@​mblayman</code></a>.</li>; </ul>; </li>; <li>Upgrade <code>ruby-build</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2319"">#2319</a> PR by <a href=""https://github.com/jalessio""><code>@​jalessio</code></a>.</li>; </ul>; </li>; <li>Add top level <code>default_install_hook_types</code> which will be installed when; <code>--hook-types</code> is not specified in <code>pre-commit install</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2322"">#2322</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; </ul>; <h3>Fixes</h3>; <ul>; <li>Fix typo in help message for <code>--from-ref</code> and <code>--to-ref</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2266"">#2266</a> PR by <a href=""https://github.com/leetrout""><code>@​leetrout</code></a>.</li>; </ul>; </li>; <li>Prioritize binary builds for R dependencies.; <ul>; <li><a href=""https://github-redirect.dependa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11731:7434,Upgrade,Upgrade,7434,https://hail.is,https://github.com/hail-is/hail/pull/11731,1,['Upgrade'],['Upgrade']
Deployability,.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:7690,Configurat,Configuration,7690,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Configurat'],['Configuration']
Deployability,.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:284); at org.broadinstitute.hail.expr.Type.toJSON(Type.scala:135); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:32); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:8); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:91); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.utils.package$.time(package.scala:119); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:114); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:108); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:108); at org.broadinstitute.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1204:3626,deploy,deploy,3626,https://hail.is,https://github.com/hail-is/hail/issues/1204,6,['deploy'],['deploy']
Deployability,.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqO,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:9081,deploy,deploy,9081,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['deploy'],['deploy']
Deployability,.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed. /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:477: FatalError; ------------------------------ Captured log call -------------------------------; INFO batch_client.aioclient:aioclient.py:753 created batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 1/4; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 2/4; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 3/4; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO backend.service_backend:java.py:190 krylov_fa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:23581,update,updated,23581,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['update'],['updated']
Deployability,".java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-414f3f183bd5; Error summary: RuntimeException: Class file too large!; ```; Code was:; ```; cutoff = 10. agg_expr = {; 'downsampling': hl.agg.collect(ht.downsamplings)[0]; }; locations = list(zip(('syn', 'mis', 'lof'), ('', '', '_classic_hc'))); agg_expr.update({; f'median_expected_{var}_{pop}': [hl.median(hl.agg.collect(ht[f'exp_{var}_{pop}{var_loc}'][i])) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'median_observed_{var}_{pop}': [hl.median(hl.agg.collect(ht[f'obs_{var}_{pop}{var_loc}'][i])) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'mean_expected_{var}_{pop}': [hl.agg.mean(ht[f'exp_{var}_{pop}{var_loc}'][i]) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'mean_observed_{var}_{pop}': [hl.agg.mean(ht[f'obs_{var}_{pop}{var_loc}'][i]) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'fraction_expected_{var}_{pop}': [hl.agg.fraction(ht[f'exp_{var}_{pop}{var_loc}'][i] > cutoff) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'fraction_observed_{var}_{pop}': [hl.agg.fraction(ht[f'obs_{var}_{pop}{var_loc}'][i] > cutoff) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'fraction_missing_expected_{var}_{pop}': [1 - hl.agg.fraction(ht[f'exp_{var}_{pop}{var_loc}'][i] > cutoff) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4516:2478,update,update,2478,https://hail.is,https://github.com/hail-is/hail/issues/4516,1,['update'],['update']
Deployability,".len(x)); .map(lambda i2: hl.tuple([x[i1], x[i2]])))); ); ). mt.describe(). ht = mt.annotate_rows(; variant_pairs=hl.agg.take(mt.variant_pairs_entry, 1)[0]; ).rows(). ht = ht.explode('variant_pairs'); ht = ht.key_by(v1_idx=ht.variant_pairs[0], v2_idx=ht.variant_pairs[1]); return ht.select(). ht = variant_pairs_ht(mt, ['gene']; ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'v1_idx': int32 ; 'v2_idx': int32 ; ----------------------------------------; Key: ['v1_idx', 'v2_idx']; ----------------------------------------; ht.show(). ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-53-73b5a6c78295>"", line 1, in <module>; ht.show(); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1169, in show; print(self._show(n,width, truncate, types)); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1172, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/laurent/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:78); at is.hail.ex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3744:1833,release,release,1833,https://hail.is,https://github.com/hail-is/hail/issues/3744,1,['release'],['release']
Deployability,".org/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v4.4.0</h2>; <p>Changelog: <a href=""https://www.sphinx-doc.org/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v4.3.1</h2>; <p>No release notes provided.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/sphinx-doc/sphinx/blob/5.x/CHANGES"">sphinx's changelog</a>.</em></p>; <blockquote>; <h1>Release 5.1.1 (released Jul 26, 2022)</h1>; <h2>Bugs fixed</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10701"">#10701</a>: Fix ValueError in the new <code>deque</code> based <code>sphinx.ext.napolean</code>; iterator implementation.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10702"">#10702</a>: Restore compatability with third-party builders.</li>; </ul>; <h1>Release 5.1.0 (released Jul 24, 2022)</h1>; <h2>Dependencies</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10656"">#10656</a>: Support <code>Docutils 0.19</code>_. Patch by Adam Turner.</li>; </ul>; <p>.. _Docutils 0.19: <a href=""https://docutils.sourceforge.io/RELEASE-NOTES.html#release-0-19-2022-07-05"">https://docutils.sourceforge.io/RELEASE-NOTES.html#release-0-19-2022-07-05</a></p>; <h2>Deprecated</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10467"">#10467</a>: Deprecated <code>sphinx.util.stemmer</code> in favour of <code>snowballstemmer</code>.; Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9856"">#9856</a>: Deprecated <code>sphinx.ext.napoleon.iterators</code>.</li>; </ul>; <h2>Features added</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10444"">#10444</a>: html theme: Allow specifying multiple CSS files through the <code>stylesheet</code>; setting in <code",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12165:2063,release,released,2063,https://hail.is,https://github.com/hail-is/hail/pull/12165,1,['release'],['released']
Deployability,".render()</code> method to generate the output HTML (PR submitted by Aussie Schnore,; good catch!)</p>; </li>; <li>; <p>Fixed bug in <code>python_quoted_string</code> regex.</p>; </li>; <li>; <p>Added <code>examples/bf.py</code> Brainf*ck parser/executor example. Illustrates using; a pyparsing grammar to parse language syntax, and attach executable AST nodes to; the parsed results.</p>; </li>; </ul>; <h2>Version 3.1.0b1 - April, 2023</h2>; <ul>; <li>; <p>Added support for Python 3.12.</p>; </li>; <li>; <p>API CHANGE: A slight change has been implemented when unquoting a quoted string; parsed using the <code>QuotedString</code> class. Formerly, when unquoting and processing; whitespace markers such as \t and \n, these substitutions would occur first, and</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/c09eb6e4bb283b375e53cfe851bb6a63ed3957bb""><code>c09eb6e</code></a> Minor update to HowToUsePyparsing.rst</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/395431ab3db9cfe59ddcb22b431caea74e623a1e""><code>395431a</code></a> Prep for release</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/54b39a52dce9a0e438b11c76bc6e1dddaf490333""><code>54b39a5</code></a> Fix regression in SkipTo when ignoring an ignoreExpr, and failed to also igno...</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/411c8ab6f697cb7b96f7fd1af37fefe9e3ce422b""><code>411c8ab</code></a> Handle case where Word(min &gt; 1) with differing init and body chars</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/1936b36823ae162698dff5184061d71e5d0bd39b""><code>1936b36</code></a> Handle case where Word(min &gt; 1) (fixes <a href=""https://redirect.github.com/pyparsing/pyparsing/issues/502"">#502</a>)</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/421e0fcdbc76fcfb43de9c97c89872bc485d8d40""><code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13345:7208,update,update,7208,https://hail.is,https://github.com/hail-is/hail/pull/13345,1,['update'],['update']
Deployability,.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed. /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:477: FatalError; ------------------------------ Captured log call -------------------------------; INFO batch_client.aioclient:aioclient.py:753 created batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 1/4; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 2/4; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 3/4; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 4/4; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO backend.service_backend:java.py:190 krylov_factorization: Iterations complete. Computing loca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:23785,update,updated,23785,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['update'],['updated']
Deployability,".serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185); at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32); at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: GC overhead limit exceeded; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:15246,install,install,15246,https://hail.is,https://github.com/hail-is/hail/issues/4780,3,['install'],['install']
Deployability,".v1.11.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=scipy&package-manager=pip&previous-version=1.9.3&new-version=1.11.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13461:5125,upgrade,upgrade,5125,https://hail.is,https://github.com/hail-is/hail/pull/13461,3,['upgrade'],['upgrade']
Deployability,".v13.5.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=rich&package-manager=pip&previous-version=12.6.0&new-version=13.5.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13575:8334,upgrade,upgrade,8334,https://hail.is,https://github.com/hail-is/hail/pull/13575,3,['upgrade'],['upgrade']
Deployability,".v13.5.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=rich&package-manager=pip&previous-version=12.6.0&new-version=13.5.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13651:8472,upgrade,upgrade,8472,https://hail.is,https://github.com/hail-is/hail/pull/13651,6,['upgrade'],['upgrade']
Deployability,".v3.18.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=zipp&package-manager=pip&previous-version=3.17.0&new-version=3.18.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14473:4411,upgrade,upgrade,4411,https://hail.is,https://github.com/hail-is/hail/pull/14473,3,['upgrade'],['upgrade']
Deployability,".v3.9.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=aiohttp&package-manager=pip&previous-version=3.9.1&new-version=3.9.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14219:7550,upgrade,upgrade,7550,https://hail.is,https://github.com/hail-is/hail/pull/14219,6,['upgrade'],['upgrade']
Deployability,".zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API:; >; > https://hail.is/pyhail/getting_started.html; >; > Please give it a spin and let us know if you run into any problems. The; > documentation for the python API is nearly complete, but the Tutorial and; > General Reference section are still being ported to python and will need; > another week or so. Thanks for your patience!; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, v",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:1419,install,install,1419,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799,1,['install'],['install']
Deployability,"//github-redirect.dependabot.com/bartdag/py4j/issues/487"">#487</a>)</li>; <li><a href=""https://github.com/py4j/py4j/commit/1c622faa81e983f5ceface5290859d6a49974849""><code>1c622fa</code></a> Migrate nosetest to pytest (<a href=""https://github-redirect.dependabot.com/bartdag/py4j/issues/481"">#481</a>)</li>; <li><a href=""https://github.com/py4j/py4j/commit/64ba89c5a680218d682161a4a6d952a969d1299b""><code>64ba89c</code></a> Add explanations for releasing Py4J for eclipse. Convert .txt to .md (<a href=""https://github-redirect.dependabot.com/bartdag/py4j/issues/479"">#479</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/bartdag/py4j/compare/0.10.9...0.10.9.7"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=py4j&package-manager=pip&previous-version=0.10.9&new-version=0.10.9.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12298:2800,update,updates,2800,https://hail.is,https://github.com/hail-is/hail/pull/12298,1,['update'],['updates']
Deployability,"//github-redirect.dependabot.com/brettcannon/gidgethub/issues/180"">#180</a>)</li>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/cf2cb85551a8aa36536dc828e830e13032e594d4""><code>cf2cb85</code></a> Bump min PyJWT v2.4.0 (<a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/179"">#179</a>)</li>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/9096d1b79447a3ef81b331457ea39c43f43e2f2d""><code>9096d1b</code></a> Release v5.1.0 (<a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/175"">#175</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/brettcannon/gidgethub/compare/v4.2.0...v5.2.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=gidgethub&package-manager=pip&previous-version=4.2.0&new-version=5.2.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12328:9686,update,updates,9686,https://hail.is,https://github.com/hail-is/hail/pull/12328,1,['update'],['updates']
Deployability,"//github-redirect.dependabot.com/googleapis/python-storage/issues/679"">#679</a>)</li>; <li><a href=""https://github.com/googleapis/python-storage/commit/71a453531603b442e26f1c78ab519cc0248e16c8""><code>71a4535</code></a> samples: add async upload sample (<a href=""https://github-redirect.dependabot.com/googleapis/python-storage/issues/665"">#665</a>)</li>; <li><a href=""https://github.com/googleapis/python-storage/commit/4dafc815470480ce9de7f0357e331d3fbd0ae9b7""><code>4dafc81</code></a> feat: add turbo replication support and samples (<a href=""https://github-redirect.dependabot.com/googleapis/python-storage/issues/622"">#622</a>)</li>; <li><a href=""https://github.com/googleapis/python-storage/commit/8aa4130ee068a1922161c8ca54a53a4a51d65ce0""><code>8aa4130</code></a> feat: remove python 3.6 support (<a href=""https://github-redirect.dependabot.com/googleapis/python-storage/issues/689"">#689</a>)</li>; <li><a href=""https://github.com/googleapis/python-storage/commit/fe1855a5f23dd49f5d2f830d9ae39b8f2f0a4aaf""><code>fe1855a</code></a> chore(python): update release.sh to use keystore (<a href=""https://github-redirect.dependabot.com/googleapis/python-storage/issues/692"">#692</a>)</li>; <li><a href=""https://github.com/googleapis/python-storage/commit/b53feaaaf0af92ea9e1d16abd317f798f2b6e17a""><code>b53feaa</code></a> build: switch to release-please for tagging (<a href=""https://github-redirect.dependabot.com/googleapis/python-storage/issues/691"">#691</a>)</li>; <li><a href=""https://github.com/googleapis/python-storage/commit/7f771077511d6b446686724f48514d5b903ec036""><code>7f77107</code></a> chore(deps): update dependency google-cloud-storage to v2 (<a href=""https://github-redirect.dependabot.com/googleapis/python-storage/issues/690"">#690</a>)</li>; <li><a href=""https://github.com/googleapis/python-storage/commit/7891dcc6c0bd90da192691025c910cf30c98407b""><code>7891dcc</code></a> chore(main): release 2.0.0 (<a href=""https://github-redirect.dependabot.com/googleapis/python-storage/issue",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11520:7434,update,update,7434,https://hail.is,https://github.com/hail-is/hail/pull/11520,2,"['release', 'update']","['release', 'update']"
Deployability,"//github-redirect.dependabot.com/numpy/numpy/issues/20682"">#20682</a> from charris/backport-20416</li>; <li><a href=""https://github.com/numpy/numpy/commit/5399c03d4a069fe81a1616be0184c9749d7271ee""><code>5399c03</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20681"">#20681</a> from charris/backport-20954</li>; <li><a href=""https://github.com/numpy/numpy/commit/f9c45f8ebf31340b1a5a0371bfca25afcfc4794e""><code>f9c45f8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20680"">#20680</a> from charris/backport-20663</li>; <li><a href=""https://github.com/numpy/numpy/commit/794b36f7e1bf2a8c42774ab0db86a74bd32f674b""><code>794b36f</code></a> Update armccompiler.py</li>; <li><a href=""https://github.com/numpy/numpy/commit/d93b14e3d7abaa1d837825e51671f817788e120f""><code>d93b14e</code></a> Update test_public_api.py</li>; <li><a href=""https://github.com/numpy/numpy/commit/7662c0789cc6a70d5ad4d950ee2e95f3afef7df6""><code>7662c07</code></a> Update <strong>init</strong>.py</li>; <li><a href=""https://github.com/numpy/numpy/commit/311ab52488a7d096ac3bc4c2de0fdae17ecd13ef""><code>311ab52</code></a> Update armccompiler.py</li>; <li>Additional commits viewable in <a href=""https://github.com/numpy/numpy/compare/v1.21.6...v1.22.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=numpy&package-manager=pip&previous-version=1.21.6&new-version=1.22.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br /",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11939:4723,Update,Update,4723,https://hail.is,https://github.com/hail-is/hail/pull/11939,2,['Update'],['Update']
Deployability,"//github-redirect.dependabot.com/prometheus/client_python/issues/754"">#754</a>)</li>; <li><a href=""https://github.com/prometheus/client_python/commit/b3271a3f1842dbbddeab822063a3f08911f3c190""><code>b3271a3</code></a> Add missing functions/classes to <strong>all</strong> (<a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/757"">#757</a>)</li>; <li><a href=""https://github.com/prometheus/client_python/commit/304745f63c9217656a7cf6d13e6edb4e06a07d32""><code>304745f</code></a> Use Iterable[Sample] for all sample collection</li>; <li><a href=""https://github.com/prometheus/client_python/commit/cd4cf2eebba87c2db4c608ad3db075656ba54f0e""><code>cd4cf2e</code></a> Add mypy linting to the CI pipeline</li>; <li><a href=""https://github.com/prometheus/client_python/commit/4b7811a393fad0738b6c0494164027734ab4daa7""><code>4b7811a</code></a> Allow labelvalues and labelkwargs to be Any type</li>; <li><a href=""https://github.com/prometheus/client_python/commit/69e915cdfbb81569179f2a2b1fbccfb9e148ad57""><code>69e915c</code></a> Release 0.13.0</li>; <li><a href=""https://github.com/prometheus/client_python/commit/da15e4a4d671b8aea0e60fc859d5df8102be3897""><code>da15e4a</code></a> Change to imports to fix go-to-declaration in editors (<a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/747"">#747</a>)</li>; <li><a href=""https://github.com/prometheus/client_python/commit/3ef865e1cccae66f63ae764762a700c5775a5190""><code>3ef865e</code></a> Allow to add labels inside a context manager (<a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/730"">#730</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/prometheus/client_python/compare/v0.11.0...v0.13.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=prometheus-client&package-manager=pip&previous-version=0.11.0&new-version=0.13.1)](ht",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11515:4616,Release,Release,4616,https://hail.is,https://github.com/hail-is/hail/pull/11515,1,['Release'],['Release']
Deployability,"//github.com/PyCQA/pylint) from 2.12.2 to 2.13.3.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/PyCQA/pylint/blob/main/ChangeLog"">pylint's changelog</a>.</em></p>; <blockquote>; <h1>What's New in Pylint 2.13.3?</h1>; <p>Release date: 2022-03-29</p>; <ul>; <li>; <p>Fix false positive for <code>unnecessary-ellipsis</code> when using an ellipsis as a default argument.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5973"">#5973</a></p>; </li>; <li>; <p>Fix crash involving unbalanced tuple unpacking.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5998"">#5998</a></p>; </li>; <li>; <p>Fix false positive for 'nonexistent-operator' when repeated '-' are; separated (e.g. by parens).</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5769"">#5769</a></p>; </li>; </ul>; <h1>What's New in Pylint 2.13.2?</h1>; <p>Release date: 2022-03-27</p>; <ul>; <li>; <p>Fix crash when subclassing a <code>namedtuple</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5982"">#5982</a></p>; </li>; <li>; <p>Fix false positive for <code>superfluous-parens</code> for patterns like; &quot;return (a or b) in iterable&quot;.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5803"">#5803</a></p>; </li>; <li>; <p>Fix a false negative regression in 2.13.0 where <code>protected-access</code> was not; raised on functions.</p>; <p>Fixes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5989"">#5989</a></p>; </li>; <li>; <p>Better error messages in case of crash if pylint can't write the issue template.</p>; <p>Refer to <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5987"">#5987</a></p>; </li>; </ul>; <h1>What's New in Pylint 2.13.1?</h1>; <p>Release date: 2022-03-26</p>; <ul>; <li>; <p>Fix a regression in 2.13.0 where <code>used-before-assignment</code> was e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11702:978,Release,Release,978,https://hail.is,https://github.com/hail-is/hail/pull/11702,1,['Release'],['Release']
Deployability,"//github.com/PyMySQL/PyMySQL/commit/bbd049f40db9c696574ce6f31669880042c56d79""><code>bbd049f</code></a> Support error packet without sqlstate (<a href=""https://redirect.github.com/PyMySQL/PyMySQL/issues/1160"">#1160</a>)</li>; <li><a href=""https://github.com/PyMySQL/PyMySQL/commit/9694747ae619e88b792a8e0b4c08036572452584""><code>9694747</code></a> pyupgrade</li>; <li><a href=""https://github.com/PyMySQL/PyMySQL/commit/1f0b7856de4008e7e4c1e8c1b215d5d4dfaecd1a""><code>1f0b785</code></a> chore(deps): update codecov/codecov-action action to v4 (<a href=""https://redirect.github.com/PyMySQL/PyMySQL/issues/1158"">#1158</a>)</li>; <li><a href=""https://github.com/PyMySQL/PyMySQL/commit/1e28be81c24dde66f8acbf4c5e24f60d6b5e72e7""><code>1e28be8</code></a> chore(deps): update github/codeql-action action to v3 (<a href=""https://redirect.github.com/PyMySQL/PyMySQL/issues/1154"">#1154</a>)</li>; <li><a href=""https://github.com/PyMySQL/PyMySQL/commit/f13f054abcc18b39855a760a84be0a517f0da658""><code>f13f054</code></a> chore(deps): update actions/setup-python action to v5 (<a href=""https://redirect.github.com/PyMySQL/PyMySQL/issues/1152"">#1152</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/PyMySQL/PyMySQL/compare/v1.1.0...v1.1.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pymysql&package-manager=pip&previous-version=1.1.0&new-version=1.1.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14556:7139,update,update,7139,https://hail.is,https://github.com/hail-is/hail/pull/14556,1,['update'],['update']
Deployability,"//github.com/boto/boto3/blob/develop/CHANGELOG.rst"">boto3's changelog</a>.</em></p>; <blockquote>; <h1>1.21.13</h1>; <ul>; <li>api-change:<code>synthetics</code>: [<code>botocore</code>] Allow custom handler function.</li>; <li>api-change:<code>transfer</code>: [<code>botocore</code>] Add waiters for server online and offline.</li>; <li>api-change:<code>devops-guru</code>: [<code>botocore</code>] Amazon DevOps Guru now integrates with Amazon CodeGuru Profiler. You can view CodeGuru Profiler recommendations for your AWS Lambda function in DevOps Guru. This feature is enabled by default for new customers as of 3/4/2022. Existing customers can enable this feature with UpdateEventSourcesConfig.</li>; <li>api-change:<code>macie</code>: [<code>botocore</code>] Amazon Macie Classic (macie) has been discontinued and is no longer available. A new Amazon Macie (macie2) is now available with significant design improvements and additional features.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] Documentation updates for Amazon EC2.</li>; <li>api-change:<code>sts</code>: [<code>botocore</code>] Documentation updates for AWS Security Token Service.</li>; <li>api-change:<code>connect</code>: [<code>botocore</code>] This release updates the *InstanceStorageConfig APIs so they support a new ResourceType: REAL_TIME_CONTACT_ANALYSIS_SEGMENTS. Use this resource type to enable streaming for real-time contact analysis and to associate the Kinesis stream where real-time contact analysis segments will be published.</li>; </ul>; <h1>1.21.12</h1>; <ul>; <li>api-change:<code>greengrassv2</code>: [<code>botocore</code>] Doc only update that clarifies Create Deployment section.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for data repository associations to use root (&quot;/&quot;) as the file system path</li>; <li>api-change:<code>kendra</code>: [<code>botocore</code>] Amazon Kendra now suggests spell corrections for a query. For more ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11504:1175,update,updates,1175,https://hail.is,https://github.com/hail-is/hail/pull/11504,1,['update'],['updates']
Deployability,"//github.com/boto/boto3/commit/6e6df92c650a91b013bd82df8549ebae9d0a56f7""><code>6e6df92</code></a> Bumping version to 1.26.9</li>; <li><a href=""https://github.com/boto/boto3/commit/4d51ef07becef163a8c4fe1af2be1ba8b63b9979""><code>4d51ef0</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/a177681a29a7dd039cf9dacce7bb810b748d27de""><code>a177681</code></a> Merge branch 'release-1.26.8'</li>; <li><a href=""https://github.com/boto/boto3/commit/531635e52549072a6d847d4f25734d8d3c4f91fd""><code>531635e</code></a> Merge branch 'release-1.26.8' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/894a5c591fa4b56f6e1dfa369948c3b6d25e4178""><code>894a5c5</code></a> Bumping version to 1.26.8</li>; <li><a href=""https://github.com/boto/boto3/commit/dde20184baf312a4f5ca7df08a0d7ce2c5c6e697""><code>dde2018</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/2d82a0c13d4510a5950dd24b4664e23584a5a364""><code>2d82a0c</code></a> Merge branch 'release-1.26.7'</li>; <li><a href=""https://github.com/boto/boto3/commit/b35796f0522b13bc2f9f293ec93697afe09873e2""><code>b35796f</code></a> Merge branch 'release-1.26.7' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/b0e241282f308cee430d340dee119af9100325ff""><code>b0e2412</code></a> Bumping version to 1.26.7</li>; <li>Additional commits viewable in <a href=""https://github.com/boto/boto3/compare/1.26.6...1.26.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=boto3&package-manager=pip&previous-version=1.26.6&new-version=1.26.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12466:6311,release,release-,6311,https://hail.is,https://github.com/hail-is/hail/pull/12466,1,['release'],['release-']
Deployability,"//github.com/boto/boto3/commit/894a5c591fa4b56f6e1dfa369948c3b6d25e4178""><code>894a5c5</code></a> Bumping version to 1.26.8</li>; <li><a href=""https://github.com/boto/boto3/commit/dde20184baf312a4f5ca7df08a0d7ce2c5c6e697""><code>dde2018</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/2d82a0c13d4510a5950dd24b4664e23584a5a364""><code>2d82a0c</code></a> Merge branch 'release-1.26.7'</li>; <li><a href=""https://github.com/boto/boto3/commit/b35796f0522b13bc2f9f293ec93697afe09873e2""><code>b35796f</code></a> Merge branch 'release-1.26.7' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/b0e241282f308cee430d340dee119af9100325ff""><code>b0e2412</code></a> Bumping version to 1.26.7</li>; <li><a href=""https://github.com/boto/boto3/commit/72c2893319ee63b513a31581ca9332b9530c7a22""><code>72c2893</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/01b1b2329ce790b0fda5d9f4bc80d2f416641790""><code>01b1b23</code></a> Merge branch 'release-1.26.6' into develop</li>; <li>See full diff in <a href=""https://github.com/boto/boto3/compare/1.26.6...1.26.8"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=boto3&package-manager=pip&previous-version=1.26.6&new-version=1.26.8)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate thi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12458:5008,release,release-,5008,https://hail.is,https://github.com/hail-is/hail/pull/12458,1,['release'],['release-']
Deployability,"//github.com/eranl""><code>@​eranl</code></a>.</li>; <li><a href=""https://redirect.github.com/java-native-access/jna/issues/1472"">#1472</a>: Fix incorrect bitmask in <code>c.s.j.Pointer#createConstant(int)</code> - <a href=""https://github.com/dbwiddis""><code>@​dbwiddis</code></a>.</li>; <li><a href=""https://redirect.github.com/java-native-access/jna/issues/1481"">#1481</a>: Fix NPE in NativeLibrary when unpacking from classpath is disabled - <a href=""https://github.com/trespasserw""><code>@​trespasserw</code></a>.</li>; <li><a href=""https://redirect.github.com/java-native-access/jna/pull/1489"">#1489</a>: Fixes typo in <code>OpenGL32Util#wglGetProcAddress</code>, instead of parameter <code>procName</code> the hardcoded value <code>wglEnumGpusNV</code> was used - <a href=""https://github.com/soywiz""><code>@​soywiz</code></a>.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/java-native-access/jna/commit/4962fd7758493b7395e86578705d8a32f6238872""><code>4962fd7</code></a> Release 5.13.0</li>; <li><a href=""https://github.com/java-native-access/jna/commit/a56504611b00cc7d90c165f924c3915cb7a6f759""><code>a565046</code></a> Adjust release directions</li>; <li><a href=""https://github.com/java-native-access/jna/commit/f7017c4f957d7fc13c7455efcae200e29407a729""><code>f7017c4</code></a> Remove artifacts classified as &quot;-jpms&quot;, there are the jna-jpms and jna-platfo...</li>; <li><a href=""https://github.com/java-native-access/jna/commit/a5f47cd359d5fe62a0e5d6c2bd9d649874be955d""><code>a5f47cd</code></a> Merge pull request <a href=""https://redirect.github.com/java-native-access/jna/issues/1494"">#1494</a> from matthiasblaesing/pr-1492</li>; <li><a href=""https://github.com/java-native-access/jna/commit/1af6eb14e0059c7acd5d5ee71fd62e519536fac5""><code>1af6eb1</code></a> Improve documentation, ensure osgi.version is defined, wrap create-export-pac...</li>; <li><a href=""https://github.com/java-native-access/jna/commit/65",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12886:3958,Release,Release,3958,https://hail.is,https://github.com/hail-is/hail/pull/12886,1,['Release'],['Release']
Deployability,"//github.com/googleapis/google-api-python-client/commit/5399bd3bf34ef31302a77ff484ef0067a18709a6""><code>5399bd3</code></a> chore(deps): update dependency google-api-python-client to v2.38.0 (<a href=""https://github-redirect.dependabot.com/googleapis/google-api-python-client/issues/1703"">#1703</a>)</li>; <li><a href=""https://github.com/googleapis/google-api-python-client/commit/8f2c948ddd238726b4af5506e7f44337f21e74c5""><code>8f2c948</code></a> chore(main): release 2.38.0 (<a href=""https://github-redirect.dependabot.com/googleapis/google-api-python-client/issues/1696"">#1696</a>)</li>; <li><a href=""https://github.com/googleapis/google-api-python-client/commit/07bfa5c5308f432272213c6c4a395cc14c4c5b0d""><code>07bfa5c</code></a> chore: Update discovery artifacts (<a href=""https://github-redirect.dependabot.com/googleapis/google-api-python-client/issues/1701"">#1701</a>)</li>; <li><a href=""https://github.com/googleapis/google-api-python-client/commit/58ef3e0171d10c7884523faec8e45907a8ff3032""><code>58ef3e0</code></a> chore: Update discovery artifacts (<a href=""https://github-redirect.dependabot.com/googleapis/google-api-python-client/issues/1700"">#1700</a>)</li>; <li><a href=""https://github.com/googleapis/google-api-python-client/commit/835818d4e815b6e56cab2c554e0938b3342e0519""><code>835818d</code></a> chore: reduce commits in discovery document update PR (<a href=""https://github-redirect.dependabot.com/googleapis/google-api-python-client/issues/1699"">#1699</a>)</li>; <li><a href=""https://github.com/googleapis/google-api-python-client/commit/a47764bc0ee296365e196daa39d038035325d5ed""><code>a47764b</code></a> docs: fix typo and unnecessary word in docstring (<a href=""https://github-redirect.dependabot.com/googleapis/google-api-python-client/issues/1692"">#1692</a>)</li>; <li><a href=""https://github.com/googleapis/google-api-python-client/commit/755cff661f95430dee01e676f63267ae0b97119c""><code>755cff6</code></a> chore(deps): update dependency google-api-python-client to v2.37.0 (<",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11541:33939,Update,Update,33939,https://hail.is,https://github.com/hail-is/hail/pull/11541,1,['Update'],['Update']
Deployability,"//github.com/googleapis/google-auth-library-python/commit/3c72365d8407bb097568919123cd7232c1a49f4f""><code>3c72365</code></a> chore: update user cred for system test (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/966"">#966</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/52c8ef90058120d7d04d3d201adc111664be526c""><code>52c8ef9</code></a> feat: ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/962"">#962</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/83b20f0b4d32b2ff1183a9c2926afd37f3baf92b""><code>83b20f0</code></a> chore: update user creds for system test (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/963"">#963</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/3c9feff3e9037a15bf07496623e3a810f117adcf""><code>3c9feff</code></a> chore(main): release 2.5.0 (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/960"">#960</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/a8eb4c8693055a3420cfe9c3420aae2bc8cd465a""><code>a8eb4c8</code></a> feat: ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/956"">#956</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/87706fd9561aeb651ef551f3576f236a73fad27a""><code>87706fd</code></a> chore: update user cred for system test (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/957"">#957</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/5a09454703bd004d23355a6f660ec8579597d981""><code>5a09454</code></a> chore(main): release 2.4.1 (<a href=""https://g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546:10965,release,release,10965,https://hail.is,https://github.com/hail-is/hail/pull/11546,1,['release'],['release']
Deployability,"//github.com/hail-is/hail/blob/40d8882470af71f2d08dd1aa6b723357ca8a1245/batch/sql/estimated-current.sql#L449-L453) in the jobs_after_update trigger. Looking at the second transaction in context now, it looks like that is probably another MJC transaction toward the end of its run after it updated the jobs table. I think it would make sense then that T2 would still hold the lock for `instances_free_cores_mcpu` but I'm not sure where the contention for `batch_inst_coll_cancellable_resources` is coming from, as I don't see how T1 could be holding any form of lock on it. Either way it seems like how we use these tables is similarly a mess. ```; *** (1) TRANSACTION:; TRANSACTION 644409381, ACTIVE 0 sec starting index read; mysql tables in use 1, locked 1; LOCK WAIT 39 lock struct(s), heap size 3520, 50 row lock(s), undo log entries 28; MySQL thread id 1941960, OS thread handle 140297909716736, query id 1869168359 10.32.3.8 dgoldste updating; UPDATE instances_free_cores_mcpu; SET free_cores_mcpu = free_cores_mcpu + cur_cores_mcpu; WHERE instances_free_cores_mcpu.name = in_instance_name; *** (1) WAITING FOR THIS LOCK TO BE GRANTED:; RECORD LOCKS space id 1263041 page no 3 n bits 264 index PRIMARY of table `dgoldste`.`instances_free_cores_mcpu` trx i; d 644409381 lock_mode X locks rec but not gap waiting; Record lock, heap no 192 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d64676f6c647374652d7374616e64617264; asc batch-worker-dgoldste-standard; (tot; al 36 bytes);; 1: len 6; hex 00002668e81a; asc &h ;;; 2: len 7; hex 710000071136b3; asc q 6 ;;; 3: len 4; hex 800029fe; asc ) ;;. *** (2) TRANSACTION:; TRANSACTION 644409370, ACTIVE 0 sec inserting; mysql tables in use 6, locked 6; 39 lock struct(s), heap size 3520, 51 row lock(s), undo log entries 30; MySQL thread id 1941930, OS thread handle 140298159240960, query id 1869168731 10.32.3.8 dgoldste update; INSERT INTO batch_inst_coll_cancellable_resources (batch_id, inst_coll,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116:1270,UPDATE,UPDATE,1270,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116,1,['UPDATE'],['UPDATE']
Deployability,"//github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1422"">#1422</a>: Load jawt library relative to <code>sun.boot.library.path</code> system on unix OSes - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1427"">#1427</a>: Rebuild all binaries with fix from <a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1422"">#1422</a> and <a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1323"">#1323</a> - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; </ul>; <h1>Release 5.10.0</h1>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/java-native-access/jna/commit/3705b849892aa3c37e5608e640eff19047811a5c""><code>3705b84</code></a> Release 5.12.1</li>; <li><a href=""https://github.com/java-native-access/jna/commit/2f919e56bad203494fe9589206d6d23f27ef4f26""><code>2f919e5</code></a> Null-check cleanable in Memory#close (<a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1447"">#1447</a>)</li>; <li><a href=""https://github.com/java-native-access/jna/commit/1eec7dd76830af97ed64ecb2d8d39a56db104dcd""><code>1eec7dd</code></a> Prepare next development iteration</li>; <li><a href=""https://github.com/java-native-access/jna/commit/0d7499f105e4495bdea15fc21f5b1046e81ca822""><code>0d7499f</code></a> Release 5.12.0</li>; <li><a href=""https://github.com/java-native-access/jna/commit/fa86166d4f75ef4478de7ad9d7d6c0b6b6933ee0""><code>fa86166</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1445"">#1445</a> from matthiasblaesing/aix</li>; <li><a href=""https://github.com/java-native-access/jna/commit/4cca4405f7",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12438:5702,Release,Release,5702,https://hail.is,https://github.com/hail-is/hail/pull/12438,1,['Release'],['Release']
Deployability,"//github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/230"">tox-dev/sphinx-autodoc-typehints#230</a></li>; <li>Support and require nptyping 2.1.1 by <a href=""https://github.com/gaborbernat""><code>@​gaborbernat</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/232"">tox-dev/sphinx-autodoc-typehints#232</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2"">https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2</a></p>; <h2>1.18.1</h2>; <p>No release notes provided.</p>; <h2>1.18.0</h2>; <p>No release notes provided.</p>; <h2>1.17.1</h2>; <p>No release notes provided.</p>; <h2>typehints_use_rtype support and handle TypeError</h2>; <p>No release notes provided.</p>; <h2>1.16.0</h2>; <p>No release notes provided.</p>; <h2>1.15.3</h2>; <p>No release notes provided.</p>; <h2>1.15.2</h2>; <p>No release notes provided.</p>; <h2>1.15.1</h2>; <p>No release notes provided.</p>; <h2>1.15.0</h2>; <p>No release notes provided.</p>; <h2>1.14.1</h2>; <p>No release notes provided.</p>; <h2>Added document_defaults config option</h2>; <p>No release notes provided.</p>; <h2>Fix NewType is inserting a reference as first argument</h2>; <p>No release notes provided.</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/blob/main/CHANGELOG.md"">sphinx-autodoc-typehints's changelog</a>.</em></p>; <blockquote>; <h2>1.18.3</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2.1.2</code></li>; </ul>; <h2>1.18.2</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2.1.1</code></li>; </ul>; <h2>1.18.1</h2>; <ul>; <li>Fix mocked module import not working when used as guarded import</li>; </ul>; <h2>1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11909:1918,release,release,1918,https://hail.is,https://github.com/hail-is/hail/pull/11909,1,['release'],['release']
Deployability,"//github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/230"">tox-dev/sphinx-autodoc-typehints#230</a></li>; <li>Support and require nptyping 2.1.1 by <a href=""https://github.com/gaborbernat""><code>@​gaborbernat</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/232"">tox-dev/sphinx-autodoc-typehints#232</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2"">https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2</a></p>; <h2>1.18.1</h2>; <p>No release notes provided.</p>; <h2>1.18.0</h2>; <p>No release notes provided.</p>; <h2>1.17.1</h2>; <p>No release notes provided.</p>; <h2>typehints_use_rtype support and handle TypeError</h2>; <p>No release notes provided.</p>; <h2>1.16.0</h2>; <p>No release notes provided.</p>; <h2>1.15.3</h2>; <p>No release notes provided.</p>; <h2>1.15.2</h2>; <p>No release notes provided.</p>; <h2>1.15.1</h2>; <p>No release notes provided.</p>; <h2>1.15.0</h2>; <p>No release notes provided.</p>; <h2>1.14.1</h2>; <p>No release notes provided.</p>; <h2>Added document_defaults config option</h2>; <p>No release notes provided.</p>; <h2>Fix NewType is inserting a reference as first argument</h2>; <p>No release notes provided.</p>; <h2>Python 3.10 support and PEP-563, drop 3.6</h2>; <p>No release notes provided.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/blob/main/CHANGELOG.md"">sphinx-autodoc-typehints's changelog</a>.</em></p>; <blockquote>; <h2>1.18.2</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2.1.1</code></li>; </ul>; <h2>1.18.1</h2>; <ul>; <li>Fix mocked module import not working when used as guarded import</li>; </ul>; <h2>1.18.0</h2>; <ul>; <li>Support and require <code>nptyping&",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11893:1415,release,release,1415,https://hail.is,https://github.com/hail-is/hail/pull/11893,1,['release'],['release']
Deployability,"//github.com/sass/libsass/releases/tag/3.6.3</a></p>; <h2>Version 0.19.3</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/sass/libsass-python/commit/13c0d60e244c694dec88d8ac8370d7aae6dce4d0""><code>13c0d60</code></a> 0.21.0</li>; <li><a href=""https://github.com/sass/libsass-python/commit/5c94c2a72dab34367758e229c487de50f1430283""><code>5c94c2a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/344"">#344</a> from sass/3_6_5</li>; <li><a href=""https://github.com/sass/libsass-python/commit/ad69f6e023a6d8fdde4428b7a497b60fb5515215""><code>ad69f6e</code></a> update libsass to 3.6.5</li>; <li><a href=""https://github.com/sass/libsass-python/commit/38735e2fdc30ecb21f6eebb253c1b7a9a45dc757""><code>38735e2</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/343"">#343</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/7f01591fdbca66375a61d70e505a286550a1c1b1""><code>7f01591</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/814d42df9787494f01474116940782ab67da083f""><code>814d42d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/342"">#342</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/b08f9ca307ce64867070a3ca5ee5f1a6c5742069""><code>b08f9ca</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/89d6a1dda507abde79ff79b3fd95b9d013eaa02d""><code>89d6a1d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/340"">#340</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/93c70a9a9f350b2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11508:4192,update,update-config,4192,https://hail.is,https://github.com/hail-is/hail/pull/11508,1,['update'],['update-config']
Deployability,"//github.com/urllib3/urllib3/commit/aa3def7d242525e6e854991247c4b68583d15135""><code>aa3def7</code></a> Release 1.26.11</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/6f93b8f450b18b4c9f4c6333d759a911a63d15ae""><code>6f93b8f</code></a> Fix <code>OverflowError</code> when TLS is used on some Python versions</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/0a5f34d2c2ee6457e8365543243eccd3d1dc9430""><code>0a5f34d</code></a> Set GHA token permissions to be read-only</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/ac61b73da703df53707c31030b4ea51aab22d43c""><code>ac61b73</code></a> Backport publish workflow and process to 1.26.x</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1fd77edc1a1373c9a7e762de148f19f1e2edd418""><code>1fd77ed</code></a> Release 1.26.10</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/37ba00248424ea3cdf556cc3e7aa81ce0bf40382""><code>37ba002</code></a> [1.26] Update paid contributor program with early feedback</li>; <li>Additional commits viewable in <a href=""https://github.com/urllib3/urllib3/compare/1.26.9...1.26.12"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.9&new-version=1.26.12)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12140:4613,Update,Update,4613,https://hail.is,https://github.com/hail-is/hail/pull/12140,1,['Update'],['Update']
Deployability,"//jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-2"">https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-2</a></li>; <li>Milestone: <a href=""https://github.com/pallets/jinja/milestone/13?closed=1"">https://github.com/pallets/jinja/milestone/13?closed=1</a></li>; </ul>; <h2>3.1.1</h2>; <ul>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-1"">https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-1</a></li>; <li>Milestone: <a href=""https://github.com/pallets/jinja/milestone/12?closed=1"">https://github.com/pallets/jinja/milestone/12?closed=1</a></li>; </ul>; <h2>3.1.0</h2>; <p>This is a feature release, which includes new features and removes previously deprecated features. The 3.1.x branch is now the supported bugfix branch, the 3.0.x branch has become a tag marking the end of support for that branch. We encourage everyone to upgrade, and to use a tool such as <a href=""https://pypi.org/project/pip-tools/"">pip-tools</a> to pin all dependencies and control upgrades. We also encourage upgrading to MarkupSafe 2.1.1, the latest version at this time.</p>; <ul>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-0"">https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-0</a></li>; <li>Milestone: <a href=""https://github.com/pallets/jinja/milestone/8?closed=1"">https://github.com/pallets/jinja/milestone/8?closed=1</a></li>; <li>MarkupSafe changes: <a href=""https://markupsafe.palletsprojects.com/en/2.1.x/changes/#version-2-1-1"">https://markupsafe.palletsprojects.com/en/2.1.x/changes/#version-2-1-1</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/blob/main/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 3.1.2</h2>; <p>Released 2022-04-28</p>; <ul>; <li>Add parameters to <code>Environment.overlay</code> to match <code>__init__</code>.; :issue:<code>1645",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12173:1445,upgrade,upgrades,1445,https://hail.is,https://github.com/hail-is/hail/pull/12173,1,['upgrade'],['upgrades']
Deployability,"//redirect.github.com/googleapis/java-storage/issues/2194"">#2194</a>) (<a href=""https://github.com/googleapis/java-storage/commit/8880d94c3d1a737dd4492cf66a16ba5e08633a70"">8880d94</a>)</li>; <li>Follow-up CLI Improvements (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2184"">#2184</a>) (<a href=""https://github.com/googleapis/java-storage/commit/d9859768081ea6f872097851d3e318b5bad384d9"">d985976</a>)</li>; <li>Initial CLI for SSB integration and Workload 1 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2166"">#2166</a>) (<a href=""https://github.com/googleapis/java-storage/commit/a349735e7fe108e623a330afec0c8cd608ebeef9"">a349735</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>A resumable session without a Range header should be interpreted as 0 length (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2182"">#2182</a>) (<a href=""https://github.com/googleapis/java-storage/commit/53022011d83e6a8515a5ba008fc45fc2dae39cea"">5302201</a>)</li>; <li>Update User-Agent handling for resumable uploads (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2168"">#2168</a>) (<a href=""https://github.com/googleapis/java-storage/commit/665b714f421d3c13b557d0ff71460c328c010856"">665b714</a>)</li>; <li>Update version resolution logic to be more resilient (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2169"">#2169</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c89d27508039a014ea5a6dd8d4889f63d07db73f"">c89d275</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update actions/checkout action to v4 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2188"">#2188</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c10267e176bda21cd5755dfb0e96d0504fbc1d54"">c10267e</a>)</li>; <li>Update actions/checkout action to v4 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2189"">#2189</a>) (<a href=""https://github.com/googleapis/java-storage/comm",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13624:1575,Update,Update,1575,https://hail.is,https://github.com/hail-is/hail/pull/13624,2,['Update'],['Update']
Deployability,"//redirect.github.com/jaraco/zipp/issues/106"">#106</a>.</li>; <li><a href=""https://github.com/jaraco/zipp/commit/4cceb497c278ad0ecb11a9472e58f4130f5ff16b""><code>4cceb49</code></a> Add special accounting for pypy when computing the stack level for text encod...</li>; <li><a href=""https://github.com/jaraco/zipp/commit/2ec3ed8567d0842675c38fd8ef0a28db668e602d""><code>2ec3ed8</code></a> Add another test at another magnitude.</li>; <li><a href=""https://github.com/jaraco/zipp/commit/d9bf5aab8b39c6a124d9499ae0315d3bf2ac2f46""><code>d9bf5aa</code></a> Fix name generator for width=1</li>; <li>Additional commits viewable in <a href=""https://github.com/jaraco/zipp/compare/v3.17.0...v3.18.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=zipp&package-manager=pip&previous-version=3.17.0&new-version=3.18.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14473:3134,update,updates,3134,https://hail.is,https://github.com/hail-is/hail/pull/14473,1,['update'],['updates']
Deployability,"//redirect.github.com/jupyter/notebook/pull/7142"">#7142</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Adopt ruff format <a href=""https://redirect.github.com/jupyter/notebook/pull/7132"">#7132</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>[7.0.x] Install stable JupyterLab 4.0 in the releaser hook <a href=""https://redirect.github.com/jupyter/notebook/pull/7183"">#7183</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; <li>Update publish-release workflow for PyPI trusted publisher <a href=""https://redirect.github.com/jupyter/notebook/pull/7176"">#7176</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/notebook/graphs/contributors?from=2023-10-17&amp;to=2024-01-19&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Abrichet+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​brichet</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ad5423197+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​d5423197</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Agithub-actions+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ajtpio+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Akrassowski+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ameeseeksmachine+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​meeseeksmachine</code></a></p>; <!-- raw HTML omitted -->; </blockquote>; </details>; <detail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14182:4838,update,updated,4838,https://hail.is,https://github.com/hail-is/hail/pull/14182,1,['update'],['updated']
Deployability,"//redirect.github.com/jupyter/notebook/pull/7142"">#7142</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Adopt ruff format <a href=""https://redirect.github.com/jupyter/notebook/pull/7132"">#7132</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>[7.0.x] Install stable JupyterLab 4.0 in the releaser hook <a href=""https://redirect.github.com/jupyter/notebook/pull/7183"">#7183</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; <li>Update publish-release workflow for PyPI trusted publisher <a href=""https://redirect.github.com/jupyter/notebook/pull/7176"">#7176</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/notebook/graphs/contributors?from=2023-10-17&amp;to=2024-01-19&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Abrichet+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​brichet</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ad5423197+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​d5423197</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Agithub-actions+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ajtpio+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Akrassowski+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ameeseeksmachine+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​meeseeksmachine</code></a></p>; </blockquote>; </details>; <details>; <summary>Changelog</sum",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14182:1974,update,updated,1974,https://hail.is,https://github.com/hail-is/hail/pull/14182,1,['update'],['updated']
Deployability,"//www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v4.3.1</h2>; <p>No release notes provided.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/sphinx-doc/sphinx/blob/5.x/CHANGES"">sphinx's changelog</a>.</em></p>; <blockquote>; <h1>Release 5.0.1 (released Jun 03, 2022)</h1>; <h2>Bugs fixed</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10498"">#10498</a>: gettext: TypeError is raised when sorting warning messages if a node; has no line number</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10493"">#10493</a>: html theme: :rst:dir:<code>topic</code> directive is rendered incorrectly with; docutils-0.18</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10495"">#10495</a>: IndexError is raised for a :rst:role:<code>kbd</code> role having a separator</li>; </ul>; <h1>Release 5.0.0 (released May 30, 2022)</h1>; <h2>Dependencies</h2>; <p>5.0.0 b1</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10164"">#10164</a>: Support <code>Docutils 0.18</code>_. Patch by Adam Turner.</li>; </ul>; <p>.. _Docutils 0.18: <a href=""https://docutils.sourceforge.io/RELEASE-NOTES.html#release-0-18-2021-10-26"">https://docutils.sourceforge.io/RELEASE-NOTES.html#release-0-18-2021-10-26</a></p>; <h2>Incompatible changes</h2>; <p>5.0.0 b1</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10031"">#10031</a>: autosummary: <code>sphinx.ext.autosummary.import_by_name()</code> now raises; <code>ImportExceptionGroup</code> instead of <code>ImportError</code> when it failed to import; target object. Please handle the exception if your extension uses the; function to import Python object. As a workaround, you can disable the; behavior via <code>grouped_exception=False</code> keyword argument until v7.0.</li>; <li><a href=""https://github-redirect.depe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11885:1765,release,released,1765,https://hail.is,https://github.com/hail-is/hail/pull/11885,1,['release'],['released']
Deployability,"/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:6957,install,install,6957,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['install'],['install']
Deployability,"/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,945"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/close HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/close HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:862"", ""message"": ""batch 9 cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,958"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,967"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,969"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:501"", ""message"": ""job (9, 1, 'main') changed state: Ready -> Cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,974"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""_delete_pvc:251"", ""message"": ""deleting persistent volume claim batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6617:2211,PATCH,PATCH,2211,https://hail.is,https://github.com/hail-is/hail/issues/6617,1,['PATCH'],['PATCH']
Deployability,"/1593"">#1593</a>)</li>; <li>See full diff in <a href=""https://github.com/samtools/htsjdk/compare/3.0.2...3.0.4"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.github.samtools:htsjdk&package-manager=gradle&previous-version=3.0.2&new-version=3.0.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12700:4590,upgrade,upgrade,4590,https://hail.is,https://github.com/hail-is/hail/pull/12700,3,['upgrade'],['upgrade']
Deployability,"/175"">#175</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/brettcannon/gidgethub/compare/v4.2.0...v5.2.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=gidgethub&package-manager=pip&previous-version=4.2.0&new-version=5.2.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12328:10838,upgrade,upgrade,10838,https://hail.is,https://github.com/hail-is/hail/pull/12328,3,['upgrade'],['upgrade']
Deployability,"/3593. ## Testing. - Adding a ""wait `n` seconds"" method that slept for n seconds, and returned the value of an environment variable. This environment variable meant I could track which version of the deployment my script ran against.; - Taking the `deploy.yaml` from the `deploy query` step of the dev deploy, adding the `TEST_VALUE` environment variable with some value and saving it as `new-deploy.yaml`; - Issuing the first wait request (for 50 seconds) (`https://internal.hail.populationgenomics.org.au/$NAMESPACE/query/api/v1alpha/wait?duration=50`); - Issuing the new deploy with:; ```bash; kubectl -n $NAMESPACE apply -f new-deploy.yaml; kubectl -n $NAMESPACE rollout status --timeout=10m deployment query; ```; - When the new pod is created (seen with `kubectl --namespace $NAMESPACE get pod`), issue the second request to the wait method.; - If all goes well, you should have:; - termination logs like those below,; - the first request successfully fulfilled with the response of env value being None (filled by the first pod); - The second request successfully filled, but has the value of the environment value, the one you set in the deploy.yaml (it got scheduled to the new node). Termination logs:. ```; {""severity"": ""INFO"", ""levelname"": ""INFO"", ""asctime"": ""2021-02-24 23:22:40,472"", ""filename"": ""query.py"", ""funcNameAndLine"": ""on_shutdown:253"", ""message"": ""On shutdown request received, with 2 tasks left"", ""hail_log"": 1}; ++ term; ++ kill -TERM 7; + true; + '[' no == yes ']'; + trap - SIGTERM SIGINT; + wait 7; {""severity"": ""INFO"", ""levelname"": ""INFO"", ""asctime"": ""2021-02-24 23:23:26,004"", ""filename"": ""hail_logging.py"", ""funcNameAndLine"": ""log:40"", ""message"": ""https GET /michaelfranklin/query/api/v1alpha/wait done in 50.029999999998836s: 200"", ""remote_address"": ""10.28.127.3"", ""request_start_time"": ""[24/Feb/2021:23:22:35 +0000]"", ""request_duration"": 50.029999999998836, ""response_status"": 200, ""x_real_ip"": ""124.170.20.28"", ""hail_log"": 1}; {""severity"": ""INFO"", ""levelname"": ""INF",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10106:1467,deploy,deploy,1467,https://hail.is,https://github.com/hail-is/hail/pull/10106,1,['deploy'],['deploy']
Deployability,"/665"">#665</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp-session/commit/29ddff39290f4e2fbc7b9feb94eb622763e156e2""><code>29ddff3</code></a> Bump pytest-aiohttp from 0.3.0 to 1.0.3 (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_session/issues/668"">#668</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp-session/commit/3bc517092aff39330f2f96315e6f542e23415831""><code>3bc5170</code></a> Bump multidict from 5.2.0 to 6.0.2 (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_session/issues/670"">#670</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/aio-libs/aiohttp_session/compare/v2.7.0...v2.11.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=aiohttp-session&package-manager=pip&previous-version=2.7.0&new-version=2.11.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11544:4715,update,updates,4715,https://hail.is,https://github.com/hail-is/hail/pull/11544,1,['update'],['updates']
Deployability,/742170/container_logs && exit 1; rm -rf /io/*; set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources -t read_with_index_p1000; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 2; memory: 7G; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-12728-job-287-742170 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5000/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.cleanup_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 50,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7016:1949,pipeline,pipeline,1949,https://hail.is,https://github.com/hail-is/hail/issues/7016,6,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,"/77"">#77</a>)</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/258a8832d9518340386e584206d7b5116185b182""><code>258a883</code></a> DOC: adjust test badge to point to Github Actions (<a href=""https://github-redirect.dependabot.com/hagenw/sphinxcontrib-katex/issues/76"">#76</a>)</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/ef894b2bf6ae4b1eaa0c5adec7ab5c1540da97cd""><code>ef894b2</code></a> Support Python 3.10 (<a href=""https://github-redirect.dependabot.com/hagenw/sphinxcontrib-katex/issues/74"">#74</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/hagenw/sphinxcontrib-katex/compare/0.5.1...v0.9.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinxcontrib-katex&package-manager=pip&previous-version=0.5.1&new-version=0.9.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12241:6673,update,updates,6673,https://hail.is,https://github.com/hail-is/hail/pull/12241,1,['update'],['updates']
Deployability,"/797b57a4ac8da86c13e52bf60586cd2432864400""><code>797b57a</code></a> Fixed pyproject.toml and setup.py.</li>; <li><a href=""https://github.com/mrabarnett/mrab-regex/commit/16bcce0d56e84367f61c24b369e23e73a3e9ad9e""><code>16bcce0</code></a> Add changelog.txt.</li>; <li><a href=""https://github.com/mrabarnett/mrab-regex/commit/d235c2c17f2335dd7699f3c29a6ae6db6dbe6dab""><code>d235c2c</code></a> pyproject.toml was missing.</li>; <li><a href=""https://github.com/mrabarnett/mrab-regex/commit/78460dc755b09966ee6e87d04c8dcfca7212256b""><code>78460dc</code></a> Added pyproject.toml.</li>; <li>See full diff in <a href=""https://github.com/mrabarnett/mrab-regex/compare/2023.3.23...2023.5.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=regex&package-manager=pip&previous-version=2023.3.23&new-version=2023.5.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12989:3232,update,updates,3232,https://hail.is,https://github.com/hail-is/hail/pull/12989,1,['update'],['updates']
Deployability,"/8.2/eshadoop-8.2.0.html</a></p>; <h2>Elasticsearch Hadoop 8.1.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/a7873eb46985d849286ab89fa0a51f8b5374e02e""><code>a7873eb</code></a> Update index.adoc (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2000"">#2000</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2002"">#2002</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/9f44fe66d0ff82f18a13a38cae6abf3f72183a94""><code>9f44fe6</code></a> Bump to version 8.4.3</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/c9e3b114b98bb0e340555311c82e2d9f32c880b6""><code>c9e3b11</code></a> [DOCS] Add 8.4.2 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1998"">#1998</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1999"">#1999</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/d10ae2a9d38eefc9fefb5cdb16f2fec61c5160b6""><code>d10ae2a</code></a> Bump to version 8.4.2</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/ee6bd94c0a54f6226fa30e0681287cc72cf26b83""><code>ee6bd94</code></a> [DOCS] Added RNs for 8.4.1 (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1996"">#1996</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/8327792db7605353c6d4d43e85c8ad7cb31f2e51""><code>8327792</code></a> Bump to version 8.4.1</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/4d2e4b12b83f84521ce36634a3eeb0904137c89b""><code>4d2e4b1</code></a> [DOCS] Add 8.4.0 release notes (<a href=""https://github-redirect.dep",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:5162,release,release,5162,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['release'],['release']
Deployability,"/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/e665cd918aa8d0ba7806b92e16881edb96180d48""><code>e665cd9</code></a> Update Spark to 3.2.3 (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2056"">#2056</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2057"">#2057</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/07380b0e17c7d908d50d59fc69ac2953adfa5a0d""><code>07380b0</code></a> Use DRA repository for build-tools dependencies</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/77bce30bfefb39c39bd34a6f147b17fb0df4701c""><code>77bce30</code></a> Bump to version 8.6.1</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/92ac2d5e61b7b8cc16b6a9f29ad1454497f604ba""><code>92ac2d5</code></a> [DOCS] Add 8.6.0 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2053"">#2053</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2054"">#2054</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/da4f3c3f209aea47d69c4faf90029a6933bd3a60""><code>da4f3c3</code></a> [DOCS] Add 8.5.3 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2045"">#2045</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2050"">#2050</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/79d592abdce1cd90845d153afab8b66069e2a172""><code>79d592a</code></a> [DOCS] Add 8.5.2 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2042"">#2042</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2049"">#2049</a>)</li>; <li><a href=""https://github.com/elastic/elastic",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12623:3206,release,release,3206,https://hail.is,https://github.com/hail-is/hail/pull/12623,1,['release'],['release']
Deployability,"/9319"">#9319</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/bfa4d95f0f356f2d535efd5c775e0fb3efe90ef2""><code>bfa4d95</code></a> changelog for 41.0.3 (<a href=""https://redirect.github.com/pyca/cryptography/issues/9320"">#9320</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/0da7165aa73c0a4865b0a4d9e019db3c16eea55a""><code>0da7165</code></a> backport fix the memory leak in fixedpool (<a href=""https://redirect.github.com/pyca/cryptography/issues/9272"">#9272</a>) (<a href=""https://redirect.github.com/pyca/cryptography/issues/9309"">#9309</a>)</li>; <li>See full diff in <a href=""https://github.com/pyca/cryptography/compare/41.0.2...41.0.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=cryptography&package-manager=pip&previous-version=41.0.2&new-version=41.0.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13357:2226,update,updates,2226,https://hail.is,https://github.com/hail-is/hail/pull/13357,3,['update'],['updates']
Deployability,"/AzureAD/microsoft-authentication-extensions-for-python/issues/110"">#110</a>)</li>; <li>Enhancement: Make all platform-dependent parameters optional (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/103"">#103</a>)</li>; <li>Enhancement: Provide <code>PersistenceEncryptError</code> and <code>PersistenceDecryptError</code>, currently raised when encryption on Windows fails. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/108"">#108</a>)</li>; <li>Enhancement: The data file will be created with <code>600</code> permission when running in Unix-like systems. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/107"">#107</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/commit/a88fa673af3602fe7c8c922314599b0c245e7add""><code>a88fa67</code></a> Merge branch 'release-1.0.0'</li>; <li><a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/commit/bd5b4074dbb7d03c9d91ce6a75378851be92552a""><code>bd5b407</code></a> Update README to reflect the new APIs</li>; <li><a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/commit/6f77b1e70be086aae752dcf7e08d7f06bcabdcd7""><code>6f77b1e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/109"">#109</a> from AzureAD/release-1.0.0</li>; <li><a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/commit/50bf9674f9c65229a1573be39ef4ef507eee17fa""><code>50bf967</code></a> MSAL EX for Python 1.0.0</li>; <li><a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/commit/6b904af1a3d4fc0e28e3f090fa3dd8492f79e6bf""><code>6b904af</code></a> Merge pull reque",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11992:1983,release,release-,1983,https://hail.is,https://github.com/hail-is/hail/pull/11992,1,['release'],['release-']
Deployability,"/PyCQA/astroid/blob/main/ChangeLog"">astroid's changelog</a>.</em></p>; <blockquote>; <h1>What's New in astroid 2.12.8?</h1>; <p>Release date: 2022-09-06</p>; <ul>; <li>; <p>Fixed a crash in the <code>dataclass</code> brain for <code>InitVars</code> without subscript typing.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7422"">PyCQA/pylint#7422</a></p>; </li>; <li>; <p>Fixed parsing of default values in <code>dataclass</code> attributes.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7425"">PyCQA/pylint#7425</a></p>; </li>; </ul>; <h1>What's New in astroid 2.12.7?</h1>; <p>Release date: 2022-09-06</p>; <ul>; <li>; <p>Fixed a crash in the <code>dataclass</code> brain for uninferable bases.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7418"">PyCQA/pylint#7418</a></p>; </li>; </ul>; <h1>What's New in astroid 2.12.6?</h1>; <p>Release date: 2022-09-05</p>; <ul>; <li>; <p>Fix a crash involving <code>Uninferable</code> arguments to <code>namedtuple()</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7375"">PyCQA/pylint#7375</a></p>; </li>; <li>; <p>The <code>dataclass</code> brain now understands the <code>kw_only</code> keyword in dataclass decorators.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7290"">PyCQA/pylint#7290</a></p>; </li>; </ul>; <h1>What's New in astroid 2.12.5?</h1>; <p>Release date: 2022-08-29</p>; <ul>; <li>; <p>Prevent first-party imports from being resolved to <code>site-packages</code>.</p>; <p>Refs <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7365"">PyCQA/pylint#7365</a></p>; </li>; <li>; <p>Fix <code>astroid.interpreter._import.util.is_namespace()</code> incorrectly; returning <code>True</code> for frozen stdlib modules on PyPy.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1755"">#1755</a></p>; </li>; </ul>; <h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12158:1103,Release,Release,1103,https://hail.is,https://github.com/hail-is/hail/pull/12158,1,['Release'],['Release']
Deployability,"/PyCQA/pylint/commit/c42fe73a1613bbfb52a5ba9129efa45a3fd76401""><code>c42fe73</code></a> Fix false negative for <code>protected-access</code> on functions (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5990"">#5990</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/dec241b1787e6c99a092bb9ef6a993abf51fea91""><code>dec241b</code></a> Add regression test for <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5982"">#5982</a> upgrade astroid to 2.11.2 (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5988"">#5988</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/b25859c4a56ccce61087f7a1270f40deaed68169""><code>b25859c</code></a> Fix false positive for <code>superfluous-parens</code> for <code>return (a or b) in iterable</code>...</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/0e1ca11ac65cbe5a65437518fca1e25f1ad0e48e""><code>0e1ca11</code></a> Bump pylint to 2.13.1, update changelog</li>; <li>Additional commits viewable in <a href=""https://github.com/PyCQA/pylint/compare/v2.12.2...v2.13.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pylint&package-manager=pip&previous-version=2.12.2&new-version=2.13.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` wi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11702:4565,update,update,4565,https://hail.is,https://github.com/hail-is/hail/pull/11702,1,['update'],['update']
Deployability,"/PyCQA/pylint/issues/6029"">#6029</a>)</li>; <li>See full diff in <a href=""https://github.com/PyCQA/pylint/compare/v2.13.3...v2.13.4"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pylint&package-manager=pip&previous-version=2.13.3&new-version=2.13.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11723:4602,upgrade,upgrade,4602,https://hail.is,https://github.com/hail-is/hail/pull/11723,3,['upgrade'],['upgrade']
Deployability,"/__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-4f13f27cd28d; NOTE: This is a beta version. Interfaces may change; during the beta period. We recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(740 + 1) / 741]2018-11-10 22:55:07 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 24) / 741]Exception in thread ""refresh progress"" Exception in thread ""LeaseRenewer:farrell@scc"" java.lang.OutOfMemoryError: GC overhead limit exceeded; java.lang.OutOfMemoryError: GC overhead limit exceeded; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>(); 4 vcf=""/project/ukbiobank/imputation/ad.v1/vcf/ukbb.hg38.imputed.chr""+chr+"".dose.vcf.bgz""; 5 mt=""/project/ukbiobank/imputation/ad.v1/mt/ukbb.hg38.imputed.chr""+chr; ----> 6 hl.import_vcf(vcf).write(mt). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548; 549 update_wrapper(wrapper, f). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, _codec_spec); 2111 """"""; 2112; -> 2113 self._jvds.write(output, overwrite, _codec_spec); 2114; 2115 def globals_table(self) -> Table:. /share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava sta",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755:1839,install,install,1839,https://hail.is,https://github.com/hail-is/hail/issues/4755,1,['install'],['install']
Deployability,"/a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Clean up lint handling <a href=""https://redirect.github.com/jupyter/notebook/pull/7142"">#7142</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Adopt ruff format <a href=""https://redirect.github.com/jupyter/notebook/pull/7132"">#7132</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>[7.0.x] Install stable JupyterLab 4.0 in the releaser hook <a href=""https://redirect.github.com/jupyter/notebook/pull/7183"">#7183</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; <li>Update publish-release workflow for PyPI trusted publisher <a href=""https://redirect.github.com/jupyter/notebook/pull/7176"">#7176</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/notebook/graphs/contributors?from=2023-10-17&amp;to=2024-01-19&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Abrichet+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​brichet</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ad5423197+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​d5423197</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Agithub-actions+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ajtpio+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Akrassowski+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ameeseeksmachine+updated%3A2023-10-17..2024-0",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14182:1871,release,release,1871,https://hail.is,https://github.com/hail-is/hail/pull/14182,2,['release'],['release']
Deployability,"/a> Fix a flake8 warning</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/aee4e42b81d56c57e1311176ce175ba3374baa0a""><code>aee4e42</code></a> extlink: Strip a leading backslash on compiling pattern</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/8a1830ca36ddea80f8bcbc20c1090280a0a5197a""><code>8a1830c</code></a> Update CHANGES for PR <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10178"">#10178</a></li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/4a496bfc98feced56e9e84eb6cf96264982d0e7a""><code>4a496bf</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10178"">#10178</a> from stephenfin/issue-10177</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/c93b95d685aea6c8e4392d624e4668587b9e5726""><code>c93b95d</code></a> Merge CHANGES entry for 4.4.1 to 4.5.0</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/a001bf47d66ae804a9a6e5d754de9b5eda4d0eb9""><code>a001bf4</code></a> Update CHANGES for PR <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10107"">#10107</a></li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/b20e04968e73234da9fff7d19b12dfbeebebe944""><code>b20e049</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10107"">#10107</a> from Jean-Abou-Samra/intl-warnings</li>; <li>Additional commits viewable in <a href=""https://github.com/sphinx-doc/sphinx/compare/v3.5.4...v4.5.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinx&package-manager=pip&previous-version=3.5.4&new-version=4.5.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11714:6998,Update,Update,6998,https://hail.is,https://github.com/hail-is/hail/pull/11714,2,['Update'],['Update']
Deployability,"/a> Name as 'options' in lambda_eval and unsafe_eval, but '_dict' in deprecated eval</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/facf3af93dabcbdd8cdbda8c3b50eefafa3bb04c""><code>facf3af</code></a> Added release notes</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/2a93aba5cfcf6e241ab4f9392c13e3b74032c061""><code>2a93aba</code></a> Use strncpy to avoid buffer overflow</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/a670597bc30e9d489656fc9d807170b8f3d7ca57""><code>a670597</code></a> Update CHANGES.rst [ci skip]</li>; <li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/10.2.0...10.3.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=10.2.0&new-version=10.3.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:15517,update,updates,15517,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['update'],['updates']
Deployability,"/a> Slight perf enhancement in Empty</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/801863aa4582a8ce5e6a7408d4966afcd247ea90""><code>801863a</code></a> Make htmlStripper.py and html_table_parser examples use PEP-8 names, add comm...</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/7d4da80b2bca8a2767134f4a181ea9aac4bbb230""><code>7d4da80</code></a> Prep for release</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/be0310a83436bb4893d0068bb5da3059199e4c0b""><code>be0310a</code></a> Add bf parser/executor example</li>; <li>Additional commits viewable in <a href=""https://github.com/pyparsing/pyparsing/compare/pyparsing_3.0.9...3.1.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pyparsing&package-manager=pip&previous-version=3.0.9&new-version=3.1.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13334:9162,update,updates,9162,https://hail.is,https://github.com/hail-is/hail/pull/13334,1,['update'],['updates']
Deployability,"/a> Test all constructors</li>; <li><a href=""https://github.com/apache/commons-codec/commit/3535c17eccb2251fc518aa545a800b4922c8dc35""><code>3535c17</code></a> Test encode of null and empty array with an offset</li>; <li><a href=""https://github.com/apache/commons-codec/commit/e42dfe1ff2f273926fd759abea82b1c7b3021985""><code>e42dfe1</code></a> Fix test names</li>; <li><a href=""https://github.com/apache/commons-codec/commit/536587931cb77538709c57455165379a74e2f04f""><code>5365879</code></a> Test the codec policy property</li>; <li>Additional commits viewable in <a href=""https://github.com/apache/commons-codec/compare/commons-codec-1.11...rel/commons-codec-1.15"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=commons-codec:commons-codec&package-manager=gradle&previous-version=1.11&new-version=1.15)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12385:4991,update,updates,4991,https://hail.is,https://github.com/hail-is/hail/pull/12385,1,['update'],['updates']
Deployability,"/a> Update translations for 1.0 release</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/2254b1f1e871f05036c47324825c601c422702c5""><code>2254b1f</code></a> Update docs and versions for 1.0.0 release</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/acada32f5d3c57d38413434a6ee83d28b5d61cd8""><code>acada32</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/readthedocs/sphinx_rtd_theme/issues/1220"">#1220</a> from readthedocs/nienn/fix-sphinx-4-pre-overflow</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/e319184c89b0e11dba77441241fe9a735855fedb""><code>e319184</code></a> Merge branch 'master' into nienn/fix-sphinx-4-pre-overflow</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/20f205fc2a5c7dc813faa0b05ff1152e6ba5a4a6""><code>20f205f</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/readthedocs/sphinx_rtd_theme/issues/1217"">#1217</a> from readthedocs/agj/release-labels</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/2774670572c44555c6d6ecc280dfe34827ef62d4""><code>2774670</code></a> Add CSS max-width to dl.property</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/b557851511509c75cfffd10e1ede1e2266249c0a""><code>b557851</code></a> Make section labels verbose to avoid numeric labels</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/73d1707e791712efb837167065c4173ce9b380f8""><code>73d1707</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/readthedocs/sphinx_rtd_theme/issues/1088"">#1088</a> from readthedocs/Blendify/fix-717</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/3a031121ed86bc2b857f734eede0b48d8164545b""><code>3a03112</code></a> Fix build</li>; <li>Additional commits viewable in <a href=""https://github.com/readthedocs/sphinx_rtd_theme/compare/0.4.2...1.0.0"">compare view</a></li>; </ul>; </detail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11464:5962,release,release-labels,5962,https://hail.is,https://github.com/hail-is/hail/pull/11464,2,['release'],['release-labels']
Deployability,"/a> remove unneeded variable</li>; <li>Additional commits viewable in <a href=""https://github.com/tqdm/tqdm/compare/v4.42.1...v4.64.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tqdm&package-manager=pip&previous-version=4.42.1&new-version=4.64.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12260:7167,upgrade,upgrade,7167,https://hail.is,https://github.com/hail-is/hail/pull/12260,3,['upgrade'],['upgrade']
Deployability,"/a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f5682a4f6d6d5372a2d382ae3e47dace490ca0d"">4f5682a</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.25.0...v2.26.0"">2.26.0</a> (2023-08-03)</h2>; <h3>Features</h3>; <ul>; <li>Implement BufferToDiskThenUpload BlobWriteSessionConfig (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2139"">#2139</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4dad2d5c3a81eda7190ad4f95316471e7fa30f66"">4dad2d5</a>)</li>; <li>Introduce new BlobWriteSession (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2123"">#2123</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e0191b518e50a49fae0691894b50f0c5f33fc6af"">e0191b5</a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/googleapis/java-storage/commit/cc6503155201c0aae6d79517a91712c834689ce5""><code>cc65031</code></a> chore(main): release 2.27.0 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2167"">#2167</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/8880d94c3d1a737dd4492cf66a16ba5e08633a70""><code>8880d94</code></a> feat: add new JournalingBlobWriteSessionConfig usable with gRPC transport (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2"">#2</a>...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/1fa49db2810f6ffbd46755b4eb1f5efdcf980edb""><code>1fa49db</code></a> deps: update dependency com.google.apis:google-api-services-storage to v1-rev...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/5e76f1963db18c9d081133755b5572e186cd1b34""><code>5e76f19</code></a> chore: Update the Java code generator (gapic-generator-java) to 2.25.0 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2198"">#2198</a>)</li>; <li><a href=""https://github.com/googlea",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13624:12719,release,release,12719,https://hail.is,https://github.com/hail-is/hail/pull/13624,1,['release'],['release']
Deployability,"/a>).</li>; </ul>; <h2>Misc</h2>; <ul>; <li>Moved <code>test_imports.py</code>, <code>test_internals.py</code> and <code>test_utils.py</code> to; pytest. Reported and fixed by <a href=""https://github.com/jpurviance""><code>@​jpurviance</code></a> (gh pr <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/978"">#978</a>)</li>; <li>Added project_urls for documentation and source. Patch by <a href=""https://github.com/andriyor""><code>@​andriyor</code></a> (gh pr; <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/975"">#975</a>).</li>; <li>Simplified handling of bytes and bytearray in <code>_parser._timelex</code>. Reported</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/dateutil/dateutil/blob/master/NEWS"">python-dateutil's changelog</a>.</em></p>; <blockquote>; <h1>Version 2.8.2 (2021-07-08)</h1>; <h2>Data updates</h2>; <ul>; <li>Updated tzdata version to 2021a. (gh pr <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/1128"">#1128</a>)</li>; </ul>; <h2>Bugfixes</h2>; <ul>; <li>Fixed a bug in the parser where non-<code>ValueError</code> exceptions would be raised; during exception handling; this would happen, for example, if an; <code>IllegalMonthError</code> was raised in <code>dateutil</code> code. Fixed by Mark Bailey.; (gh issue <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/981"">#981</a>, pr <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/987"">#987</a>).</li>; <li>Fixed the custom <code>repr</code> for <code>dateutil.parser.ParserError</code>, which was not; defined due to an indentation error. (gh issue <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/991"">#991</a>, gh pr <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/993"">#993</a>)</li>; <li>Fixed a bug that caused <code>b'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11518:4644,update,updates,4644,https://hail.is,https://github.com/hail-is/hail/pull/11518,2,"['Update', 'update']","['Updated', 'updates']"
Deployability,"/a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/68d7b8992d77ad00cdd985bfd764b81f42085fe3""><code>68d7b89</code></a> Eagerly Convert Headers Always in Download (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32173"">#32173</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/c10e612d913b03f044ddd58aa591850615b61ecd""><code>c10e612</code></a> Sync eng/common directory with azure-sdk-tools for PR 4701 (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32168"">#32168</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/44682b71c0216aae1530af287e745803feeec2fc""><code>44682b7</code></a> Regenerate Storage Blobs with Fix for Download to File (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32163"">#32163</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/11f065d4d592d14977d178ddd58f6a6ec6b16276""><code>11f065d</code></a> Increment package versions for keyvault releases (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32151"">#32151</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/44d679faee209611bee14fcea08207f9753bb466""><code>44d679f</code></a> Increment versions for appcomplianceautomation releases (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32155"">#32155</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/020145e20dff874555f4e610bc9b5b39213b1740""><code>020145e</code></a> move processor-lifecycle-manager from messaging to service module (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/32152"">#32152</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/69d2f672399502021a8f8918a7d5962c3005f687""><code>69d2f67</code></a> [Automation] Generate Fluent Lite from appcomplianceautomation#package-2022-1...</li>; <li><a href=""https://",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12477:4033,release,releases,4033,https://hail.is,https://github.com/hail-is/hail/pull/12477,1,['release'],['releases']
Deployability,"/a>)</li>; <li><a href=""https://github.com/python/typing_extensions/commit/9683c1a82add3182be967050d164349da426a20f""><code>9683c1a</code></a> Backport test case from #python/cpython/96358 (<a href=""https://github-redirect.dependabot.com/python/typing_extensions/issues/71"">#71</a>)</li>; <li><a href=""https://github.com/python/typing_extensions/commit/db79268673ac10412b4aad19efea03948869b7db""><code>db79268</code></a> Silence a <code>flake8-bugbear</code> warning (<a href=""https://github-redirect.dependabot.com/python/typing_extensions/issues/72"">#72</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/python/typing_extensions/compare/4.3.0...4.4.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=typing-extensions&package-manager=pip&previous-version=4.3.0&new-version=4.4.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12288:4454,update,updates,4454,https://hail.is,https://github.com/hail-is/hail/pull/12288,1,['update'],['updates']
Deployability,"/a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/febde39c7cd5f56fe19979adeee05e1e16eadfe2""><code>febde39</code></a> Release 2.0.0rc3 (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1535"">#1535</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/32310a819012589211a68db8d8060fd1e3a499f5""><code>32310a8</code></a> Fix <a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1522"">#1522</a>: fix <code>'str' object has no attribute 'attributes'</code> (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1528"">#1528</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/8ce23cec96f628ac0d29737bf1cf8cc2e750f068""><code>8ce23ce</code></a> Version bump for 2.0rc3 development (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1521"">#1521</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/46f5307dbd32ebb339c3a76514ce5791826ec381""><code>46f5307</code></a> Release 2.0rc2 (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1520"">#1520</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/5838e6aa545863fac10c08314f90feb6d7ac7757""><code>5838e6a</code></a> Add support for <code>docutils==0.20.x</code> (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1517"">#1517</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/readthedocs/sphinx_rtd_theme/compare/1.3.0...2.0.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinx-rtd-theme&package-manager=pip&previous-version=1.3.0&new-version=2.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you do",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14502:3381,Release,Release,3381,https://hail.is,https://github.com/hail-is/hail/pull/14502,1,['Release'],['Release']
Deployability,"/a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/aio-libs/aiohttp_session/compare/v2.7.0...v2.11.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=aiohttp-session&package-manager=pip&previous-version=2.7.0&new-version=2.11.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11544:5867,upgrade,upgrade,5867,https://hail.is,https://github.com/hail-is/hail/pull/11544,3,['upgrade'],['upgrade']
Deployability,"/a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/aio-libs/aiohttp_session/compare/v2.7.0...v2.12.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=aiohttp-session&package-manager=pip&previous-version=2.7.0&new-version=2.12.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12499:6434,upgrade,upgrade,6434,https://hail.is,https://github.com/hail-is/hail/pull/12499,3,['upgrade'],['upgrade']
Deployability,"/a>, <a href=""https://github.com/alculquicondor""><code>@​alculquicondor</code></a>)</li>; <li>Kube-apiserver: Fixes handling of CRD schemas containing literal null values in enums. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104969"">kubernetes/kubernetes#104969</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>)</li>; <li>Kube-apiserver: The <code>rbac.authorization.k8s.io/v1alpha1</code> API version is removed; use the <code>rbac.authorization.k8s.io/v1</code> API, available since v1.8. The <code>scheduling.k8s.io/v1alpha1</code> API version is removed; use the <code>scheduling.k8s.io/v1</code> API, available since v1.14. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104248"">kubernetes/kubernetes#104248</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>)</li>; <li>Kube-scheduler: support for configuration file version <code>v1beta1</code> is removed. Update configuration files to v1beta2(xref: <a href=""https://github-redirect.dependabot.com/kubernetes/enhancements/issues/2901"">kubernetes/enhancements#2901</a>) or v1beta3 before upgrading to 1.23. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104782"">kubernetes/kubernetes#104782</a>, <a href=""https://github.com/kerthcet""><code>@​kerthcet</code></a>)</li>; <li>KubeSchedulerConfiguration provides a new field <code>MultiPoint</code> which will register a plugin for all valid extension points (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105611"">kubernetes/kubernetes#105611</a>, <a href=""https://github.com/damemi""><code>@​damemi</code></a>) [SIG Scheduling and Testing]</li>; <li>Kubelet should reject pods whose OS doesn't match the node's OS label. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105292"">kubernetes/kubernetes#105292</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>) [SIG Apps a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:8443,Update,Update,8443,https://hail.is,https://github.com/hail-is/hail/pull/11957,2,"['Update', 'configurat']","['Update', 'configuration']"
Deployability,"/a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.github.samtools:htsjdk&package-manager=gradle&previous-version=3.0.4&new-version=4.0.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13576:9778,upgrade,upgrade,9778,https://hail.is,https://github.com/hail-is/hail/pull/13576,3,['upgrade'],['upgrade']
Deployability,"/a></p>; <!-- raw HTML omitted -->; <h2>8.0.1</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v8.0.0...dc6113c360e05122430b8e130374e9f4e4b701d7"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Fix json_output in kernelspec app <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/921"">#921</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2023-01-26&amp;to=2023-01-26&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2023-01-26..2023-01-26&amp;type=Issues""><code>@​blink1073</code></a></p>; <h2>8.0.0</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.3.5...760a7835d8b20a9daea3737759b1751d5e55dad8"">Full Changelog</a>)</p>; <p>This release is primarily focused on improving <code>asyncio</code> support, while aiming to have minimal API changes.</p>; <h3>Enhancements made</h3>; <ul>; <li>Remove nest-asyncio dependency <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/835"">#835</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Bugs fixed</h3>; <ul>; <li>Allow interrupt during restart of pending kernels <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/898"">#898</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Fix connection reconciliation to handle restarts <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/882"">#882</a> (<a href=""https://github.com/kevin-bates""><code>@​kevin-bates</code></a>)</li>; <li>Reconcile connection information <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/879"">#879</a> (<a href=""https://github.com/kevin-bates""><code>@​kevin-bates",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12656:5174,release,release,5174,https://hail.is,https://github.com/hail-is/hail/pull/12656,1,['release'],['release']
Deployability,"/add-pyproject.toml</li>; <li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/9.5.0...10.0.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=9.5.0&new-version=10.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:16173,upgrade,upgrade,16173,https://hail.is,https://github.com/hail-is/hail/pull/13321,3,['upgrade'],['upgrade']
Deployability,"/aio-libs/aiorwlock/issues/250"">#250</a>)</li>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/63f68eb11d293a5e15133ef933304eddf61753e7""><code>63f68eb</code></a> Bump pytest-asyncio from 0.16.0 to 0.17.0 (<a href=""https://github-redirect.dependabot.com/aio-libs/aiorwlock/issues/249"">#249</a>)</li>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/34092ffd1f8927d68dc3e3e9f2a0bfddbdc3a382""><code>34092ff</code></a> Bump flake8-bugbear from 21.11.29 to 22.1.11 (<a href=""https://github-redirect.dependabot.com/aio-libs/aiorwlock/issues/248"">#248</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/aio-libs/aiorwlock/compare/v1.0.0...v1.3.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=aiorwlock&package-manager=pip&previous-version=1.0.0&new-version=1.3.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11514:3855,update,updates,3855,https://hail.is,https://github.com/hail-is/hail/pull/11514,1,['update'],['updates']
Deployability,"/aiohttp/issues/5219) &lt;https://github.com/aio-libs/aiohttp/issues/5219&gt;</code>_</p>; </li>; <li>; <p>Added <code>client_max_size</code> to <code>BaseRequest.clone()</code> to allow overriding the request body size. -- :user:<code>anesabml</code>.</p>; <p><code>[#5704](https://github.com/aio-libs/aiohttp/issues/5704) &lt;https://github.com/aio-libs/aiohttp/issues/5704&gt;</code>_</p>; </li>; <li>; <p>Added a middleware type alias <code>aiohttp.typedefs.Middleware</code>.</p>; <p><code>[#5898](https://github.com/aio-libs/aiohttp/issues/5898) &lt;https://github.com/aio-libs/aiohttp/issues/5898&gt;</code>_</p>; </li>; <li>; <p>Exported <code>HTTPMove</code> which can be used to catch any redirection request; that has a location -- :user:<code>dreamsorcerer</code>.</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/45b2c2c5773f0ee0d35fce8ff5716c78e91d9135""><code>45b2c2c</code></a> Release v3.9.0 (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7843"">#7843</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/5d59d3d6ac073a7db5e5d2234e03a67da5dec48a""><code>5d59d3d</code></a> Release v3.9.0rc0 (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7840"">#7840</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/c806814a8aaad1661d75e6e2b8d619d6c44d331d""><code>c806814</code></a> Release v3.9.0rc0 (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7838"">#7838</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/e07a1bdaacfb83fda3ea8f668edacb36c6c125df""><code>e07a1bd</code></a> Use timestamp instead of datetime to achieve faster cookie expiration… (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7837"">#7837</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/53476dfd4ef4fb1bb74a267714bbc39eda71b403""><code>53476df</code></a> Disa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14027:7005,Release,Release,7005,https://hail.is,https://github.com/hail-is/hail/pull/14027,6,['Release'],['Release']
Deployability,"/b490b5d51af6ed29709c357a00fcdb6bda26df78""><code>b490b5d</code></a> fix missing arg passed to C psutil_debug()</li>; <li><a href=""https://github.com/giampaolo/psutil/commit/eb2f74c153987b4e0d03aa16931d97e8137d9257""><code>eb2f74c</code></a> Fix CI tests / wheels / workflow (<a href=""https://github-redirect.dependabot.com/giampaolo/psutil/issues/2024"">#2024</a>)</li>; <li><a href=""https://github.com/giampaolo/psutil/commit/a1ae994cabff37eb86c6ca4564b4f193a73a7b0d""><code>a1ae994</code></a> fix <a href=""https://github-redirect.dependabot.com/giampaolo/psutil/issues/2023"">#2023</a> [Linux] cpu_freq() return order is wrong on systems with &gt; 9 CPUs.</li>; <li><a href=""https://github.com/giampaolo/psutil/commit/875d2195fc8efa642c7bca714d468551d1805c6c""><code>875d219</code></a> Handle missing dependencies on MidnightBSD (<a href=""https://github-redirect.dependabot.com/giampaolo/psutil/issues/2019"">#2019</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/giampaolo/psutil/compare/release-5.8.0...release-5.9.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=psutil&package-manager=pip&previous-version=5.8.0&new-version=5.9.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11459:6157,release,release-,6157,https://hail.is,https://github.com/hail-is/hail/pull/11459,1,['release'],['release-']
Deployability,"/b64ec22effafffc6a1371e544c560e6bfc24b56e""><code>b64ec22</code></a> Add explicit name in setup.py</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/4f8ef056177513ea599597d4089fed4275ae5d12""><code>4f8ef05</code></a> chore(deps): update actions/checkout action to v3 (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/121"">#121</a>)</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/6389c5acb15153df43b0681cb1333bbd892c3a16""><code>6389c5a</code></a> chore: allow automerge for official GitHub Actions</li>; <li>Additional commits viewable in <a href=""https://github.com/thibaudcolas/curlylint/compare/v0.12.0...v0.13.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=curlylint&package-manager=pip&previous-version=0.12.0&new-version=0.13.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11713:9401,update,updates,9401,https://hail.is,https://github.com/hail-is/hail/pull/11713,1,['update'],['updates']
Deployability,"/backport-10217-to-7.1.x</li>; <li>Additional commits viewable in <a href=""https://github.com/pytest-dev/pytest/compare/7.1.1...7.1.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pytest&package-manager=pip&previous-version=7.1.1&new-version=7.1.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12187:7385,upgrade,upgrade,7385,https://hail.is,https://github.com/hail-is/hail/pull/12187,3,['upgrade'],['upgrade']
Deployability,"/blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.13 (2022-11-23)</h2>; <ul>; <li>Deprecated the <code>HTTPResponse.getheaders()</code> and <code>HTTPResponse.getheader()</code> methods.</li>; <li>Fixed an issue where parsing a URL with leading zeroes in the port would be rejected; even when the port number after removing the zeroes was valid.</li>; <li>Fixed a deprecation warning when using cryptography v39.0.0.</li>; <li>Removed the <code>&lt;4</code> in the <code>Requires-Python</code> packaging metadata field.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/64b7f792c8ab62e301147d4115c4bca98529593a""><code>64b7f79</code></a> Release 1.26.13</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/977b8438970fdce695d9c0c41b03fcfe845fddc6""><code>977b843</code></a> Update publish workflow on 1.26.x to match main</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/64a3767e548f830fdc667abb1a943617d371a7e4""><code>64a3767</code></a> Bump cryptography to fix docs build</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/455960b7463fd719b96ea27be935809669faee22""><code>455960b</code></a> Support cryptography 39</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/955da4d03eaa6785aef40a34f440a67d736a4793""><code>955da4d</code></a> [1.26] Strip leading zeros from ports</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/b8c5d457fc42821b951ea58bec4ad685a0183c02""><code>b8c5d45</code></a> [1.26] Deprecate HTTPResponse.getheaders() and .getheader() methods</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/8b8e4b5a148d0eb706daf5ac48b4423b434495f5""><code>8b8e4b5</code></a> Temporary fix for SLSA generator</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/cc9b0dc10eaf83b1242d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12506:1757,Update,Update,1757,https://hail.is,https://github.com/hail-is/hail/pull/12506,1,['Update'],['Update']
Deployability,"/brettcannon/gidgethub/issues/74&gt;</code>_).</li>; <li>Add support for GitHub Actions Environment Files with :meth:<code>gidgethub.actions.setenv</code>; and :meth:<code>gidgethub.actions.addpath</code>.; (<code>Issue [#137](https://github.com/brettcannon/gidgethub/issues/137) &lt;https://github.com/brettcannon/gidgethub/issues/132&gt;</code>_).</li>; <li>Make router callback execution order non-deterministic to avoid relying on; registration order.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/9660d1e1c0187d9def32c473c8ceefcd130fe26f""><code>9660d1e</code></a> Add .DS_Store to .gitignore file</li>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/ef0368998fe40769f4f20a6c4b6ccfea27fe8ca9""><code>ef03689</code></a> Bump the version number</li>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/1f80a51670555acda0db0e42189d00bb58bb3b45""><code>1f80a51</code></a> Release 5.2.1</li>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/89ade8859539212e0663e91f0777ad8a39ecf323""><code>89ade88</code></a> Fix cgi and importlib_resources deprecations (<a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/185"">#185</a>)</li>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/64888cbe83e3f11af3c6f25294adff26dc2f557a""><code>64888cb</code></a> Add support for Python 3.11 and drop EOL Python 3.6 (<a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/184"">#184</a>)</li>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/8c60e56029b7e10b7be9879e64dfbf97bbeda2b8""><code>8c60e56</code></a> Add variable mapping to fix 'Session tests-3.10-dev skipped: Python interpret...</li>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/07040aa3d7dc3466308e92625bb889abe53ff0a9""><code>07040aa</code></a> Update a link</li>; <li><a href=""https://",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12328:7538,Release,Release,7538,https://hail.is,https://github.com/hail-is/hail/pull/12328,1,['Release'],['Release']
Deployability,"/code>: [<code>botocore</code>] AWS introduces the new Amazon EventBridge Scheduler. EventBridge Scheduler is a serverless scheduler that allows you to create, run, and manage tasks from one central, managed service.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/a177681a29a7dd039cf9dacce7bb810b748d27de""><code>a177681</code></a> Merge branch 'release-1.26.8'</li>; <li><a href=""https://github.com/boto/boto3/commit/894a5c591fa4b56f6e1dfa369948c3b6d25e4178""><code>894a5c5</code></a> Bumping version to 1.26.8</li>; <li><a href=""https://github.com/boto/boto3/commit/dde20184baf312a4f5ca7df08a0d7ce2c5c6e697""><code>dde2018</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/2d82a0c13d4510a5950dd24b4664e23584a5a364""><code>2d82a0c</code></a> Merge branch 'release-1.26.7'</li>; <li><a href=""https://github.com/boto/boto3/commit/b35796f0522b13bc2f9f293ec93697afe09873e2""><code>b35796f</code></a> Merge branch 'release-1.26.7' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/b0e241282f308cee430d340dee119af9100325ff""><code>b0e2412</code></a> Bumping version to 1.26.7</li>; <li><a href=""https://github.com/boto/boto3/commit/72c2893319ee63b513a31581ca9332b9530c7a22""><code>72c2893</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/01b1b2329ce790b0fda5d9f4bc80d2f416641790""><code>01b1b23</code></a> Merge branch 'release-1.26.6' into develop</li>; <li>See full diff in <a href=""https://github.com/boto/boto3/compare/1.26.6...1.26.8"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=boto3&package-manager=pip&previous-version=1.26.6&new-version=1.26.8)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12458:4534,release,release-,4534,https://hail.is,https://github.com/hail-is/hail/pull/12458,1,['release'],['release-']
Deployability,"/code>: [<code>botocore</code>] AWS introduces the new Amazon EventBridge Scheduler. EventBridge Scheduler is a serverless scheduler that allows you to create, run, and manage tasks from one central, managed service.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/fa603d91ec4b97a31c17ee28318b3e0d691377ce""><code>fa603d9</code></a> Merge branch 'release-1.26.9'</li>; <li><a href=""https://github.com/boto/boto3/commit/6e6df92c650a91b013bd82df8549ebae9d0a56f7""><code>6e6df92</code></a> Bumping version to 1.26.9</li>; <li><a href=""https://github.com/boto/boto3/commit/4d51ef07becef163a8c4fe1af2be1ba8b63b9979""><code>4d51ef0</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/a177681a29a7dd039cf9dacce7bb810b748d27de""><code>a177681</code></a> Merge branch 'release-1.26.8'</li>; <li><a href=""https://github.com/boto/boto3/commit/531635e52549072a6d847d4f25734d8d3c4f91fd""><code>531635e</code></a> Merge branch 'release-1.26.8' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/894a5c591fa4b56f6e1dfa369948c3b6d25e4178""><code>894a5c5</code></a> Bumping version to 1.26.8</li>; <li><a href=""https://github.com/boto/boto3/commit/dde20184baf312a4f5ca7df08a0d7ce2c5c6e697""><code>dde2018</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/2d82a0c13d4510a5950dd24b4664e23584a5a364""><code>2d82a0c</code></a> Merge branch 'release-1.26.7'</li>; <li><a href=""https://github.com/boto/boto3/commit/b35796f0522b13bc2f9f293ec93697afe09873e2""><code>b35796f</code></a> Merge branch 'release-1.26.7' into develop</li>; <li><a href=""https://github.com/boto/boto3/commit/b0e241282f308cee430d340dee119af9100325ff""><code>b0e2412</code></a> Bumping version to 1.26.7</li>; <li>Additional commits viewable in <a href=""https://github.com/boto/boto3/compare/1.26.6...1.26.9"">compare view</a></li>; </ul>; </details>; <b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12466:5837,release,release-,5837,https://hail.is,https://github.com/hail-is/hail/pull/12466,1,['release'],['release-']
Deployability,"/code>: [<code>botocore</code>] Update stepfunctions client to latest version</li>; <li>api-change:<code>transfer</code>: [<code>botocore</code>] Adds a NONE encryption algorithm type to AS2 connectors, providing support for skipping encryption of the AS2 message body when a HTTPS URL is also specified.</li>; </ul>; <h1>1.26.12</h1>; <ul>; <li>api-change:<code>amplify</code>: [<code>botocore</code>] Adds a new value (WEB_COMPUTE) to the Platform enum that allows customers to create Amplify Apps with Server-Side Rendering support.</li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow simplifies the preparation and cataloging of SaaS data into the AWS Glue Data Catalog where your data can be discovered and accessed by AWS analytics and ML services. AppFlow now also supports data field partitioning and file size optimization to improve query performance and reduce cost.</li>; <li>api-change:<code>appsync</code>: [<code>botocore</code>] This release introduces the APPSYNC_JS runtime, and adds support for JavaScript in AppSync functions and AppSync pipeline resolvers.</li>; <li>api-change:<code>dms</code>: [<code>botocore</code>] Adds support for Internet Protocol Version 6 (IPv6) on DMS Replication Instances</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/f38ce50a317baf6715870b2706100d43b80b0c73""><code>f38ce50</code></a> Merge branch 'release-1.26.16'</li>; <li><a href=""https://github.com/boto/boto3/commit/33d7d6f020510890b93edf49de3f81c0ba208cb3""><code>33d7d6f</code></a> Bumping version to 1.26.16</li>; <li><a href=""https://github.com/boto/boto3/commit/fb642196bd5dda0f48636e3eeae5f983835fcef5""><code>fb64219</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/cc2984fc4fe2a399404a81711eb9ece3fb8d6eb7""><code>cc2984f</code></a> Merge branch 'release-1.26.15'</li>; <l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12502:6302,release,release,6302,https://hail.is,https://github.com/hail-is/hail/pull/12502,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"/code>: [<code>botocore</code>] Update stepfunctions client to latest version</li>; <li>api-change:<code>transfer</code>: [<code>botocore</code>] Adds a NONE encryption algorithm type to AS2 connectors, providing support for skipping encryption of the AS2 message body when a HTTPS URL is also specified.</li>; </ul>; <h1>1.26.12</h1>; <ul>; <li>api-change:<code>amplify</code>: [<code>botocore</code>] Adds a new value (WEB_COMPUTE) to the Platform enum that allows customers to create Amplify Apps with Server-Side Rendering support.</li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow simplifies the preparation and cataloging of SaaS data into the AWS Glue Data Catalog where your data can be discovered and accessed by AWS analytics and ML services. AppFlow now also supports data field partitioning and file size optimization to improve query performance and reduce cost.</li>; <li>api-change:<code>appsync</code>: [<code>botocore</code>] This release introduces the APPSYNC_JS runtime, and adds support for JavaScript in AppSync functions and AppSync pipeline resolvers.</li>; <li>api-change:<code>dms</code>: [<code>botocore</code>] Adds support for Internet Protocol Version 6 (IPv6) on DMS Replication Instances</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds a new optional parameter &quot;privateIpAddress&quot; for the CreateNatGateway API. PrivateIPAddress will allow customers to select a custom Private IPv4 address instead of having it be auto-assigned.</li>; <li>api-change:<code>elbv2</code>: [<code>botocore</code>] Update elbv2 client to latest version</li>; <li>api-change:<code>emr-serverless</code>: [<code>botocore</code>] Adds support for AWS Graviton2 based applications. You can now select CPU architecture when creating new applications or updating existing ones.</li>; <li>api-change:<code>ivschat</code>: [<code>botocore</code>] Adds LoggingConfiguration APIs for IVS Chat - a feature that allows customers to sto",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12498:5831,release,release,5831,https://hail.is,https://github.com/hail-is/hail/pull/12498,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"/code></a> Release v0.13.1</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/5bd2922633791277f3073135f779fee3e6684bb4""><code>5bd2922</code></a> Update <code>patch_click</code> to fix compatibility issue with click 8.1.0. Fix <a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/132"">#132</a> (#...</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/e16056828523d9af3e13b67243d62830ff03d89d""><code>e160568</code></a> chore(deps): update actions/cache action to v3</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/bce95021c33e9206104512c412751ee435a6606b""><code>bce9502</code></a> chore(deps): update dependency prettier to v2.6.1</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/c7ebbaedf5bc6a8c562a46941681d7bc8598497b""><code>c7ebbae</code></a> chore(deps): update dependency prettier to v2.6.0</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/efd710b42cf0982582bb8a4f345ccfa967866b97""><code>efd710b</code></a> chore(deps): update dependency coverage to v6.3.2</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/438cf6131c1784de8bc9b34970beace1ec7c52af""><code>438cf61</code></a> Update tested Python versions on GitHub (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/122"">#122</a>)</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/b64ec22effafffc6a1371e544c560e6bfc24b56e""><code>b64ec22</code></a> Add explicit name in setup.py</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/4f8ef056177513ea599597d4089fed4275ae5d12""><code>4f8ef05</code></a> chore(deps): update actions/checkout action to v3 (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/121"">#121</a>)</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/6389c5acb15153df43b0681cb1333bbd892c3a16""><code>6389c5a</code></a> chore: allow automerge for official GitHub Actions</li>; <li>Additional com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11713:8009,update,update,8009,https://hail.is,https://github.com/hail-is/hail/pull/11713,1,['update'],['update']
Deployability,"/commit/ab96ea88e829af05e1491c30214b924c9553697b""><code>ab96ea8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/10258"">#10258</a> from pytest-dev/backport-10252-to-7.1.x</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/fc0e024b118fa63e84637bd5c9242b2b382e58fd""><code>fc0e024</code></a> [7.1.x] Fix regendoc</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/8f5088f4126b61ff76ac9809d5eb27cdbc31f07b""><code>8f5088f</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/10249"">#10249</a> from pytest-dev/backport-10231-to-7.1.x</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/aae93d6127c43a7f9036556ba7482019d389e21d""><code>aae93d6</code></a> Ignore type-errors related to attr.asdict</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/71b79fcda5313624544ae501a2045186c1c72244""><code>71b79fc</code></a> [7.1.x] Ignore editable installation modules</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/89f7518cb131579608387936c55f96cd4d3e9d3f""><code>89f7518</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/10222"">#10222</a> from pytest-dev/backport-10171-to-7.1.x</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/88fc45bd57d217a9dc6f53e419c94d28e2d5392f""><code>88fc45b</code></a> [7.1.x] Update fixtures.rst w/ finalizer order</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/d0b53d6ba7b41dcb200a35af5f5733f591cd929b""><code>d0b53d6</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/10221"">#10221</a> from pytest-dev/backport-10217-to-7.1.x</li>; <li>Additional commits viewable in <a href=""https://github.com/pytest-dev/pytest/compare/7.1.1...7.1.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_scor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12187:5033,install,installation,5033,https://hail.is,https://github.com/hail-is/hail/pull/12187,1,['install'],['installation']
Deployability,"/commit/ad0d0f907010fbc8b66cdbe8ce0af2683881a309""><code>ad0d0f9</code></a> REL: set 1.9.2 released [wheel build]</li>; <li><a href=""https://github.com/scipy/scipy/commit/d9ad9801323653a2015b4d3e80d6d3ea93b6c021""><code>d9ad980</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17150"">#17150</a> from tylerjereddy/treddy_scipy_192_more_backports</li>; <li><a href=""https://github.com/scipy/scipy/commit/6b098c25223e224ff44101f86bbc86efecffe1d9""><code>6b098c2</code></a> TST: optimize.milp: remove problematic timeout/iteration test</li>; <li><a href=""https://github.com/scipy/scipy/commit/24dce9760b87934f1be046ec817c758b0f3952dc""><code>24dce97</code></a> DOC: stats.pearsonr: typo in coeffic<em>i</em>ent (<a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17153"">#17153</a>)</li>; <li><a href=""https://github.com/scipy/scipy/commit/a6ba7cad3b54c35d2ccb55c595691689004742c1""><code>a6ba7ca</code></a> MAINT: misc 1.9.2 updates</li>; <li><a href=""https://github.com/scipy/scipy/commit/ed9760e60a28b8f13e5644494033e2dab9aafbcd""><code>ed9760e</code></a> MAINT: stats.pearson3: fix ppf for negative skew (<a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17055"">#17055</a>)</li>; <li><a href=""https://github.com/scipy/scipy/commit/6fb67007dd7105755057f3379fb7ef423eae524e""><code>6fb6700</code></a> FIX: optimize.milp: return feasible solution if available on timeout/node lim...</li>; <li><a href=""https://github.com/scipy/scipy/commit/bcfce27fc061cbde6ac6531799362e0420ea4796""><code>bcfce27</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17132"">#17132</a> from tylerjereddy/treddy_192_backports</li>; <li><a href=""https://github.com/scipy/scipy/commit/2bc973a2c28c4b6b5bea0e288631834fe34b526e""><code>2bc973a</code></a> BLD: set version to 1.9.2.dev0 (and trigger wheel build CI)</li>; <li>Additional commits viewable in <a href=""https://github.com/scipy/scipy/compare/v1.2.1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12352:2391,update,updates,2391,https://hail.is,https://github.com/hail-is/hail/pull/12352,1,['update'],['updates']
Deployability,"/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3142,deploy,deploy-config,3142,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['deploy'],['deploy-config']
Deployability,"/d03f201762df7138c6da157b5cbb8e634acef45f""><code>d03f201</code></a> Suggest using upper bound for unbound tvar (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/13730"">#13730</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/5b17cc6c393280326ed15d763e599cbaeefbc0e6""><code>5b17cc6</code></a> Fix overload overlap check for UninhabitedType (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/13461"">#13461</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/c7b4714e1f5e3cb8f3fec7426b6538fe1a3dcab1""><code>c7b4714</code></a> Update version to 0.981</li>; <li><a href=""https://github.com/python/mypy/commit/2bd7da21462a59643f2aec546304db1a624ba285""><code>2bd7da2</code></a> [0.980 backport] build changes (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/13688"">#13688</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/2b2953a1392368f623331d5168ccdfd39e37bbee""><code>2b2953a</code></a> [0.980 backport] Update pos-only unit tests for Python 3.10.7 (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/13660"">#13660</a>) (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/13665"">#13665</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/ada007841f6a96f68d114769624a0f7b523814a7""><code>ada0078</code></a> Remove dev from version</li>; <li><a href=""https://github.com/python/mypy/commit/efd1d38fb1db188e56fe6068ebe69d2164462b34""><code>efd1d38</code></a> [0.980 backport] Fix stubtest custom_typeshed_dir regression (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/13656"">#13656</a>) (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/13658"">#13658</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/70bc34837ecbafc528e511a46219262736781d43""><code>70bc348</code></a> [0.980 backport] Allow unpacking from TypeVars with iterable bounds (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/13425"">#13425</a>) ...</li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12291:1577,Update,Update,1577,https://hail.is,https://github.com/hail-is/hail/pull/12291,1,['Update'],['Update']
Deployability,"/details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **461/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.5 | Generation of Error Message Containing Sensitive Information <br/>[SNYK-PYTHON-JUPYTERSERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3MThjYjgyZC1jNGU3LTRlNWEtODgzZi02NjQ0NjlmYzA4MGEiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjcxOGNiODJkLWM0ZTctNGU1YS04ODNmLTY2NDQ2OWZjMDgwYSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14070:1816,upgrade,upgraded,1816,https://hail.is,https://github.com/hail-is/hail/pull/14070,1,['upgrade'],['upgraded']
Deployability,"/discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-63d60cc. ### What you did:; Tried to run sample_qc on mt table imported from Delly vcf. ; hl.sample_qc(ds). ### What went wrong (all error messages here, including the full java stack trace):. The sample_qc had a problem when alt ref is <DEL>. ```; [Stage 3:> (0 + 140) / 415]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail.qc/delly-qc.py"", line 35, in <module>; ds = hl.sample_qc(ds); File ""<decorator-gen-902>"", line 2, in sample_qc; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 490, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/methods/qc.py"", line 91, in sample_qc; return MatrixTable(Env.hail().methods.SampleQC.apply(require_biallelic(dataset, 'sample_qc')._jvds, name)); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: invalid allele ""<DEL>"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 160, scc-q01.scc.bu.edu, executor 4): is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCombiner$.alleleIndices(SampleQC.scala:44); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413:1111,install,install,1111,https://hail.is,https://github.com/hail-is/hail/issues/3413,1,['install'],['install']
Deployability,/dist-packages/hail/backend/service_backend.py:477: FatalError; ------------------------------ Captured log call -------------------------------; INFO batch_client.aioclient:aioclient.py:753 created batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 _make_tsm: found 1000 variants after filtering out monomorphic sites.; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 1/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 2/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 3/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 4/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 5/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 6/10; INFO batch_client.aioclient:aioclient.py:770 updated,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12980:5633,update,updated,5633,https://hail.is,https://github.com/hail-is/hail/issues/12980,1,['update'],['updated']
Deployability,"/e6f6869fddbf307e3267ac164561c669039d557a""><code>e6f6869</code></a> Release v0.13.0 (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/78"">#78</a>)</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/2afa4b2934c3b61b96259d548c26cd49e03daf24""><code>2afa4b2</code></a> Implement --template-tags CLI flag</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/28316eea40acb6f67f267a0fa1545d81fb3ed59a""><code>28316ee</code></a> Re-add repo token</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/27b6bf942a0e9205d918efff1c6bab87ed80a460""><code>27b6bf9</code></a> Add website .nvmrc for Netlify</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/fd8964ab2ecc7662c6b88575f5b89de7a2fbde3d""><code>fd8964a</code></a> Revert &quot;Switch to official GitHub Actions coveralls integration&quot;</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/bc4ec76844853c0fe57b595b18116b2ca33b10bd""><code>bc4ec76</code></a> Use correct lcov path for Coveralls integration</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/7a081706e70480e23ea27c567cbc25121489938e""><code>7a08170</code></a> Add more basic CLI tests</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/08bdb0ce9e044e008eb487f32162865740c25232""><code>08bdb0c</code></a> Switch to official GitHub Actions coveralls integration</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/3a92cc4b4e6f5951bea72234f57b32bef133ab75""><code>3a92cc4</code></a> Tentatively declare support for Python 3.10</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/145983f7a3764743a653ef61595dd3ea33f24620""><code>145983f</code></a> Declare support for Python 3.9</li>; <li>Additional commits viewable in <a href=""https://github.com/thibaudcolas/curlylint/compare/v0.12.0...v0.13.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/bad",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11494:6337,integrat,integration,6337,https://hail.is,https://github.com/hail-is/hail/pull/11494,2,['integrat'],['integration']
Deployability,"/em> mode. If you're upgrading from v0.19 and you haven't configured <code>asyncio_mode = legacy</code>, you can upgrade without taking any additional action. If you're upgrading from an earlier version or you have explicitly enabled <em>legacy</em> mode, you need to switch to <em>auto</em> or <em>strict</em> mode before upgrading to this version.</li>; <li>Deprecate use of pytest v6.</li>; <li>Fixed an issue which prevented fixture setup from being cached. <code>[#404](https://github.com/pytest-dev/pytest-asyncio/issues/404) &lt;https://github.com/pytest-dev/pytest-asyncio/pull/404&gt;</code>_</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/c8d017407d39dd81d6864fa9a58ba1240d54be9f""><code>c8d0174</code></a> fix: Do not warn about outdated pytest version when pytest&gt;=7 is installed. (...</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/6450ddbe974f5359d56317ba8bdda8b2ab48655a""><code>6450ddb</code></a> Prepare release of v0.20.0. (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-asyncio/issues/428"">#428</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/150f29c107fbd76641de47e040d43840769ef92c""><code>150f29c</code></a> Build(deps): Bump hypothesis in /dependencies/default (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-asyncio/issues/427"">#427</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/adc88090f341d9872e9e9b4d22a94cdadf60b3bc""><code>adc8809</code></a> Build(deps): Bump typing-extensions in /dependencies/default (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-asyncio/issues/425"">#425</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/4abf9d1df228ed8b083721d7affa73e4a08d13c3""><code>4abf9d1</code></a> Build(deps): Bump zipp from 3.8.1 to 3.9.0 in /dependencies/default (<a href=""https://github-redirect.dependabo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12390:4205,release,release,4205,https://hail.is,https://github.com/hail-is/hail/pull/12390,1,['release'],['release']
Deployability,"/em>; :issue:<code>8014</code>.</p>; </li>; <li>; <p>Added runtime type check for <code>ClientSession</code> <code>timeout</code> parameter.</p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>8021</code>.</p>; </li>; <li>; <p>Fixed an unhandled exception in the Python HTTP parser on header lines starting with a colon -- by :user:<code>pajod</code>.</p>; <p>Invalid request lines with anything but a dot between the HTTP major and minor version are now rejected.; Invalid header field names containing question mark or slash are now rejected.; Such requests are incompatible with :rfc:<code>9110#section-5.6.2</code> and are not known to be of any legitimate use.</p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>8074</code>.</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/24a6d64966d99182e95f5d3a29541ef2fec397ad""><code>24a6d64</code></a> Release v3.9.2 (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8082"">#8082</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/9118a5831e8a65b8c839eb7e4ac983e040ff41df""><code>9118a58</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8079"">#8079</a>/1c335944 backport][3.9] Validate static paths (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8080"">#8080</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/435ad46e6c26cbf6ed9a38764e9ba8e7441a0e3b""><code>435ad46</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/3955"">#3955</a>/8960063e backport][3.9] Replace all tmpdir fixtures with tmp_path (...</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/d33bc21414e283c9e6fe7f6caf69e2ed60d66c82""><code>d33bc21</code></a> Improve validation in HTTP parser (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8074"">#8074</a>) (<a hr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14212:3827,Release,Release,3827,https://hail.is,https://github.com/hail-is/hail/pull/14212,6,['Release'],['Release']
Deployability,"/em></p>; <blockquote>; <h2>1.18.2</h2>; <h2>What's Changed</h2>; <ul>; <li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/230"">tox-dev/sphinx-autodoc-typehints#230</a></li>; <li>Support and require nptyping 2.1.1 by <a href=""https://github.com/gaborbernat""><code>@​gaborbernat</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/232"">tox-dev/sphinx-autodoc-typehints#232</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2"">https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2</a></p>; <h2>1.18.1</h2>; <p>No release notes provided.</p>; <h2>1.18.0</h2>; <p>No release notes provided.</p>; <h2>1.17.1</h2>; <p>No release notes provided.</p>; <h2>typehints_use_rtype support and handle TypeError</h2>; <p>No release notes provided.</p>; <h2>1.16.0</h2>; <p>No release notes provided.</p>; <h2>1.15.3</h2>; <p>No release notes provided.</p>; <h2>1.15.2</h2>; <p>No release notes provided.</p>; <h2>1.15.1</h2>; <p>No release notes provided.</p>; <h2>1.15.0</h2>; <p>No release notes provided.</p>; <h2>1.14.1</h2>; <p>No release notes provided.</p>; <h2>Added document_defaults config option</h2>; <p>No release notes provided.</p>; <h2>Fix NewType is inserting a reference as first argument</h2>; <p>No release notes provided.</p>; <h2>Python 3.10 support and PEP-563, drop 3.6</h2>; <p>No release notes provided.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/blob/main/CHANGELOG.md"">sphinx-autodoc-typehints's changelog</a>.</em></p>; <blockquote>; <h2>1.18.2</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2.1.1</code></li>; </ul>; <h2>1.18.1</h2>; <ul>; <li>Fix moc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11893:1311,release,release,1311,https://hail.is,https://github.com/hail-is/hail/pull/11893,1,['release'],['release']
Deployability,"/en/3.1.x/changes/#version-3-1-0"">https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-0</a></li>; <li>Milestone: <a href=""https://github.com/pallets/jinja/milestone/8?closed=1"">https://github.com/pallets/jinja/milestone/8?closed=1</a></li>; <li>MarkupSafe changes: <a href=""https://markupsafe.palletsprojects.com/en/2.1.x/changes/#version-2-1-1"">https://markupsafe.palletsprojects.com/en/2.1.x/changes/#version-2-1-1</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/blob/main/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 3.1.2</h2>; <p>Released 2022-04-28</p>; <ul>; <li>Add parameters to <code>Environment.overlay</code> to match <code>__init__</code>.; :issue:<code>1645</code></li>; <li>Handle race condition in <code>FileSystemBytecodeCache</code>. :issue:<code>1654</code></li>; </ul>; <h2>Version 3.1.1</h2>; <p>Released 2022-03-25</p>; <ul>; <li>The template filename on Windows uses the primary path separator.; :issue:<code>1637</code></li>; </ul>; <h2>Version 3.1.0</h2>; <p>Released 2022-03-24</p>; <ul>; <li>; <p>Drop support for Python 3.6. :pr:<code>1534</code></p>; </li>; <li>; <p>Remove previously deprecated code. :pr:<code>1544</code></p>; <ul>; <li><code>WithExtension</code> and <code>AutoEscapeExtension</code> are built-in now.</li>; <li><code>contextfilter</code> and <code>contextfunction</code> are replaced by; <code>pass_context</code>. <code>evalcontextfilter</code> and; <code>evalcontextfunction</code> are replaced by <code>pass_eval_context</code>.; <code>environmentfilter</code> and <code>environmentfunction</code> are replaced; by <code>pass_environment</code>.</li>; <li><code>Markup</code> and <code>escape</code> should be imported from MarkupSafe.</li>; <li>Compiled templates from very old Jinja versions may need to be; recompiled.</li>; <li>Legacy resolve mode for <code>Context</code> subclasses is no longer; supporte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12173:2559,Release,Released,2559,https://hail.is,https://github.com/hail-is/hail/pull/12173,1,['Release'],['Released']
Deployability,"/f0dd67f9b7cc2791f301f3fd135f0c97d9c66bae""><code>f0dd67f</code></a> Merge branch 'release-1.29.16'</li>; <li><a href=""https://github.com/boto/botocore/commit/22c3cb362c0ef00c6de404140f06a14d0e195f39""><code>22c3cb3</code></a> Bumping version to 1.29.16</li>; <li><a href=""https://github.com/boto/botocore/commit/4aa5f864b62b6193ed0729a4ac71c010877fe377""><code>4aa5f86</code></a> Update to latest models</li>; <li><a href=""https://github.com/boto/botocore/commit/a7e153d9c822fae5c55d30ef476bdf4f55a4d027""><code>a7e153d</code></a> Merge branch 'release-1.29.15'</li>; <li><a href=""https://github.com/boto/botocore/commit/a942b57854dd35a37766d7973c3fb980a2de4068""><code>a942b57</code></a> Merge branch 'release-1.29.15' into develop</li>; <li><a href=""https://github.com/boto/botocore/commit/4d972ecc0b008bc6e8d9dbacc285524a5fa82e3f""><code>4d972ec</code></a> Bumping version to 1.29.15</li>; <li><a href=""https://github.com/boto/botocore/commit/4e5ec200284b1bbce09a973de61de159888dd657""><code>4e5ec20</code></a> Update to latest partitions and endpoints</li>; <li><a href=""https://github.com/boto/botocore/commit/238d938b4f519a5efce58403848fd480cf99d331""><code>238d938</code></a> Update to latest models</li>; <li><a href=""https://github.com/boto/botocore/commit/7b4b3bbb13a5d59097e6d5f178de58e280fdb553""><code>7b4b3bb</code></a> Resolve endpoint with default partition when no region is set (<a href=""https://github-redirect.dependabot.com/boto/botocore/issues/2818"">#2818</a>)</li>; <li><a href=""https://github.com/boto/botocore/commit/cc3f1c22f55ba50ca792eb73e7a6f721abdcc5ee""><code>cc3f1c2</code></a> Fix: S3 Object Lambda requests miss x-amz-content-sha256 headers (<a href=""https://github-redirect.dependabot.com/boto/botocore/issues/2819"">#2819</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/boto/botocore/compare/1.29.13...1.29.16"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12503:2697,Update,Update,2697,https://hail.is,https://github.com/hail-is/hail/pull/12503,1,['Update'],['Update']
Deployability,"/github-redirect.dependabot.com/grpc/grpc/issues/30326"">#30326</a>)</li>; <li><a href=""https://github.com/grpc/grpc/commit/4c51abf12053e3c43a62059c693322ea992b35ce""><code>4c51abf</code></a> Bump version to 1.48.0-pre1 (on v1.48.x branch) (<a href=""https://github-redirect.dependabot.com/grpc/grpc/issues/30194"">#30194</a>)</li>; <li><a href=""https://github.com/grpc/grpc/commit/46bd0be2c99aa8228ec5d93d8a27f20ab0c61956""><code>46bd0be</code></a> Bump core version to 26.0.0 for upcoming release (<a href=""https://github-redirect.dependabot.com/grpc/grpc/issues/30163"">#30163</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/grpc/grpc/compare/v1.47.0...v1.48.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=grpcio&package-manager=pip&previous-version=1.47.0&new-version=1.48.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12201:6406,update,updates,6406,https://hail.is,https://github.com/hail-is/hail/pull/12201,1,['update'],['updates']
Deployability,"/github-redirect.dependabot.com/jaraco/zipp/issues/62"">#62</a>)</li>; <li><a href=""https://github.com/jaraco/zipp/commit/a4f5b769793af19f7b858816889c1bf026f55f5c""><code>a4f5b76</code></a> Update base URL for PEPs (<a href=""https://github-redirect.dependabot.com/jaraco/zipp/issues/61"">#61</a>)</li>; <li><a href=""https://github.com/jaraco/zipp/commit/10bf1b1fb9e09e9836bea9e2edec620cd9eea7f9""><code>10bf1b1</code></a> Add Python 3.11 into the matrix using workaround from <a href=""https://github-redirect.dependabot.com/actions/setup-python/issues/21"">actions/setup-python#21</a>...</li>; <li>Additional commits viewable in <a href=""https://github.com/jaraco/zipp/compare/v3.8.0...v3.8.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=zipp&package-manager=pip&previous-version=3.8.0&new-version=3.8.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12108:2942,update,updates,2942,https://hail.is,https://github.com/hail-is/hail/pull/12108,1,['update'],['updates']
Deployability,"/github-redirect.dependabot.com/plotly/plotly.py/issues/3518"">#3518</a></li>; <li>Deprecated <code>ff.create_annotated_heatmap</code>, <code>ff.create_county_choropleth</code>, <code>ff.create_gantt</code> <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/issues/3518"">#3518</a></li>; <li><code>div_id</code> argument to <code>pio.to_html</code>, <code>pio.write_html</code>, <code>fig.to_html</code> and <code>fig.write_html</code> to optionally make its IDs deterministic <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/issues/3487"">#3487</a> with thanks to <a href=""https://github.com/Skn0tt""><code>@​Skn0tt</code></a></li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fixed ValueError when <code>ff.create_annotated_heatmap</code> passes <code>rgba()</code> colors into <code>to_rgb_color_list</code> <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/issues/3478"">#3478</a> with thanks to <a href=""https://github.com/janosh""><code>@​janosh</code></a></li>; </ul>; <h3>Updated</h3>; <ul>; <li>Updated Plotly.js to from version 2.6.3 to version 2.8.3. See the <a href=""https://github.com/plotly/plotly.js/blob/master/CHANGELOG.md#280----2021-12-10"">plotly.js CHANGELOG</a> for more information. Notable changes include:; <ul>; <li>Horizontal color bars</li>; <li><code>texttemplate</code> for histogram-like and heatmap-like traces</li>; </ul>; </li>; </ul>; <h2>[5.4.0] - 2021-11-15</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed error when serializing dict with mix of string and non-string keys <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/issues/3380"">#3380</a></li>; </ul>; <h3>Updated</h3>; <ul>; <li>The JSON serialization engines no longer sort their keys <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/issues/3380"">#3380</a></li>; <li>Updated Plotly.js to from version 2.4.2 to version 2.6.3. See the <a href=""https://github.com/plotly/plotly.js/blob/master/CHANGELOG.md#263----2021-11-12"">plotly.js CHANGELOG</a> for more i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11535:2651,Update,Updated,2651,https://hail.is,https://github.com/hail-is/hail/pull/11535,2,['Update'],['Updated']
Deployability,"/github-redirect.dependabot.com/spyder-ide/qtpy/pull/353"">PR 353</a> - PR: Add note to readme about use with Pyright, by <a href=""https://github.com/CAM-Gerlach""><code>@​CAM-Gerlach</code></a> (<a href=""https://github-redirect.dependabot.com/spyder-ide/qtpy/issues/352"">352</a>)</li>; <li><a href=""https://github-redirect.dependabot.com/spyder-ide/qtpy/pull/350"">PR 350</a> - PR: Restore <code>WEBENGINE</code> constant in <code>QtWebEngineWidgets</code>, by <a href=""https://github.com/ccordoba12""><code>@​ccordoba12</code></a></li>; <li><a href=""https://github-redirect.dependabot.com/spyder-ide/qtpy/pull/346"">PR 346</a> - PR: Add workaround for <code>mode</code> argument in QTextCursor.movePosition (PySide6), by <a href=""https://github.com/rear1019""><code>@​rear1019</code></a></li>; <li><a href=""https://github-redirect.dependabot.com/spyder-ide/qtpy/pull/344"">PR 344</a> - PR: Add missing imports and modules, by <a href=""https://github.com/DaelonSuzuka""><code>@​DaelonSuzuka</code></a></li>; </ul>; <p>In this release 7 pull requests were closed.</p>; <hr />; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/spyder-ide/qtpy/commit/7841f4d398c9dbdc3b479a1df9f997dc14101088""><code>7841f4d</code></a> Release 2.2.0</li>; <li><a href=""https://github.com/spyder-ide/qtpy/commit/78ee3de13b7eba94cffa5ccc508a6da07cb693aa""><code>78ee3de</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/spyder-ide/qtpy/issues/357"">#357</a> from dalthviz/fixes_issue_61</li>; <li><a href=""https://github.com/spyder-ide/qtpy/commit/469e8d1229a9f8c40b52aa7c0cba59016dd21809""><code>469e8d1</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/spyder-ide/qtpy/issues/358"">#358</a> from tlambert03/typing</li>; <li><a href=""https://github.com/spyder-ide/qtpy/commit/4ae7625f55b17f7eda6290c5c05c7fad12854432""><code>4ae7625</code></a> Update qtpy/QtCore.py</li>; <li><a href=""https://github.com/spyder-ide/qtpy/com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12194:6294,release,release,6294,https://hail.is,https://github.com/hail-is/hail/pull/12194,1,['release'],['release']
Deployability,"/github.com/michel-kraemer/gradle-download-task/commit/b3fa29f9ffb4d4544e13ef84601e371fb2778ddf""><code>b3fa29f</code></a> Revert &quot;Update Apache HttpClient to 5.2.1&quot;</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/01f05e046be0dca18f506723c79e88f208336e71""><code>01f05e0</code></a> Add integration tests for Gradle 6.9.3 and 7.6</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a998a544908a8b39f713f4526f717fcb328c06eb""><code>a998a54</code></a> Upgrade Gradle to 7.6</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/5.3.0...5.3.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=5.3.0&new-version=5.3.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:3571,update,updates,3571,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['update'],['updates']
Deployability,"/github.com/psf/black/commit/afed2c01903465f9a486ac481a66aa3413cc1b01""><code>afed2c0</code></a> Load .gitignore and exclude regex at time of use</li>; <li><a href=""https://github.com/psf/black/commit/e269f44b25737360e0dc65379f889dfa931dc68a""><code>e269f44</code></a> Lazily import parallelized format modules</li>; <li><a href=""https://github.com/psf/black/commit/c47b91f513052cd39b818ea7c19716423c85c04e""><code>c47b91f</code></a> Fix misdetection of project root with <code>--stdin-filename</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3216"">#3216</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/psf/black/compare/22.3.0...22.8.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=black&package-manager=pip&previous-version=22.3.0&new-version=22.8.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12174:11562,update,updates,11562,https://hail.is,https://github.com/hail-is/hail/pull/12174,1,['update'],['updates']
Deployability,"/github.com/pygments/pygments/blob/master/CHANGES"">pygments's changelog</a>.</em></p>; <blockquote>; <h2>Version 2.15.1</h2>; <p>(released April 18th, 2023)</p>; <ul>; <li>; <p>Updated lexers:</p>; <ul>; <li>Java properties: Fix catastrophic backtracking (<a href=""https://redirect.github.com/pygments/pygments/issues/2356"">#2356</a>, <a href=""https://redirect.github.com/pygments/pygments/issues/2404"">#2404</a>)</li>; </ul>; </li>; <li>; <p>Fix Python console traceback lexing being too strict; and sometimes reordering output (<a href=""https://redirect.github.com/pygments/pygments/issues/2407"">#2407</a>, <a href=""https://redirect.github.com/pygments/pygments/issues/2410"">#2410</a>, <a href=""https://redirect.github.com/pygments/pygments/issues/2412"">#2412</a>)</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pygments/pygments/commit/04a75bd5a75bfe27f0b582dd83c85e62f9475581""><code>04a75bd</code></a> Prepare 2.15.1 release.</li>; <li><a href=""https://github.com/pygments/pygments/commit/fdf182a7af85b1deeeb637ca970d31935e7c9d52""><code>fdf182a</code></a> Improve Java properties lexer (<a href=""https://redirect.github.com/pygments/pygments/issues/2404"">#2404</a>)</li>; <li><a href=""https://github.com/pygments/pygments/commit/c5a2b23adaaadc08a7586a5eda72e9f7d6171012""><code>c5a2b23</code></a> Update CHANGES</li>; <li><a href=""https://github.com/pygments/pygments/commit/c97762448b1e4eac8d74b8d88415f23c32aa0cdd""><code>c977624</code></a> Refactor PythonConsoleLexer as a DelegatingLexer (<a href=""https://redirect.github.com/pygments/pygments/issues/2412"">#2412</a>)</li>; <li><a href=""https://github.com/pygments/pygments/commit/50dd4d80e25c4c4afab503d41b471a536ed2af13""><code>50dd4d8</code></a> Python console: do not require output that looks like a traceback to be valid...</li>; <li><a href=""https://github.com/pygments/pygments/commit/96a0cdf200ab8a36dc5f6f748f3b9d01c05cb91b""><code>96a0cdf</code></a> PythonTra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12909:1991,release,release,1991,https://hail.is,https://github.com/hail-is/hail/pull/12909,1,['release'],['release']
Deployability,"/github.com/python-jsonschema/jsonschema/commit/76b2e597d691e4cf5e9ebb7f3d1cff4f5da0115a""><code>76b2e59</code></a> Merge commit '095a009acc1938caf9596085d5581e7196021f66'</li>; <li><a href=""https://github.com/python-jsonschema/jsonschema/commit/aeecae37b17b430c328d3c3e15bec90d30c8848b""><code>aeecae3</code></a> Squashed 'json/' changes from d40b3e62f..cf78d97d0</li>; <li><a href=""https://github.com/python-jsonschema/jsonschema/commit/2f3a79c61176f60c9244d07fa8afb728218270ff""><code>2f3a79c</code></a> Merge commit 'aeecae37b17b430c328d3c3e15bec90d30c8848b'</li>; <li>Additional commits viewable in <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.6.0...v4.6.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jsonschema&package-manager=pip&previous-version=4.6.0&new-version=4.6.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11981:4250,update,updates,4250,https://hail.is,https://github.com/hail-is/hail/pull/11981,1,['update'],['updates']
Deployability,"/github.com/sass/libsass-python/releases"">libsass's releases</a>.</em></p>; <blockquote>; <h2>libsass 0.21.0</h2>; <ul>; <li>Fix build on OpenBSD. [#310 by Denis Fondras].</li>; <li>Produce abi3 wheels on windows. [#322 by Anthony Sottile]</li>; <li>Make the manpage build reproducible. [#319 by Chris Lamb]</li>; <li>Follow up the libsass upstream: 3.6.5 --- See the release notes of LibSass <a href=""https://github.com/sass/libsass/releases/tag/3.6.5"">3.6.5</a>. [#344 by Anthony Sottile]</li>; </ul>; <h2>libsass 0.20.1</h2>; <ul>; <li>(no changes, re-releasing to test build automation)</li>; </ul>; <h2>libsass 0.20.0</h2>; <ul>; <li>Produce abi3 wheels on macos / linux [#307 by Anthony Sottile]</li>; <li>Follow up the libsass upstream: 3.6.4 --- See the release notes of LibSass <a href=""https://github.com/sass/libsass/releases/tag/3.6.4"">3.6.4</a>. [#313 by Anthony Sottile]</li>; </ul>; <h2>libsass 0.19.4</h2>; <ul>; <li>Follow up the libsass upstream: 3.6.3 --- See the release notes of LibSass <a href=""https://github.com/sass/libsass/releases/tag/3.6.3"">3.6.3</a>. [#304 by Anthony Sottile]</li>; </ul>; <h2>libsass 0.19.3</h2>; <ul>; <li>Follow up the libsass upstream: 3.6.2 --- See the release notes of LibSass <a href=""https://github.com/sass/libsass/releases/tag/3.6.2"">3.6.2</a>. [#302 by Anthony Sottile]</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/sass/libsass-python/blob/master/docs/changes.rst"">libsass's changelog</a>.</em></p>; <blockquote>; <h1>Changelog</h1>; <h2>Version 0.21.1</h2>; <p>Released on May 20, 2021.</p>; <ul>; <li>Fix build on OpenBSD. [:issue:<code>310</code> by Denis Fondras].</li>; <li>Produce abi3 wheels on windows. [:issue:<code>322</code> by Anthony Sottile]</li>; <li>Make the manpage build reproducible. [:issue:<code>319</code> by Chris Lamb]</li>; <li>Follow up the libsass upstream: 3.6.5 --- See the release notes of LibSass; 3.6.5__. [:issue:<code>344</cod",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11508:1144,release,release,1144,https://hail.is,https://github.com/hail-is/hail/pull/11508,1,['release'],['release']
Deployability,"/github.com/scipy/scipy/commit/b5d8bab88af61d61de09641243848df63380a67f""><code>b5d8bab</code></a> REL: 1.8.0 release commit.</li>; <li><a href=""https://github.com/scipy/scipy/commit/d84f731d4df03915ce3dd8013a86eab0db5ec60e""><code>d84f731</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/15521"">#15521</a> from tylerjereddy/treddy_prep_180_final</li>; <li><a href=""https://github.com/scipy/scipy/commit/315dd5340cd2825682087f195663d2f63deb3f24""><code>315dd53</code></a> DOC: update 1.8.0 relnotes.</li>; <li><a href=""https://github.com/scipy/scipy/commit/b54b7ae0240cad126a1a9baf2ed71c97c15ddc15""><code>b54b7ae</code></a> MAINT: fix broken link and remove CI badges</li>; <li><a href=""https://github.com/scipy/scipy/commit/920e27b282583531cbf5b5ad348845f0c4bddeed""><code>920e27b</code></a> REL: 1.8.0 unreleased.</li>; <li><a href=""https://github.com/scipy/scipy/commit/ea004bd338738183ff4761427246198f84071ba1""><code>ea004bd</code></a> REL: 1.8.0rc4 released.</li>; <li><a href=""https://github.com/scipy/scipy/commit/4f3969d70fd5cd8f7fcc7ffb207879c4d482b642""><code>4f3969d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/15479"">#15479</a> from tylerjereddy/treddy_180rc4</li>; <li><a href=""https://github.com/scipy/scipy/commit/8ed6aa905efebe9c6eed7cd026c7dc10f87c725d""><code>8ed6aa9</code></a> DOC: update 1.8.0 relnotes.</li>; <li><a href=""https://github.com/scipy/scipy/commit/efe4ca57dd5923717a238081ff856444b51b825d""><code>efe4ca5</code></a> MAINT: PR 15479 revisions</li>; <li><a href=""https://github.com/scipy/scipy/commit/180391391388a3f288abe3b77c987dffb7e39d32""><code>1803913</code></a> MAINT: remove non-default settings (except <code>shallow</code>) in <code>.gitmodules</code></li>; <li>Additional commits viewable in <a href=""https://github.com/scipy/scipy/compare/v1.2.1...v1.8.0"">compare view</a></li>; </ul>; </details>; <br />. Dependabot will resolve any conflicts with this PR as lon",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11538:3683,release,released,3683,https://hail.is,https://github.com/hail-is/hail/pull/11538,1,['release'],['released']
Deployability,"/github.com/tornadoweb/tornado/commit/7dfe8b597f2d179334d7b528f61e9449ac131273""><code>7dfe8b5</code></a> httpserver_test: Add ExpectLog to fix CI</li>; <li><a href=""https://github.com/tornadoweb/tornado/commit/217295b1dd30f556ea374d62007f6821688f00f0""><code>217295b</code></a> http1connection: Make content-length parsing more strict</li>; <li><a href=""https://github.com/tornadoweb/tornado/commit/e3aa6c5e2943242d8ab25448c2798365b3cb9945""><code>e3aa6c5</code></a> Merge pull request <a href=""https://redirect.github.com/tornadoweb/tornado/issues/3267"">#3267</a> from bdarnell/branch6.3</li>; <li>See full diff in <a href=""https://github.com/tornadoweb/tornado/compare/v6.3.2...v6.3.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tornado&package-manager=pip&previous-version=6.3.2&new-version=6.3.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13432:2811,update,updates,2811,https://hail.is,https://github.com/hail-is/hail/pull/13432,2,['update'],['updates']
Deployability,"/google-api-python-client/commit/39e9bd18d0813281f2d87f0bd897d355f348610b</a> (<a href=""https://github.com/googleapis/google-api-python-client/commit/58ef3e0171d10c7884523faec8e45907a8ff3032"">58ef3e0</a>)</li>; <li><strong>cloudchannel:</strong> update the api <a href=""https://github.com/googleapis/google-api-python-client/commit/63c9fbcd252b0e480a43f3b2eee480faa925269a"">https://github.com/googleapis/google-api-python-client/commit/63c9fbcd252b0e480a43f3b2eee480faa925269a</a> (<a href=""https://github.com/googleapis/google-api-python-client/commit/58ef3e0171d10c7884523faec8e45907a8ff3032"">58ef3e0</a>)</li>; <li><strong>clouddeploy:</strong> update the api <a href=""https://github.com/googleapis/google-api-python-client/commit/7e7dd3465e8b2bcdecc7e3d00ca846ed293db06a"">https://github.com/googleapis/google-api-python-client/commit/7e7dd3465e8b2bcdecc7e3d00ca846ed293db06a</a> (<a href=""https://github.com/googleapis/google-api-python-client/commit/58ef3e0171d10c7884523faec8e45907a8ff3032"">58ef3e0</a>)</li>; <li><strong>cloudfunctions:</strong> update the api <a href=""https://github.com/googleapis/google-api-python-client/commit/7e2bd793ef1a36e25503ae39a1e89279d012de39"">https://github.com/googleapis/google-api-python-client/commit/7e2bd793ef1a36e25503ae39a1e89279d012de39</a> (<a href=""https://github.com/googleapis/google-api-python-client/commit/58ef3e0171d10c7884523faec8e45907a8ff3032"">58ef3e0</a>)</li>; <li><strong>cloudidentity:</strong> update the api <a href=""https://github.com/googleapis/google-api-python-client/commit/557608ad1fd8d62fcb60a16a864cd842649e7386"">https://github.com/googleapis/google-api-python-client/commit/557608ad1fd8d62fcb60a16a864cd842649e7386</a> (<a href=""https://github.com/googleapis/google-api-python-client/commit/58ef3e0171d10c7884523faec8e45907a8ff3032"">58ef3e0</a>)</li>; <li><strong>cloudsearch:</strong> update the api <a href=""https://github.com/googleapis/google-api-python-client/commit/8f4100e6398a969b840d09870fe49d1f77a6fe2c"">https://githu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11541:13986,update,update,13986,https://hail.is,https://github.com/hail-is/hail/pull/11541,2,['update'],['update']
Deployability,"/googleapis/google-api-python-client/commit/55a2fdd36ed7e8cec07f1fd7839a003542aaaaae</a> (<a href=""https://github.com/googleapis/google-api-python-client/commit/c9d0e01dd8eaeddb1c628fc6433fee5473891270"">c9d0e01</a>)</li>; <li><strong>firebase:</strong> update the api <a href=""https://github.com/googleapis/google-api-python-client/commit/ea9f4257b1043d70cfff70fab72d947cecd16a82"">https://github.com/googleapis/google-api-python-client/commit/ea9f4257b1043d70cfff70fab72d947cecd16a82</a> (<a href=""https://github.com/googleapis/google-api-python-client/commit/c9d0e01dd8eaeddb1c628fc6433fee5473891270"">c9d0e01</a>)</li>; <li><strong>iam:</strong> update the api <a href=""https://github.com/googleapis/google-api-python-client/commit/f1ba7e60420a1a6f941ebd0043ac7ea86a8c6cc3"">https://github.com/googleapis/google-api-python-client/commit/f1ba7e60420a1a6f941ebd0043ac7ea86a8c6cc3</a> (<a href=""https://github.com/googleapis/google-api-python-client/commit/c9d0e01dd8eaeddb1c628fc6433fee5473891270"">c9d0e01</a>)</li>; <li><strong>managedidentities:</strong> update the api <a href=""https://github.com/googleapis/google-api-python-client/commit/69fb750010d7b91f55bdc99e42f3b3a7c9a664da"">https://github.com/googleapis/google-api-python-client/commit/69fb750010d7b91f55bdc99e42f3b3a7c9a664da</a> (<a href=""https://github.com/googleapis/google-api-python-client/commit/c9d0e01dd8eaeddb1c628fc6433fee5473891270"">c9d0e01</a>)</li>; <li><strong>mybusinessbusinessinformation:</strong> update the api <a href=""https://github.com/googleapis/google-api-python-client/commit/9d7a59eb7848462d10ecd9433e76eacd7c6fb23e"">https://github.com/googleapis/google-api-python-client/commit/9d7a59eb7848462d10ecd9433e76eacd7c6fb23e</a> (<a href=""https://github.com/googleapis/google-api-python-client/commit/c9d0e01dd8eaeddb1c628fc6433fee5473891270"">c9d0e01</a>)</li>; <li><strong>networkmanagement:</strong> update the api <a href=""https://github.com/googleapis/google-api-python-client/commit/1402417ac4577229bc1d07f67cd2ec82",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11541:6534,update,update,6534,https://hail.is,https://github.com/hail-is/hail/pull/11541,2,['update'],['update']
Deployability,"/googleapis/google-api-python-client/commit/61bf78b268cfcbec154f94f6deab9a2069d1ad38</a> (<a href=""https://github.com/googleapis/google-api-python-client/commit/c9d0e01dd8eaeddb1c628fc6433fee5473891270"">c9d0e01</a>)</li>; <li><strong>compute:</strong> update the api <a href=""https://github.com/googleapis/google-api-python-client/commit/92a4e01199ad06930e77bea3caf2de70158b0ba6"">https://github.com/googleapis/google-api-python-client/commit/92a4e01199ad06930e77bea3caf2de70158b0ba6</a> (<a href=""https://github.com/googleapis/google-api-python-client/commit/c9d0e01dd8eaeddb1c628fc6433fee5473891270"">c9d0e01</a>)</li>; <li><strong>container:</strong> update the api <a href=""https://github.com/googleapis/google-api-python-client/commit/56b30805e34dc8e59176fdd69145ed2fbc75ad0b"">https://github.com/googleapis/google-api-python-client/commit/56b30805e34dc8e59176fdd69145ed2fbc75ad0b</a> (<a href=""https://github.com/googleapis/google-api-python-client/commit/c9d0e01dd8eaeddb1c628fc6433fee5473891270"">c9d0e01</a>)</li>; <li><strong>content:</strong> update the api <a href=""https://github.com/googleapis/google-api-python-client/commit/803d36fd01c92a67098eb2cfe5b2480849bc99dc"">https://github.com/googleapis/google-api-python-client/commit/803d36fd01c92a67098eb2cfe5b2480849bc99dc</a> (<a href=""https://github.com/googleapis/google-api-python-client/commit/c9d0e01dd8eaeddb1c628fc6433fee5473891270"">c9d0e01</a>)</li>; <li><strong>datamigration:</strong> update the api <a href=""https://github.com/googleapis/google-api-python-client/commit/b8836f7828485aa4e3bcc17f88ddf6a7667a7de8"">https://github.com/googleapis/google-api-python-client/commit/b8836f7828485aa4e3bcc17f88ddf6a7667a7de8</a> (<a href=""https://github.com/googleapis/google-api-python-client/commit/c9d0e01dd8eaeddb1c628fc6433fee5473891270"">c9d0e01</a>)</li>; <li><strong>dialogflow:</strong> update the api <a href=""https://github.com/googleapis/google-api-python-client/commit/f59636069d852f20f3472ad40539530063ea38b3"">https://github.com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11541:3333,update,update,3333,https://hail.is,https://github.com/hail-is/hail/pull/11541,2,['update'],['update']
Deployability,"/googleapis/python-cloud-core/issues/201"">#201</a>)</li>; <li><a href=""https://github.com/googleapis/python-cloud-core/commit/1bc7354261e2fcb8426d269d0083d2d993fafe66""><code>1bc7354</code></a> chore(main): release 2.3.1 (<a href=""https://github-redirect.dependabot.com/googleapis/python-cloud-core/issues/195"">#195</a>)</li>; <li><a href=""https://github.com/googleapis/python-cloud-core/commit/1b79d69cdb156ae21dddb5ab7579f6c08fd701b8""><code>1b79d69</code></a> docs: fix changelog header to consistent size (<a href=""https://github-redirect.dependabot.com/googleapis/python-cloud-core/issues/194"">#194</a>)</li>; <li><a href=""https://github.com/googleapis/python-cloud-core/commit/66d0e8692123021ae76d5d0802c130ba05cce181""><code>66d0e86</code></a> chore(python): auto approve template changes (<a href=""https://github-redirect.dependabot.com/googleapis/python-cloud-core/issues/193"">#193</a>)</li>; <li><a href=""https://github.com/googleapis/python-cloud-core/commit/a41c3a78fd6611a4e0e0044f25b1af36bcf3ca6b""><code>a41c3a7</code></a> chore: [autoapprove] update readme_gen.py to include autoescape True (<a href=""https://github-redirect.dependabot.com/googleapis/python-cloud-core/issues/191"">#191</a>)</li>; <li><a href=""https://github.com/googleapis/python-cloud-core/commit/3f64dde56a5ebd3941cac91f68c2be250978a9c6""><code>3f64dde</code></a> chore(python): use ubuntu 22.04 in docs image (<a href=""https://github-redirect.dependabot.com/googleapis/python-cloud-core/issues/190"">#190</a>)</li>; <li><a href=""https://github.com/googleapis/python-cloud-core/commit/70b348bdfd4c11c77793b907468c206ee457bd32""><code>70b348b</code></a> chore(deps): update all dependencies (<a href=""https://github-redirect.dependabot.com/googleapis/python-cloud-core/issues/188"">#188</a>)</li>; <li><a href=""https://github.com/googleapis/python-cloud-core/commit/6014db08ca2078a1c8ebb37ee46c53dae381e7d5""><code>6014db0</code></a> chore(main): release 2.3.0 (<a href=""https://github-redirect.dependabot.com/googleapis/pyth",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12175:8879,update,update,8879,https://hail.is,https://github.com/hail-is/hail/pull/12175,1,['update'],['update']
Deployability,"/h1>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/PyCQA/astroid/commit/65bca39bbf254bc760ac9d388e5a09333eaf5c87""><code>65bca39</code></a> Bump astroid to 2.12.8, update changelog</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/fab511c1477d13262e9e33b015906d4bca683953""><code>fab511c</code></a> Fix crash in <code>dataclass</code> brain (<a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1770"">#1770</a>)</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/0720cbcd05a3938bdf8141328a1ceed1e2f38bed""><code>0720cbc</code></a> Parse default values in <code>dataclass</code> attributes correctly (<a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1771"">#1771</a>)</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/b2dbf7bdaa02436962c4c5ccbc6bbb8b8e0c3295""><code>b2dbf7b</code></a> Bump astroid to 2.12.7, update changelog</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/fbe7859ec56ab64cc4d1b2604082878fdfdc8a14""><code>fbe7859</code></a> Fix crash in <code>dataclass</code> brain (<a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1768"">#1768</a>)</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/e194631088aee587140c029a0404f8d40c6765b5""><code>e194631</code></a> Bump astroid to 2.12.6, update changelog</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/1f5dc457729d7219178ace9705d8445e89513472""><code>1f5dc45</code></a> Handle <code>dataclass</code> <code>kw_only</code> keyword correctly (<a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1764"">#1764</a>)</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/8f8448ee70d968784c3e2b9d622f2b1e8fe61f5d""><code>8f8448e</code></a> Fix a crash involving <code>Uninferable</code> args to <code>namedtuple</code> (<a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1763"">#1763</a>)</li>; <li><",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12158:3171,update,update,3171,https://hail.is,https://github.com/hail-is/hail/pull/12158,1,['update'],['update']
Deployability,"/h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/361"">#361</a>: Avoid potential REDoS in <code>EntryPoint.pattern</code>.</li>; </ul>; <h1>v4.10.0</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/354"">#354</a>: Removed <code>Distribution._local</code> factory. This; functionality was created as a demonstration of the; possible implementation. Now, the; <code>pep517 &lt;https://pypi.org/project/pep517&gt;</code>_ package; provides this functionality directly through; <code>pep517.meta.load &lt;https://github.com/pypa/pep517/blob/a942316305395f8f757f210e2b16f738af73f8b8/pep517/meta.py#L63-L73&gt;</code>_.</li>; </ul>; <h1>v4.9.0</h1>; <ul>; <li>Require Python 3.7 or later.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python/importlib_metadata/commit/99a2ec4489da45407d8224be2804ff323a164ac0""><code>99a2ec4</code></a> Update changelog.</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/dbe114cbdc49ff42026974e48ca7178a091e7530""><code>dbe114c</code></a> Add docstring with tests for EntryPoint.matches. Ref <a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/373"">#373</a>.</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/ee566d048c0061b4f846f100ebfd93eefbcbf608""><code>ee566d0</code></a> Remove cast of path items to strings. Ref <a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/372"">#372</a>.</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/14cce75299645467adcd17352cb07caada32c444""><code>14cce75</code></a> Prefer re.findall, which returns materialized results. Fixes <a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/369"">#369</a>.</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/b4661fd898",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11596:2411,Update,Update,2411,https://hail.is,https://github.com/hail-is/hail/pull/11596,1,['Update'],['Update']
Deployability,"/h2>; <p>Added a function <code>decoratorx</code> using the <code>FunctionMaker</code> and thus; preserving the signature of <code>__code__</code> objects. Then fixed three small bugs:</p>; <ul>; <li>Sphinx was printing a few warnings when building the documentation, as; signaled by Tomasz Kłoczko</li>; <li>functions decorated with <code>decorator.contextmanager</code> were one-shot,; as discovered by Alex Pizarro.</li>; <li><code>decorator.decorator</code> was not passing the kwsyntax argument.</li>; </ul>; <h2>5.0.9 (2021-05-16)</h2>; <p>Fixed a test breaking PyPy. Restored support for Sphinx.</p>; <h2>5.0.8 (2021-05-15)</h2>; <p>Made the decorator module more robust when decorating builtin functions; lacking dunder attributes, like <code>dict.__setitem__</code>.</p>; <h2>5.0.7 (2021-04-14)</h2>; <p>The decorator module was not passing correctly the defaults inside the; <code>*args</code> tuple, thanks to Dan Shult for the fix. Also fixed some mispellings; in the documentation and integrated codespell in the CI, thanks to; Christian Clauss.</p>; <h2>5.0.6 (2021-04-08)</h2>; <p>The decorator module was not copying the <strong>module</strong> attribute anymore.; Thanks to Nikolay Markov for the notice.</p>; <h2>5.0.5 (2021-04-04)</h2>; <p>Dropped support for Python &lt; 3.5 with a substantial simplification of; the code base (now building a decorator does not require calling &quot;exec&quot;).; Added a way to mimic functools.wraps-generated decorators.; Ported the Continuous Integration from Travis to GitHub.</p>; <h2>4.4.2 (2020-02-29)</h2>; <p>Sylvan Mosberger (<a href=""https://github.com/Infinisil"">https://github.com/Infinisil</a>) contributed a patch to; some doctests that were breaking on NixOS.; John Vandenberg (<a href=""https://github.com/jayvdb"">https://github.com/jayvdb</a>) made a case for removing the usage</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://gi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11490:1658,integrat,integrated,1658,https://hail.is,https://github.com/hail-is/hail/pull/11490,1,['integrat'],['integrated']
Deployability,"/h2>; <p>Added a function <code>decoratorx</code> using the <code>FunctionMaker</code> and thus; preserving the signature of <code>__code__</code> objects. Then fixed three small bugs:</p>; <ul>; <li>Sphinx was printing a few warnings when building the documentation, as; signaled by Tomasz Kłoczko</li>; <li>functions decorated with <code>decorator.contextmanager</code> were one-shot,; as discovered by Alex Pizarro.</li>; <li><code>decorator.decorator</code> was not passing the kwsyntax argument.</li>; </ul>; <h2>5.0.9 (2021-05-16)</h2>; <p>Fixed a test breaking PyPy. Restored support for Sphinx.</p>; <h2>5.0.8 (2021-05-15)</h2>; <p>Made the decorator module more robust when decorating builtin functions; lacking dunder attributes, like <code>dict.__setitem__</code>.</p>; <h2>5.0.7 (2021-04-14)</h2>; <p>The decorator module was not passing correctly the defaults inside the; <code>*args</code> tuple, thanks to Dan Shult for the fix. Also fixed some mispellings; in the documentation and integrated codespell in the CI, thanks to; Christian Clauss.</p>; <h2>5.0.6 (2021-04-08)</h2>; <p>The decorator module was not copying the <strong>module</strong> attribute anymore.; Thanks to Nikolay Markov for the notice.</p>; <h2>5.0.5 (2021-04-04)</h2>; <p>Dropped support for Python &lt; 3.5 with a substantial simplification of; the code base (now building a decorator does not require calling &quot;exec&quot;).; Added a way to mimic functools.wraps-generated decorators.; Ported the Continuous Integration from Travis to GitHub.</p>; <h2>4.4.2 (2020-02-29)</h2>; <p>Sylvan Mosberger (<a href=""https://github.com/Infinisil"">https://github.com/Infinisil</a>) contributed a patch to; some doctests that were breaking on NixOS.; John Vandenberg (<a href=""https://github.com/jayvdb"">https://github.com/jayvdb</a>) made a case for removing the usage</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11799:1627,integrat,integrated,1627,https://hail.is,https://github.com/hail-is/hail/pull/11799,1,['integrat'],['integrated']
Deployability,"/h2>; <p>This is release 1.48.1 (<a href=""https://github.com/grpc/grpc/blob/master/doc/g_stands_for.md"">garum</a>) of gRPC Core.</p>; <p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>; <p>This release contains refinements, improvements, and bug fixes, with highlights listed below.</p>; <h2>Core</h2>; <ul>; <li>Backport EventEngine Forkables. (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/30605"">#30605</a>)</li>; </ul>; <h2>Release v1.48.0</h2>; <p>This is release 1.48.0 (<a href=""https://github.com/grpc/grpc/blob/master/doc/g_stands_for.md"">garum</a>) of gRPC Core.</p>; <p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>; <p>This release contains refinements, improvements, and bug fixes, with highlights listed below.</p>; <h2>Core</h2>; <ul>; <li>Upgrade Abseil to LTS 20220623.0 . (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/30155"">#30155</a>)</li>; <li>Call: Send cancel op down the stack even when no ops are sent. (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/30004"">#30004</a>)</li>; <li>FreeBSD system roots implementation. (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/29436"">#29436</a>)</li>; <li>xDS: Workaround to get gRPC clients working with istio. (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/29841"">#29841</a>)</li>; </ul>; <h2>Python</h2>; <ul>; <li>Set Correct Platform Tag in Wheels on Mac OS with Python 3.10. (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/29857"">#29857</a>)</li>; <li>[Aio] Ensure Core channel closes when deallocated. (<a href=""https://github-redirect.dependabot.com/grpc/grpc/pull/29797"">#29797</a>)</li>; <li>[Aio] Fix the wait_for_termination return value. (<a href=""https://github-redirect.dependabot.com/grpc/grpc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12201:1249,Upgrade,Upgrade,1249,https://hail.is,https://github.com/hail-is/hail/pull/12201,1,['Upgrade'],['Upgrade']
Deployability,"/h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2022-11-10&amp;to=2022-11-15&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ameeseeksmachine+updated%3A2022-11-10..2022-11-15&amp;type=Issues""><code>@​meeseeksmachine</code></a></p>; <h2>v7.4.5</h2>; <h2>7.4.5</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.4.4...d27c8a497c6cbb1a232fbbe75cb1fd0f53faa9b0"">Full Changelog</a>)</p>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>[7.x] Handle Jupyter Core Warning <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/875"">#875</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Clean up 7.x workflows <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/865"">#865</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2022-10-25&amp;to=2022-11-10&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2022-10-25..2022-11-10&amp;type=Issues""><code>@​blink1073</code></a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_client/blob/v7.4.6/CHANGELOG.md"">jupyter-client's changelog</a>.</em></p>; <blockquote>; <h2>7.4.6</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.4.5...3394591f161be4a19f9e61c66ba510d7e29afd59"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Reconcile connection information <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/879"">#879</a> (<a href=""https://github.com/kevin-bates""><code>@​kevin-bates</code></a>)</li>; </ul>;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12467:1736,release,release,1736,https://hail.is,https://github.com/hail-is/hail/pull/12467,1,['release'],['release']
Deployability,"/hagenw/sphinxcontrib-katex/commit/da43ec0d98471d0cf02292cc03c05ed526777bf0""><code>da43ec0</code></a> Release 0.8.4</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/358887325d6d9cbae839fd77bebb9de0fae3b474""><code>3588873</code></a> Increase top padding of equations (<a href=""https://github-redirect.dependabot.com/hagenw/sphinxcontrib-katex/issues/61"">#61</a>)</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/5ef309a8c9c7f9f12c3c17693adef49f593eb5de""><code>5ef309a</code></a> TST: re-enable link checks (<a href=""https://github-redirect.dependabot.com/hagenw/sphinxcontrib-katex/issues/60"">#60</a>)</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/e29e793911d2d71bd50a9ecb7248f51832ad8ea6""><code>e29e793</code></a> DOC: update copyright year (<a href=""https://github-redirect.dependabot.com/hagenw/sphinxcontrib-katex/issues/59"">#59</a>)</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/3ea5d66cc6985f4aa021c845ceeadd9a26f39a37""><code>3ea5d66</code></a> Release 0.8.3</li>; <li><a href=""https://github.com/hagenw/sphinxcontrib-katex/commit/d7636e1c52f3cc9a586171f5c9318b338acfbaaf""><code>d7636e1</code></a> Readd manifest to fix compilation of latest docs (<a href=""https://github-redirect.dependabot.com/hagenw/sphinxcontrib-katex/issues/58"">#58</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/hagenw/sphinxcontrib-katex/compare/0.5.1...v0.8.6"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinxcontrib-katex&package-manager=pip&previous-version=0.5.1&new-version=0.8.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11458:5473,Release,Release,5473,https://hail.is,https://github.com/hail-is/hail/pull/11458,2,['Release'],['Release']
Deployability,"/hail-all-spark.jar; --; 872 | amazon-ebs: rm -rf build/deploy; 873 | amazon-ebs: mkdir -p build/deploy; 874 | amazon-ebs: mkdir -p build/deploy/src; 875 | amazon-ebs: cp ../README.md build/deploy/; 876 | amazon-ebs: rsync -r \; 877 | amazon-ebs: --exclude '.eggs/' \; 878 | amazon-ebs: --exclude '.pytest_cache/' \; 879 | amazon-ebs: --exclude '__pycache__/' \; 880 | amazon-ebs: --exclude 'benchmark_hail/' \; 881 | amazon-ebs: --exclude '.mypy_cache/' \; 882 | amazon-ebs: --exclude 'docs/' \; 883 | amazon-ebs: --exclude 'dist/' \; 884 | amazon-ebs: --exclude 'test/' \; 885 | amazon-ebs: --exclude '*.log' \; 886 | amazon-ebs: python/ build/deploy/; 887 | amazon-ebs: # Clear the bdist build cache before building the wheel; 888 | amazon-ebs: cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; 889 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-ebs: Downloading aiohttp_session-2.7.0-py3-none-any.whl (14 kB); 899 | amazon-ebs: Collecting asyncinit<0.3,>=0.2.4; 900 | amazon-ebs: Downloading asyncinit-0.2.4-py3-none-an",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:1191,install,installer,1191,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['install'],['installer']
Deployability,"/hail/pull/14170#discussion_r1473442106). Since this is merely propagating existing bad behavior, let's change it separately so as to keep the conceptual overhead of this change as small as possible. Please make an issue and paste the link into each comment so that we can later track how we resolved each comment. . > Ok. Still working on getting the tests to pass and cleaning things up. However, I ran into a small snag. The code below needs to be ironed out. Should the number of jobs and state of the job group be recursive or specific to that job group? It's a bit weird for the billing and cancellation to be nested, but the number of jobs etc. are not. More concretely, if a child batch is running, should the parent also be running even if it has no direct child jobs that are running? Thoughts?; > ; > cc: @daniel-goldstein; > ; > ```sql; > UPDATE batches SET; > `state` = 'running',; > time_completed = NULL,; > n_jobs = n_jobs + expected_n_jobs; > WHERE id = in_batch_id;; > ; > ### FIXME FIXME what should the state be of nested job groups?; > UPDATE job_groups; > INNER JOIN (; > SELECT batch_id, job_group_id, CAST(COALESCE(SUM(n_jobs), 0) AS SIGNED) AS staged_n_jobs; > FROM job_groups_inst_coll_staging; > WHERE batch_id = in_batch_id AND update_id = in_update_id; > GROUP BY batch_id, job_group_id; > ) AS t ON job_groups.batch_id = t.batch_id AND job_groups.job_group_id = t.job_group_id; > SET `state` = 'running', time_completed = NULL, n_jobs = n_jobs + t.staged_n_jobs;; > ```. When you say ""billing and cancellation [is] nested"" do you mean that the bill for a group is the sum of the bill for all jobs directly in the group with all jobs in any descendent group?. Since we decided that groups are nested, my inclination is for everything to represent a sum total over the direct jobs and jobs within any descendant groups. From here on out ""sum total"" means exactly that. OK, so:. 1. In the UI (database should do what makes sense and is fast), the number of jobs should be t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14170#issuecomment-1940341021:1109,UPDATE,UPDATE,1109,https://hail.is,https://github.com/hail-is/hail/pull/14170#issuecomment-1940341021,2,['UPDATE'],['UPDATE']
Deployability,"/hailctl/dataproc/0.2.129"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/init_notebook.py python/hailtop/hailctl/dataproc/resources/vep-GRCh37.sh python/hailtop/hailctl/dataproc/resources/vep-GRCh38.sh build/deploy/dist/hail-0.2.129-py3-none-any.whl gs://hail-common/hailctl/dataproc/0.2.129; gcloud storage objects update -",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:1278,deploy,deploy,1278,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,1,['deploy'],['deploy']
Deployability,"/ijl/orjson/releases"">orjson's releases</a>.</em></p>; <blockquote>; <h2>3.9.15</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14357:1169,Update,Update,1169,https://hail.is,https://github.com/hail-is/hail/pull/14357,3,['Update'],['Update']
Deployability,"/issues/13425"">#13425</a>) ...</li>; <li>Additional commits viewable in <a href=""https://github.com/python/mypy/compare/v0.950...v0.982"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=mypy&package-manager=pip&previous-version=0.950&new-version=0.982)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12291:4174,upgrade,upgrade,4174,https://hail.is,https://github.com/hail-is/hail/pull/12291,3,['upgrade'],['upgrade']
Deployability,"/issues/15499"">#15499</a>: Adopt ruff format <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15564"">#15564</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab/graphs/contributors?from=2023-11-18&amp;to=2023-12-29&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aafshin+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​afshin</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Abrichet+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​brichet</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Adavidbrochart+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​davidbrochart</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aecharles+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​echarles</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Afcollonval+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​fcollonval</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ag547315+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​g547315</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Agabalafou+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​gabalafou</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3AGabrielaVives+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​GabrielaVives</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Agithub-actions+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/searc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14184:10041,update,updated,10041,https://hail.is,https://github.com/hail-is/hail/pull/14184,1,['update'],['updated']
Deployability,"/issues/205"">#205</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/af050bb8b129a6648030ec11c1b0625cfd7a29dd""><code>af050bb</code></a> chore(deps): update sphinx requirement (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/204"">#204</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/7e7999c19300d32c04f90cc0b56f0c5488e9d787""><code>7e7999c</code></a> [chore] pin k8s version to 1.23.6 in e2e tests. (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/206"">#206</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/308f153f91b7942476f2d4ddda3dc8c99933d598""><code>308f153</code></a> ci: add workflow to publish sdist/wheel to PyPI (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/202"">#202</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/961624063383cbcdc78a61b1d18448429a61a489""><code>9616240</code></a> [chore] update changelog</li>; <li>Additional commits viewable in <a href=""https://github.com/tomplus/kubernetes_asyncio/compare/v19.15.1...23.6.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=kubernetes-asyncio&package-manager=pip&previous-version=19.15.1&new-version=23.6.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:15375,update,update,15375,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['update'],['update']
Deployability,"/issues/30163"">#30163</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/grpc/grpc/compare/v1.47.0...v1.48.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=grpcio&package-manager=pip&previous-version=1.47.0&new-version=1.48.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12201:7558,upgrade,upgrade,7558,https://hail.is,https://github.com/hail-is/hail/pull/12201,3,['upgrade'],['upgrade']
Deployability,"/issues/995"">#995</a></li>; <li>Additional commits viewable in <a href=""https://github.com/microsoft/debugpy/compare/v1.6.0...v1.6.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=debugpy&package-manager=pip&previous-version=1.6.0&new-version=1.6.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12103:6854,upgrade,upgrade,6854,https://hail.is,https://github.com/hail-is/hail/pull/12103,6,['upgrade'],['upgrade']
Deployability,"/latest/dg/query-spell-check.html"">https://docs.aws.amazon.com/kendra/latest/dg/query-spell-check.html</a></li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] Launching Amazon AppFlow Marketo as a destination connector SDK.</li>; <li>api-change:<code>timestream-query</code>: [<code>botocore</code>] Documentation only update for SDK and CLI</li>; </ul>; <h1>1.21.11</h1>; <ul>; <li>api-change:<code>gamelift</code>: [<code>botocore</code>] Minor updates to address errors.</li>; <li>api-change:<code>cloudtrail</code>: [<code>botocore</code>] Add bytesScanned field into responses of DescribeQuery and GetQueryResults.</li>; <li>api-change:<code>athena</code>: [<code>botocore</code>] This release adds support for S3 Object Ownership by allowing the S3 bucket owner full control canned ACL to be set when Athena writes query results to S3 buckets.</li>; <li>api-change:<code>keyspaces</code>: [<code>botocore</code>] This release adds support for data definition language (DDL) operations</li>; <li>api-change:<code>ecr</code>: [<code>botocore</code>] This release adds support for tracking images lastRecordedPullTime.</li>; </ul>; <h1>1.21.10</h1>; <ul>; <li>api-change:<code>mediapackage</code>: [<code>botocore</code>] This release adds Hybridcast as an available profile option for Dash Origin Endpoints.</li>; <li>api-change:<code>rds</code>: [<code>botocore</code>] Documentation updates for Multi-AZ DB clusters.</li>; <li>api-change:<code>mgn</code>: [<code>botocore</code>] Add support for GP3 and IO2 volume types. Add bootMode to LaunchConfiguration object (and as a parameter to UpdateLaunchConfigurationRequest).</li>; <li>api-change:<code>kafkaconnect</code>: [<code>botocore</code>] Adds operation for custom plugin deletion (DeleteCustomPlugin) and adds new StateDescription field to DescribeCustomPlugin and DescribeConnector responses to return errors from asynchronous resource creation.</li>; </ul>; <h1>1.21.9</h1>; <ul>; <li>api-change:<code>finspace-data</cod",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11486:1700,release,release,1700,https://hail.is,https://github.com/hail-is/hail/pull/11486,4,['release'],['release']
Deployability,"/lepture/mistune/releases"">mistune's releases</a>.</em></p>; <blockquote>; <h2>Version 2.0.2</h2>; <p>Fix <code>escape_url </code> via <a href=""https://github-redirect.dependabot.com/lepture/mistune/pull/295"">lepture/mistune#295</a></p>; <h2>Version 2.0.1</h2>; <p>Fix XSS for image link syntax.</p>; <h2>Version 2.0.0</h2>; <p>First release of Mistune v2.</p>; <h2>Version 2.0.0 RC1</h2>; <p>In this release, we have a <strong>Security Fix</strong> for harmful links.</p>; <h2>Version 2.0.0 Alpha 1</h2>; <p>This is the first release of v2. An alpha version for users to have a preview of the new mistune.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/lepture/mistune/blob/master/docs/changes.rst"">mistune's changelog</a>.</em></p>; <blockquote>; <h2>Changelog</h2>; <p>Here is the full history of mistune v2.</p>; <p>Version 2.0.4</p>; <pre><code>; Released on Jul 15, 2022; <ul>; <li>Fix <code>url</code> plugin in <code>&amp;lt;a&amp;gt;</code> tag</li>; <li>Fix <code>*</code> formatting</li>; </ul>; <p>Version 2.0.3; </code></pre></p>; <p>Released on Jun 27, 2022</p>; <ul>; <li>Fix <code>table</code> plugin</li>; <li>Security fix for CVE-2022-34749</li>; </ul>; <p>Version 2.0.2</p>; <pre><code>; Released on Jan 14, 2022; <p>Fix <code>escape_url</code></p>; <p>Version 2.0.1; </code></pre></p>; <p>Released on Dec 30, 2021</p>; <p>XSS fix for image link syntax.</p>; <p>Version 2.0.0</p>; <pre><code>; Released on Dec 5, 2021; <p>This is the first non-alpha release of mistune v2.</p>; <p>Version 2.0.0rc1; </code></pre></p>; <p>Released on Feb 16, 2021</p>; <p>Version 2.0.0a6</p>; <pre><code>; &lt;/tr&gt;&lt;/table&gt; ; </code></pre>; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/lepture/mistune/commit/3f422f1e84edae0f39756c45be453ecde534b755""><code>3f422f1</code></a> Version bump 2.0.3</li>; <li><a href=""https://github.com/l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12064:1092,Release,Released,1092,https://hail.is,https://github.com/hail-is/hail/pull/12064,1,['Release'],['Released']
Deployability,"/lepture/mistune/releases"">mistune's releases</a>.</em></p>; <blockquote>; <h2>Version 2.0.2</h2>; <p>Fix <code>escape_url </code> via <a href=""https://github-redirect.dependabot.com/lepture/mistune/pull/295"">lepture/mistune#295</a></p>; <h2>Version 2.0.1</h2>; <p>Fix XSS for image link syntax.</p>; <h2>Version 2.0.0</h2>; <p>First release of Mistune v2.</p>; <h2>Version 2.0.0 RC1</h2>; <p>In this release, we have a <strong>Security Fix</strong> for harmful links.</p>; <h2>Version 2.0.0 Alpha 1</h2>; <p>This is the first release of v2. An alpha version for users to have a preview of the new mistune.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/lepture/mistune/blob/master/docs/changes.rst"">mistune's changelog</a>.</em></p>; <blockquote>; <h2>Changelog</h2>; <p>Here is the full history of mistune v2.</p>; <p>Version 2.0.4</p>; <pre><code>; Released on Jul 15, 2022; <ul>; <li>Fix <code>url</code> plugin in <code>&amp;lt;a&amp;gt;</code> tag</li>; <li>Fix <code>*</code> formatting</li>; </ul>; <p>Version 2.0.3; </code></pre></p>; <p>Released on Jun 27, 2022</p>; <ul>; <li>Fix <code>table</code> plugin</li>; <li>Security fix for CVE-2022-34749</li>; </ul>; <p>Version 2.0.2</p>; <pre><code>; Released on Jan 14, 2022; <p>Fix <code>escape_url</code></p>; <p>Version 2.0.1; </code></pre></p>; <p>Released on Dec 30, 2021</p>; <p>XSS fix for image link syntax.</p>; <p>Version 2.0.0</p>; <pre><code>; Released on Dec 5, 2021; <p>This is the first non-alpha release of mistune v2.</p>; <p>Version 2.0.0rc1; </code></pre></p>; <p>Released on Feb 16, 2021</p>; <p>Version 2.0.0a6</p>; <pre><code>; &lt;/tr&gt;&lt;/table&gt; ; </code></pre>; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/lepture/mistune/commit/b92a5febd4da3d7097a3d2b8d7cac6f5d57ea20c""><code>b92a5fe</code></a> Version bump 2.0.4</li>; <li><a href=""https://github.com/l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12066:1092,Release,Released,1092,https://hail.is,https://github.com/hail-is/hail/pull/12066,2,['Release'],['Released']
Deployability,"/li>; </ul>; <h2>Fixes</h2>; <ul>; <li>Remove del from Redis (Fixes <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1115"">#1115</a>) (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1227"">#1227</a>)</li>; <li>fix socket.error raises (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1129"">#1129</a>)</li>; <li>Fix buffer is closed error when using PythonParser class (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1213"">#1213</a>)</li>; </ul>; <h2>Version v2.0.0</h2>; <p>Version 2.0 is a complete rewrite of aioredis. Starting with this version, aioredis now follows the API of <a href=""https://github.com/andymccurdy/redis-py"">redis-py</a>, so you can easily adapt synchronous code that uses redis-py for async applications with aioredis-py.</p>; <p><strong>NOTE:</strong> This version is <em>not</em> compatible with earlier versions of aioredis. If you upgrade, you will need to make code changes.</p>; <p>For more details, read our <a href=""https://aioredis.readthedocs.io/en/latest/migration/"">documentation on migrating to version 2.0</a>.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aioredis-py/blob/master/CHANGELOG.md"">aioredis's changelog</a>.</em></p>; <blockquote>; <h2>2.0.1 - (2021-12-20)</h2>; <h3>Features</h3>; <ul>; <li>Added Python 3.10 to CI &amp; Updated the Docs; (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1160"">#1160</a>)</li>; <li>Enable mypy in CI (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1101"">#1101</a>)</li>; <li>Synchronized reading the responses from a connection; (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1106"">#1106</a>)</li>; </ul>; <h3>Fixes</h3>; <ul>; <li>Remove <strong>del</strong> from Redis (Fixes <a href=""https://github-redirec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11569:1736,upgrade,upgrade,1736,https://hail.is,https://github.com/hail-is/hail/pull/11569,1,['upgrade'],['upgrade']
Deployability,"/li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11770"">#11770</a> [component: bokehjs] [BUG] Linking an axis range can lead to other axis range autoscaling improperly</li>; </ul>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/bokeh/bokeh/commit/17a0b288052afac80ebcf0aa74e3915452fce3ca""><code>17a0b28</code></a> Deployment updates for release 3.0.1</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/3f9d1fa63050d1f15238223cda9ac06ba4245cb6""><code>3f9d1fa</code></a> Merge deployment staging branch staging-3.0.1rc1</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/07246bb64c9f2cb46952eeeec07d1f09fa82835a""><code>07246bb</code></a> Deployment updates for release 3.0.1rc1</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/46fbb8edfcba1c26eb62c2f48eac7697f397dfe9""><code>46fbb8e</code></a> updates for 3.0.1 release (<a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/12552"">#12552</a>)</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/14860e167477dc8700f69e04a9a9823e63a55c12""><code>14860e1</code></a> update mybinder links (<a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/12549"">#12549</a>)</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/a13f3c1e7336dd4c4a6d6d859371bfa063a97461""><code>a13f3c1</code></a> remove runtime dependency on typing_extensions (<a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/12539"">#12539</a>)</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/2925a464c219aaba73921384c91ffec5866aaaae""><code>2925a46</code></a> Apply blackbody example label edits (<a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/12534"">#12534</a>) to the ts example as well (<a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/12546"">#12546</a>)</li>; <li><a href=""https://github.com/bokeh/bokeh/commit/9f00012c81ac04105",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12454:8282,release,release,8282,https://hail.is,https://github.com/hail-is/hail/pull/12454,1,['release'],['release']
Deployability,"/li>; <li><a href=""https://github.com/pallets/jinja/commit/466a200ea40642b674db77588d13889abbad55f5""><code>466a200</code></a> update requirements</li>; <li><a href=""https://github.com/pallets/jinja/commit/990602f719b4086540287e95f601baefd830d790""><code>990602f</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1647"">#1647</a> from Tom-Brouwer/202204/add-missing-overlay-options</li>; <li><a href=""https://github.com/pallets/jinja/commit/5d3d2414710c1439105d84efc58e4aba8e453cb3""><code>5d3d241</code></a> fix flake8-bugbear finding</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/3.0.3...3.1.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=3.0.3&new-version=3.1.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12173:6857,update,updates,6857,https://hail.is,https://github.com/hail-is/hail/pull/12173,1,['update'],['updates']
Deployability,"/li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/85c2ef19a3e11faad0cb405a5ab66bdca7e49f45""><code>85c2ef1</code></a> Minor formatting change, bug-fix on 0000 time</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/2fc41a02f32b4f7a769f036daec58ba4f233b106""><code>2fc41a0</code></a> Update ci.yml</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/08e7cfdb94fecf2d99c324856c37ec66fac0eeed""><code>08e7cfd</code></a> Minor changes in examples, conversion to PEP8 names, etc.</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/a8b05ccbe380117dac40b4cf6d9ffe08266fd7ed""><code>a8b05cc</code></a> Slight perf enhancement in Empty</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/801863aa4582a8ce5e6a7408d4966afcd247ea90""><code>801863a</code></a> Make htmlStripper.py and html_table_parser examples use PEP-8 names, add comm...</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/7d4da80b2bca8a2767134f4a181ea9aac4bbb230""><code>7d4da80</code></a> Prep for release</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/be0310a83436bb4893d0068bb5da3059199e4c0b""><code>be0310a</code></a> Add bf parser/executor example</li>; <li>Additional commits viewable in <a href=""https://github.com/pyparsing/pyparsing/compare/pyparsing_3.0.9...3.1.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pyparsing&package-manager=pip&previous-version=3.0.9&new-version=3.1.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13334:8534,release,release,8534,https://hail.is,https://github.com/hail-is/hail/pull/13334,1,['release'],['release']
