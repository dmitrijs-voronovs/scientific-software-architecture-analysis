quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Integrability,"Completing https://github.com/broadinstitute/hellbender/issues/673 will take care of the case of ""missing reference for CRAM"", but we also need to make sure we're handling the case of ""wrong reference"" elegantly (where elegantly means ""throw a `UserException` with a descriptive error message). We want a test case with a reference that is the wrong reference for a CRAM, but has a compatible sequence dictionary (so that it won't be caught by the sequence dictionary validation). Both the wrong and missing reference cases should have a simple integration test that runs, eg., `PrintReads` with `expectedExceptions = UserException.class`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374:285,message,message,285,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374,2,"['integrat', 'message']","['integration', 'message']"
Integrability,"Dijkstra origina algorithm is about finding the single shortest route (or one of the in case of a tie), here we need the one that finds the K-shortest routes which is described [here](https://en.wikipedia.org/wiki/K_shortest_path_routing). Is this one implanted in Jgraph? In that case, yes we could.... . Otherwise if we have to implement the it from scratch... then there is no guaranteed the code is going to be simpler.... it could simpler just because I didn't bother to make the current one as simple as it could be.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3561#issuecomment-328169271:64,rout,route,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3561#issuecomment-328169271,2,['rout'],"['route', 'routes']"
Integrability,"FilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/reso",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:2144,integrat,integration,2144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,1,['integrat'],['integration']
Integrability,"For reporting the number of reads that fails each of the filters, the composed filter could be changed by a `CountingReadFilter`; using the `getSummaryLine()` method will provide the number of reads failing each of the components. Developers could have in their tools a field with the `WellFormedReadFilter` and call a new method for reporting the summary, to log a warning/debug line. For exploding depending on the tool, maybe an advance/hidden argument can be added to the filter (something like `--failOnMalformed`) to throw an exception if true; developers might add a default filter with this value equals to true if they want to enforce by default this behaviour. I think that this a simpler idea for allow the developer to choose, and give some flexibility to the user to change the behaviour as its own risk (they can disable all filters anyway, which is also risky).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699:400,depend,depending,400,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699,1,['depend'],['depending']
Integrability,"Hello Geraldine:. > On 1/Mar/2017, at 7:56 PM, Geraldine Van der Auwera <notifications@github.com> wrote:; > ; > @chlangley One thing we could potentially do to attract attention to this issue and solicit feedback from the community would be to feature it on the GATK blog. If you were to write a concise case study detailing the impact of the problem on your results, others may be motivated to look at their own results, and if it causes problems there, add their voices to yours. We're willing to bring this to public attention, we just don't have the bandwidth to do the legwork. I started to work on this a bit and found myself blocked. . At this point I have a simple question: The GATK blog is separate from the forum (?). When I am on the blog page I can’t seem to find a button to submit a new post. I must be missing something or the route to blog posting is only via the forum?. Sorry to bother you with such mundane question. Cheers,; Chuck",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/269#issuecomment-287541436:844,rout,route,844,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-287541436,1,['rout'],['route']
Integrability,"Hello everyone: I am realizing that the GATK framework is going to have a lot of dependencies from java and python even if the simpler framework is the one need it. Maybe it is a good idea to start thinking about sub-modules within the same repository for the engine (maybe even separate the Spark framework), CNV...and create an independent artifact for every of them, and one combined one. Does it sound reasonable?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-346051614:81,depend,dependencies,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-346051614,1,['depend'],['dependencies']
Integrability,"Hey @jemunro,. Thanks for sharing your fix. I tried it on my data but now I have this ERROR message:; ```; ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.8-0-ge9d806836):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: For input string: ""NaN\1SOR=0.693""; ##### ERROR ------------------------------------------------------------------------------------------; INFO 13:46:00,793 HelpFormatter - ---------------------------------------------------------------------------------- ; ```; Would not be enough to use this code instead?:; ```; bcftools view in.vcf.gz |; sed 's/=nan/=NaN/g' |; bgzip > fixed.vcf.gz; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5582#issuecomment-630137734:92,message,message,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5582#issuecomment-630137734,2,['message'],['message']
Integrability,"Hi @Neato-Nick @davidbenjamin . Apologies for posting this message here. I have posted this message few days before at the regular GATK forum and also using the direct inbox option but have got no response so maybe something wrong with my account. The issue is - I have done variant calling on 384 potato samples following, mostly, GATK best ##practices and have applied hard filters to select SNPs for further usage. However, I am noticing that '--max-nocall-fraction', '--max-nocall-number' and '--max-fraction-filtered-genotypes' arguments for 'SelectVariants' are not working properly. I have tried with various cutoff settings and every time I am observing SNPs with a much larger number of genotypes (~246 out of 384 with 0.10 setting) with 'no call' than the set thresholds. I have searched the forum first but couldn't find any relevant threads. I am using the latest GATK version (4.0.7.0). I am attaching three example sets of (1) log files (2) subset vcf files and (3) vcf index file for the three main vcfs. I would appreciate if you could provide any feedback on this issue and/or if this behaviour has been observed by some other users also. The link to the original post is here:; https://gatkforums.broadinstitute.org/gatk/discussion/12688/possible-bug-in-selectvariants-tool#latest; [SelectVariantBugReport.zip](https://github.com/broadinstitute/gatk/files/2291206/SelectVariantBugReport.zip). Regards,; Sanjeev",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-413285177:59,message,message,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-413285177,2,['message'],['message']
Integrability,"Hi @jean-philippe-martin ,. A `Feature` in our codebase has a specific meaning that is different from ""interval"": it is a record that 1. has a location on the genome plus (typically) some metadata information about that location and 2. is in one of the formats supported by our file-parsing framework tribble and is the product of a tribble codec. A VCF record is an example of a `Feature`. . The common interface between `Feature` and `SimpleInterval` is called `Locatable`. I recommend (for now) that you simply alter your uprooted version of BQSR to take a `List<? extends Locatable>` instead of a `List<? extends Feature>` in `apply()`. This should require no code changes beyond changing method parameter types, and it will allow you to feed BQSR `SimpleIntervals` for the known sites for now, and `Features` like VCF records later on when we're ready for that. Please do return `ArtificialTestFeature` to the `FeatureDataSourceUnitTest` from which it came -- this is a very incomplete class meant only for testing purposes and not for external use.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247:404,interface,interface,404,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247,1,['interface'],['interface']
Integrability,"Hi @lbergelson and thanks for considering my issue,. I'm sorry but I'm not familiar to artifactory dependency, if necessary I'll deepen about it; so I just inserted this dependency in the project's pom.xml; ```; <dependency>; <groupId>org.broadinstitute</groupId>; <artifactId>gatk</artifactId>; <version>4.beta.6-18-g2ee7724-20171025.162137-1</version>; </dependency>; ```; as reported in the [artifact repository](https://broadinstitute.jfrog.io/broadinstitute/webapp/#/artifacts/browse/tree/General/libs-snapshot-local/org/broadinstitute/gatk/4.beta.6-18-g2ee7724-SNAPSHOT/gatk-4.beta.6-18-g2ee7724-20171025.162137-1.jar), but when I execute `mvn clear install` in my folder project, I receive this error: ; ```; [ERROR] Failed to execute goal on project GATKpipe: ; Could not resolve dependencies for project uk.ac.ncl:GATKpipe:jar:0.0.1-SNAPSHOT: ; Could not find artifact org.broadinstitute:gatk:jar:4.beta.6-18-g2ee7724-20171025.162137-1 -> [Help 1]; ```. Am I making any mistake?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-339624024:99,depend,dependency,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-339624024,5,['depend'],"['dependencies', 'dependency']"
Integrability,"Hi @qindan2008 - is this the full log file that is produced, or is there more to it? If there is more to the log file can you post it? Would you mind posting one or two of your variants as well? They can be simplified - I only the need position and alleles. Also, did you happen to make any modifications to the data sources? If you enabled gnomAD, Funcotator will try to read the gnomAD data sources via the Google Cloud API, which may be slow or fail depending on your internet connection and settings. You could experience similar issues if you added another web-facing data source.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7135#issuecomment-799646869:453,depend,depending,453,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7135#issuecomment-799646869,1,['depend'],['depending']
Integrability,"Hi David,. Thanks for your response and effort developing the best practices pipeline and GATK. I'm not certain, but I would suspect that a significant percentage of your users may also not use the best practices pipeline for one reason or another. In my particular case, I intersect calls from multiple variant callers and prefer to run this pipeline without the added abstraction of Terra (or WDL) for the sake of simplicity. This was easy to fix on my end, thanks again. Andrew. @davidbenjamin. > On Sep 3, 2019, at 4:16 PM, David Benjamin <notifications@github.com> wrote:; > ; > @lbergelson The stats file is not optional, but the argument is optional because by default FilterMutectCalls looks for the stats file produced automatically by Mutect2 in the same directory as the output vcf.; > ; > @andrewrech The official best practices pipeline -- that is, mutect2.wdl in this repo and hosted on Terra (formerly Firecloud) -- handles this automatically. We generally discourage users from writing their own pipelines because it takes very long and can easily yield inferior results. Is the official pipeline missing a feature that you need?; > ; > As for backwards compatibility, while we can guarantee that Mutect2 and FilterMutectCalls from the same GATK release will always work together we do not make any promises about the interoperability of different releases.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6124#issuecomment-527643415:1334,interoperab,interoperability,1334,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6124#issuecomment-527643415,1,['interoperab'],['interoperability']
Integrability,"Hi Marissa - I think we're all in agreement that we'd like to find a way to make Intel-TF the default, but whether or not we can have CNNScoreVariants require AVX to run is less clear. Naturally, we'd prefer to not have to provide a custom TF distribution for a fallback, but there are 3 cases where we may not have a choice: user with old hardware, Travis/CI testing, and GCE. We may need to provide a fallback environment for those (I'll try to get resolution on that). If it turns out we do, I'm actually not suggesting the fallback be automatic (3 in your list), just that we have a graceful failure mode and an instructive error message. . In the meantime, there is still the issue that this PR fails to even build on Travis. It looks like it produces so much output building the Docker image that it exceeds the allowable Travis build log size. That will need to be resolved, and we'll also need to understand the impact of this change on the size of our Docker image, which is already large, and continues to be a challenge for us.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429451059:634,message,message,634,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429451059,1,['message'],['message']
Integrability,"Hi all, thanks again for working to integrate this code!. Saw some confusion in the comments above and just wanted to clarify: if you take a look at the VQSR-lite PR https://github.com/broadinstitute/gatk/pull/7954/commits that the current branch is rebased upon, you'll see that it contains a version of the Joint Genotyping WDL (which was put together by Megan for Ultima) along with Java code for the tools (which was written by me). Both the WDL and the code have been updated in subsequent PRs. The WDL was rewritten by me in #8074; the main difference is that we no longer run SNPs and indels filtering in ""series"", but instead run them in a single step. However, this requires that you use the same annotations for both SNPs and indels; GVS might not be ready for that just yet, since the default WARP implementation uses different annotations. (But see also the comment here: https://github.com/broadinstitute/gatk/pull/8074#issue-1423991277. The gist is we can easily reimplement Megan's/WARP's ""serial"" SNP-then-indel workflow using the simpler single-step workflow.) (EDIT: I was originally confused here, Megan’s WDL simply runs SNPs and indels separately—thanks to George for correcting me here!). Note also that test infrastructure was moved from Travis to Github Actions between these PRs, so the Travis references above have already been cleaned up. There have also been a few additional minor PRs merged in the interim, with a couple more incoming. These PRs do not fundamentally change the interfaces of the tools/WDL, however, so I think you can update to them when you're ready. Punchline: this branch should suffice for a first cut of a VQSR/VQSR-lite bakeoff, and although it is already slightly out of date, it shouldn't be too much work to get things updated after the first cut is done.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8157#issuecomment-1412640649:36,integrat,integrate,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8157#issuecomment-1412640649,2,"['integrat', 'interface']","['integrate', 'interfaces']"
Integrability,"Hmm, I started taking a stab at the LL score implementation, but I think it's going to complicate the code quite a bit and add some branching options to the tool interfaces. Compounding this with a change in the use of ""truth"" and ""validation"" terminology, I fear that the resulting differences from the legacy strategy might be a bit much for users to digest!. So I'd want to better understand the cost/benefit before we proceed. How critical is automatic tuning of the hard threshold? And what's the relative importance to method changes that increase AUC (i.e., as opposed to figuring out where on the curve to hard threshold)? Is there a clear path forward for evaluating such a tuning process? @meganshand would be glad to chat more!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065524909:162,interface,interfaces,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065524909,1,['interface'],['interfaces']
Integrability,"I added integration tests for simple output and including features or verbose. While doing it, I realized that GATK 3.5 included some filters that wasn't included here, and that indels weren't tracked, so I changed also the code to fit the previous implementation. Back to you @akiezun.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1836#issuecomment-221651158:8,integrat,integration,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1836#issuecomment-221651158,1,['integrat'],['integration']
Integrability,I can confirm that the fix works for me: I now see a user-friendly error message. Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/357#issuecomment-91289762:73,message,message,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/357#issuecomment-91289762,1,['message'],['message']
Integrability,"I don't think that will work as the key needs to be `GATKRead` to take advantage of the `SAMRecordToGATKReadAdapterSerializer`. How about writing a new `Comparator<GATKRead>` that wraps a `SAMRecordCoordinateComparator`? That should be pretty simple and won't require a new serializer. BTW minor correction: `ReadSparkSink` operates on `JavaPairRDD<GATKRead, Void>` (not `JavaPairRDD<GATKRead, SAMRecordWritable>`) at the moment - the values are null so as to not duplicate the amount of data going through the shuffle.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1249#issuecomment-162024954:180,wrap,wraps,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1249#issuecomment-162024954,1,['wrap'],['wraps']
Integrability,"I finished the implementation for the draft `SlidingWindowWalker` (I should implement an example and an integration test, but I would like to wait till some issues are solved). made a ""TODO"" about the way in which the intervals are constructed, because I will need a that `ReadShard` have a way to construct a shard without `ReadSource` (either null or empty source), just in case that the implemented `SlidingWindowWalker` does not require reads. @droazen, could you review and give me some feedback about this, because this class is important for other parts of GATK?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-215710163:104,integrat,integration,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-215710163,1,['integrat'],['integration']
Integrability,"I propose to still hide from the command line and docs the example walkers. They are meant only for developers, to show how to use some kind of walkers and have a running tool for integration tests. Having then in the command line will generate software users to run them instead of use them for developmental purposes... In addition, I think that this is a good moment to also generate a sub-module structure (as I suggested in #3838) to separate artifact for different pipelines/framework bits (e.g., engine, Spark-engine, experimental, example-code, CNV pipeline, general-tools, etc.). For the aim of this issue, this will be useful for setting documentation guidelines in each of the sub-modules: e.g., example-code should be documented for developers, but not for the final user; experimental module should have the `@Experimental` barclay annotation in every `@DocumentedFeature`; etc.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346291829:180,integrat,integration,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346291829,1,['integrat'],['integration']
Integrability,"I talked to comms and we agreed that a ""mitochondria-mode"" argument to Mutect2 was the right balance of clarity (you're really running Mutect2 not a wrapper) and simplicity (you don't need a laundry list of arguments to change which mode you're in if you just want to run with optimized defaults). . @ldgauthier @davidbenjamin @takutosato @rcmajovski Could you please take another look? Removing the wrapper tools has cleaned up the code so there are fewer changes now. I also changed TLOD to LOD in this version, but I'm happy to take that out and have that be future work if anyone is worried about it being a breaking change.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-428195077:149,wrap,wrapper,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-428195077,2,['wrap'],['wrapper']
Integrability,"I think that it is necessary to have a way for downstream projects to override some of the top-level arguments in the base CLP class. For example, the config file is for documentation purposes, but I don't want to expose users to that argument because I will set the defaults programmatically. Another example is the GCS retries, which might not be useful for a software that is not planning to support GCS even if it is already implemented (or does not want to expose). As a downstream developer, for me it is important to being able to configure arguments and expose/hide them to my final users; with the current implementation, my main issue is to have an argument that are irrelevant for the toolkit user and that I get questions about why and how to use them (the most clear example, the config file). If the main problem is to change an interface, a default value for new methods can be added to keep the same behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183:843,interface,interface,843,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183,1,['interface'],['interface']
Integrability,"I think we can generally enable this by pushing the option up to VariantWalker / GATKTool and integrating it with the createVCFWriter method. . It can optionally return a writer wrapped in a decorator that only outputs sites within the given intervals. We might want to rename the option in that case to something like ""only-output-variants-starting-in-intervals"" so it's clear that it only effects variant outputs. Or make it work with generated bamWriters too...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6339#issuecomment-568100260:94,integrat,integrating,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6339#issuecomment-568100260,2,"['integrat', 'wrap']","['integrating', 'wrapped']"
Integrability,"I tried a quick upgrade of our guava dependency from 18 -> 22, but it ends in test failures. It looks like at least one of our hadoop dependencies requires guava <= 18. I'm not totally clear if it's an issue for hadoop-core or only in hadoop-minicluster which is a library we use for running tests. If you're not using hdfs I think you won't have any problems including 22, but I'm afraid we can't upgrade our default version without some work. . Hopefully hadoop 3.x will solve the problem in general by shading their internal version of guava. ; https://issues.apache.org/jira/browse/HADOOP-14284, https://issues.apache.org/jira/browse/HADOOP-10101. It sounds like you have a reasonable workaround, let us know if you have further issues with it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516:37,depend,dependency,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516,2,['depend'],"['dependencies', 'dependency']"
Integrability,"I tried to do it, and I'm afraid that it won't be trivial as I expected: because it is a facade, there is not accesibility to `Logger.setLogLevel()` and this is required to set the verbosity level in the command line. After explore a bit the code, it seems that [`LoggingUtils`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java) is the only place where a concrete implementation should be used. My suggestion is to move this class to a package that could be excluded by the backend user (because it contains methods to change the logging of log4j, I suggest `org.apache.logging.log4j`), which implements a simple interface/abstract class `org.broadinstitute.hellbender.utils.LoggingUtils` to set the log level (LoggingUtils.setLoggingLevel(final Log.LogLevel verbosity)`. The default implementation (that could be used by final users callid`super.setLoggingLevel(final Log.LogLevel verbosity)`) could setup the htsjdk and the java.util.logger.Logger. This implementation requires to change the `CommandLineProgram` to have a setter for the `LoggingUtils` to use, that could be set in `Main` (as in my PR for improve the extensibility of this class). The only pronblem is that it requires to be initialize with a simple implementation class of `LoggingUtils`, which should use the default. I think that this design does not break the behaviour of GATK, but introduce more complexity in the code. If you think that this is worthy, I could implement it today. @lbergelson, I'm not able to run Spark tools in a cluster yet, neither in gcloud dataproc, sorry. I'll wait for your answers on this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062:683,interface,interface,683,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062,1,['interface'],['interface']
Integrability,"I was outputting to .vcf.gz. . I reran the command to output to just. vcf and it runs without error:; ```; /gatk-launch FilterByOrientationBias --artifactModes 'G/T' -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P ~/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk FilterByOrientationBias --artifactModes G/T -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; 01:16:16.916 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.4.3.jar!/com/intel/gkl/native/libgkl_compression.dylib; [June 6, 2017 1:16:16 AM EDT] FilterByOrientationBias --output test_filterbyorientationbias.vcf --preAdapterDetailFile /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics --artifactModes G/T --variant /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891:421,wrap,wrapper,421,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891,1,['wrap'],['wrapper']
Integrability,I wasn't able to reproduce this exact issue. However at least I can make the error message a bit more user-friendly (submitting #2417 for review). Crucially this will now give the name of the file we're having issues with (thus removing the uncertainty about whether it's the data or the index file we cannot access).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286:83,message,message,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286,1,['message'],['message']
Integrability,"I'd rather keep the message more generic, and think of the check as simply defining what a valid `CopyRatio` object can be: an interval associated with a finite double value. One might imagine that someone would try to create such an object that does not originate from a BAM (perhaps for test data, or for imputing missing values in pre-existing data, etc.). This check says that they must create it with some finite value. A more appropriate place for the sort of message you suggest is in the relevant denoising method. In the edge case you encountered, you used a BAM that was almost completely uncovered in all bins at the specified resolution, resulting in a sample median of zero. Since one of the steps in standardization is dividing by the sample median, this results in a divide by zero. I've added the corresponding check.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4292#issuecomment-365726746:20,message,message,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4292#issuecomment-365726746,2,['message'],['message']
Integrability,"I'm not completely opposed to that way of dealing with this, but I'm not yet convinced either. . I'm not sure I see how having an extra argument is somehow shorter than having one special value that is included in the description of the original argument. As in:. --trimWhatever | -trimWvr -- bla bla bla; default w; min x max y; to disable trimming, use z. . As for the documentation auto-generator showing the two args together, that is dependent on setting up the arguments so that the code specifies they are related, and adding some logic to the auto-generation to pull related arguments together. (As a contributing developer to a documentation auto-generator --the GATKDocs-- I can tell you that is not necessarily trivial and adds even more moving parts.) This also generates additional complexity for third-party developers of wrappers (such as Galaxy). Finally, it can be a source of confusion for users who are trying to look up an argument called ""-dont-Trim-whatever"" since presumably it's only going to be listed under T (-Trim-whatever) and not under D in the alphabetical list. Or should it be listed twice? . A reference manual can be very ""nice"" and helpful, and it must be organized in the most intuitive way possible, especially since there is no way we can provide examples that cover every single use case under the sun (trust me, there's not).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/143#issuecomment-71122930:439,depend,dependent,439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/143#issuecomment-71122930,2,"['depend', 'wrap']","['dependent', 'wrappers']"
Integrability,"If you're interested in BWASpark tool I might wait a bit. There are a lot of issues with it as it currently stands, it's one of the least tested tools we have. We have someone working on a different more efficient implementation of the bwa bindings that may eventually be integrated into mainline gatk, so we've sort of stopped most development on BWASparkEngine until we're clear on the direction that the new work is going to take.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119998:272,integrat,integrated,272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119998,1,['integrat'],['integrated']
Integrability,"It's not a technical problem but it's more than a question of style. It's a user experience problem. Happy to go into detail at some point (just not now). I hear you on the internal wiring rationale; but I think we should explore whether it's possible to fix that, potentially through changes to WDL itself. Clearly the language isn't allowing you to do what you need as an author and what I need as a user -- which I would characterize as ""conditional optionality"", ie things are made optional or not depending on a condition. Would be good to get redteam involved to see what they can suggest.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334049665:502,depend,depending,502,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334049665,1,['depend'],['depending']
Integrability,"Looking at the existing code in Hadoop_BAM it makes the assumption that coordinates are always of the form ```chr:start-stop``` never things like ```chr```, ```chr:pos```, ```char:star+```... I've just generalized a bit more so that it can handle ':' and '-' inside the ```chr``` in a PR. I guess is not ideal but In any case this addresses the current fire and dependants have a clear work around which is to provide their intervals in the expected ```chr:start-stop``` format.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3360#issuecomment-331202249:362,depend,dependants,362,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360#issuecomment-331202249,1,['depend'],['dependants']
Integrability,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:162,depend,dependencies,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261,7,"['depend', 'message']","['dependencies', 'dependency', 'message']"
Integrability,"Looks like this failed on travis. I think given that given the lateness of the hour (release wise), we might want to take the original change that removes the libgcc-ng dependency, since that passed on travis, and rely on the simple workarounds for osx, which we'll have to convey out-of-band. Anything that requires changing the docker image seems risky at this point, not to mention that the image is already at 5.2 gig, which is way over our desired target. @samuelklee Any thoughts on this ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4087#issuecomment-356131086:169,depend,dependency,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4087#issuecomment-356131086,1,['depend'],['dependency']
Integrability,"Merging this now to have usable VCF NIO support in master -- continuous tests to prove that the wrapper is applied will be added in a separate PR, but my ad-hoc tests on the latest version of this branch suggest it's working fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2393#issuecomment-277697090:96,wrap,wrapper,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2393#issuecomment-277697090,1,['wrap'],['wrapper']
Integrability,"Need some guidance here. The CompareSAMs tool was not propagating the validation stringency. I have a fix for that, but that alone doesn't fix the compareBAMFiles test in BaseRecalibrationIntegrationTest.java, since that uses SamAssertionUtils.assertSamsEqual, which also doesn't propagate (or accept) a validation stringency. Changing SamAssertionUtils to use either SILENT or LENIENT does fix the integration test, and all the other tests pass, but it seems like a relaxing of the stringency, and I'm not sure it should be necessary to the BQSR test. If relaxing the stringency for BQSR test _IS_ the right path, one possibility is to add a new method to SamAssertionUtils that accepts a validation stringency argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266:399,integrat,integration,399,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266,1,['integrat'],['integration']
Integrability,"Nice that HTSJDK is moving forward to version 3. The points that I would like to address are similar to yours, with some inclussions. * Regarding NIO support, I would go to remove completely `File` support. If API users need to use the `File` abstraction, they should convert to a `java.nio.Path` using the `toPath` method.; * In addition, I would like that HTTP/S and FTP is handled also with NIO. For HTTP/S, I am working in a simple `FileSystemProvider` that should be good enough for using in combination with HTSJDK ([jsr203-http](https://github.com/magicDGS/jsr203-http)), and I can speed up the development there for needs in HTSJDK; for FTP, maybe [ftp-fs](https://github.com/robtimus/ftp-fs) can be used or a simple implementation can be derived from the HTTP/S implementation (without credentials). This will remove the special handling of HTTP/S and FTP paths in HTSJDK in favor of a consistent and pluggable manner.; * Interfaces for the data types are great, and maybe it will be good to have codec interfaces for both encoding and decoding. For example, I am missing encoders in tribble (an attempt in https://github.com/samtools/htsjdk/pull/822 for writing support).; * For VCF, I would like to have a less diploid-centric interface and design, or at least a way of configure the catching of genotype-related attributes. Currently there are methods for homozygotes/heterozygotes that aren't really useful for triploids or even VCFs without variation (for example, in Pool-Seq data).; * Modular design for artifacts: thus, a project with only SAM/BAM requirements will require only `htsjdk-sam`, and if they also want CRAM support, `htsjdk-cram`. See https://github.com/samtools/htsjdk/issues/896 for more info about it.; * Common license for all HTSJDK, or at least for each module. This will be good for taking into account legal concerns when including the library, because now there is a mixture depending on the files that are used. This is what is coming to my mind now. Maybe I ad",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-363390940:1012,interface,interfaces,1012,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-363390940,1,['interface'],['interfaces']
Integrability,"OK, I experimented a bit with removing the R install from the base image and adding the R dependencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:90,depend,dependencies,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954,5,['depend'],"['depend', 'dependencies']"
Integrability,"Okay tranche filtering and training script are in. They're pure python right now but it would be simple to wrap them in java CLP via PythonScriptExecutor. These scripts add several dependencies which will probably make the already big docker quite a bit bigger. Long term I think we can get rid of most of them as we already have for inference, but we want to have some training functionality available by AGBT which is the week after next. Ready for a first round review @cmnbroad.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4245#issuecomment-362679008:107,wrap,wrap,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4245#issuecomment-362679008,2,"['depend', 'wrap']","['dependencies', 'wrap']"
Integrability,"One note that might be useful (or known already to the team): simply calling `cache()` doesn't cause any action. It seems that one might need to force the computation to be done on the RDD (e.g. `count()`), for caching to work, if the predicate depends on the results of computation. (ref last comment in #1877)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1811#issuecomment-225204823:245,depend,depends,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1811#issuecomment-225204823,1,['depend'],['depends']
Integrability,"One of our goals for alpha (https://github.com/broadinstitute/gatk/issues/961) is actually to wrap `spark-submit` and its many options to make it easier to run hellbender tools on spark. We want users to be able to type a simple command like `./hellbender ToolName [toolArgs] --sparkMaster X`, and have hellbender figure out whether to invoke `spark-submit` or `gcloud dataproc` on their behalf, and provide sensible defaults for all relevant spark options. . Perhaps there is a way in `SparkCommandLineProgram` to detect whether an option has already been set externally, and allow the default to be overridden if it has been?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1070#issuecomment-152538633:94,wrap,wrap,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1070#issuecomment-152538633,1,['wrap'],['wrap']
Integrability,"Overall, this looks fine, just a few minor comments. Two broader questions/issues:; - Does the test data exercise all of the code paths and edge cases?; - In general, putting the majority of testing in integration tests instead of unit tests is bad pattern. It have several bad consequences (1) it becomes less clear which cases are being tested (2) it's slower than just running unit tests and (3) it makes it unhelpful to (perhaps someday) move to a testing framework that only runs tests relevant to the code directly affected (because all integration tests must be run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490:202,integrat,integration,202,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490,2,['integrat'],['integration']
Integrability,"Provider; # used by the testGATKPythonEnvironmentPackagePresent test in PythonEnvironmentIntegrationTest needs to be updated; # to reflect the changes.; #; name: gatk; channels:; # if channels other than conda-forge are added and the channel order is changed (note that conda channel_priority is currently set to flexible),; # verify that key dependencies are installed from the correct channel and compiled against MKL; - conda-forge; - defaults; dependencies:. # core python dependencies; - conda-forge::python=3.6.10 # do not update; - pip=20.0.2 # specifying channel may cause a warning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge::keras=2.2.4 # updated from pip-installed 2.2.0, which caused various conflicts/clobbers of conda-installed packages; # conda-installed 2.2.4 appears to be the most recent version with a consistent API and without conflicts/clobbers; # if you wish to update, note that versions of conda-forge",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:1976,depend,dependencies,1976,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,1,['depend'],['dependencies']
Integrability,"Rationale for engine changes:; This tool opens a large number of feature files (TSVs, not VariantContexts) and iterates over them simultaneously. No querying, just a single pass through each.; Issue 1: When a feature file lives in the cloud, it takes unacceptably long (several seconds, typically) to initialize it. A few seconds doesn't seem like a long time, but when there are large numbers of feature files to open, it adds up. This is caused by a large number of codecs (mostly the vcf-processing codecs) opening and reading the first few bytes of the file in the canDecode method. To avoid this I've reversed the order in which we test each codec, checking first if it produces the correct subtype of Feature, and only then calling canDecode. If you don't know what specific subtype you need, you can just ask for any Feature by passing Feature.class. It's much faster that way.; Issue 2: Each open feature source soaks up a huge amount of memory. That's because text-based feature reading is optimized for VCFs, which can have enormously long lines. So huge buffers are allocated. The problem is compounded for cloud-based feature files for which we allocate a large cloud prefetch buffer. (Though that feature can be turned off, which helps a little.) But the biggest memory hog is the TabixReader, which always reads in the index, regardless of whether it's used or not. Tabix indices are very large. To avoid this, I've created a smaller, simpler FeatureReader subclass called a TextFeatureReader that loads the index only when necessary. The revisions allow the new tool to run using an order of magnitude less memory. Faster, too.; Issue 3: The code in FeatureDataSource that creates a FeatureReader is brittle, and tests for various subclasses. To allow use of the new TextFeatureReader, I added a FeatureReaderFactory interface that allows one to ask the codec for an appropriate FeatureReader.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1284340770:1832,interface,interface,1832,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1284340770,1,['interface'],['interface']
Integrability,"Regarding the non-Docker integration tests failing earlier today, I think this was because the R packages were added to the Travis cache in #3101. @cmnbroad cleared the cache to see if we could reproduce a compiler error introduced in #3934 on Travis (for the record, we could reproduce it on my local Ubuntu machine and gsa5, but not on Travis). This removed the cached getopt dependency, which then caused tests to fail. See #4246.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4209#issuecomment-359999441:25,integrat,integration,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4209#issuecomment-359999441,2,"['depend', 'integrat']","['dependency', 'integration']"
Integrability,"So the long term timeline is fairly up in the air. You can check out the alpha and beta milestones for some idea of what's been prioritized. Alpha milestone is due for completion ~this week. Beta is much more up in the air and will depend at least in part on feedback from user. . Incidentally, if you're interested in CNV calling take a look at https://github.com/broadinstitute/gatk-protected/ which has some tools for CNV calling built on this engine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1198#issuecomment-160717606:232,depend,depend,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1198#issuecomment-160717606,1,['depend'],['depend']
Integrability,"Some comments/questions for the review:; - I'll add a separate ticket to rewrite the integration tests, all of which pass and most of which are disabled since they require access to large files on the broad file system. In the meantime I need to add a couple of small tests to get the coverage back up, and would like to get the CR process started.; - I ported a bunch of support files but need feedback on whether they're in the right location.; - Somewhere I saw something that said GATK no longer supports .ped files ? If not, what should the replacement be in the tests require pedigree input?; - Is it a requirement to support Ploidy > 2 ? The current GATK tool, and thus the HB tool, do not; - I did not port the WalkerTestSpec.disableShadowVCF? Is that needed in Hellbender ?; - Are there other headers I should be applying to the output variant file ?. Command Line Arguments:; - I didn't port the GATK command line argument ""-no_cmd_line_in_header"". Should I ? And if not, should the command line args automatically be propagated to the output vcf file ? I didn't see GATK do this anywhere.; - There was one test that used --variant:dbsnp on the command line but I couldn't find the code that processed that in GATK, not sure what the means on the command line.; - I replaced ""-U LENIENT_VCF_PROCESSING"" with ""--lenient"" (testFileWithoutInfoLineInHeaderWithOverride needs this to pass).; - I replaced ""-L"" with --interval since HB seems to use -L for ""lane"" ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027:85,integrat,integration,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027,1,['integrat'],['integration']
Integrability,"Some offline discussions have led us to the conclusion that this is best handled by tools upstream. Adapters should not be simply soft-clipped, so it shouldn't be the responsibility of M2 or HC to include logic to remove adapters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6346#issuecomment-575334816:221,adapter,adapters,221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6346#issuecomment-575334816,1,['adapter'],['adapters']
Integrability,"Sorry, I wasn't very clear: Spark doesn't return the user exception to the driver even as the 'cause' exception (only the exception message is preserved). So it won't be possible to do the unwrapping in the same way at the moment. I agree that #551 will help catch regressions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/574#issuecomment-113196502:132,message,message,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/574#issuecomment-113196502,1,['message'],['message']
Integrability,"Tests with `TEST_DOCKER = true` failed, I'm not entirely clear why. Here's a bit of the log:. > Building 85% > :test > Resolving dependencies ':jacocoAgent'aven.org/maven2/org/jacoco/org.jacoco.agent/0.7.7.201606060606/org.jacoco.agent-0.7.7.201606060606.jar; > Building 85% > :test > 207 KB/233 KB downloaded> Building 85% > :test > 0 tests completed> Resolving dependencies ':testRuntime':test FAILED",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367:129,depend,dependencies,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367,2,['depend'],['dependencies']
Integrability,Thank you @lbergelson for the fix! For once a dependency conflict that turns out to be simple!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7203#issuecomment-841279259:46,depend,dependency,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7203#issuecomment-841279259,1,['depend'],['dependency']
Integrability,"Thank you @vruano for your diligent review. I've implemented logger classes to encapsulate the metrics classes. Unfortunately the metrics classes must remain public in order to write output using `MetricsUtils.saveMetrics()`, but at least the tools aren't using them directly. There are two logging class groups - one for Filter and one Score. For Filter, there is an interface `PSFilterLogger` that is implemented by a file-logging class `PSFilterFileLogger` and a dummy class `PSFilterEmptyLogger` that does nothing. There are analogous classes for Score, but there is no Empty logger because it's not actually necessary. This adds a lot of new classes (maybe you can think of a better way) but usage has been greatly simplified. As we discussed in person, I don't think there is a faster way to count the reads in Spark. If you wanted to count the reads as they pass through, you would have to use some kind of atomic type that would be slow. Also it may be impossible to account for cases when tasks fail and restart. @lbergelson @droazen In this PR, I wanted to use htsjdk's MetricsFile and MetricBase classes for writing metrics to a file. I notice that these classes are mostly used for picard-related things. Is this the preferred way to do things? They do force you to expose public variables and also use an upper-case naming convention. On the other hand, they are somewhat convenient.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-334308160:368,interface,interface,368,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-334308160,1,['interface'],['interface']
Integrability,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:373,integrat,integration,373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791,1,['integrat'],['integration']
Integrability,"Thanks @lbergelson! I agree that it might be good to break into more layers—could be worth talking to SV team and seeing what lessons they learned in putting together their hierarchy of images. Also, note that I pushed the install of miniconda into the base, but I did not push down the setup of the GATK conda environment itself (which takes the bulk of the time during the main-image build, as it requires lots of downloading). I think I commented elsewhere that a good strategy might be to set up the conda environment with the non-GATK python dependencies in the base, and then update the environment via a pip install of the GATK python packages in the main image. This would let us make python code changes without having to rebuild the base, but might require a bit of scripting to create a final yml for non-Docker users. I also agree that it would be nice to cut down the Travis time, might be worth taking a look at other strategies to do that—could save everyone a lot of time!. Will try to add the test you suggested sometime tomorrow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-621487662:547,depend,dependencies,547,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-621487662,1,['depend'],['dependencies']
Integrability,"Thanks for the quick review, @ldgauthier!. I don't think my fix will address any non-determinism in the integration tests. I'm inclined to just do better with the new tools---there does seem to be enough duct tape in the integration tests regarding re/setting the RNG so that the exact-match tests consistently pass. As for learning how to run the WARP tests, I think that would indeed be pretty useful---for anyone that might have to update code for VQSR or the new tools in the future! Can we teach everyone to fish? Isn't this what CARROT is for?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1061830649:104,integrat,integration,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1061830649,2,['integrat'],['integration']
Integrability,"Thanks for the response, @droazen! Technically, yes, that would be satisfactory & accurate... and if that's easiest, I'm fine with that. . From a user perspective though, it might be beneficial to report the first occurrence of this error, as that's most likely where I would go back to do future testing & troubleshooting. That being said, all of the overlapping intervals are already outputted to stderr, so all the information is retained regardless, and I could just look through the logs to find that first problematic interval. As an aside, I find it a bit weird that the overlapping interval message shows up as a _warning_ even when using the `-no-overlaps` option (I would assume it would be an error, not a warning). In my experience, most errors cause the program to quit immediately. So, perhaps instead, if this warning were an _error_ when using the `-no-overlaps` option, the program would stop after the first occurrence of this error... and then the error message would be accurate. Maybe that was the original intent of this code. But, again, if that requires much more testing & changes, when a quick rewording would also suffice, there's no need. If it's simply a rewording, I'm happy to make a pull request. Let me know what you think.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8103#issuecomment-1329747570:599,message,message,599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103#issuecomment-1329747570,2,['message'],['message']
Integrability,"Thanks for the review and running those tests, @ldgauthier! Will restore the aforementioned GnarlyGenotyperIntegrationTests and update a few other exact matches in the rebase this afternoon. You also asked above if there was a theoretical reason to change the threshold. Since it seems the original was relatively arbitrary (at least from what I've been told, happy to be corrected), I think we can leave it. The new annotation is strictly larger, so we will then be slightly more conservative about keeping sites if we leave the threshold fixed. You can think of this as a slight change in the decision boundary in genotype-count space---perhaps I can add some plots to this thread this afternoon to demonstrate. In practice, what we care about is whether: 1) many sites flicker across the change in boundary after hard filtering, and/or 2) these sites result in discrepancies post-VQSR. I think the tests you ran suggest that we don't need to worry much about the second issue, and I can take a closer look later to check about the first (which will depend simply on the number of samples and the allele frequency spectrum). We can also take a basic look at how things might change with e.g. more samples using the aforementioned plots.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-914471272:1052,depend,depend,1052,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-914471272,1,['depend'],['depend']
Integrability,"Thanks for your feedback, @cmnbroad. In my case, I think that `IntegrationTestSpec` is a good way of avoid complicated code to test tool results, but it is true that it have some problems (one that I had was the usage for testing programs where the outputs are determined by a prefix in the command line, but with different suffixes). I think, from the API user point of view, that a class like `IntegrationTestSpec` to facilitate program output testing (including user exceptions) will be nice for developing purposes. Nevertheless, this is just a convenience that I asked for here, but I can try to solve the issues with the `BaseTest` instead. By the way, I would love to have this interface in GATK at least for now, because several of my tools rely on the `IntegrationTestSpecs` for development...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889:685,interface,interface,685,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889,1,['interface'],['interface']
Integrability,"Thanks! Just to be clear, the PR is incomplete. We need to determine the additional dependencies (which were previously installed along with R) required for AVX, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-413599300:84,depend,dependencies,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-413599300,1,['depend'],['dependencies']
Integrability,"Thanks, @cmnbroad!. - You're right about gatkbase-2.1.0, that image is coming from #5026, which needs some more work. We can delete it for the time being if you think it'll cause confusion.; - Correct, I think the import statement for `reshape` in BQSR.R was always incorrect/extraneous. `reshape2` is the correct dependency for `ggplot2` (which is itself imported), and `reshape` is not explicitly used in BQSR.R. So to recap: I removed the installation of this unnecessary package, but failed to remove an unnecessary import statement since it was in an untested code path, which was then caught when users tried to run the tool. Investigation of this issue then revealed that `ggplot2` was not installed correctly in the current base image, due to a completely unrelated dependency issue.; - Good call on clearing the Travis cache. Not actually sure how to do that, do I just delete the cache at https://travis-ci.org/broadinstitute/gatk/caches for this particular branch?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408447924:314,depend,dependency,314,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408447924,2,['depend'],['dependency']
Integrability,That would be a clearer error message.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/489#issuecomment-99104999:30,message,message,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/489#issuecomment-99104999,1,['message'],['message']
Integrability,"That's why I am not using in ReadTools and other developmental toolkit the base class from GATK, due to the polluted command line with unused arguments. I think that for give flexibility, some of that arguments should be configurable by extending classes. For example, some tools that does not require reads at all should be able to turn off the read arguments. That will be very useful, although I am not sure how to do it in a proper way without adding more and more interfaces for argument collections. In context case of this PR, I think that adding it does not have any real effect on the GATK codebase, and a lot is gained by downstream projects. For example, if the wrapper script adds another argument that should be parsed in `Main` and documented, the GATK team just add it to its class. If a toolkit has a similar wrapper script, it can also add its own only-doc argument by simply overriding the method...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371822090:469,interface,interfaces,469,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371822090,3,"['interface', 'wrap']","['interfaces', 'wrapper']"
Integrability,The PR at googleapis/google-cloud-java#5789 makes it possible to add a BigQuery dependency without having to move to the unshaded version. This should make our lives simpler.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5928#issuecomment-516181784:80,depend,dependency,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5928#issuecomment-516181784,1,['depend'],['dependency']
Integrability,"The advantage of using SLF4J is that it is a general facade, so it makes simpler to change for one logging system to other if the bound is implemented. For the most common logging systems (log4j, jul, JLC, etc.), there are this implementation and even no-op logging. One of the nice things from slf4j is that it allows to use the logging format set by the software to every library dependency, controlling the verbosity of other libraries too. . After having a look to the gradle dependencies, it seems that ADAM and Spark use slf4j. This will allow better integration with the two libraries: now the `slf4-jdk` is completely removed, and I don't know if this will blow up at some point if some of the ADAM/Spark classes try to load them. In addition, it will make GATK4 more general. Regarding features, I'm not using more that what log4j is providing, but I'm quite familiar with logback and I have a bias to use it if possible, but the GATK framework as it is implemented now ""force"" to use log4j. But anyway, I'm happy also with using log4j and I was only suggesting this for make GATK4 more general (and to come back in my work to logback, but that is just personal taste). @lbergelson, feel free to close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054:382,depend,dependency,382,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054,3,"['depend', 'integrat']","['dependencies', 'dependency', 'integration']"
Integrability,"The new pipeline is in a complete state. Nearly all tools and scripts were rewritten, many from scratch. I've tried to minimize interaction with old `tools/exome` code (notably, `ReadCountCollection` and `SegmentUtils`). There are still some improvements that can be made (especially in segment union and the subsequent modeling), but we should be ready to go for a first validation. Some notes:. WDL:; - I've moved the old pipeline to `somatic_old` folders.; - There is now just a matched-pair workflow and a panel workflow. We can add a single BAM case workflow or expand the matched-pair workflow to handle this, depending on the discussion at https://github.com/broadinstitute/gatk/issues/3657.; - WES/WGS is toggled by providing an optional target-file input.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:616,depend,depending,616,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['depend'],['depending']
Integrability,"The previos message was written a bit quick from my phone. The concrete PR is https://github.com/samtools/htsjdk-next-beta/pull/12. @lbergelson - would like to have a look to it, or do you prefer that I open another one with the interface on top of the CIGAR part? I prefer to go step by step, as it is clearer than adding too many classes to review at once...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5267#issuecomment-428735904:12,message,message,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5267#issuecomment-428735904,2,"['interface', 'message']","['interface', 'message']"
Integrability,"The problem is that the catch block in `CommandLineProgram` is calling both `commandLineParser.usage()` and `printDecoratedUserExceptionMessage()` -- it should only be calling `commandLineParser.usage()`, and letting the catch block in `Main.mainEntry()` call `printDecoratedUserExceptionMessage()`. Otherwise there are cases where a `CommandLineException` will be caught without printing any error message. This is a bug and should be fixed. The distinction between ""errors that are the user's fault"" and ""errors that are not the user's fault"" is very important for our support team -- it allows them to deal with bug reports and forum questions much more efficiently. Whatever solution we come up with here should maintain that distinction, and clearly label errors like ""bad argument value"" as being a user error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938:399,message,message,399,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938,1,['message'],['message']
Integrability,"There are pretty significant incompatibilities between java 8 and 11 that make it hard to run the same code on both. It affects a number of our dependencies which use features which were removed/altered from java 8 -> 11. Unfortunately despite there being significant pain in switching to 11 there aren't particularly compelling new features after 8 so there isn't much incentive for developers to move forward. That said, you CAN now run gatk on java 11 if you build it using java 11, the jars built on 8 are incompatible with 11 and vice a versa. We consider running on 11 to be a beta feature and would love to hear feedback about either success or failure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6298#issuecomment-561371535:144,depend,dependencies,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6298#issuecomment-561371535,1,['depend'],['dependencies']
Integrability,"This can occur in cases where there was a mixup with the samples, meaning the user intended to run a properly matched normal/tumor pair, but there is a provenance error. This is how @asmoe4 and myself hit this issue. So this is not the same use case as #5821, where they know there's a deliberate mismatch. While we're not expecting the contamination check to provide something sensible in this case, may I suggest that the tool provides a user-friendly message to help debug, rather than a stack traceback. This could happen to other people if they have an accidental mismatch.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5880#issuecomment-483276300:454,message,message,454,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5880#issuecomment-483276300,1,['message'],['message']
Integrability,This should be fixed in the next release as we are now on Picard 2.25.4 in master via #7255. If you need a docker build with an updated picard dependency I would suggest checking out our nightly builds gs://gatk-nightly-builds which should have an up-to-date version of master soon or simply waiting for the next release.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7254#issuecomment-841405791:143,depend,dependency,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7254#issuecomment-841405791,1,['depend'],['dependency']
Integrability,"To add some commentary to why this is happening: It looks like multiple threads are hitting this line simultaneously and based on the overload of `ArrayList.add()` this error could be triggered by multiple calls to `ensureCapacityInternal()` inside the add method:; ```; final List<ReadsPathDataSource> readSources = new ArrayList<>(threads);; final ThreadLocal<ReadsPathDataSource> threadReadSource = ThreadLocal.withInitial(; () -> {; final ReadsPathDataSource result = new ReadsPathDataSource(readArguments.getReadPaths(), factory);; readSources.add(result);; return result;; });; ```; The fix should be simple you just have to make sure ti synchronize the initialization or swap out the readSources object to one that is itself thread safe. @vruano",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899732721:644,synchroniz,synchronize,644,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899732721,1,['synchroniz'],['synchronize']
Integrability,"To add, just in case it wasn't clear, note that this is almost certainly overkill for most somatic applications. However, if this is going to double as a more lightweight germline pipeline (as it is for the time being, as we are using some of the results to prototype SV integration), it might be worthwhile.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386269562:271,integrat,integration,271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386269562,1,['integrat'],['integration']
Integrability,"To clarify this ticket: in `GATKTool.initializeReads()`, just check `readArguments.getReadFiles()` for files ending with a cram extension (should see if there's a canonical method in htsjdk for checking whether a file is cram) -- if you find any and we don't have a reference according to `hasReference()`, throw a `UserException` with a clear error message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/673#issuecomment-125265449:350,message,message,350,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/673#issuecomment-125265449,1,['message'],['message']
Integrability,"To summarize current state of discussions - we're going to have 3 repos, as originally planned (1 for the interfaces and 2 for Intel and IBM implementations, respectively). There will be a bit code duplication but many other aspects (some technical, some organizational) are massively simplified by such architecture. . @droazen @lbergelson @gspowley @paolonarvaez @frank-y-liu @t-ogasawara - please use this ticket to comment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1788#issuecomment-216545820:106,interface,interfaces,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1788#issuecomment-216545820,1,['interface'],['interfaces']
Integrability,"Unfortunately I don't think it's practical to try to enforce that ""only the arg parser can throw `CommandLineException`"" -- there's too much downstream code and too many tools that do so already, so it would be a bit painful to treat it as a bug. Instead I think what we should do is:. * Make sure that barclay uses a separate exception class for internal errors that are not the user's fault. This internal exception class should not be usable outside of barclay (perhaps we can make it package-private?).; * Catch `CommandLineException` in GATK and present it as a user error (output should say ""A USER ERROR HAS OCCURRED"").; * Move the `printDecoratedUserExceptionMessage()` call for caught `CommandLineExceptions` from `CommandLineProgram.parseArgs()` to `Main.mainEntry()`, to ensure that a message always gets printed for `CommandLineException`. As @cmnbroad said, this will have to wait until after the holiday break (most of us are going to be away until early January).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268828854:796,message,message,796,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268828854,1,['message'],['message']
Integrability,"Using the latest version of ADAM (which has a Scala 2.12 version) fixes the 2bit failures. I also added a fix for the `java.nio.ByteBuffer.clear()` problem. All unit tests are passing, and the only integration test failures are the `Could not serialize lambda` problems. It should be possible to fix these by making the relevant classes implement `Serializable` (like in https://github.com/samtools/htsjdk/pull/1408).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527483090:198,integrat,integration,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527483090,1,['integrat'],['integration']
Integrability,"Very funny! Closing, since this is clearly meant as a joke. Let's discuss after alpha ways to actually slim down our dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1120#issuecomment-157435278:117,depend,dependencies,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1120#issuecomment-157435278,1,['depend'],['dependencies']
Integrability,"While we normally don't recommend ignoring that wrapper, this seems like a good reason to do so. . The wrapper is pretty simple, most of what it's doing is some munging of the input to allow it to be more standardized in several different gatk use cases. The only thing I can think of that you would want to be sure to copy is that it sets a number of properties. . We set these spark `--conf` properties with the wrapper. I don't actually know how important some of them are anymore. If it works without them then you're probably good.; ```; ""spark.kryoserializer.buffer.max"" : ""512m"",; ""spark.driver.maxResultSize"" : ""0"",; ""spark.driver.userClassPathFirst"" : ""false"",; ""spark.io.compression.codec"" : ""lzf"",; ""spark.executor.memoryOverhead"" : ""600"",; ""spark.driver.extraJavaOptions"" : EXTRA_JAVA_OPTIONS_SPARK,; ""spark.executor.extraJavaOptions"" : EXTRA_JAVA_OPTIONS_SPARK; ```. These are htsjdk properties we want to set for spark. ; ```; EXTRA_JAVA_OPTIONS_SPARK= ""-DGATK_STACKTRACE_ON_USER_EXCEPTION=true "" \; ""-Dsamjdk.use_async_io_read_samtools=false "" \; ""-Dsamjdk.use_async_io_write_samtools=false "" \; ""-Dsamjdk.use_async_io_write_tribble=false "" \; ""-Dsamjdk.compression_level=2 ""; ```. If you can get this value into your spark environment variables it prevents and anying warning output. `SUPPRESS_GCLOUD_CREDS_WARNING=true`. Let us know how it works for you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6198#issuecomment-539073054:48,wrap,wrapper,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6198#issuecomment-539073054,3,['wrap'],['wrapper']
Integrability,"Yeah, I don't like these new interface methods -- they make `GATKRead` significantly worse. We should cache `isUnmapped`, etc. in the adapter to accomplish the same thing, as @lbergelson suggests. Not that hard, and we can just unconditionally invalidate the cached values (using `Boolean` fields set to null) whenever the read is mutated in any way in order to simplify the logic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282:29,interface,interface,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282,2,"['adapter', 'interface']","['adapter', 'interface']"
Integrability,"Yep, sorry. Just learned that as you were closing it. On Tue, Aug 14, 2018 at 11:51 AM, Louis Bergelson <notifications@github.com>; wrote:. > It's useful to put something like fixes #5104 in the commit message. That; > way it automatically closes the issue and shows a link from the PR to the; > Issue.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4932#issuecomment-412920923>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AoMkWAqeYbmnX12i6s9k_5bEWy149CPFks5uQvITgaJpZM4UyozK>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4932#issuecomment-412923593:202,message,message,202,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4932#issuecomment-412923593,1,['message'],['message']
Integrability,"abadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 01:42:05 2017 -0500. hybrid ADVI abstract argument collection w/ flexible default values; hybrid ADVI argument collection for contig ploidy model; hybrid ADVI argument collection for germline denoising and calling model. commit 56e21bf955d3dc0c52aceb384f28cf6173959de0; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 23:18:39 2017 -0500. rewritten python-side coverage metadata table reader using pandas to fix the issues with comment line; change criterion for cohort/case based on whether a contig-ploidy model is provided or not; simulated test files for ploidy determination tool; proper integration test for ploidy determination tool and all edge cases; updated docs for ploidy determination tool. commit 7fa104b2e9170770cfc5b338835e41215d7fd39c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 18:43:17 2017 -0500. kabab case for gCNV-related tools; removed short args (this also partially affected PlotDenoisedCopyRatios and PlotModeledSegments and their integration tests). commit f02cb024331a986213cfd9fae2da706bbc5ddbd9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 18:02:40 2017 -0500. synced with mb_gcnv_python_kernel. commit 2963bbf8c90418d9b88545c93771ae51cf542db9; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 11:38:05 2017 -0500. Fixing typo in travis.yml. commit 6cf589999c716ec66404eb0a2ae4310dd130a772; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 11:13:58 2017 -0500. editable, full path. commit d998f2d5c2b33dd41e291be9bfeaea72fe479b8a; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:56:24 2017 -0500. revert Dockerfile, change yml. commit 930d7486b7d2cf918fcb16dd03394bb9c9f0611b; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:34:46 2017 -0500. more Dockerfile. commit 94112131526b514ef254bcc2c50a239dbae35aa1; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:25:13 2017",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:8122,integrat,integration,8122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['integrat'],['integration']
Integrability,"and likely to require some iteration so I'd be ok with starting with just the minimal ""porting"" changes to keep things simple, and then doing a code hygiene pass at the end. The ""porting"" changes should include things like updated javadoc, GATK4-style command line arguments, updating of outdated GATK3 terminology such as ""ROD"", Utils.nonNull assertions, etc. The finals and curly braces can wait for a separate pass (we will want to do those in this PR though). If you're not sure what to include or not just ask. I like the idea of keeping the GATK3 tests working as we go along. We should make a clear distinction between the old and new tests though. Ideally the GATK3 tests would be in a separate commit that we can just delete at the end, but that can get unwieldy if the files in the commit need to change as we go along. Alternatively you could isolate them into a separate directory. They should either be disabled or made dependent on a test method (see the `@Test` annotation properties `enabled` and `dependsOn`) that is easily toggled so they can be run locally, but don't run on the CI server. Otherwise the CI server build will always fail. In general, its really helpful to have the first commit in the PR contain the completely unmodified GATK3 source files. It makes it much easier for the reviewer to see what changed for the port. I noticed that you have 2 new plugins included in this. I'm not sure if that was suggested by someone on the GATK team (I'm wondering if we want to go down that path...) but I can tell you that the existing plugins required an enormous amount of test development and review iteration. If we do decide to make them plugins, I think it would be a good idea to do so in a separate PR. Also, if we choose to make an AbstractPlugin base class, we may want that to live in the Barclay repo. As @magicdgs points out, master already has your previous commits, so you should start by rebasing on that. Ideally, the branch would have the following commits bef",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352:1155,depend,dependent,1155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352,2,['depend'],"['dependent', 'dependsOn']"
Integrability,"avadoc.doclet](https://docs.oracle.com/en/java/javase/11/docs/api/jdk.javadoc/jdk/javadoc/doclet/package-summary.html). The javadoc tools in `org.broadinstitute.hellbender.utils.help` may need to be re-written (and it's not clear if it's possible to support Java 8 and Java 11 simultaneously).; * Travis build. Getting this to build and test on Java 11 in addition to the current builds may be fairly involved as the matrix is already quite complicated. (The current PR just changes Java 8 to Java 11 for testing purposes - we'd need a way of getting both to run.). The vast majority of tests are passing on Java 11, the following are failing:; * Missing `TwoBitRecord` (from ADAM); * `ReferenceMultiSparkSourceUnitTest`; * `ImpreciseVariantDetectorUnitTest`; * `SVVCFWriterUnitTest`; * `DiscoverVariantsFromContigAlignmentsSAMSparkIntegrationTest`; * `StructuralVariationDiscoveryPipelineSparkIntegrationTest`; * `SvDiscoverFromLocalAssemblyContigAlignmentsSparkIntegrationTest`; * `java.lang.NoSuchMethodError: java.nio.ByteBuffer.clear()Ljava/nio/ByteBuffer;`; * `SeekableByteChannelPrefetcherTest`; * `GatherVcfsCloudIntegrationTest`; * `Could not serialize lambda`; * `ExampleAssemblyRegionWalkerSparkIntegrationTest`; * `PileupSparkIntegrationTest`; * Native HMM library code caused the tests to crash on my Mac:; ```; Running Test: Test method testLikelihoodsFromHaplotypes[0](org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM@6282d367, true)(org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest); dyld: lazy symbol binding failed: can't resolve symbol __ZN13shacc_pairhmm9calculateERNS_5BatchE in /private/var/folders/cj/wyp4zgw17vj4m9qdmddvmcc80000gn/T/libgkl_pairhmm13775554937319419112.dylib because dependent dylib #1 could not be loaded; dyld: can't resolve symbol __ZN13shacc_pairhmm9calculateERNS_5BatchE in /private/var/folders/cj/wyp4zgw17vj4m9qdmddvmcc80000gn/T/libgkl_pairhmm13775554937319419112.dylib because dependent dylib #1 could not be loaded; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359:2504,depend,dependent,2504,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359,2,['depend'],['dependent']
Integrability,"b UI at http://xx.xx.xx.xx:4040; 18/04/24 17:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:42:02 ERROR TransportRequestHandler: Error sending result StreamResponse{streamId=/jars/gatk-package-4.0.3.0-spark.jar, byteCount=138618122, body=FileSegmentManagedBuffer{file=/scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar, offset=0, length=138618122}} to /xx.xx.xx.25:57139; closing connection; java.io.IOException: Connection reset by peer; at sun.nio.ch.FileChannelImpl.transferTo0(Native Method); at sun.nio.ch.FileChannelImpl.transferToDirectlyInternal(FileChannelImpl.java:428); at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:493); at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:608); at io.netty.channel.DefaultFileRegion.transferTo(DefaultFileRegion.java:139); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:287); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:314); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:802); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.forceFlush(AbstractNioChannel.java:319); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:637); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorat",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:37226,protocol,protocol,37226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['protocol'],['protocol']
Integrability,"c 5 12:51:17 2017 -0500. Updates to handle SAM header changes from sl_wgs_acnv_headers and updates to mb_gcnv_python_kernel. commit d02d04df684a2820308a1d1c2bfda4b7d1c5f05e; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Mon Nov 13 12:52:33 2017 -0500. Added CLIs and WDL for python gCNV pipeline. commit 66ed74b68375d43514ef84658e7a6c771ed9053c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Nov 15 01:50:03 2017 -0500. Polished code, ready for review; ; gCNV computational kernel (initial release); ; renaming gammas_s to psi_s to uniformity (sample-specific unexplained variance); ; renamed determine_ploidy_and_depth.py to cohort_determine_ploidy_and_depth.py; finite-temperature forward-backward algorithm; in the ploidy model, replaced alpha_j (NB over-dispersion) with psi_j (unexplained variance) for uniformity. Also, added the possibility of sample-specific unexplained variance in the germline contig ploidy model; ; updated I/O routines and CLIs according to team discussion; ; updated I/O routines and CLIs according to team discussion; ; changed the output layout of the ploidy determination tool; refactored parts of io.py; upped the version to 0.3 as it is not backwards compatible anymore; ; case ploidy determination tool from a given ploidy model; major code cleanup and refactoring of I/O module; refactoring of common CLI script snippets; ; removed all ""targets""; some code cleanup; ; pad flat class bitmask w/ a given padding value in the hybrid q_c_expectation_mode; option to disable annealing and keep the temperature fixed; ; bugfix in finite-temperature forward-backward; further refactoring of model I/O; ; the option to take a previously trained model as starting point in cohort CLI; the option to take previous calls as a starting point in cohort CLI; ; option to save and load adamax moments; ; import/export adamax bias correction tensor; ; refactoring related to fancy opt I/O; added average ploidy column to read depth; updated docs of hybrid ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:10911,rout,routines,10911,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,2,['rout'],['routines']
Integrability,clearing the milestone - our alpha does not depend on this pull req.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/936#issuecomment-151705716:44,depend,depend,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/936#issuecomment-151705716,1,['depend'],['depend']
Integrability,"e canonical name for this metric actually is, but it doesn't appear to be ""LL score""---perhaps someone else knows or has better Google-fu and can figure it out) before moving on to their methods for estimating F1. Doing a literature search for other discussions of optimizing F1 or other metrics in the context of positive-unlabeled learning might be worthwhile, but I think most methods will probably involve some sort of estimation of the base rate in unlabeled data. I think we may have to add some mechanism for holding out a validation set during training if we want to automatically tune thresholds in a rigorous fashion. Shouldn't be too bad---we can just have the training tool randomly mask out a set of the truth and pass the mask to the scoring tool (or maybe just determine the threshold in the training tool, if we are running in positive/negative mode and have access to unlabeled data)---but does add a couple of parameters to the tool interfaces. This also adds additional dependence on the quality of the truth resources. I think an implicit assumption in any use of the truth---even just thresholding/calibrating by sensitivity---is that it is a random sample; however, I'm not sure how true this is in actual use. For example, in malaria, it looks like we may have to resort to using a callset that has been very conservatively filtered as truth, which will bias us towards high scores and the peaks of the positive distribution. Perhaps we can also experiment with just treating training/truth on an equal footing (I think the distinction between the two is somewhat blurry in the original VQSR design, anyway). Perhaps @davidbenjamin has some thoughts? I see some related stuff going on in ThresholdCalculator, but I have to admit that I can't tell whether that's used in a similar PU context. Also note that depending on the model used, we might not have well calibrated posteriors---the IsolationForest simply outputs scores in a unit interval, and we simply report the differe",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241:1716,depend,dependence,1716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241,1,['depend'],['dependence']
Integrability,"e spec"". Third, although I don't know in detail about the different execution ; environments you are trying to support, there is a general strategy that ; I haven't seen discussed in these threads.; Perhaps it's impractical, but I'll mention it anyway. It seems like ; another approach would be to create (internal to the implementation) a ; ""header tag"" that could be efficiently serialized; and passed as part of the SAMRecord when you need to distribute it. The ; header tag could be used by the receiver to reattach the SAMRecord to ; its header (either proactively or on demand), but transparently to ; application code that is running against the SAMRecord API.; This would allow SAM headers to be transmitted out-of-band in a way that ; depends on the execution environment. Depending on the environment, ; this might be done by proactive broadcast, or you could think of the ; header tag as a promise to retrieve the header if/when it is needed. ; The size and complexity of the header tag might also depend on the ; execution environment. If the execution environment only supports a ; small finite number of headers, the header tag could be a small integer, ; or in a different execution environment it could be; a unique hash of the header or something like that. Memory footprint in ; the receiver is minimized because many SAMRecords can all share the same ; header object.; This requires more work to support in each execution environment, but it ; seems like it could be efficient and allows application code written to ; operate on SAMRecords to be portable; across different execution environments without having to contend with ; the possible presence of headerless SAMRecords. -Bob. On 9/17/15 4:28 PM, droazen wrote:. > @davidadamsphd https://github.com/davidadamsphd, @lbergelson ; > https://github.com/lbergelson, and myself met for an hour or two ; > just now to discuss this issue, and after reviewing all the options I ; > think we were convinced by the following argument:; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:2038,depend,depend,2038,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518,1,['depend'],['depend']
Integrability,"ePicture()`; - [x] `extractSimpleChimera()`. ### bump test coverage; Once code above is consolidated, bump test coverage, particularly for the classes above and the following poorly-covered classes; - [x] `ChimericAlignment`; - [x] `isForwardStrandRepresentation()`; - [x] `splitPairStrongEnoughEvidenceForCA()` ; - [x] `parseOneContig()` (needs testing because we need it for simple-re-interpretation for CPX variants) Note that `nextAlignmentMayBeInsertion()` is currently broken in the sense that when using this to filter out alignments whose ref span is contained by another, check if the two alignments involved are head/tail. - [x] `BreakpointsInference` & `BreakpointComplications`. - [x] `NovelAdjacencyAndAltHaplotype`; - [x] `toSimpleOrBNDTypes()`. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence`; - [x] serialization test. - [x] `AnnotatedVariantProducer`; - [x] `produceAnnotatedBNDmatesVcFromNovelAdjacency()`. - [x] `BreakEndVariantType`. - [ ] `SvDiscoverFromLocalAssemblyContigAlignmentsSpark` integration test; . ### update how variants are represented ; Implement the following representation changes that should make type-based evaluation easier; - [x] change `INSDUP` to`INS` when the duplicated ref region, denoted with annotation `DUP_REPEAT_UNIT_REF_SPAN`, is shorter than 50 bp.; - [x] change scarred deletion calls, which currently output as `DEL` with `INSSEQ` annotation, to one of these; - [x] `INS`/`DEL`, when deleted/inserted bases are < 50 bp and annotate accordingly; when type is determined as`INS`, the `POS` will be 1 base before the micro-deleted range and `END` will be end of the micro-deleted range, where the `REF` allele will be the corresponding reference bases.; - [x] two records `INS` and `DEL` when both are >= 50, share the same `POS`, and link by `EVENT`; - [ ] we are making a choice that treats duplication expansion as insertion. If decide to treat `DUP` as a separate 1st class type, we need to ; - [ ] shift the left breakpoint to the ri",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021:2183,integrat,integration,2183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021,1,['integrat'],['integration']
Integrability,efConfidenceGenotypes(ReferenceConfidenceVariantContextMerger.java:543); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:130); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:310); at org.broadinstitute.hellbender.engine.VariantLocusWalker.lambda$traverse$0(VariantLocusWalker.java:136); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:423); at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:134); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadins,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640:9382,wrap,wrapAndCopyInto,9382,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640,1,['wrap'],['wrapAndCopyInto']
Integrability,"endencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnondeterminism-perl; libfont-afm-perl libfontenc1 libgcc-5-dev libgdbm3 libgettextpo-dev; libget",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:1078,depend,dependencies,1078,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954,1,['depend'],['dependencies']
Integrability,"ergelson please comment on the following proposal. The proposal is that we would spin off native PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class type of JNIReadDataHolderClass; * @param haplotypeDataHolder",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:1101,interface,interface,1101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864,1,['interface'],['interface']
Integrability,"fter which the usual modelling and smoothing steps are performed. For the 75% tumor + 25% normal mixture, this yields 122 segments (up from 83):; ![N-25-T-75-SJS modeled](https://user-images.githubusercontent.com/11076296/76558618-015bd180-6474-11ea-996a-48d39770149b.png). For the 25% tumor + 75% normal mixture, this yields 105 segments (up from 50):; ![N-75-T-25-SJS modeled](https://user-images.githubusercontent.com/11076296/76560726-34a05f80-6478-11ea-9027-a54726c46b9e.png). One could imagine that smoothing could be disabled (so that all samples retain the common segmentation after modeling) or made more aggressive (so that private events don't get inadvertently introduced into other samples due to noise, perhaps), depending on the use case. It looks like the joint segmentation allows some additional events to be resolved, although I haven't done any rigorous evaluations. We could probably cook up some evaluations using simulated toy data or in silico mixtures, but there's really no reason why this shouldn't work decently well, especially if the kernel-segmentation method works well on a single sample for your data. It would also be interesting to understand at which point changing segmentation parameters on a single sample can no longer yield the same performance as joint segmentation on a fixed number of samples; however, this is probably a function of various S/N ratios, and it might not be easy to characterize this behavior outside of toy data. The segmentation parameter space is big enough to make this unwieldy even for toy data, too. Perhaps we can get some feedback from test users---not only on performance, but also on the structure of the new workflow. It might also be worth gauging whether a new WDL is warranted. Otherwise, we just need to add some unit tests for correctness of the multisample-segmentation backend class, integration tests for plumbing of the new tool, and perhaps address some of the issues mentioned above. Then I'd say this is good to go.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598386823:2996,integrat,integration,2996,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598386823,1,['integrat'],['integration']
Integrability,"gic, I prefer not to do it this way, because we can add more logics in the future, and capturing/resolving these BND's that are not suitable for _THIS PARTICULAR_ logic. I guess in general my personal preference is to put less algorithm-related information in VCF for analysts (less reading for them), and produce add on files for tool developers. What's your thoughts?. > Does this even have to be a spark tool? It looks like you are just reading the variants into a parallel spark context, filtering, and then collecting them to actually process them. Why not just make this a non-spark tool and process it all in memory on one node?. Answer: Agree. It doesn't have to be, at least in theory, and it probably is going to be faster as we don't need to incur the Spark overhead for such a typically small job. But (I'm saying too many buts....) up to this point all SV tools are under the package `hellbender.tools.spark.sv`, so I'm following suit here. Note the two classes's main interface methods mentions nothing about RDDs (that's on purpose). ; On the other hand, this is an engineering question I believe, and it depends on whether we want to put as much of discovery code as possible into `StructuralVariationDiscoveryPipelineSpark` (the last commit actually hooks the two classes into it, so a single invocation of the tool produces more variants), or we go wdl in pipelining the whole process. -------. All in all, I think the comments and critics are generally about the ""filtering""/""classifying"" part, and the most serious concern about it is false negatives. Am I understanding correctly? If so, given that the filtering step is only picking the BND's that are suitable to the linking logic, I can imagine the false negative problem be solved in the future by other logics (e.g. more relaxed requirement on pair matching, or not even requiring matching INV55/INV33 pairs, etc.) In fact, that's what I'm planning on.; Another part of the problem is how much I can accomplish in this single",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:6140,interface,interface,6140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929,1,['interface'],['interface']
Integrability,"gments` optionally takes denoised copy ratio and/or allelic counts, `PlotModeledSegments` outputs only the corresponding plots appropriately.; - I added a dependency on the R package `data.table` to slightly speed up the reading of input files.; - Setting `pch="".""` also sped up the generation of scatter plots.; - Plotting now takes a couple of minutes, most of which is I/O (#3554).; - AAF (rather than MAF) is now plotted for allele fraction (#2957). Other:; - I've introduced a `LocatableCollection` class to unify how allelic counts, copy ratios, and segments are stored and read/written from/to TSV (#2836). Intervals are always output in lexicographical order for now, to be consistent with the old coverage collection (#2951). Once @asmirnov239's `CollectReadCounts` is in, we can change everything over to ordering determined by the sequence dictionary.; - Column headers and log2 copy ratio output have been standardized throughout (#2886).; - [x] I've also introduced a `NamedSampleFile` abstract class to tag files that have `#SAMPLE_NAME=...` as the first comment line. For `CollectAllelicCounts`, this simply uses code borrowed from `GetSampleName`. We should unify the reading and storing of sample names at some point (#2910).; - [x] We will need to replace `SimpleReadCountCollection` (which currently serves as the interface between the old coverage collection files and the new code) with one of these subclasses when `CollectReadCounts` is in. We can also change `NamedSampleFile` depending on what he's implemented.; - [x] We should eventually write proper SAM headers with useful tags to all TSV and HDF5 files generated by our tools that represent annotated intervals that can be associated with a single sample. Documentation:; - [x] I need to update class javadoc and example invocations throughout. The initial PR will already be quite massive, so I'll leave this until later. Perhaps @sooheelee might want to be involved?; - [ ] I will update the white paper at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:10316,interface,interface,10316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,2,"['depend', 'interface']","['depending', 'interface']"
Integrability,"hat I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) for future use. Comment by @cwhelan ; > I'm a bit confused by this comment: this method is still being called in several places, so how is it obsolete?. Reply by @SHUANG-Broad (also copied to update the ""TODO"" message; > this predicate is currently used in two places (excluding appearance in comments): `BreakpointComplications.IntraChrStrandSwitchBreakpointComplications`, where it is use to test if the input simple chimera indicates an inverse tandem duplication and trigger the logic for inferring duplicated region; and `BreakpointsInference.IntraChrStrandSwitchBreakpointInference`, where",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:2134,message,message,2134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030,1,['message'],['message']
Integrability,"i.e. uses updated kebab syntax. --- . ## CalcMetadataSpark . 1. Revise one-line summary to something like:; Collects read metrics relevant to structural variant discovery. - Notice the lack of a period at the end above.; - Not statistics but metrics?. 2. Overview and Notes could use finessing but let's leave this for next year. One thing to do now is move this statement up top:; This tool is used in development and should not be of interest to most researchers. 3. I think this tool fits under the DiagnosticsAndQCProgramGroup.java.; 4. The tool takes a SAM/BAM/CRAM and calculates fragment length statistics...; 5. ""This is the first step in the workflow""--> makes it sound like this tool is necessary in the SV workflow but you say otherwise in the debugging sentence. I find this confusing. 6. I'm noticing that the example command does not have spark options despite the tool being a Spark tool. For such cases, it would be helpful to state, e.g. ""This tool can run in both Spark and non-Spark modes, depending on if --sparkMaster is set."" Then include a second example command that shows how to utilize Spark. There is an example from ChrisW in <https://github.com/broadinstitute/gatk/issues/3853>:. ```; 	-- \; --sparkRunner GCS \; --cluster my-dataproc-spark-cluster; ```. ---; ## DiscoverVariantsFromContigAlignmentsSAMSpark. 1. ""Parse"" is vague. How about: ; Parses aligned contig assemblies of genomic breakpoints and calls structural variants. And `6. ` from above. ---; ## ExtractOriginalAlignmentRecordsByNameSpark. 1. Subsets reads by names; 2. I think you mean FilterSamReads (Picard) and not PrintReads. AFAIK, PrintReads cannot subset based on a list of read names. Rather FilterSamReads can do so as long as the reads are queryname-sorted. So then it would be good to distinguish this tool from FilterSamReads by saying (assuming true) ""Unlike FilterSamReads, this tool can take any sort-order, e.g. unsorted, to subset target reads.""; 3. ReadDataProgramGroup.java. And `6. ` fr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3948#issuecomment-351467451:1342,depend,depending,1342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3948#issuecomment-351467451,1,['depend'],['depending']
Integrability,"ies are installed from the correct channel and compiled against MKL; - conda-forge; - defaults; dependencies:. # core python dependencies; - conda-forge::python=3.6.10 # do not update; - pip=20.0.2 # specifying channel may cause a warning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge::keras=2.2.4 # updated from pip-installed 2.2.0, which caused various conflicts/clobbers of conda-installed packages; # conda-installed 2.2.4 appears to be the most recent version with a consistent API and without conflicts/clobbers; # if you wish to update, note that versions of conda-forge::keras after 2.2.5; # undesirably set the environment variable KERAS_BACKEND = theano by default; - defaults::intel-openmp=2019.4; - conda-forge::scikit-learn=0.22.2; - conda-forge::matplotlib=3.2.1; - conda-forge::pandas=1.0.3. # core R dependencies; these should only be used for plotting and do not take precedence over core python dependencies!; -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:2316,depend,dependencies,2316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,1,['depend'],['dependencies']
Integrability,"implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract methods for the three parameters (`getWindowSize()`, `getWindowStep()` and `getWindowPadding()`, and implement `ReadWindowWalker` as a extension of this interface setting `getWindowStep()` to return `getWindowSize()` and `requiresReads()` to true. I can do this once the PR #1567 is accepted and generate the two interfaces (to be sure that the integration with the HC engine is working as expected with the changes), or just implement the `SlidingWindowWalker` and you can include it in the HC PR, or update afterwards to avoid redundancy in the code. What do you think, @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:2154,interface,interface,2154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775,3,"['integrat', 'interface']","['integration', 'interface', 'interfaces']"
Integrability,"n Tools; #; # Only update this environment if there is a *VERY* good reason to do so!; # If the build is broken but could be fixed by doing something else, then do that thing instead.; # Ensuring the correct environment for canonical (or otherwise reasonable) usage of our standard Docker takes precedence over edge cases.; # If you break the environment, you are responsible for fixing it and also owe the last developer who left this in a reasonable state a beverage of their choice.; # (This may be yourself, and you'll appreciate that beverage while you tinker with dependencies!); #; # When changing dependencies or versions in this file, check to see if the ""supportedPythonPackages"" DataProvider; # used by the testGATKPythonEnvironmentPackagePresent test in PythonEnvironmentIntegrationTest needs to be updated; # to reflect the changes.; #; name: gatk; channels:; # if channels other than conda-forge are added and the channel order is changed (note that conda channel_priority is currently set to flexible),; # verify that key dependencies are installed from the correct channel and compiled against MKL; - conda-forge; - defaults; dependencies:. # core python dependencies; - conda-forge::python=3.6.10 # do not update; - pip=20.0.2 # specifying channel may cause a warning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely nec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:1300,depend,dependencies,1300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,2,['depend'],['dependencies']
Integrability,"naging and querying Supplementary alignment; > information from read alignment records:; >; > Some of the things that I think smell:; >; > 1.; >; > Querying: implemented in htsjdk consists in forging artificial; > SAMRecords that contain only the alignment info in the SA tag element... It; > seems to me that it makes more sense to create class to hold this; > information alone (e.g. ReadAlignmentInfo or ReadAlignment); SATagBuilder; > already has defined a private inner class with that in mind ""SARead"" so why; > not flesh it out and make it public.; > 2.; >; > Writing: currently SATagBuilder gets attached to a read, parsing its; > current SA attribute content into SARead instances. It provides the; > possibility adding additional SAM record one by one or clearing the list.; > ... then it actually updates the SA attribute on the original read when a; > method (setTag) is explicitly called.; >; > I don't see the need to attach the SATag Builder to a read... it could; > perfectly be free standing; the same builder could be re-apply to several; > reads for that matter and I don't see any gain in hiding the read SA tag; > setting process,... even if typically this builder output would go to the; > ""SA"" tag, perhaps at some point we would like to also write SA coordinate; > list somewhere else, some other tag name or perhaps an error message... why; > impose this single purpose limitation?; >; > I suggest to drop the notion of a builder for a more general custom; > ReadAlignmentInfo (or whatever name) list. Such list could be making; > reference to a dictionary to validate its elements, prevent duplicates,; > keep the primary SA in the first position... etc.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/3324>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AArTZft11VTCtCHT_xr89kPL7hMFYQyhks5sQNghgaJpZM4Ofpkb>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323:2115,message,message,2115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323,1,['message'],['message']
Integrability,"nd other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge::keras=2.2.4 # updated from pip-installed 2.2.0, which caused various conflicts/clobbers of conda-installed packages; # conda-installed 2.2.4 appears to be the most recent version with a consistent API and without conflicts/clobbers; # if you wish to update, note that versions of conda-forge::keras after 2.2.5; # undesirably set the environment variable KERAS_BACKEND = theano by default; - defaults::intel-openmp=2019.4; - conda-forge::scikit-learn=0.22.2; - conda-forge::matplotlib=3.2.1; - conda-forge::pandas=1.0.3. # core R dependencies; these should only be used for plotting and do not take precedence over core python dependencies!; - r-base=3.6.2; - r-data.table=1.12.8; - r-dplyr=0.8.5; - r-getopt=1.20.3; - r-ggplot2=3.3.0; - r-gplots=3.0.3; - r-gsalib=2.1; - r-optparse=1.6.4. # other python dependencies; these should be removed after functionality is moved into Java code; - biopython=1.76; - pyvcf=0.6.8; - bioconda::pysam=0.15.3 # using older conda-installed versions may result in libcrypto / openssl bugs. # pip installs should be avoided, as pip may not respect the dependencies found by the conda solver; - pip:; - gatkPythonPackageArchive.zip; ```. It seems to successfully create the environment. I'd still recommend updating the information on your README.md and the file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:3197,depend,dependencies,3197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,4,['depend'],['dependencies']
Integrability,"nds for identifying the copy-neutral state.; - [ ] @MartonKN is going to work on an improved caller for his next project. This caller should also make simple calls (not full allelic copy number, but just `0`, `+`, `-`), but should also take advantage of the copy-ratio and minor-allele fraction posteriors estimated by `ModelSegments` to generate quality scores. Plotting:; - Other than the allele-fraction model, the limiting factor was the original plotting code (some plotting runs originally took several hours for a single WGS sample...) We can now make plotting much faster with the ordering enforced by `TSVLocatableCollection` (see below).; - There are now two plotting tools, `PlotDenoisedCopyRatios` and `PlotModeledSegments`. This is in contrast to the old `PlotSegmentedCopyRatio` and `PlotACNVResults`.; - Because `ModelSegments` optionally takes denoised copy ratio and/or allelic counts, `PlotModeledSegments` outputs only the corresponding plots appropriately.; - I added a dependency on the R package `data.table` to slightly speed up the reading of input files.; - Setting `pch="".""` also sped up the generation of scatter plots.; - Plotting now takes a couple of minutes, most of which is I/O (#3554).; - AAF (rather than MAF) is now plotted for allele fraction (#2957). Other:; - I've introduced a `LocatableCollection` class to unify how allelic counts, copy ratios, and segments are stored and read/written from/to TSV (#2836). Intervals are always output in lexicographical order for now, to be consistent with the old coverage collection (#2951). Once @asmirnov239's `CollectReadCounts` is in, we can change everything over to ordering determined by the sequence dictionary.; - Column headers and log2 copy ratio output have been standardized throughout (#2886).; - [x] I've also introduced a `NamedSampleFile` abstract class to tag files that have `#SAMPLE_NAME=...` as the first comment line. For `CollectAllelicCounts`, this simply uses code borrowed from `GetSampleName`. W",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:9138,depend,dependency,9138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['depend'],['dependency']
Integrability,"ntReads cannot subset based on a list of read names. Rather FilterSamReads can do so as long as the reads are queryname-sorted. So then it would be good to distinguish this tool from FilterSamReads by saying (assuming true) ""Unlike FilterSamReads, this tool can take any sort-order, e.g. unsorted, to subset target reads.""; 3. ReadDataProgramGroup.java. And `6. ` from above. ---; ## FilterLongReadAlignmentsSAMSpark. 1. In the one-line summary, I'm not clear on what is meant by ""Filters"". Based on the result file, seems like it collects metrics on each contig alignment.; 2. ; 3. If metrics, then DiagnosticsAndQCProgramGroup.java. And `6. ` from above. ---; ## FindBadGenomicKmersSpark. 1. The term ""copy number"" should be reserved in reference to CNV analyses. So instead, how about:; Identify sequence contexts that occur at high frequency in a reference; 2. Please define a kmer. If only a reference fasta is required (as listed under Inputs) great. But if the tool also depends on a FAI index and DICT dictionary, please do include them. Also, it would be good to provide an example of how such information is used in SV discovery, e.g. ""the resulting file can be given to FindBreakpointEvidenceSpark, which will then ignore such sequence contexts during analysis."" Also would be good to mention that the default kmer size (--k-size 51) is optimized for human if indeed this is the case.; 3. ReferenceProgramGroup.java. And `6. ` from above. ---; ## FindBreakpointEvidenceSpark. 1. Assembles and aligns contigs of genomic breakpoint regions associated with structural variants ; 2. Overview and Notes could use finessing but let's leave this for next year. One thing to include is a reference to FermiLite for those seeking more information. A publication would be best. And `6. ` from above. ---; ## StructuralVariationDiscoveryPipelineSpark. 1. Runs the structural variant discovery workflow on a single sample in Spark ; 2. Fyi we sanction a ""Caveats"" section, which is likely more appropri",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3948#issuecomment-351467451:2945,depend,depends,2945,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3948#issuecomment-351467451,1,['depend'],['depends']
Integrability,"ollapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, especially given the simplicity of writing a new walker type in GATK4. For all of these reasons we don't want `AssemblyRegionWalker` to inherit from a more abstract traversal type -- it's just going to be its own standalone thing",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:1660,rout,routines,1660,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,1,['rout'],['routines']
Integrability,"on localhost (executor driver) (4/4); 17/05/05 17:03:58 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool ; 17/05/05 17:03:58 INFO DAGScheduler: ResultStage 2 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) finished in 10.370 s; 17/05/05 17:03:58 INFO DAGScheduler: Job 1 finished: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 16.702399 s; 17/05/05 17:03:58 INFO SparkUI: Stopped Spark web UI at http://172.30.0.122:46483; 17/05/05 17:03:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 17/05/05 17:03:59 INFO MemoryStore: MemoryStore cleared; 17/05/05 17:03:59 INFO BlockManager: BlockManager stopped; 17/05/05 17:03:59 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/05/05 17:03:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 17/05/05 17:03:59 INFO SparkContext: Successfully stopped SparkContext; [May 5, 2017 5:03:59 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.40 minutes.; Runtime.totalMemory()=799080448; 17/05/05 17:03:59 INFO ApplicationMaster: Final app status: FAILED, exitCode: 16, (reason: Shutdown hook called before final status was reported.); 17/05/05 17:03:59 INFO ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: Shutdown hook called before final status was reported.); 17/05/05 17:03:59 INFO ApplicationMaster: Deleting staging directory hdfs://ip-172-30-0-86.ec2.internal:8020/user/hadoop/.sparkStaging/application_1493961816416_0010; 17/05/05 17:03:59 INFO ShutdownHookManager: Shutdown hook called; 17/05/05 17:03:59 INFO ShutdownHookManager: Deleting directory /mnt1/yarn/usercache/hadoop/appcache/application_1493961816416_0010/spark-223a9e8b-0fe9-41f0-8bed-f843978f1882; 17/05/05 17:03:59 INFO ShutdownHookManager: Deleting directory /mnt/yarn/usercache/hadoop/appcache/application_1493961816416_0010/spark-573a9c53-e268-4f3b-8907-1f35e5839788; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046:18568,message,message,18568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046,1,['message'],['message']
Integrability,"ot sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage if there is no data for known indels. I will propose very soon two PRs with fully functional tools (without the n-way out feature for indel-realignment), and trying to add simple integration tests with the data already available on the repository and running with GATK3.8-1. If that is OK for you, I will proceed with this approach.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:2181,integrat,integration,2181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115,1,['integrat'],['integration']
Integrability,"ould like to address are similar to yours, with some inclussions. * Regarding NIO support, I would go to remove completely `File` support. If API users need to use the `File` abstraction, they should convert to a `java.nio.Path` using the `toPath` method.; * In addition, I would like that HTTP/S and FTP is handled also with NIO. For HTTP/S, I am working in a simple `FileSystemProvider` that should be good enough for using in combination with HTSJDK ([jsr203-http](https://github.com/magicDGS/jsr203-http)), and I can speed up the development there for needs in HTSJDK; for FTP, maybe [ftp-fs](https://github.com/robtimus/ftp-fs) can be used or a simple implementation can be derived from the HTTP/S implementation (without credentials). This will remove the special handling of HTTP/S and FTP paths in HTSJDK in favor of a consistent and pluggable manner.; * Interfaces for the data types are great, and maybe it will be good to have codec interfaces for both encoding and decoding. For example, I am missing encoders in tribble (an attempt in https://github.com/samtools/htsjdk/pull/822 for writing support).; * For VCF, I would like to have a less diploid-centric interface and design, or at least a way of configure the catching of genotype-related attributes. Currently there are methods for homozygotes/heterozygotes that aren't really useful for triploids or even VCFs without variation (for example, in Pool-Seq data).; * Modular design for artifacts: thus, a project with only SAM/BAM requirements will require only `htsjdk-sam`, and if they also want CRAM support, `htsjdk-cram`. See https://github.com/samtools/htsjdk/issues/896 for more info about it.; * Common license for all HTSJDK, or at least for each module. This will be good for taking into account legal concerns when including the library, because now there is a mixture depending on the files that are used. This is what is coming to my mind now. Maybe I added something else in https://github.com/samtools/htsjdk/issues/520",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-363390940:1238,interface,interface,1238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-363390940,2,"['depend', 'interface']","['depending', 'interface']"
Integrability,"periment with the LL score discussed there (see https://www.aaai.org/Papers/ICML/2003/ICML03-060.pdf for the original paper---although note that despite the paper's high citation count, I'm not sure what the canonical name for this metric actually is, but it doesn't appear to be ""LL score""---perhaps someone else knows or has better Google-fu and can figure it out) before moving on to their methods for estimating F1. Doing a literature search for other discussions of optimizing F1 or other metrics in the context of positive-unlabeled learning might be worthwhile, but I think most methods will probably involve some sort of estimation of the base rate in unlabeled data. I think we may have to add some mechanism for holding out a validation set during training if we want to automatically tune thresholds in a rigorous fashion. Shouldn't be too bad---we can just have the training tool randomly mask out a set of the truth and pass the mask to the scoring tool (or maybe just determine the threshold in the training tool, if we are running in positive/negative mode and have access to unlabeled data)---but does add a couple of parameters to the tool interfaces. This also adds additional dependence on the quality of the truth resources. I think an implicit assumption in any use of the truth---even just thresholding/calibrating by sensitivity---is that it is a random sample; however, I'm not sure how true this is in actual use. For example, in malaria, it looks like we may have to resort to using a callset that has been very conservatively filtered as truth, which will bias us towards high scores and the peaks of the positive distribution. Perhaps we can also experiment with just treating training/truth on an equal footing (I think the distinction between the two is somewhat blurry in the original VQSR design, anyway). Perhaps @davidbenjamin has some thoughts? I see some related stuff going on in ThresholdCalculator, but I have to admit that I can't tell whether that's used in a ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241:1678,interface,interfaces,1678,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241,1,['interface'],['interfaces']
Integrability,"ppear to be ""LL score""---perhaps someone else knows or has better Google-fu and can figure it out) before moving on to their methods for estimating F1. Doing a literature search for other discussions of optimizing F1 or other metrics in the context of positive-unlabeled learning might be worthwhile, but I think most methods will probably involve some sort of estimation of the base rate in unlabeled data. I think we may have to add some mechanism for holding out a validation set during training if we want to automatically tune thresholds in a rigorous fashion. Shouldn't be too bad---we can just have the training tool randomly mask out a set of the truth and pass the mask to the scoring tool (or maybe just determine the threshold in the training tool, if we are running in positive/negative mode and have access to unlabeled data)---but does add a couple of parameters to the tool interfaces. This also adds additional dependence on the quality of the truth resources. I think an implicit assumption in any use of the truth---even just thresholding/calibrating by sensitivity---is that it is a random sample; however, I'm not sure how true this is in actual use. For example, in malaria, it looks like we may have to resort to using a callset that has been very conservatively filtered as truth, which will bias us towards high scores and the peaks of the positive distribution. Perhaps we can also experiment with just treating training/truth on an equal footing (I think the distinction between the two is somewhat blurry in the original VQSR design, anyway). Perhaps @davidbenjamin has some thoughts? I see some related stuff going on in ThresholdCalculator, but I have to admit that I can't tell whether that's used in a similar PU context. Also note that depending on the model used, we might not have well calibrated posteriors---the IsolationForest simply outputs scores in a unit interval, and we simply report the difference between the positive and the negative scores, for example.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241:2557,depend,depending,2557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241,1,['depend'],['depending']
Integrability,"r gradle; build.gradle. 3. Significant changes to existing code to support/invoke new filter; - add arguments for XGBoostEvidenceFilter, changes for scaling density filter by coverage; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/StructuralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:1741,interface,interfaces,1741,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,1,['interface'],['interfaces']
Integrability,"ralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:2024,integrat,integration,2024,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,1,['integrat'],['integration']
Integrability,"re falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnondeterminism-perl; libfont-afm-perl libfontenc1 libgcc-5-dev libgdbm3 libgettextpo-dev; libgettextpo0 libgfortran-5-dev libgfortran3 libgomp1 libhtml-form-perl; libhtml-format-perl libhtml-parser-perl libhtml-tagset-perl; libhtml-tree-perl libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl; libhttp-message-perl libhttp-negotiate-perl libio-html-perl; libio-socket-ssl-perl libipc-system-simple-perl libisc-export160 libisl15; libitm1 libjpeg-dev libjpeg-turbo8-dev libjpeg8-dev liblapack-dev liblapack3; liblsan0 liblwp-mediatypes-perl liblwp-protocol-https-perl liblzma-dev; libmail-sendmail-perl libmailtools-perl libmnl0 libmpc3 libmpfr4 libmpx0; libncurses5-dev libnet-dbus-perl libnet-http-perl libnet-smtp-ssl-perl; libnet-ssleay-perl libpaper-utils libpaper1 libpcre16-3 libpcre3-dev; libpcre32-3 libpcrecpp0v5 libperl5.22 libpipeline1 libpng12-dev libquadmath0; libreadline-dev libreadline6-dev libsigsegv2 libstdc++-5-dev; libsys-hostname-long-perl libtcl8.6 libtext-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:2308,message,message-perl,2308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954,3,"['mediat', 'message', 'protocol']","['mediatypes-perl', 'message-perl', 'protocol-https-perl']"
Integrability,"serialization/spec/class.html#a4100):. _If not specified by the class, the value returned is a hash computed from the class's name, interfaces, methods, and fields using the Secure Hash Algorithm (SHA) as defined by the National Institute of Standards._. Now when I look at the `java.io.ObjectStreamClass.java` file for 64-bit JDK7 and JDK8 - from src.zip - both have the same code for the following parts after performing a `diff` - I didn't list all of the lines of code since they are quite long:. ```; public long getSerialVersionUID() {; // REMIND: synchronize instead of relying on volatile?; if (suid == null) {; suid = AccessController.doPrivileged(; new PrivilegedAction<Long>() {; public Long run() {; return computeDefaultSUID(cl);; }; }; );; }; return suid.longValue();; }; ... private static long computeDefaultSUID(Class<?> cl) {; ...very long code which can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase fro",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:1333,interface,interface,1333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['interface'],['interface']
Integrability,"similar same error message with ; `gatk HaplotypeCallerSpark -R ref.fa -I input.GatherBamFiles.bam -O output.g2.vcf.gz`. OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12); gatk 4.1.8.1 . ```; 07:16:06.169 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 20/08/15 07:16:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.5 MB, free 57.3 GB); 20/08/15 07:16:06 INFO SparkUI: Stopped Spark web UI at http://e1c-050:4041; 20/08/15 07:16:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 20/08/15 07:16:06 INFO MemoryStore: MemoryStore cleared; 20/08/15 07:16:06 INFO BlockManager: BlockManager stopped; 20/08/15 07:16:06 INFO BlockManagerMaster: BlockManagerMaster stopped; 20/08/15 07:16:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 20/08/15 07:16:06 INFO SparkContext: Successfully stopped SparkContext; 07:16:06.412 INFO HaplotypeCallerSpark - Shutting down engine; [August 15, 2020 7:16:06 AM EDT] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 0.11 minutes.; Runtime.totalMemory()=102900432896; Exception in thread ""main"" java.lang.StackOverflowError; at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:67); at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:505); at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:575); at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:505); at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:575); at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869#issuecomment-674384617:19,message,message,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869#issuecomment-674384617,1,['message'],['message']
Integrability,"tash@broadinstitute.org>; Date: Thu Dec 7 02:50:19 2017 -0500. update WDL scripts. commit 12bcfa192ee6fa6da21239ebf5b513633efe974f; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 02:47:33 2017 -0500. significant updates to GermlineCNVCaller; integration tests for GermlineCNVCaller w/ sim data in both run modes. commit 151416a4af735ca721bd75e4b54a780c17ac9397; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 01:42:05 2017 -0500. hybrid ADVI abstract argument collection w/ flexible default values; hybrid ADVI argument collection for contig ploidy model; hybrid ADVI argument collection for germline denoising and calling model. commit 56e21bf955d3dc0c52aceb384f28cf6173959de0; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 23:18:39 2017 -0500. rewritten python-side coverage metadata table reader using pandas to fix the issues with comment line; change criterion for cohort/case based on whether a contig-ploidy model is provided or not; simulated test files for ploidy determination tool; proper integration test for ploidy determination tool and all edge cases; updated docs for ploidy determination tool. commit 7fa104b2e9170770cfc5b338835e41215d7fd39c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 18:43:17 2017 -0500. kabab case for gCNV-related tools; removed short args (this also partially affected PlotDenoisedCopyRatios and PlotModeledSegments and their integration tests). commit f02cb024331a986213cfd9fae2da706bbc5ddbd9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 18:02:40 2017 -0500. synced with mb_gcnv_python_kernel. commit 2963bbf8c90418d9b88545c93771ae51cf542db9; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 11:38:05 2017 -0500. Fixing typo in travis.yml. commit 6cf589999c716ec66404eb0a2ae4310dd130a772; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 11:13:58 2017 -0500. editable, full path. commit d998f2d5c2b33dd41e291b",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:7729,integrat,integration,7729,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['integrat'],['integration']
Integrability,"this for a first workflow to target?. 1) Run ExtractVariantAnnotations on a training set of chromosomes. You can keep training/truth labels as in Best Practices, for now.; 2) Run TrainVariantAnnotationsModel on that. We'll use the truth scores generated here for any sensitivity conversions---i.e., we'll be calibrating scores only to the truth sites that are contained in the training set of chromosomes.; 3) Use the trained model to run a single shard of ScoreVariantAnnotations on a validation set of chromosomes.; 4) Run some variation of the above script on the resulting outputs to determine SNP and INDEL score thresholds for optimizing the corresponding LL scores. We can also add some code to the script to use the truth scores from step 2 to convert these score thresholds into truth-sensitivity thresholds.; 5) Provide these truth-sensitivity thresholds to ScoreVariantAnnotations and use them to hard filter. Evaluate on a test set of chromosomes. If all looks good, we can later move steps 3-4 into the train tool and automate the passing of sensitivities in 5 via outputs in the model directory. This will let us keep the basic interface of ScoreVariantAnnotations the same, but we'll have to add a few basic parameters to TrainVariantAnnotationsModel to control the train/validation split. So I think all this branch is missing is step 5---we'll simply need to add command-line parameters for the SNP/INDEL sensitivity thresholds and then do the hard filtering in the VCF writing method highlighted above. Do you think you can handle implementing that in this branch, and then the rest at the WDL level? I can help with the python script for the LL stuff (or anything else), if needed. Not sure if you got a chance to check out what your collaborators are doing in the methods you're looking to compare against, but it would be good to understand if this basic scheme for train/validation/test splitting can be replicated over there. You'll want to compare apples to apples, after all!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084:1221,interface,interface,1221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084,1,['interface'],['interface']
Integrability,"tually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) for future use. Comment by @cwhelan ; > I'm a bit confused by this comment: this method is still being called in several places, so how is it obsolete?. Reply by @SHUANG-Broad (also copied to update the ""TODO"" message; > this predicate is currently used in two places (excluding appearance in comments): `BreakpointComplications.IntraChrStrandSwitchBreakpointComplications`, where it is use to test if the input simple chimera indicates an inverse tandem duplication and trigger the logic for inferring duplicated region; and `BreakpointsInference.IntraChrStrandSwitchBreakpointInference`, where it is used for breakpoint inference. The problem is, the contig will not even be sent here, because `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` defines a simple chimera that has strand switch and the two alignments overlaps on reference as ""incomplete"", so in practice the two uses are not going to be triggered. But when we come back later and see what can be extracted from such ""incomplete"" contigs, these code could be useful again. So it is kept. ------------; ### On the problem of writing out SAM records of ""Unknown"" contigs efficiently. First round comment by @cwhelan ; > This seems like a very inef",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:2748,message,message,2748,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030,1,['message'],['message']
Integrability,"utput from chr1. The output shows the Maximum resident set size (kbytes): **2630440**. Using GATK jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar defined in environment variable GATK_LOCAL_JAR; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx200g -Xms16g -jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace; Command being timed: ""gatk --java-options -Xmx200g -Xms16g GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace""; User time (seconds): 270716.45; System time (seconds): 1723.34; Percent of CPU this job got: 99%; Elapsed (wall clock) time (h:mm:ss or m:ss): 76:08:24; Average shared text size (kbytes): 0; Average unshared data size (kbytes): 0; Average stack size (kbytes): 0; Average total size (kbytes): 0; Maximum resident set size (kbytes): 2630440; Average resident set size (kbytes): 0; Major (requiring I/O) page faults: 5; Minor (reclaiming a frame) page faults: 206030721; Voluntary context switches: 11129822; Involuntary context switches: 176522; Swaps: 0; File system inputs: 627981312; File system outputs: 466730160; Socket messages sent: 0; Socket messages received: 0; Signals delivered: 0; Page size (bytes): 4096; Exit status: 0. ```. So using the import on reblocked gvcfs using --bypass-feature-reader was the fastest way to import our 3500 gVCFs and minimize memory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687:3022,message,messages,3022,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687,2,['message'],['messages']
Modifiability," burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class type of JNIReadDataHolderClass; * @param haplotypeDataHolderClass class type of JNIHaplotypeDataHolderClass; */; void jniInitializeClassFields(Class<JNIReadDataHolderClass> readDataHolderClass, Class<JNIHaplotypeDataHolderClass> haplotypeDataHolderClass);. /**; * Real compute kernel; */; void jniComputeLikelihoods(int numReads, int numHaplotypes, JNIReadDataHolderClass[] readDataArray,; JNIHaplotypeDataHolderClass[] haplotypeDataArray, double[] likelihoodArray, int maxNumThreadsToUse);. /**; * Print final profiling information from native code. ; */; default void close() { jniClose(); }. void jniClose();; }; ```. and a class that implements those as native methods",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:1680,extend,extends,1680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864,1,['extend'],['extends']
Modifiability," really know what's happening. We wouldn't expect gatk4 haplotype caller to be that much slower. . It looks like they're running beta2 which is kind of old as well. Can you ask them what exact version they're using?. Can you ask if they have the log (stdout + stderr) for the gatk4 non-spark run? I can't tell what pairhmm they're actually running with and the logs would help with that. . Can you also find out what sort of hardware they're running on? Specifically, is it an intel machine with support for AVX?. A good setting for` --nativePairHmmThreads` is probably 4-8, you won't see any improvement after that. I also noticed that they're setting -XX:+UseParallelGC -XX:ParallelGCThreads=32 for the gatk3. They would be better off setting it to 2-4 threads. Performance gets worse beyond that typically from what I've seen. They can set the same thing for gatk4 using`--javaOptions ' -XX:+UseParallelGC -XX:ParallelGCThreads=4'`. Their spark configuration looks wrong in a number of ways which is probably a big part of why they're not seeing any improvement. In general you want executors with ~4-8 cores and at least 4g of memory per core. I don't know how much memory their nodes have, and I don't know if they're running with autoscaling turned on, but I suspect they're only allocating 1 executor on 1 node and then it's thrashing memory because it's trying to run 32 threads at once. Spark tuning for haplotype caller is going to be complicated though and I don't know how to do it will yet, we will be revisiting it in the next quarter probably. They're also running withs spark 2.1.0, we currently require spark 2.0.2 which is an unfortunately specific version, we're planning on upgrading to spark 2.2.+ in the next quarter. . You should make it clear to them that the results will not be the same between 3, 4, and 4-spark yet and that 4 is in rapid state of flux and has known performance issues that we're planning on working soon. Even so though, that slowdown they're seeing is bi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332879964:971,config,configuration,971,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332879964,1,['config'],['configuration']
Modifiability," unnecessary copies of the data (now fixed: https://github.com/cloudera/spark-dataflow/pull/60), which caused OOM errors when trying to broadcast the 3GB reference data. With this fixed, I ran a [pipeline called JoinReferencesDataflow](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/tools/dataflow/pipelines/JoinReferencesDataflow.java) on a small cluster that broadcasts the reference as a dataflow view. The code is a modified version of CountReadsDataflow that simply sends the view, and then doesn't use it, so we can see the cost of doing a broadcast (See the rest of the code in this branch: https://github.com/tomwhite/hellbender/tree/hadoop-references). JoinReferencesDataflow took 2 min 25s to run, of which 18s were for reading the reference from the local filesystem in the driver. For comparison, CountReadsDataflow took 17s on the same cluster. So broadcasting the reference takes less than 2 minutes. Note that this was just for one task, but Spark has [an efficient protocol for sending broadcast variables](http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf), which scales well with the number of nodes, so the approach looks feasible. Having said all that, we might still want to use the sharding approach, in order to share more code between the Google and Spark dataflow implementations. One way this could work would be to generalize `RefAPISource` and `RefAPIMetadata` to support reading reference data from a [ReferenceHadoopSource](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/engine/dataflow/datasources/ReferenceHadoopSource.java), which is in line with @droazen's last comment. Am I right in thinking that the read pipeline work is being completed in https://github.com/broadinstitute/hellbender/tree/da_read_pipeline? Is that at a point where I could try with pipeline on Spark, or should I wait until it's merged?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353:1189,variab,variables,1189,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353,1,['variab'],['variables']
Modifiability," what intervals were used for the jobs. I tried using running GenomicsDBImport with -L over a small region, or I ran SelectVariants on the gVCF first (which behaves a little differently), and then used that subset gVCF as input to GenomicsDBImport, where GenomicsDBImport is given the entire contig as the interval. The resulting workspaces will be slightly different, with the latter containing information over a wider region (GenomicsDBIport truncates start/end of the input records to just the target interval). . So if either of these workspaces is passed to GenotypeGVCFs, using --only-output-calls-starting-in-intervals and -L 1:1050-1150:. I think any upstream padding doesnt matter. If you have a multi-nucleotide polymorphism that starts upstream of 1050 but spans 1050, this job wouldnt be responsible for calling that. The prior job, which has an interval set upstream of this one should call it. I think GenomicsDbImport's behavior is fine here. If you have a multi-NT variant that starts within 1050-1150, but extends outside (i.e. deletion or insertion starting at 1148), this could be a problem. The GenomicsDB workspace created with the interval 1:1050-1150 lacks the information to score that, right? The workspace created using the more permissive SelectVariants->GenomicsDBImport contains that downstream information and presumably would make the same call as if GenotypeGVCFs was given the intact chromosome as input, right?. However, it seems that if I simply create the workspace with a reasonably padded interval (adding 1kb should be more than enough for Illumina, right?), and then run GenotypeGVCFs with the original, unpassed interval, then the resulting workspace should contain all available information and GenotypeGVCFs should be able to make the same call as if it was given a whole-chromosome workspace as input. . Does that logic seem right? . ```; # The Input gVCF; 1	1040	.	A	<NON_REF>	.	.	END=1046	GT:DP:GQ:MIN_DP:PL	0/0:15:24:14:0,24,360; 1	1047	.	T	<NON_REF>	.	",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221558244:1618,extend,extends,1618,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221558244,1,['extend'],['extends']
Modifiability,"![IMG_9960](https://user-images.githubusercontent.com/11076296/95899038-ee88e280-0d5d-11eb-86bf-272687eb9ac0.jpg). Decided to just sit down and go through the exercise of threading all of the parameter sets by hand after biffing it once. Reproducing above; might be helpful for the reviewer if this goes in, but they may want to independently check it. (Is there a way I could've gotten IntelliJ to do this for me?). I would hope that we could do some refactoring to simplify this a bit, if not model ablation or consolidation of parameters, but I won't attempt it for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-707919816:452,refactor,refactoring,452,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-707919816,1,['refactor'],['refactoring']
Modifiability,- Picard Version: 2.18.16; 11:33:26.275 INFO CountReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:33:26.275 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:33:26.275 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 11:33:26.276 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:33:26.276 INFO CountReadsSpark - Deflater: IntelDeflater; 11:33:26.276 INFO CountReadsSpark - Inflater: IntelInflater; 11:33:26.276 INFO CountReadsSpark - GCS max retries/reopens: 20; 11:33:26.276 INFO CountReadsSpark - Requester pays: disabled; 11:33:26.277 WARN CountReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CountReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 11:33:26.277 INFO CountReadsSpark - Initializing engine; 11:33:26.277 INFO CountReadsSpark - Done initializing engine; 2019-01-07 11:33:26 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:26 INFO SparkContext:54 - Running Spark version 2.3.0; 2019-01-07 11:33:26 INFO SparkContext:54 - Submitted application: CountReadsSpark; 2019-01-07 11:33:26 INFO SecurityManager:54 - Changing view acls to: farrell; 2019-01-07 11:33:26 INFO SecurityManager:54 - Changing modify acls to: farrell; 2019-01-07 11:33:26 INFO SecurityManager:54 - Changing view acls groups to:; 2019-01-07 11:33:26 INFO SecurityManager:54 - Changing modify acls groups to:; 2019-01-07 11:33:26 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-07 11:33:27 INFO Utils:54 -,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:4625,config,configuration,4625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['config'],['configuration']
Modifiability,- Picard Version: 2.18.16; 13:35:11.511 INFO CountReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 13:35:11.511 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 13:35:11.511 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 13:35:11.511 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:35:11.511 INFO CountReadsSpark - Deflater: IntelDeflater; 13:35:11.511 INFO CountReadsSpark - Inflater: IntelInflater; 13:35:11.512 INFO CountReadsSpark - GCS max retries/reopens: 20; 13:35:11.512 INFO CountReadsSpark - Requester pays: disabled; 13:35:11.512 WARN CountReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CountReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 13:35:11.512 INFO CountReadsSpark - Initializing engine; 13:35:11.512 INFO CountReadsSpark - Done initializing engine; 2019-01-09 13:35:11 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:11 INFO SparkContext:54 - Running Spark version 2.3.0; 2019-01-09 13:35:11 INFO SparkContext:54 - Submitted application: CountReadsSpark; 2019-01-09 13:35:11 INFO SecurityManager:54 - Changing view acls to: farrell; 2019-01-09 13:35:11 INFO SecurityManager:54 - Changing modify acls to: farrell; 2019-01-09 13:35:11 INFO SecurityManager:54 - Changing view acls groups to:; 2019-01-09 13:35:11 INFO SecurityManager:54 - Changing modify acls groups to:; 2019-01-09 13:35:11 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-09 13:35:12 INFO Utils:54 -,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:4364,config,configuration,4364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['config'],['configuration']
Modifiability,".codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /opt/Software/gatk/build/libs/gatk-package-4.beta.5-70-gdc3237e-SNAPSHOT-spark.jar PrintReadsSpark -I /gatk4/output.bam -O /gatk4/output_3.bam --sparkMaster yarn-client; Warning: Master yarn-client is deprecated since 2.0. Please use master ""yarn"" with specified deploy mode instead.; 18:11:33.604 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:11:33.737 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/Software/gatk/build/libs/gatk-package-4.beta.5-70-gdc3237e-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [October 13, 2017 6:11:33 PM CST] PrintReadsSpark --output /gatk4/output_3.bam --input /gatk4/output.bam --sparkMaster yarn-client --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 13, 2017 6:11:33 PM CST] Executing as hdfs@mg on Linux 3.10.0-514.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:1672,variab,variables,1672,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"3 general classes: simple -> SimpleNovelAdjacency, complex -> ComplexVariantCanonicalRepresentation, and unknown -> SAM records of the contigs); and a groupBy() operation is necessary in the middle using these objects as keys; due to the fact that different contigs may produce the same variant; So what I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) for future use. Comment by @cwhelan ; > I'm a bit confused by this comment: this method is still being called in several places, so how is it obsolete?. Reply by @SHUANG-Broad (also copied to update the ""TODO"" message; > this predicate is currently used in two places (excluding appearance in com",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:1977,inherit,inheritance,1977,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030,1,['inherit'],['inheritance']
Modifiability,"64e; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 08:04:19 2017 -0500. mkl. commit 43e2a65201286161fcd5bfe7dbb21ae888e19dac; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 06:56:20 2017 -0500. added cpu argument for germline tasks. commit 4433a62c2173c7f29d0f264c084bbaf2f6738782; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 02:45:38 2017 -0500. revert travis yml forks; verbose logging germline wdl. commit ae05801e33c37b3bf2685fba202032a270804873; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 00:55:14 2017 -0500. updated somatic PoNs for PreprocessIntervals drop Ns. commit cff64984d9fb42364001bda4c73d54cf68d85a5c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 00:37:24 2017 -0500. sudo travis yml. commit 89025941febd2089d426cfa1e0f0aa6a6712e2a9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 00:23:22 2017 -0500. travis/Docker config update (g++-6, Miniconda3); python test group assignment. commit 31f96398106c2b8577b8c25d110abea3ebe7f836; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:44:53 2017 -0500. WDL test bugfix. commit 9b2fb820536ec355bea0256471bd093d547f5c99; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:20:36 2017 -0500. update WDL test JSON files. commit e3d97644d1a2c40a5c364a96f8b67246154179c9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:18:14 2017 -0500. assertions in inference task base; removed a ASCII > 128 character in log messages. commit 526cf92e623a3bbd5f9d375132b6ca046fc47620; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:03:04 2017 -0500. redirect tqdm progress bar to python logger. commit 2e45bd30968b921fae225de3901fb97ece690b0c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:45:49 2017 -0500. more arg related fixes. commit bb89a3bb338d88199881e8aca65f656f2acd7c0a; Author: ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:4206,config,config,4206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['config'],['config']
Modifiability,6f6105708ded7f86c96c830781; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:35:33 2017 -0500. annotated intervals kebab case; updated germline WDL workflows. commit 29cc6234dbfb8db12559217a650c6ceb170c5797; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:15:28 2017 -0500. cleanup test files. commit 08a35bb4e65eceb735adcd41a91132e9a34d2b66; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 02:50:19 2017 -0500. update WDL scripts. commit 12bcfa192ee6fa6da21239ebf5b513633efe974f; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 02:47:33 2017 -0500. significant updates to GermlineCNVCaller; integration tests for GermlineCNVCaller w/ sim data in both run modes. commit 151416a4af735ca721bd75e4b54a780c17ac9397; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 01:42:05 2017 -0500. hybrid ADVI abstract argument collection w/ flexible default values; hybrid ADVI argument collection for contig ploidy model; hybrid ADVI argument collection for germline denoising and calling model. commit 56e21bf955d3dc0c52aceb384f28cf6173959de0; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 23:18:39 2017 -0500. rewritten python-side coverage metadata table reader using pandas to fix the issues with comment line; change criterion for cohort/case based on whether a contig-ploidy model is provided or not; simulated test files for ploidy determination tool; proper integration test for ploidy determination tool and all edge cases; updated docs for ploidy determination tool. commit 7fa104b2e9170770cfc5b338835e41215d7fd39c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 18:43:17 2017 -0500. kabab case for gCNV-related tools; removed short args (this also partially affected PlotDenoisedCopyRatios and PlotModeledSegments and their integration tests). commit f02cb024331a986213cfd9fae2da706bbc5ddbd9; Author: Mehrtash Babadi <mehrtash@broadins,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:7178,flexible,flexible,7178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['flexible'],['flexible']
Modifiability,"> How can F be a probability when it takes on negative values?. It's the probability of alleles being IBD *provided that inbreeding is the only source of deviation from HWE* and in the limit of infinite sample size washing out statistical noise. Under these assumptions it's always positive. How about I rewrite the docs to be much, much clearer about this?. > Also, I've never heard of it being called the Fixation Score. I hadn't heard of it, either, but Wikipedia told me so: https://en.wikipedia.org/wiki/F-statistics",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5768#issuecomment-470249511:304,rewrite,rewrite,304,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5768#issuecomment-470249511,1,['rewrite'],['rewrite']
Modifiability,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:131,adapt,adapter,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307,2,['adapt'],['adapter']
Modifiability,"@akiezun Yes, it's true that there's no way to prevent that, short of doing deep copies every time a read is wrapped in an adapter. But we can make it clear in the GATKRead contract that the backing reads should not be directly modified after wrapping within an adapter, and if they are they need to be re-wrapped in a new adapter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718:123,adapt,adapter,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718,3,['adapt'],['adapter']
Modifiability,"@cmnbroad : first - would it be possible to kick off travis tests? i refactored this and dont seem to be able to do that. Second, yes, I was trying to reorder and condense the commits but clearly didnt work. I think the problem was trying to put your GATK3 commit first (which would seem to make sense). in any case, I just recreated this, putting a pristine GATK3 first, following a consolidated set of my commits with 1) the limited core changes, 2) the meat of the VariantEval port, and 3) A separate commit with a port of GATK3 VariantEvalIntegrationTest which is useful for validation but should not be merged. To your points:. 1) I substantially cut down the incoming large files, mostly by limiting the intervals of new large VCFs. 2) On the plugin: this was discussed above, and I initially also pointed out this should ultimately go into Barclay. You are actually the one who proposed staging it in GATK. I am not entirely sure I understand the reticence on plugins; however, my goal is to get VariantEval ported by touching as little of it as possible. This is already sucking up a ton of time. I flipped VariantEvalUtils to gather a list of classes from the appropriate package instead of a full-on plugin. That should satisfy that concern?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431913735:69,refactor,refactored,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431913735,4,"['plugin', 'refactor']","['plugin', 'plugins', 'refactored']"
Modifiability,"@cmnbroad I refactored the training java wrapper into separate wrappers to write tensors (CNNVariantWriteTensors.java) and to train (CNNVariantTrain.java) I think this simplified the meaning/necessity of many of the arguments, which was unclear when all those tools were rolled together. . I'm working on a release-style integration test that chains all the tools together, like @droazen discussed a few meetings ago, but for this PR I think I will have to do something simpler. Because of some issues with the GSA5 environment and GPU, I still have to write in a Python2/3 agnostic way, which precludes the use of type hints. I would like to update, but I'm blocked by BITs in the short term.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4245#issuecomment-367449432:12,refactor,refactored,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4245#issuecomment-367449432,1,['refactor'],['refactored']
Modifiability,"@cmnbroad could the failing WDL test simply be due to some Spark configuration issue, rather than memory? Locally, for both 1) the WDL test within the Docker and 2) CreateReadCountPanelOfNormalsIntegrationTest using 17.0.3 without the Docker, I seem to hit the exception discussed here: https://stackoverflow.com/questions/72724816/running-unit-tests-with-spark-3-3-0-on-java-17-fails-with-illegalaccesserror-cl. Not sure why CreateReadCountPanelOfNormalsIntegrationTest seems to pass in the CI environments, but perhaps it'll be more obvious to you?. Just for context, note that this tool relies on the Spark MLlib implementation of PCA.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8035#issuecomment-1409180990:65,config,configuration,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8035#issuecomment-1409180990,1,['config'],['configuration']
Modifiability,"@cmnbroad thanks for the review. I think addressed all comments except the arguments/weights simplification, which I would prefer to save for the PEP8 refactor we discussed. Back to you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5175#issuecomment-426366833:151,refactor,refactor,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5175#issuecomment-426366833,1,['refactor'],['refactor']
Modifiability,"@cmnbroad 👍 to adding an advanced command line option for it. . @magicDGS Our goal is to make it unnecessary for normal users to ever need to see a stacktrace. We're definitely not at the point yet where every UserException produces either a) the complete information necessary to debug, or even b) the correct information. We're trying to fix all those cases, but there's a lot of possible failure modes between cloud access, spark, filesystem plugins, etc, so it's going to be an issue for a while. . I don't think it will hurt the user experience to have an extra commandline argument for it. Printing the stacktrace when debug is on isn't a bad idea, but I think it's better to have finer grained control over it. . The other issue is that it's easier to explain to an unsophisticated user how to set an extra commmand line argument rather than trying to get them to set the environment variable which well be helpful for our support team when they're trying to debug someone's problem. (especially since setting the environment variables may be different on a spark cluster than on a local run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394:445,plugin,plugins,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394,3,"['plugin', 'variab']","['plugins', 'variable', 'variables']"
Modifiability,"@cmnbroad, that's not wholly unreasonable, but i'd like to push back on a number of these points. . 1) First - would GATK consider simply letting us take over VariantEval and maintain as a GATK4-based tool in another repo? My understanding from GATK4 issues is that plan was to never migrate VariantEval (i think in favor of other picard/gatk QC tools). There is a bit of a conflict between keeping a lean core engine and having all these tools built off it. I would think there's an argument for keeping your core engine and the many tools built off it separated (GATK3 seemed to include some dead tools, for example). I appreciate we're the ones pushing this migration, but I hope on the other side you can appreciate the bar is pretty significant on our time. . 2) What new plugins are you talking about? VariantStratification and VariantEvaluator are part of GATK3's VariantEval? Yeah, I wrote a base PluginDescriptor class patterned on how ReadFilters work. It probably should exist in a more core position in code. While there's some good ideas in the argument-parsing/plugin code of GATK/Barlcay, frankly seems like much of it isnt fully developed yet, which is why I kept this separated at the moment. . 3) Be aware, the GATK3 tests depend on ~30GB of files. I dont know the limits of git lfs, but I did not currently have plans to check those in. I assumed I would convert these to use GATK4 chr20/21 data for a final commit, but felt there was a lot of value in using unaltered GATK3 data to confirm parity (and it was during the migration).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407123968:777,plugin,plugins,777,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407123968,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"@cwhelan Thanks for the review! And I apologize for not clearly stating what problem is getting fixed here. I've addressed the comments in separate commit, changed the implementation, made more improvements that were discovered while reviewing the variants ; (https://github.com/SHuang-Broad/GATK-SV-callset-regressionTest/tree/master/Evaluation/Analysis/masterVSfeature/notes.xlsx); The implemented fixes are:; * for removing the hard-coded/explicit mentioning of ""chr"" in non-canonical versions, it is now fixed in 5eff782e4d582d516004fba2cee7535d984b1540; * for contigs whose alignments paint ambiguous picture, i.e. multiple alignment configurations offer equally good explanation:; 	1. if only one configuration has all alignment with MQ above a specified threshold, it is favored; this is implemented in ecc31f5fbec4e524b401fc9474a3a1b7ab08c561; 	2. if one configuration has alignment to non-canonical chromosome that explains the contig better than would-be-event-inducing mappings to canonical chromosomes, the canonical mappings are saved but the better non-canonical mappings are saved as SA tag as in SAM spec, and the VCF record produced is annotated accordingly; this is implemented in 65cdb523a2f9fa2026334713fed45381d76ffc82; * fixed a bug where sometimes an assembly contig as several alignments, only one of which has non-mediocre MQ but at the sametime this alignment contains a large gap, such contigs were previously incorrectly filtered away, they are now salvaged by commit b6b2f197b112981e00efd9d415f010c024d31b36. So, for the FN variants (FN in the sense that they are captured in the stable version of our interpretation tool but now goes missing in the experimental interpretation tool); that were curated in the above-mentioned review, only the following ones are not salvaged, with plans or comments attached. ```; asm012854:tig00000	missing	classified as ""incomplete""; fixable by finishing the last TODO in AssemblyContigAlignmentSignatureClassifier (same problem as face ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-370923522:639,config,configurations,639,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-370923522,2,['config'],"['configuration', 'configurations']"
Modifiability,"@davidadamsphd Sure, here is a quick guide to the code:. -`BaseRecalibratorSpark` is the standalone BQSR tool, and calls into the `BaseRecalibratorSparkFn` (which is also called from `ReadsPipelineSpark`). -`ApplyBQSRSpark` is the standalone ApplyBQSR tool, and calls into the `ApplyBQSRSparkFn` (also called from `ReadsPipelineSpark`). -Integration tests for the above are in `BaseRecalibratorSparkIntegrationTest` and `ApplyBQSRSparkIntegrationTest`. -Almost all other changes in the branch are related to the BQSR engine refactoring, which I summarize below:; - We pulled out the guts of the walker `BaseRecalibrator` tool, combined it with all of the code from the former `RecalibrationEngine` class (now deleted) to make a new `BaseRecalibrationEngine` class under `utils/recalibration`.; - We stripped out all copies of the code in `BaseRecalibrationEngine` from the walker, dataflow, and spark versions of BQSR, and modified them to call into `BaseRecalibrationEngine`.; - We moved all auxiliary classes needed by the `BaseRecalibrationEngine` (eg., the covariates, etc.) into `utils/recalibration`.; - We refactored the argument collections. Now there is a single shared `RecalibrationArgumentCollection` that contains **only** the parameters for the `BaseRecalibrationEngine` itself, and this argument collection is exposed by all 3 versions of the tool. Input/output arguments have been removed from this argument collection and put into the individual implementations of BQSR, since they vary between the walker, dataflow, and spark versions of the tool. This eliminates awkward problems such as having both a `knownSites` argument AND a `BQSRKnownVariants` exposed at the same time, with only 1 of them usable for a given version of a tool. The dataflow-only `BaseRecalibrationArgumentCollection` has been deleted completely as no longer needed.; - We tweaked the names of some tool arguments to enforce consistency between the 3 versions of the tool as well as the rest of hellbender (eg.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073:524,refactor,refactoring,524,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073,1,['refactor'],['refactoring']
Modifiability,@davidbenjamin I have refactored this branch to account for changes to the codebase adjacent to this code. In the interest of not possibly harming any of the old results I have made this a toggle and I have also made the setting apply symmetrically to tails and heads and added a few simple tests in the existing framework.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6113#issuecomment-640870830:22,refactor,refactored,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6113#issuecomment-640870830,1,['refactor'],['refactored']
Modifiability,@davidbenjamin I think that this issue will be addressed by the AFCalculator refactoring one way or another (e.g. by lifting up the max-alt-allele restrictions or simply avoid adding the NON-REF allele before calling the AFCalculator).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1858#issuecomment-221770394:77,refactor,refactoring,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1858#issuecomment-221770394,1,['refactor'],['refactoring']
Modifiability,"@davidbenjamin I thought you had implemented something a little more sophisticated initially, but then reverted to the ReCapSeg caller for some reason?. Anything that is relatively simple to implement yet sufficiently more principled than the ReCapSeg caller would be reasonable for this rewrite. Thought you might've had something that fit the bill originally, but maybe I'm remembering wrong. If so, then we can try leaving it as is for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324142206:288,rewrite,rewrite,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324142206,1,['rewrite'],['rewrite']
Modifiability,"@droazen and @cmnbroad: i completely understand that this is outside the main GATK dev cycle and priorities; however, do you have any guess as to when you might be able to review? I dont know how active development is, but I'd especially like to get that change in VariantWalker and MultiVariantWalker (which are currently really simple refactors) in before other development on them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-378318089:337,refactor,refactors,337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-378318089,1,['refactor'],['refactors']
Modifiability,"@igordot It used to exist because `AssemblyBasedCallerArgumentCollection` used to extend `StandardCallerArgumentCollection`, causing `Mutect2` to have a bunch of `HaplotypeCaller` arguments that it didn't use. This was fixed in PR #5758. `FilterMutectCalls` also lost a few arguments as part of a huge change to the entire filtering model in PR #5688. I'm working on a blog post about this but for now the Mutect2 docs at https://github.com/broadinstitute/gatk/blob/master/docs/mutect/mutect.pdf are up-to-date and more user-friendly than they used to be.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5845#issuecomment-478008792:82,extend,extend,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5845#issuecomment-478008792,1,['extend'],['extend']
Modifiability,"@jamesemery Great, thanks for checking. Could you do a review pass on this when you get a chance? It's not clear that the approach taken here of sending the owner config file around is what we want....it seems like instead we need a way to load the owner config from the launcher script itself.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188:163,config,config,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188,2,['config'],['config']
Modifiability,"@jamesemery and now the overview of the more complex changes:. - `AssemblyResultSet`: the code for adding and removing haplotypes based on pileup alleles has become a `void` method of this class, where it belongs. Here and elsewhere I introduce snappy variable and function named referring to ""good"" and ""bad"" alleles, which I find visually much clearer. The code is basically the same as before but somewhat streamified. I extracted a `makeHaplotypeWithInsertedEvent` method to eliminate some code duplication between GGA and pileup force-calling.; - `HaplotypeCallerEngine` and `Mutect2Engine`: Force-calling alleles are split into biallelic `Events`. Duplicated code for finding all pileup events, then sifting them into good event to force-call and bad events to remove is extracted as `PileupBasedAlleles.goodAndBadPileupEvents`. Computing `allVariationEvents` is much simpler because 1) it now uses `Event` instead of `VariantContext` and 2) `Event` overrides `equals` and `hashCode`.; - `PileupBasedAlleles`: `getPileupVariantContexts` and sorting into good and bad pileup variants has been unified into `goodAndBadPileupEvents()`. It has additionally been somewhat rewritten for conciseness. Also, instead of the somewhat kludgy method of making `VariantContext` with four temporary attributes, then filtering based on those attributes, it calculates the filtering status immediately and uses `Events`. Also fixed the somewhat-misleading use of the word `alt` to mean `SNP`.; - `AssemblyBasedCallerUtils`: `applyPileupEventsAsForcedAlleles`, along with several helper methods that it calls, has been moved into `AssemblyResultResult`, where it is now a void member method.; - `GATKVariantContextUtils` mainly just using `Event` instead of `VariantContext`, which simplifies the code for splitting a `VariantContext` into biallelics. After going through this exercise I realize that it's not actually so much. The diff's bark is worse than its bite. The overwhelming majority of changes are eit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8332#issuecomment-1574175702:252,variab,variable,252,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8332#issuecomment-1574175702,1,['variab'],['variable']
Modifiability,"@jamesemery sorry to bug on this topic, but I'm hoping to make a push early this year to fully migrate my lab off GATK3 . I looked more closely at the specific annotations we need to migrate. I decided that I will implement our walker, 'DiscvrVariantAnnotator', which is basically a light wrapper around VariantAnnotation. This will make it easier to spike in custom annotations. In that walker, I will override makeVariantAnnotations(). I will make a new marker interface for EngineAwareAnnotation, and test that on all the Annotation classes, and use this to inject FeatureManager. So no core GATK changes needed. I did find one thing I'd like to propose. You probably know PedigreeAnnotation is special-cased in GATK. Annotations that use it have automatic argument validation and have the SampleDB injected. Currently, PedigreeAnnotation is a subclass of InfoFieldAnnotation, so isnt available to GenotypeAnnotations. There doesnt appear to be a solid reason why. I tried to fix that and my best idea is the proposal here: #7041 . The core idea is to convert InfoFieldAnnotation and GenotypeAnnotation to interfaces. This is generally a trivial switch in existing code. With that, it becomes possible for classes that currently extend PedigreeAnnotation (which I switched to no longer extend InfoFieldAnnotation) to simply PedigreeAnnotation and implement InfoFieldAnnotation. This makes it possible for future classes to extend PedigreeAnnotation and implement GenotypeAnnotation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-760424063:1232,extend,extend,1232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-760424063,3,['extend'],['extend']
Modifiability,"@jfarrell Do you recognize ""scc"" as a local host name ? ""hdfs:///project/casa/gcad/adsp.cc/sv"" looks reasonable enough as a file URI, except that the hadoop file system provider requires an authority component (the part of the uri between the second and third slash: ""hdfs://authority-component/..."") be provided in such URIs. Since you didn't include one as part of the hdfs path on the command line, it looks like transform along the way resulted in one being added (the authority component looks like ""host:port""), resulting in the port number -1. So I'm not clear if its a configuration issue, or a bad code code path, or both. But I would suggest trying an hdfs path with a valid authority component (one that works with the hadoop shell). @SHuang-Broad I do see some code paths in `StructuralVariationDiscoveryPipelineSpark` that call `Paths.get directly`, rather than `IOUtils.getPath()`. I would also suggest replacing the direct calls to `makeSAMOrBAMWriter` in `SVFileUtils` with the GATK wrapper code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-493980166:577,config,configuration,577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-493980166,1,['config'],['configuration']
Modifiability,"@jonn-smith, I did see XsvLocatableTableCodec and the .config file path, but this does not appear to work. To be clear this is something like:; ```; gatk IndexFeatureFile -I ./hg19/testTextSource.config; ```; In IndexFeatureFile (https://github.com/broadinstitute/gatk/blob/abe8148bda234edf6bd00fa51df44d456e8e2641/src/main/java/org/broadinstitute/hellbender/tools/IndexFeatureFile.java#L118), it does identify the correct codec; however, it then calls:. IndexFactory.createDynamicIndex(featurePath.toPath(), ...). where featurePath is the config file. This calls IndexFactory to open a lineReader on the config file (not the backing data source): https://github.com/samtools/htsjdk/blob/6d3fc7bc1f613ecfce1c22d368f3ae17cb86823d/src/main/java/htsjdk/tribble/index/IndexFactory.java#L598. . This then fails during XsvLocatableTableCodec.readActualHeader(), since this is trying to read the config file, not the TXT file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1591678472:55,config,config,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1591678472,5,['config'],['config']
Modifiability,"@koncheto-broad this is one of the VQSR-lite PRs you will want to eventually rebase on. It's still awaiting review (I was waiting until the dust from updating to Java 17 in #8035 settles), but if anyone from your team wants to take a first crack, feel free! Not too many code changes, so hopefully it should be pretty manageable. Just so it's all in one place: your #8157 GVS branch is currently rebased on #7954, which contains the ""serial SNP-then-indel"" version of the Joint Genotyping WDL (written by Megan for Ultima) and the Java code for the tools. Some minor updates were made to the Java code in #8049 and the WDL was rewritten by me to do SNPs and indels in a single pass in #8074. (EDIT: I was originally confused here, the WDL that was replaced in this PR simply ran SNPs and indels separately, rather than serially—thanks to George for correcting me here.). The PR here makes relatively minor updates to both the Java code and the WDL and might require very minor updates to GVS code or JSON configurations. And finally, the larger PR at #8132 adds a Pure Java BGMM backend. As we discussed during my mobbing presentation, this is provided merely as a convenience for those users that might not be able to control their python environment (hopefully a small number, these days!), so getting it merged is probably less urgent and should not affect any GVS work.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1414056344:1005,config,configurations,1005,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1414056344,1,['config'],['configurations']
Modifiability,"@laserson the `SAMRecord` vs. Google `Read` is a loooooong story.; The super-short version:; We had a bunch of utilities written for `SAMRecord` that @droazen refactored over months to take the GATKRead interface. As it happens, the SAM spec and the GA4GH spec are not 100% compatible. So, it's not possible to losslessly convert from A -> B -> A (where A is `SAMRecord` or Google `Read`). The cases where it doesn't work are edge cases, but they exist. Second, @jean-philippe-martin found that converting to Google `Read` was fairly expensive. Between those two points, I think we're probably better off with SAM-backed reads. (Also, right now the Google `Read` is serialized via JSON, so it's not that small anyway.). @tomwhite and @jean-philippe-martin, I think adding the header back will be fine for us engineers working on the engine, but it will make for a poorer user experience for newcomers and Comp Bios to burden them with having to care about what happens with shuffles (when they just want to prototype something). . That said, I think this is probably the best approach we have at our disposal. If we do, we need to do an excellent job of throwing errors if users try to perform actions that would require the header. The error message should explain what really happened and ideally point to some documentation we write explaining the stripping of the header and how to fix it. If this error occurs, it needs to be simple for anyone to fix it. @droazen @lbergelson, what do you two think? (also @laserson, do you have any ideas or thoughts on the header since we're probably stuck with `SAMRecord`?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025:159,refactor,refactored,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025,1,['refactor'],['refactored']
Modifiability,"@lbergelson - In gradle, I first resolve with maven central and then with your artifactory:. ```gradle; repositories {; mavenCentral(); maven {; url ""https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/""; }; }; ```. In the case of maven, for several repositories this should be done following [this](https://maven.apache.org/guides/mini/guide-multiple-repositories.html). I think that the configuration for the repositories should look like this (if I remember correctly):. ```xml; <repositories>; <repository>; <id>central</id>; <name>Maven Repository Switchboard</name>; <layout>default</layout>; <url>http://repo1.maven.org/maven2</url>; <snapshots>; <enabled>false</enabled>; </snapshots>; </repository>; <repository>; <id>snapshots</id>; <snapshots>; <enabled>true</enabled>; </snapshots>; <name>libs-snapshot</name>; <url>https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot</url>; </repository>; </repositories>; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-340482852:398,config,configuration,398,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-340482852,1,['config'],['configuration']
Modifiability,"@lbergelson Sorry to be unclear---this isn't a GATK issue. For Cromwell, you can configure various options for each backend. For example, if you are running on a local backend with Docker, you can set a `submit-docker` attribute to specify the string that runs the Docker container; so to solve the above problem, you'd set this to include `--shm-size` and set it accordingly. However, according to @jsotobroad, you're not allowed such an attribute when submitting to Google cloud. If that's the case, then this is more of an issue with the Cromwell/Google Pipelines interface than the data.table package (although, as the discussion in the GitHub issue above shows, it'd be a simple fix on the data.table end, so I'm not sure why it's not addressed yet...) Changing the R script to get around the issue in this particular case is not unacceptably ugly, but you could imagine we might run into a similar problem in the future if anything else exceeds the 64MB /dev/shm limit and also cannot specify tmpfs. So perhaps we should take a look at the underlying issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357375691:81,config,configure,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357375691,1,['config'],['configure']
Modifiability,"@lbergelson You're right, it would be easier to read that way, but it leaves a dangling ""Optional Arguments"" string in the output even when there are no optional arguments. If you're still not sold I can rewrite it to iterate once to count the optional args, but this is a cheap, simple way to get the right output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/566#issuecomment-112561579:204,rewrite,rewrite,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/566#issuecomment-112561579,1,['rewrite'],['rewrite']
Modifiability,@lbergelson can you review - it's a simple enhancement to the CompareBaseQualities tool,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1773#issuecomment-214757716:43,enhance,enhancement,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1773#issuecomment-214757716,1,['enhance'],['enhancement']
Modifiability,"@lbergelson. Our simple s3 nio library is not currently open source, but we would be willing to make it so after testing it properly. We found that to get the s3 nio library work with GATK, a few minor changes needed to be made in the GATK source. This is especially true for the spark tools because, on AWS EMR Spark clusters, s3 uris can be treated exactly as if they are HDFS uris. Therefore, it was not quite as simple for us to just add the s3 nio library to the classpath and have everything work as expected. For that reason we put the project on hold until GATK is closer to release. Thanks,; David. ________________________________; From: Louis Bergelson <notifications@github.com>; Sent: Monday, July 31, 2017 11:57 AM; To: broadinstitute/gatk; Cc: David Brown; Mention; Subject: Re: [broadinstitute/gatk] update com.google.guava version (#3102). @david-wb<https://github.com/david-wb> Is your s3 plugin available as an open source plugin that others could use? We had another question about s3 support in gatk and I thought you might have some insight about it. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319146368>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ABxO-d5XTtUyeAI0GzCFLP5eVGYiyQJEks5sThWegaJpZM4N31U->.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834:907,plugin,plugin,907,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834,2,['plugin'],['plugin']
Modifiability,"@ldgauthier Some parts of taking splitting MNPs at the end of HaplotypeCaller are easy: breaking eg one DNP at position n into a SNP at n and a SNP at n + 1, letting the SNPs inherit the PLs, AF, and AD (okay, this isn't quite right because a read might end in the middle of the MNP, but close enough) of the parent MNP. . . but the general problem of splitting annotations seems like it might be too tricky. I'm leaning toward instead just modifying `AssemblyBasedCallerGenotypingEngine.phaseCalls()`. It seems that this phasing relies very heavily on perfect phasing or anti-phasing and that even one questionable haplotype with incorrect phasing can spoil things. I would guess that we could improve the phasing by making some simple guess as to which haplotypes are real. Basically, the problem is that while HaplotypeCaller imposes ploidy on alleles, it does not do so on haplotypes, and so phasing information is diluted. With your permission I would like to merge this PR and open a new issue for improving `phaseCalls`. After all, the issue is fixed in M2, and HC now has a perfectly good MNP mode, with the caveat that it doesn't interact nicely with GVCF mode.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-384836262:175,inherit,inherit,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-384836262,1,['inherit'],['inherit']
Modifiability,"@magicDGS It looks like you have triggered a few new compiler errors in the last branch, namely in the following places:. ```; /gatk/src/test/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/SVDiscoveryTestDataProvider.java:33: error: cannot find symbol; BaseTest.b38_reference_20_21, ReferenceWindowFunctions.IDENTITY_FUNCTION);; ^; symbol: variable BaseTest; location: class SVDiscoveryTestDataProvider; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/formats/SampleLocatableCollectionUnitTest.java:30: error: cannot find symbol; private static final String TEST_SUB_DIR = toolsTestDir + ""copynumber/formats"";; ^; symbol: variable toolsTestDir; location: class SampleLocatableCollectionUnitTest; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedregion/SimpleAnnotatedGenomicRegionUnitTest.java:18: error: cannot find symbol; private static final String TEST_FILE = publicTestDir + ""org/broadinstitute/hellbender/tools/copynumber/utils/combine-segment-breakpoints-with-legacy-header-learning-combined-copy-number.tsv"";; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259:353,variab,variable,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259,2,['variab'],['variable']
Modifiability,"@magicDGS The GATK versioning scheme is not related to the API -- it is targeted at end users rather than projects using GATK as a library. Here's a slide that explains it:. <img width=""824"" alt=""gatk_versioning"" src=""https://user-images.githubusercontent.com/798637/38042254-e5bb85a4-3281-11e8-8d83-017bb6b73fda.png"">. As the slide mentions, we have given some thought to supplementing the main version number with an ""API version number"", but we'd have to more clearly define what constitutes the official public API for the GATK before doing so. On a side note, now that we're in general release it may be easier for you to get PRs for things like new walker types merged into the GATK proper, particularly if they are fairly self-contained and don't involve refactoring lots of engine classes. I was planning to ask whether you wanted to resurrect your `SlidingWindowWalker` PR at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4603#issuecomment-376946968:762,refactor,refactoring,762,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4603#issuecomment-376946968,1,['refactor'],['refactoring']
Modifiability,"@magicDGS The HaplotypeCaller traversal has undergone some changes in the past few weeks to improve performance and bring the output of the tool closer to GATK3. There is now an `AssemblyRegionWalker` that divides the intervals into active and inactive regions, in a greatly simplified version of the GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:373,extend,extend,373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,2,"['adapt', 'extend']","['adapted', 'extend']"
Modifiability,"@magicdgs You could include this filter in ReadTools and the plugin would discover it - after all, thats part of the purpose of plugins ;-). Anyway, at a minimum we should make sure the doc clearly explains when/how to use this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5367#issuecomment-434681393:61,plugin,plugin,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5367#issuecomment-434681393,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"@mbabadi I've updated my PR to use miniconda3. @mbabadi @lucidtronix @samuelklee I think we should aim for tools that at least run out-of-box, without depending on any out-of-band configuration other than the conda env. On top of that we can provide guidance/configs for users on how to enable further optimizations, like g++. Does that sound like an achievable goal ?. As for the docker, we're going to have strike the right balance between image bloat and performance(including test performance). I think we're around 4+ gig now, and counting. Before the Python integration we were at 1.9G, and trying to find ways to reduce it. So lets see where we wind up but keep that in mind. Finally, we need to find a way to install the (GATK) python package(s) without depending on access to the GATK repo. Right now I think the gCNV branch has a ""pip install from source"" added to the conda env .yml. That will work on the docker at the moment (and thus on travis), but that won't work for non-docker users how don't have source/repo access. Also, one of the proposals to reduce the size of the docker is to remove the repo clone that is currently there. My proposal is that we change the gradle build to create an archive/zip of the python source (this would include the VQSR-CNN package code as well as gCNV kernel). We can then copy that on to the docker image, and pip-install it from the copy. That would retain the ability to always run travis tests based on the code in the repo, and also keep the nightly docker image in sync. We'll also have deliver the archive as an artifact somehow (perhaps including PyPi) for non-docker users.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277:180,config,configuration,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277,2,['config'],"['configs', 'configuration']"
Modifiability,"@ronlevine I know this a port from gatk3, but I think theres a bit of refactoring that can be done. It seems like it's more complicated than it needs to be. Could you take a look and see? In particular I'm not sure why things get converted to a bitset, it looks like you should just be able to derive the indecies directly and avoid creating a bitset. If I'm missing some detail and it can't be simplified let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049:70,refactor,refactoring,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049,1,['refactor'],['refactoring']
Modifiability,"@samuelklee ; My understanding: the code that **can** (and I think should) be borrowed from VCF is `CHROM`, `POS`, `ID`, `INFO`, with `END` from `INFO` extracted to be its own column. ; Then; * `FILTER` can be optional.; * `QUAL` can be optional but it is a nice-to-have feature as a quick-glance confidence measure, if that applies.; * `FORMAT` is going to be hard, because I understand the complaint that they can be wasting space, but I have seen VCF files that have rows with different numbers of fields in `FORMAT`, and that is spec-compliant. If this flexibility is allowed, i.e. allowing sample specific information to be missing on several rows, then the `FORMAT` column can be shared. Recap: only `REF`, `ALT` are missing, which is not much code I believe. I think VCF just happens to have a name that starts with V. Stripping out the `REF`, `ALT`, it is quite flexible for describing any annotated interval (OK, 0-length is up for debate) on a piecewise linear coordinate. And I just made myself sound like a VCF-lover. I simply think much of it can be reused.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481753416:870,flexible,flexible,870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481753416,1,['flexible'],['flexible']
Modifiability,"@samuelklee DRAGEN STRE model doesn't actually make any alterations to the smith waterman parameters or how they work, it just works by adjusting the indel gap penalties that are used for the PairHMM. At one point we were concerned about SW parameters being different with dragen but as it turns out the biggest visible effect of the SW parameters on the output (the alignment we perform after haplotypes discovery) is irrelevant since they don't realign their reads internally. We kept the default gatk alignment behavior and thus the SW parameters that are used (for dangling head recovery which I believe are the old arguments) still match. As far as unifying the parameters I suspect it could be done though one wonders if there aren't risks where the different contexts in which we use the parameters will not perform as well with a unified set. Speculation on my part though. I agree with David that we should be cautious about making changes that will affect the HaplotypeCaller before November. . I support including an argument in any case (possibly multiple) to include the SW parameters. I would actually advocate we read these files in as tables of parameters where you simply point to on the command line to configure new parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993:1221,config,configure,1221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993,1,['config'],['configure']
Modifiability,"@samuelklee It wasn't just a rebase, it was a complete rewrite because the old code had since become completely entangled with DRAGEN code. But I did it! Everything is passing, the code is dramatically simpler, and it's even a bit faster. I have done my best to make a coherent commit history. I would recommend reviewing one commit at a time in side-by-side diff mode. Note that some commits rip out old code and replace it with pseudocode, deferring the new code to a later commit. Other commits tell a story of what all the different caches meant in order to motivate the simpification of later commits. The baroqueness of the old code was motivated by three considerations:; * cache-friendliness -- traversing all arrays by incrementing the innermost index, reads. This is absolutely essential.; * flattening 3D arrays into 1D arrays. This was a premature optimization.; * Precomputing addition operations -- this was misguided. The DRAGEN code relied on these caches in a rather complex way, which fortunately turned out not to be necessary and which could be dramatically simplified. My notes on tracking all the variables from the parent genotype calculator down to the DRAGEN calculator are in this google doc: https://docs.google.com/document/d/1v6s57mUAwfj38nL3VdktjA059kYBkJfokq18IDy79E8/edit?usp=sharing. Good luck and don't hesitate to ask me to explain anything.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1023647476:55,rewrite,rewrite,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1023647476,2,"['rewrite', 'variab']","['rewrite', 'variables']"
Modifiability,"@t-ogasawara @frank-y-liu @gspowley @paolonarvaez @droazen @lbergelson please comment on the following proposal. The proposal is that we would spin off native PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:265,extend,extend,265,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864,1,['extend'],['extend']
Modifiability,@tedsharpe @SHuang-Broad I've tried to address your comments -- want to have a another look? . Due to issues in the class I backed out my usage and refactoring of SATagAlignmentBuilder and SATagAlignment and just went with my own simple little parser.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2684#issuecomment-301569060:148,refactor,refactoring,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684#issuecomment-301569060,1,['refactor'],['refactoring']
Modifiability,"@tovanadler Review complete. Looks good, just a few comments. I have a few comments about the organization of duplicate marking. I think you've inherited some very old style code that could maybe use some refactoring. I think we do need to also include the histogram and the metrics headers. Those could be done in a separate ticket though. I'm a bit worried that the test is indeterministic. Unless I overlooked something which is likely, it seems like it might depend on the ordering of a PCollection which is undefined. This isn't problematic for the actual metric file, but might be for the tests. What do you think about reorganizing to output an annotation on only 1 of the ""best"" reads with the count of all optical duplicates in it's group. That would simplify the code, and since we only care about the global count it wouldn't change the information content.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958:144,inherit,inherited,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958,2,"['inherit', 'refactor']","['inherited', 'refactoring']"
Modifiability,"@vilay-nference Thank you for doing this work. It's nitpicky annoying stuff to figure out.; ; I have one additional request. Instead of addding additional direct implementation dependencies, could we specify the transtive version requirements in a [gradle constraints block](https://docs.gradle.org/current/userguide/dependency_constraints.html)? . That will: ; 1. make it clear that we don't rely on these directly; 2. prevent us from keeping them around if we do something like remove hadoop in the future; 3. lets us rewrite those force blocks to instead define minimum versions so if the libraries move forward in the future we're not accidentally holding on a to an old version",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8950#issuecomment-2297074810:520,rewrite,rewrite,520,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8950#issuecomment-2297074810,1,['rewrite'],['rewrite']
Modifiability,"Actually what you said is correct: it is very painful to extend the `Main` class and use the current implementation. The simplest `Main` class that I'm using it's not extending the GATK one just because of the static methods (see the code [here](https://github.com/magicDGS/thaplv/blob/master/src/main/java/org/magicdgs/thaplv/Main.java)), that's why I would like to include this change. In few minutes I will commit the changes that you propose, including the method and the change from static. Thanks for the feedback, @lbergelson!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255841430:57,extend,extend,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255841430,2,['extend'],"['extend', 'extending']"
Modifiability,"Additional feedback from the user for the mutect2 workflow. > ""Of note, it is really difficult and not really 'user-friendly' to have to predict disc space and runtime for Funcotator, which seem to depend (based on calculations you copied above from other Functotator workflows) on outputs of Mutect2 (eg vcf sizes), when here Mutect and Funcotator and bundled together. So I cannot see output of Mutect to predict values for Funcotator - especially not when I get to run this over hundreds of samples. It is also pricey to have jobs failing because of this. It would be much better to have these variables encoded, so that the algorithm uses Mutect outputs to predict memory etc. that it will need to run Funcotator downstream. If this is really how things work (and this is my current understanding), I really do not know how to estimate this for many samples without 'trial and error' that is both costly and it will take extremely long time....""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6680#issuecomment-651354230:597,variab,variables,597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6680#issuecomment-651354230,1,['variab'],['variables']
Modifiability,"Another thing that just come to my mind is to rely on [SLF4J](https://www.slf4j.org/) for logging - downstream projects can configure which logger they want to use, and they can have their own ways of setting logging verbosity. If the logging system from HTSJDK wants to be maintain, it can also add a simple implementation of SLF4J with the verbosity levels that are in the current implementation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-371401288:124,config,configure,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-371401288,1,['config'],['configure']
Modifiability,"Apart of the amount of work in both Barclay and GATK, I think that this shouldn't be implemented for 2 reasons:. * After #3486, some tools are hidden from the command line (and they will be most likely undocumented too). If the bash-completion works with undocumented tools that are hidden from the command line, there will appear anyway after pressing tab-tab. If that tools are treated in a different way, then it requires even more work - Barclay does not use the omitFromCommandLine at all, and that means that GATK should extend the bash-completion to take it into account.; * If a tool can bash-complete but it does not show in the online help pages (the main source for help, taking into account that in the CLI is a bit messy when the parameter space grows), then it will be really difficult to really understand how the tool work. Even if it shows the parameters with tab-tab, the only way of checking what the meaning of each of them is look at the CLI help. Because the bash-completion is a sub-type of help-doclet, it should require the `@DocumentedFeature` annotation: that is the marker interface in Barclay for mark classes as parsed/added to the ""help"" generated by doclets....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3596#issuecomment-331112758:527,extend,extend,527,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3596#issuecomment-331112758,1,['extend'],['extend']
Modifiability,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:341,extend,extend,341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661,1,['extend'],['extend']
Modifiability,"BucketUtils was a solution before we had Filesystem providers. It's stuck around as a parallel set of code because we couldn't trust the providers at first. In the long run it should be removed and replaced entirely by `Files` operations. We need to test that all the functionality exists / works as expected though, and it hasn't been a high priority to do so. Particularly, I'm not sure we have a lot of faith in the HDFS NIO plugin, so we may need to keep around special cases for that. It could definitely at least be simplified a lot though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3569#issuecomment-328993020:428,plugin,plugin,428,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3569#issuecomment-328993020,1,['plugin'],['plugin']
Modifiability,"By doing the following, I was able to get a JointGenotyping result for my 343 samples:; - increased the amount of memory allocated to the Java heap in ImportGvcfs to 50000m; - modified the runtime attributes for all the joint genotyping tasks to match the format that Cromwell accepts for HPC environments (https://cromwell.readthedocs.io/en/stable/tutorials/HPCIntro/#specifying-the-runtime-attributes-for-your-hpc-tasks); - increasing the runtime memory attribute for ImportGvcfs and GenotypeGvcfs from 26000 MiB to 60 G; - executing the workflow with the following sbatch parameters:; nodes=4; ntasks=32; mem=248g; tmp=429G; - manually tar'ing up all the genomicsdb directories from the execution directories of all 10 shards of ImportGvcfs after they successfully completed GenomicsDBImport and failed with the error message: ; pure virtual method called ; terminate called without active exception; - running an abbreviated version of JointGenotyping which started at GenotypeGvcfs and executed the remainder of the JointGenotyping workflow unchanged.; ; I think this pretty clearly demonstrates that, whatever is going on, it occurs between GenomicsDBImport's successful creation of genomicsdb and the tar -cf of same. The failure is 100% reproducible with a number of different runtime configurations. The error messages are from C++ and seem to be occurring at the point where native C++ code is handing execution back to Java.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1314069533:1293,config,configurations,1293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1314069533,1,['config'],['configurations']
Modifiability,"By the way, I thought @vdauwera was opposed to using optional inputs in this way at some point (see #3657). Was that question ever decided? (I'm still of the opinion that they *should* be used in this way, but this is one of the reasons I didn't for this iteration of the WDL.). To be clear, the pair WDL right now does not allow all of the workflow paths (tumor-only, no PoN, etc.) that the new tools make possible. It only allows the one that we will most likely run in production (matched-normal + PoN). We should probably make the WDL a little more flexible to cover the most common use cases, but I'm fine if it doesn't completely expose all of the possible workflow paths---this would probably just make the WDL harder to maintain. Users can write their own WDLs in this case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362696132:553,flexible,flexible,553,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362696132,1,['flexible'],['flexible']
Modifiability,"CKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/wgs.hg38/pipelines/hc/cram.test/GRCh38_full_analysis_set_plus_decoy_hla.fa.gz --spark-master yarn; 2019-01-09 13:35:04 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:05 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 13:35:09.640 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:35:09.799 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:11.507 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.508 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:35:11.508 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:35:11.508 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:35:11.508 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:35:11.509 INFO CountReadsSpark - Start Date/Time: January 9, 2019 1:35:09 PM EST; 13:35:11.509 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.509 INF",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:2095,variab,variables,2095,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"Can you give a bit more information here? If I'm understanding correctly, it's not clear that the same issue is at play here. The original issue was that duplicate/incomplete fragments were causing queries to the workspace to fail. . In this latest instance, it seems you are appending additional samples to the existing workspace. Is that right? If so,; - are you seeing the same/similar error? That is, it's a core dump? Can you share the error messages, any logs, core dump files etc?; - did you clean up the workspace before importing? That is, remove the incomplete fragment @nalinigans identified and the duplicated ones?. My first instinct is that even if the incomplete/duplicated fragments weren't cleaned up, the incremental import shouldn't have an issue -- at least not till it gets to the consolidate phase, which only happens after all batches are imported. Sounds like you were seeing an issue at batch 3 of 4, so might have something to do with the samples in that batch...or some other import issue. You mentioned that previous imports to this particular contig failed -- were those just transient failures that worked when rerun, or was there some configuration that you changed to get that to work?. For completeness, the way I identified duplicate fragments was to do an md5sum check on some of the internal files. If any pair of fragments have the same md5sum they are likely duplicates. So, from the workspace directory, something like:. ```; find . -name ""ALT.tdb"" -exec md5sum {} \;|sort; ```; That will highlight the fragments that are potentially duplicate. To confirm that the fragments are indeed duplicates, you'll then want to take that list of potentially duplicate fragments and check that all corresponding files within each pair of potentially duplicate fragments actually have the same md5sum. I have a crude bash script that I can share if you want.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722541707:1166,config,configuration,1166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722541707,1,['config'],['configuration']
Modifiability,"Considering that this PR has lasted through my absence and the holiday season, I want to take the chance of summarizing the concerns you have issued here:. --------. Resolved as requested (or at least made efforts to):. * documenting the logic and methods; documented. * emit VCF instead of custom file format; emit both VCF custom file format now. * bug in determining if alignment signature satisfies `allMiddleAlignmentsDisjointFromAlphaOmega`; bug fixed in commit b4f7568b03b91eb77d256bcfe8117001bce040ec. --------. Unresolved yet:; the fact that gap split happens after the alignment configuration scoring step is considered backwards. I agree in principle but due to AS and MQ were used in the scoring step, and split-copy leads to technically wrong AS & MQ, I originally decided to score first, then split. Splitting the gapped alignments was introduced originally to have a centralized logic in inferring type and location of the events. . The tension is that AS is used in the scoring but becomes practically useless after that. >> Correct, but I am having thoughts about this now (not to pick only one—that; would be wrong—but to ditch them altogether probably under some condition; and redo the alignment step), exactly because of this behavior I observe.; Think about the case where one originating gapped (say insertion); alignment, after splitting, has one of the two children contained in; another alignment (not its sibling, that's impossible) in terms of their; read span. Now the originating gapped alignment probably should be filtered; out, or not, because if we keep it, an insertion would be called but; apparently there are alternative explanations due to the other alignment.; I'm not sure how to deal with this case, and if this scenario is common; enough. It probably is the case that such alignments happen mostly in STR; regions, so getting the exact alignments correct there is no easy task.; ; > Is that enough of a concern to worry about. In such a case I feel like we; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-354976980:589,config,configuration,589,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-354976980,1,['config'],['configuration']
Modifiability,"Dear all,. I am a bit confused why GATK uses `[0,1,2]` for the `GT` files, even though VCF specifications clearly state that the `GT` field is `encoded as allele values separated by either of / or |`. They even say that for `diploid calls examples could be 0/1, 1 | 0, or 1/2, etc`. As it is right now, if I read a VCF from GATK CNV germline pipeline through `bcftools`, the `GT` field is changed to `-65`:; ```; 1 17345376 CNV_1_17345376_161326630 N <DUP> 101.19 . END=161326630 GT:CN:NP:QA:QS:QSE:QSS -65:3:13:11:101:3:18. 1 161332119 CNV_1_161332119_161332223 N <DEL> 3.19 . END=161332223 GT:CN:NP:QA:QS:QSE:QSS -65:1:1:3:3:3:3. 1 193091331 CNV_1_193091331_241683022 N <DUP> 268.21 . END=241683022 GT:CN:NP:QA:QS:QSE:QSS -65:3:27:34:268:36:3. 2 96919546 CNV_2_96919546_96931119 N . 62.93 . END=96931119 GT:CN:NP:QA:QS:QSE:QSS -65:2:3:38:63:38:63. 3 10183532 CNV_3_10183532_69928534 N . 469.93 . END=69928534 GT:CN:NP:QA:QS:QSE:QSS -65:2:22:31:470:19:75. 3 69986973 CNV_3_69986973_70014399 N <DUP> 10.12 . END=70014399 GT:CN:NP:QA:QS:QSE:QSS -65:3:8:4:10:4:10; ```. Any reason to not use the standaed `GT` format?. I have also noticed that GATK outputs some non-variable SVs to the VCF without any ALT allele. Why not remove them if they are actually not SVs, if `GT=0` and `CN=2`?. thanks,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6167#issuecomment-621738904:1164,variab,variable,1164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6167#issuecomment-621738904,1,['variab'],['variable']
Modifiability,"Done with my review. Thanks for doing this much-needed refactor! The BaseRecal stuff looks sound, but I have some other feedback that we should discuss/address.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142750080:55,refactor,refactor,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142750080,1,['refactor'],['refactor']
Modifiability,"First cut at a rewrite seems to be working fine and is much, much leaner. Building a small PoN with 4 simulated normals with 5M bins each, CombineReadCounts took ~1 min, CreatePanelOfNormals (with no QC) took ~4.5 minutes (although ~1 minute of this is writing target weights, which I haven't added to the new version yet) and generated a 2.7GB PoN, and NormalizeSomaticReadCounts took ~8 minutes (~7.5 minutes of which was spent composing/writing results, thanks to overhead from ReadCountCollection). In comparison, the new CreateReadCountPanelOfNormals took ~1 minute (which includes combining read-count files, which takes ~30s of I/O) and generated a 520MB PoN, and DenoiseReadCounts took ~30s (~10s of which was composing/writing results, as we are still forced to generate two ReadCountCollections). Resulting PTN and TN copy ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:15,rewrite,rewrite,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['rewrite'],['rewrite']
Modifiability,"Hello @cmnbroad. My current solution satisfy all the constraints and it's not too complicated, although is not as simple as a common generic class that just need to be extended. Have a look and if you like it I can implement some tests for `CountingVariantFilter`; if not, I could come back to a separate `CountingVariantFilter` with its own and/or/negate inner classes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272482490:168,extend,extended,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272482490,1,['extend'],['extended']
Modifiability,"Here's some old code that uses SamLocusIterator (from tfennel) that AllelicCapseg can adapt for now. From Tim:; ""They key to making this nice and simple is the SamLocusIterator class, which given a BAM file and a list of intervals, will give you pileups at each position in the intervals, filtered how you want them, and even provide convenience methods to access the exact base per read that is piled up at the site etc. The really nice things about doing it this way is that the constructor to SamLocusIterator takes a simple parameter to tell it whether to use an index/query mechanism (similar to what you're doing now) or to just read the BAM serially up until the last interval is reached and output the loci of interest. Running the below with ~100k sites on a standard exome (15GB or so) without using the index took only about 15 minutes."". ```; public void pileup(final File bam, final IntervalList intervals, final int minQ, final File outputFile) {; final int MAX_INTERVALS_FOR_INDEX = 25000; // just a guess, not sure what the right number is. final SamLocusIterator iterator = new SamLocusIterator(new SAMFileReader(bam), intervals, intervals.size() < MAX_INTERVALS_FOR_INDEX);; iterator.setEmitUncoveredLoci(false);; iterator.setQualityScoreCutoff(minQ);. final BufferedWriter out = IoUtil.openFileForBufferedWriting(outputFile); // will automatically gzip if filename ends with .gz; try {; while (iterator.hasNext()) {; final SamLocusIterator.LocusInfo locus = iterator.next();; int a=0, c=0, g=0, t=0;. for (final SamLocusIterator.RecordAndOffset rec : locus.getRecordAndPositions()) {; switch (rec.getReadBase()) {; case 'A' : ++a; break;; case 'C' : ++c; break;; case 'G' : ++g; break;; case 'T' : ++t; break;; }; }. out.append(locus.getSequenceName() + ""\t"" + locus.getPosition() + ""\t"" + a + ""\t"" + c + ""\t"" + g + ""\t"" + t + ""\t"");; }. out.close(); ; }; catch (IOException ioe) { throw new RuntimeIOException(ioe); }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420:86,adapt,adapt,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420,1,['adapt'],['adapt']
Modifiability,"Hey @bbimber I will have to think on this. The most simple solution might be to add a feature context side input for the annotation in question but looking at how that code is threaded in the variant callers it would take a little bit of work to add it to those tools and probably introduce some complicated questions, (like for example: what is the correct featurecontext to send to annotate a variant that only covers one base of the site in question where the feature context object exists?). Its possible to do something like that for variant annotator a little bit more easily but i guess the question comes down to this: How generalized do you think this annotation will be? Does it need to be annotatable with variant annotator or could you write a separate tool that does the variant -> variant association and calculates the annotation without using the plugin framework? If it needs to be generalizable I would agree with @droazen that the easiest approach would be to add the side input as an argument and make the annotation object responsible for querying the feature context. This is inelegant but might be preferable to putting the entire walker context into the `annotate()` function.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754249851:863,plugin,plugin,863,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754249851,1,['plugin'],['plugin']
Modifiability,"Hey all, I'm still interested in supporting this. We don't really have a ""plugin API"", I am in fact the API, but if you give me something usable I'll plug it in. As this is marked ""QuixoticDream"" I don't think that's likely. I'm closing the corresponding IGV issue, too many open issues, but it doesn't mean I've lost interest.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3286#issuecomment-433201230:74,plugin,plugin,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3286#issuecomment-433201230,1,['plugin'],['plugin']
Modifiability,"Hi @cwhelan , I've expanded this PR to do more than what it originally was trying to fix, and separated the patches by commits as usual:. * the originally proposed fix, which brings back the annotation that are available to simple variants but go missing due to a careless bug, is now done in commit 50f1b640a31ddb528dc763b83b26a9d98dce8556; this commit also accordingly refactors the giant class `CpxVariantDetector` into three new classes; * in the 2nd commit 734516383fb665a79796de76535560fc03cb754b, I did more refactoring on how we group the descriptions for the annotation keys, and updated the test VCF files accordingly.; * because of the refactoring, the review comments were gone, so I added them back in the 3rd commit b7619c45a949dfba21d65a5ed876bc72e832aa77, which contains the comments and my replies. They come in as TODO's but are going to be removed ultimately; * in the following commits, I added tests for the CPX code path, selecting three representative cases (there's no limit how complex the scenario can go). One particular commit 224c97c7b736e94ed6b4d8b067ec830a9f8f2403 is large but most of it is for adding a flat file that contains the chromosome names in hg38 and their lengths for building a bare bone sequence dictionary used in building test data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4330#issuecomment-372761525:371,refactor,refactors,371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4330#issuecomment-372761525,3,['refactor'],"['refactoring', 'refactors']"
Modifiability,"Hi @jean-philippe-martin ,. A `Feature` in our codebase has a specific meaning that is different from ""interval"": it is a record that 1. has a location on the genome plus (typically) some metadata information about that location and 2. is in one of the formats supported by our file-parsing framework tribble and is the product of a tribble codec. A VCF record is an example of a `Feature`. . The common interface between `Feature` and `SimpleInterval` is called `Locatable`. I recommend (for now) that you simply alter your uprooted version of BQSR to take a `List<? extends Locatable>` instead of a `List<? extends Feature>` in `apply()`. This should require no code changes beyond changing method parameter types, and it will allow you to feed BQSR `SimpleIntervals` for the known sites for now, and `Features` like VCF records later on when we're ready for that. Please do return `ArtificialTestFeature` to the `FeatureDataSourceUnitTest` from which it came -- this is a very incomplete class meant only for testing purposes and not for external use.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247:568,extend,extends,568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247,2,['extend'],['extends']
Modifiability,Hi @potter-s ; Our docker image is already built with root account only however PATH is set to be usable by all users so if you wish to keep user priviledges after execution you may add ` -u $UID:$GID` parameter to docker command line therefore the container will run using your user permissions. . This has a catch of course. Temporary folders must be set where your user has RWX permissions therefore we want users to pay attention to that. There is a writing that we posted a while ago which you may refer to for setting up your temporary files for GATK workflows. . [How to setup temporary folder for GATK local executtion](https://gatk.broadinstitute.org/hc/en-us/articles/18965297287067-How-to-setup-and-use-temporary-folder-for-GATK-local-execution). For some of the tools such as gCNV or CNN you may need to setup additional environment variables to locate python compilation directory to a place where you have read and write permissions. . I hope this helps.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2145780965:845,variab,variables,845,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2145780965,1,['variab'],['variables']
Modifiability,"Hi DarioS. FastaAlternateReferenceMaker is a really simple tool. It actually just looks at the alternate alleles at each site and uses the first non-symbolic one to make the fasta. It doesn't even look at the genotypes. So it should work fine with a multisample vcf but it will give you a mush of samples together as a single fasta. I could be extended to be smarter but it's not a high priority for us right now. . We should improve the documentation, I had to go look in the code to see what it was doing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7557#issuecomment-969237729:344,extend,extended,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7557#issuecomment-969237729,1,['extend'],['extended']
Modifiability,"Hi,. Thanks for the response. Running with -u isn’t ideal as we can’t control; how the user runs this (unless they do this on their own hardware or say a; cloud instance). However, I managed to convert the docker image into a singularity one and; that runs ‘out of the box’ in user space. Simon. On 3 Jun 2024, at 18:43, Gökalp Çelik ***@***.***> wrote:. Hi @potter-s <https://github.com/potter-s>; Our docker image is already built with root account only however PATH is; set to be usable by all users so if you wish to keep user priviledges after; execution you may add -u $UID:$GID parameter to docker command line; therefore the container will run using your user permissions. This has a catch of course. Temporary folders must be set where your user; has RWX permissions therefore we want users to pay attention to that. There; is a writing that we posted a while ago which you may refer to for setting; up your temporary files for GATK workflows. How to setup temporary folder for GATK local executtion; <https://gatk.broadinstitute.org/hc/en-us/articles/18965297287067-How-to-setup-and-use-temporary-folder-for-GATK-local-execution>. For some of the tools such as gCNV or CNN you may need to setup additional; environment variables to locate python compilation directory to a place; where you have read and write permissions. I hope this helps. —; Reply to this email directly, view it on GitHub; <https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2145780965>,; or unsubscribe; <https://github.com/notifications/unsubscribe-auth/ABU3SAWISO2HSCUNHK3SGIDZFSTK5AVCNFSM6AAAAABIWRNXGKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNBVG44DAOJWGU>; .; You are receiving this because you were mentioned.Message ID:; ***@***.***>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2155884154:1229,variab,variables,1229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2155884154,1,['variab'],['variables']
Modifiability,"I _believe_ your issue is that you are assigning 600GB to execution of cromwell, but the error is with the call to **VariantRecalibrator** in one of the tasks not having enough memory. A few tasks call **VariantRecalibrator**, do you know which task failed? Can you post the java call from the STDERR file? For me, it was task **SNPsVariantRecalibrator** which was assigned only 3.5GB of memory by default. In [joint-discovery-gatk4.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.wdl), the memory assigned for each task can be set via ""machine_mem_gb"", but it looks like the current [input.json](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.hg38.wgs.inputs.json) does not have that variable, but instead ""mem_size"" for each task. . A simple solution would be to replace ${java_mem} with a static value in calls to **VariantRecalibrator** (lines 564 & 684). For example, replace:. `${gatk_path} --java-options ""-Xmx${java_mem}g -Xms${java_mem}g""`. with. `${gatk_path} --java-options ""-Xmx100g -Xms100g""`. I'm not certain this will help, but I think it's a step in the right direction.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6165#issuecomment-571396381:785,variab,variable,785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6165#issuecomment-571396381,1,['variab'],['variable']
Modifiability,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:238,flexible,flexible,238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208,4,"['extend', 'flexible', 'plugin']","['extending', 'flexible', 'plugin']"
Modifiability,"I havent published to github yet, pending getting these core changes in; however, the purpose is pretty simple: allow VariantEval to inherit from MultiVariantWalker, but not require it to include the required argument -V. this seemed comparable to VariantWalkerBase (no arguments), and VariantWalker (specifies -V). GATK3's VariantEval uses the --eval argument and I generally tried to keep everything in this port in sync with GATK3, within reason. If there is another way to subclasses to negate some @argument defined by a superclass this would work too. If you want to see more I'll push to github.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-379803776:133,inherit,inherit,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-379803776,1,['inherit'],['inherit']
Modifiability,"I think that it is necessary to have a way for downstream projects to override some of the top-level arguments in the base CLP class. For example, the config file is for documentation purposes, but I don't want to expose users to that argument because I will set the defaults programmatically. Another example is the GCS retries, which might not be useful for a software that is not planning to support GCS even if it is already implemented (or does not want to expose). As a downstream developer, for me it is important to being able to configure arguments and expose/hide them to my final users; with the current implementation, my main issue is to have an argument that are irrelevant for the toolkit user and that I get questions about why and how to use them (the most clear example, the config file). If the main problem is to change an interface, a default value for new methods can be added to keep the same behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183:151,config,config,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183,3,['config'],"['config', 'configure']"
Modifiability,"I was thinking that if we relied on PyPI for distribution, it would only be for released builds, not a release for every repo merge commit. But, I'm increasingly inclined to think that in the short term we should just include the python archive/zip file right in the gatk distribution zip, and modify the env .yml to install from that. Then every configuration (docker image, git clone user, and end user) could use exactly the same method to establish the environment. That seems like the simplest solution for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3964#issuecomment-352279343:347,config,configuration,347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3964#issuecomment-352279343,1,['config'],['configuration']
Modifiability,"I would like to specify what passing a `ReadFilter` to some of my tools means, so maybe passing an `ArgumentCollection` will be simpler than this one, I agree. Although #2085 may solve the issue regarding the `ReadTransformer`/`ReadFilter` ordering, I would like to have in the plugin a way to specify different parameters (maybe some of then hidden before expose to users or advanced in the case of disabling). I will open a new PR for that change, but I will really appreciate if I can get something like that in this and other plugins (if implemented).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983:278,plugin,plugin,278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"I'm adding some issues and PRs for make the plugin usable in other cases too, @cmnbroad. Maybe you prefer that solution instead of make it extensible. Just let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275376544:44,plugin,plugin,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275376544,1,['plugin'],['plugin']
Modifiability,"I'm going to close this issue because it's not a bug. Several things in the code of Mutect2 and FilterMutectCalls adapt as they traverse the genome and it's possible that some learned parameter shifts minutely. For example, the assembly graph pruning algorithm uses knowledge of previously assembled regions to better distinguish between errors and somatic variation. It's also possible that somewhere we forgot to give something a fixed random seed. In full honesty, I _wish_ that I knew exactly what causes the 3142 to become 3143, and I regret that I don't have time for it. Nonetheless, in principle it is not cause for alarm.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8152#issuecomment-1983783338:114,adapt,adapt,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8152#issuecomment-1983783338,1,['adapt'],['adapt']
Modifiability,I'm not totally clear from your response but I think you've resolved the problem? . If you're encountering a bug merging bai files could you open an issue describing that with your stack trace and any relevant information about the configuration you're running?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6233#issuecomment-547956623:232,config,configuration,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6233#issuecomment-547956623,1,['config'],['configuration']
Modifiability,"I've added a new end-to-end test for SelectVariants that writes to GCS. Sadly, the IntegrationTestSpec class uses Files throughout, so it wasn't possible to do this simply without first completely refactoring IntegrationTestSpec (which should probably be its own pull request). . Doing this refactoring would have the advantage that changing existing end-to-end tests from local to GCS would be trivial. For now instead I went with an ad-hoc approach. It works, and the test passes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-455686612:197,refactor,refactoring,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-455686612,2,['refactor'],['refactoring']
Modifiability,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:235,variab,variable,235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495,6,"['config', 'variab']","['config', 'variable']"
Modifiability,"Interesting! Thanks for generating these. I am already convinced by #4519 we should at least switch over to a ‘CollectReadCounts’ strategy for initial evaluations. A few comments:. -I’m guessing that the equal insert size and uniform sampling is enhancing many of these artifacts to a level that we probably don’t see in the real world. Can we take a look at some real-world examples?. -Same goes for the fact that homs will be unlikely. -Not sure about the dropouts. Might be worth running without SNPs as a confounding factor. -How flexible is SVGen? Might be worth putting together a more realistic simulated data set. Any chance @MartonKN might be able to use it to cook up some realistic tumor data?. -I don’t recall having a `CollectBaseCallCoverage` type tool in beta—which tool are you thinking of? On a related note, it seems there is some demand to port `DepthOfCoverage` from GATK3. However, I’d prefer that we roll a CNV-specific version of the tool even if it does get ported. In any case, I think along with findings from the other issue, we should issue a quick PR for `CollectReadCounts` and go ahead to change the `CollectCounts` WDL task to call it—it’s for this very reason that the task is named generically! @sooheelee note that we may have to update the tutorials, etc. at some point, but perhaps the right time will be until all evaluations are more complete. Speaking of which, this PR should not delay getting the first round of automated evaluations up and running. Again, the whole point of those is to have a reproducible baseline metric against which we can easily experiment with and adopt these sorts of changes. Although these sorts of theoretical/simulated/thought experiments are clearly useful to us, unfortunately, they may not be as compelling to some of our users as demonstrable improvement seems on real data!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375122976:534,flexible,flexible,534,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375122976,1,['flexible'],['flexible']
Modifiability,"It's not clear to me that we want these tools in Gatk4. We deliberately didn't port them because we felt they were unnecessary going forward. . I understand that there are some legitimate use cases that require them: ex low coverage naive variant calling from high ploidy pools which haplotype caller would do poorly on. (Also, do we know that haplotype caller doesn't do well on those sorts of things? Maybe we should consider modifications there if it doesn't?) I'm not sure that supporting that use case is worth the added complexity of maintaining and supporting these tools. Especially since we don't provide a pileup based variant caller as part of gatk4... . @vdauwera Can you comment? . @sooheelee I'm not sure I agree with you that supporting this for mutect 1 is useful. ; A) We don't want to support the use of mutect 1 anymore and would like to encourage people to switch to mutect 2 which I think we now believe is a better variant caller for both snps and indels. ; B) Mutect 1 users are already using gatk3, so they have access to these tools already. Mutect 1 also requires co-cleaning which I believe is a different but related tool to indel realignment. . For the variant review issue, we have thoughts on implementing a much better solution for variant review by creating an assembly plugin for igv.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988:1303,plugin,plugin,1303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988,1,['plugin'],['plugin']
Modifiability,Just adding a note here that `FeatureCache` should eventually be refactored to use the simplified Interval class (when it exists) to track cache boundaries and compute overlap.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630:65,refactor,refactored,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630,1,['refactor'],['refactored']
Modifiability,"Let's hear what others say, but I think I would strongly prefer to simply take over VariantEval in another repo if this was something you'd consider. I'd likely do much of what you propose anyway (certainly WRT testing); however, perhaps not the microscope we went through with the core GATK changes earlier. On plugins: I like what seems to be shaping up w/ Barclay. I carried over the Stratifier and Evaluator as plugins because it seems like it would make sense to allow tools to provide extensions (VariantEval, our tool, does). If I took this PR a step further, I would have migrated many arguments currently top-level on VariantEval into the plugins themselves (a good feature in Barclay). As an aside: I dont think VariantAnnotator is migrated yet, but we have many GATK3 plugins related to annotation, and hope that tool retains Annotator plugins when it get migrated. My impressions of barclay are probably a little out of date. I agree the main argument parsing framework is pretty robust. Specifically on plugins, it seems a little less so, or at least there are not many tools I visibly see exercising that part of the code. For example, there really should be a default implmentation or base class between Barclay's plugins and ReadFilter plugins. I'm guessing if more tools in GATK4 were using plugins this would have happened. I created something like this for VariantEval, and without a ton of work that could probably get generalized; however, doing so would throw a lot higher bar on me and as noted above I'm trying to take on less, not more at the moment. If we do take over VariantEval, I'm certainly happy to try to contribute code and experiences to improve the core, through more targeted PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407202501:312,plugin,plugins,312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407202501,9,['plugin'],['plugins']
Modifiability,"Looks great!. One quick note: I don't get the idea behind `Poisson` -- shouldn't we simply use negative binomials w/ modeled `mu_sj` and `alpha_sj`, evaluated at observed counts (`tt.arange(min_count, max_count + 1)`), and weighted with the number bins for each count (`_hist_sjm`)? i.e. if one observes an empirical distribution `P_obs(x)` rather than `x` draws, then the appropriate max likelihood objective function is `\sum_x P_obs(x) log P_model(x | \theta)`. Perhaps this is exactly what you've done and I don't get it. Another quick note: what I had in mind was _either_ modeling `mu_sj` at quantized ploidy states, _or_ let the ploidy state be unrestricted w/ a penalty via. a Bernoulli process (possibly w/ different per-contig penalties to account for e.g. higher rate of X/Y loss). We have enough samples in the cohort to select the quantized model (and those samples pin down the per-contig biases `b_j`). The samples that do not conform to quantized ploidy states can then choose whatever (variable) ploidy state they wish by paying a (hefty) price. We would also need to mask contigs that have variable ploidy calls from gCNV.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376286536:1003,variab,variable,1003,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376286536,2,['variab'],['variable']
Modifiability,"Lots of refactoring was done for the Segmenter classes in #6499. At least for segmentation, all use cases (CR-only, AF-only, CR+AF, single-sample, multi-sample) now go through `MultisampleMultidimensionalKernelSegmenter`. `AlleleFractionKernelSegmenter` and `CopyRatioKernelSegmenter` classes still exist, but both simply call the `MultisampleMultidimensionalKernelSegmenter` class; this was done so preexisting tests for those two classes could be reused. I'm fine with calling this done. We can always open a new issue in the unlikely event we refactor the modelling code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5625#issuecomment-900609908:8,refactor,refactoring,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5625#issuecomment-900609908,2,['refactor'],"['refactor', 'refactoring']"
Modifiability,"Mappings = new AssemblyContigWithFineTunedAlignments(contig, tigWithInsMappings.insertionMappings);; > +; > + this.basicInfo = new BasicInfo(contig);; > +; > + annotate(refSequenceDictionary);; > + }; > +; > + private static List<AlignmentInterval> deOverlapAlignments(final List<AlignmentInterval> originalAlignments,; > + final SAMSequenceDictionary refSequenceDictionary) {; > + final List<AlignmentInterval> result = new ArrayList<>(originalAlignments.size());; > + final Iterator<AlignmentInterval> iterator = originalAlignments.iterator();; > + AlignmentInterval one = iterator.next();; > + while (iterator.hasNext()) {; > + final AlignmentInterval two = iterator.next();; > + // TODO: 11/5/17 an edge case is possible where the best configuration contains two alignments,; > + // one of which contains a large gap, and since the gap split happens after the configuration scoring,; > I agree it is backwards. But...; > ; > The reason was that the (naive) alignment configuration scoring module rightnow uses MQ and AS (aligner score) for picking the ""best"" configuration (i.e. sub-list of the alignments given by aligner), which would be technically wrong if we were to split the gap and to simply grab the originating alignment's values.; > ; > This is especially true for AS, whose recomputing takes more time, and code, and forces us to know how AS are computed in the aligner so that there's no bias in computing the scores of naive alignments vs gap-split alignments (may not matter in practice, but still takes more code to compute).; > ; > Lots of the code in the discovery stage was devoted actually to alignment related acrobatics and edge cases so that the breakpoints we could resolve are as accurate as possible.; > I've kept in mind your wisdom that different aligners may be experimented with, but it seems unlikely in the near future (their own quirkiness, lack of API for JNI, etc); it seems more and more likely to me that eventually it's inevitable to have a custom alignment m",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-350618009:1740,config,configuration,1740,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-350618009,2,['config'],['configuration']
Modifiability,"Maybe I misunderstand the underlying model, but if some Pedigree annotations only need to know which samples are founders (ExcessHet ?) , and some need to know the full relationships (PossibleDeNovo), then I'm suggesting we change the class hierarchy to reflect that:. PedigreeAnnotation; |--TrioAnnotation; |----PossibleDeNovo; |--ExcessHet (assuming ExcessHet only needs founders...); ... Then the plugin could deterministically validate whether the user has provided sufficient args for the set of requested annotations; and if so, propagate them accordingly. A TrioAnnotation could only be populated (from the command line at least) from a file, whereas the others could be populated from either a file or just a set of IDs. I think it would simplify the annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463372550:400,plugin,plugin,400,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463372550,1,['plugin'],['plugin']
Modifiability,"Not sure if this is outside the scope of a simple port, but I think it would be great if the fitting of a `GaussianMixtureModel` was made a little bit more generic and extracted. Right now the method `maximizeGaussian` takes in `List<VariantDatum>`, but it should be trivial to refactor it to take in a `double[]` or `List<Double>`. Fitting a GMM could be more generally useful for other methods, after all. It might even be useful to extract the k-means clustering code used to initialize the model, if this is retained in the port. Perhaps also outside the scope, but it'd also be nice if variable names were changed to match the notation in Bishop Ch. 10 (on which the variational-Bayes algorithm is based). I think this would make the code much easier to parse from a mathematical standpoint.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236003146:278,refactor,refactor,278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236003146,2,"['refactor', 'variab']","['refactor', 'variable']"
Modifiability,"Now, it seems like calling `contaminationDownsampling` right after `retainEvidence` could cause problems if both methods remove reads. However, one might correctly point out that although the cache invalidation I mentioned is not handled systematically, the method `removeEvidenceByIndex` _does_ have some code to update the evidence by sample and the evidence index map. It's possible that this code is totally fine and that this lead is a dead end. However, the code looks like it could be simpler and it's tough to parse. For example, try to track the `to` variable, which determines the determination of the outer `for` loop:. ```; for (int etrIndex = 1, to = nextIndexToRemove, from = to + 1; to < newEvidenceCount; etrIndex++, from++) {; if (etrIndex < evidencesToRemove.length) {; nextIndexToRemove = evidencesToRemove[etrIndex];; evidenceIndex.remove(evidences.get(nextIndexToRemove));; } else {; nextIndexToRemove = oldEvidenceCount;; }; for (; from < nextIndexToRemove; from++) {; final EVIDENCE evidence = evidences.get(from);; evidences.set(to, evidence);; evidenceIndex.put(evidence, to++);; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625030697:560,variab,variable,560,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625030697,1,['variab'],['variable']
Modifiability,"On the first question, we definitely appreciate how much work this will take. Often, porting the code is the easy part; developing new tests and test data can be a huge effort. I can try to find out if it would be possible for you to take the tool over - I know this kind of thing has come up before for other tools, but I'd have to ask around to find that out. @vdauwera do you have input on this ?. As for the plugins, currently in your branch `VariantStratification` and `VariantEvaluator` are modeled as Barclay command line plugin descriptors, and I was questioning whether thats necessary. Being a plugin is not necessarily required - `ReadFilter` and `Annotation` are both plugins, but they didn't have to be, and it takes quite a bit of work (again, mostly test development) to get a plugin right. Also, I'd consider the Barclay plugin framework to be pretty developed at this point, so I'd be curious to learn more about what issues you see. And yes, definitely don't check any of the large GATK3 test files into the repo, even temporarily. Take a look at [General guidelines for GATK4 developers](https://github.com/broadinstitute/gatk#dev_guidelines) if you haven't already. As you pointed out, new GATK4 tests that use smaller files would have to be developed. We'd want those to be included, and passing tests on the CI server, before we started reviewing the branch, so we know we're reviewing code that works and is covered by tests as much as possible. The second commit in my list above would have only your GATK3 java test files, etc (but not the big files, which you appear to have locally). The third commit would have your ported tool code, as well as the new test code, with the new tests enabled, as well as the smaller input files and expected results files. At the end we'd remove commit #2.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407185633:412,plugin,plugins,412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407185633,6,['plugin'],"['plugin', 'plugins']"
Modifiability,"Results are in:. Using the branch for PR #4971 with the value `ALIGNMENT_LOW_READ_UNIQUENESS_THRESHOLD` set to 10 and 19, while keeping the gap split children together (that is, method ; `private static GoodAndBadMappings splitGaps(final GoodAndBadMappings configuration, final boolean keepSplitChildrenTogether)` is called with `false` for its second parameter). Here are the comparisons:; ```; simple variants unique TP unique FP; size-10 filter: 10756 24 101; size-19 filter: 10755 1 0; ```. So I think your suggestion is a better trade off!. What I'll do is make that parameter an (advanced) CLI argument in PR #4971 , and experiment more to settle on a good default value.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403890890:257,config,configuration,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403890890,1,['config'],['configuration']
Modifiability,"Running a particular bam sort takes ~20minutes with hdd and 16 minutes with ssd. So it's definitely being used somehow. It looks like spark.local.dir is over ridden by the environment variable LOCAL_DIRS, and I don't see that set, but it's possible it's being set but not recorded correctly in the UI or something like that. Someone will need to poke at a bit more to be more clear about what's happening.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283481370:184,variab,variable,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283481370,1,['variab'],['variable']
Modifiability,"ScoreVariantAnnotations:. Scores variant calls in a VCF file based on site-level annotations using a previously trained model. TODOs:. - [x] Integration tests. Exact-match tests for (non-exhaustive) configurations given by the Cartesian product of the following options:; * Java Bayesian Gaussian Mixture Model (BGMM) backend vs. python sklearn IsolationForest backend; (BGMM tests to be added once PR for the backend goes in.); * non-allele-specific vs. allele-specific; * SNP-only vs. SNP+INDEL (for both of these options, we use trained models that contain both SNP and INDEL scorers as input) ; - [x] Tool-level docs. Minor TODOs:. - [x] Parameter-level docs.; - [x] Parameter/mode validation.; - [x] Double check or add behavior for handling previously filtered input, clearing present filters, etc. Future work:. - [ ] The `score_samples` method of the sklearn IsolationForest is single-threaded. See (possibly stalled) PR at https://github.com/scikit-learn/scikit-learn/pull/14001 and some workarounds using e.g. `multiprocessing` ibid.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948563:199,config,configurations,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948563,1,['config'],['configurations']
Modifiability,Seems like something like https://github.com/broadinstitute/gatk/issues/4794 could be avoided if we rewrote this. It seems like a pretty simple rewrite too...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4535#issuecomment-391044481:144,rewrite,rewrite,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4535#issuecomment-391044481,1,['rewrite'],['rewrite']
Modifiability,"Some comments/questions for the review:; - I'll add a separate ticket to rewrite the integration tests, all of which pass and most of which are disabled since they require access to large files on the broad file system. In the meantime I need to add a couple of small tests to get the coverage back up, and would like to get the CR process started.; - I ported a bunch of support files but need feedback on whether they're in the right location.; - Somewhere I saw something that said GATK no longer supports .ped files ? If not, what should the replacement be in the tests require pedigree input?; - Is it a requirement to support Ploidy > 2 ? The current GATK tool, and thus the HB tool, do not; - I did not port the WalkerTestSpec.disableShadowVCF? Is that needed in Hellbender ?; - Are there other headers I should be applying to the output variant file ?. Command Line Arguments:; - I didn't port the GATK command line argument ""-no_cmd_line_in_header"". Should I ? And if not, should the command line args automatically be propagated to the output vcf file ? I didn't see GATK do this anywhere.; - There was one test that used --variant:dbsnp on the command line but I couldn't find the code that processed that in GATK, not sure what the means on the command line.; - I replaced ""-U LENIENT_VCF_PROCESSING"" with ""--lenient"" (testFileWithoutInfoLineInHeaderWithOverride needs this to pass).; - I replaced ""-L"" with --interval since HB seems to use -L for ""lane"" ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027:73,rewrite,rewrite,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027,1,['rewrite'],['rewrite']
Modifiability,"Some offline discussions have led us to the conclusion that this is best handled by tools upstream. Adapters should not be simply soft-clipped, so it shouldn't be the responsibility of M2 or HC to include logic to remove adapters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6346#issuecomment-575334816:221,adapt,adapters,221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6346#issuecomment-575334816,1,['adapt'],['adapters']
Modifiability,"Sorry, but this bug still isn't fixed as of v4.2.6.1. Reproduce as follows:. ```; --read-filter MateDistantReadFilter; --mate-too-distant-length 1500; ```. Instead of a run-time exception (as in v4.2.5.0), HaplotypeCaller simply produces no variant calls at all. Expected behavior would be to exclude paired-end mappings whose TLEN exceeds the parameterized value. Perhaps there is an implementation bug, unrelated to the original problem, that contains faulty logic for doing this. Thanks...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1102943692:344,parameteriz,parameterized,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1102943692,1,['parameteriz'],['parameterized']
Modifiability,"Thank you @vruano for your diligent review. I've implemented logger classes to encapsulate the metrics classes. Unfortunately the metrics classes must remain public in order to write output using `MetricsUtils.saveMetrics()`, but at least the tools aren't using them directly. There are two logging class groups - one for Filter and one Score. For Filter, there is an interface `PSFilterLogger` that is implemented by a file-logging class `PSFilterFileLogger` and a dummy class `PSFilterEmptyLogger` that does nothing. There are analogous classes for Score, but there is no Empty logger because it's not actually necessary. This adds a lot of new classes (maybe you can think of a better way) but usage has been greatly simplified. As we discussed in person, I don't think there is a faster way to count the reads in Spark. If you wanted to count the reads as they pass through, you would have to use some kind of atomic type that would be slow. Also it may be impossible to account for cases when tasks fail and restart. @lbergelson @droazen In this PR, I wanted to use htsjdk's MetricsFile and MetricBase classes for writing metrics to a file. I notice that these classes are mostly used for picard-related things. Is this the preferred way to do things? They do force you to expose public variables and also use an upper-case naming convention. On the other hand, they are somewhat convenient.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-334308160:1292,variab,variables,1292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-334308160,1,['variab'],['variables']
Modifiability,"Thanks @lbergelson! I agree that it might be good to break into more layers—could be worth talking to SV team and seeing what lessons they learned in putting together their hierarchy of images. Also, note that I pushed the install of miniconda into the base, but I did not push down the setup of the GATK conda environment itself (which takes the bulk of the time during the main-image build, as it requires lots of downloading). I think I commented elsewhere that a good strategy might be to set up the conda environment with the non-GATK python dependencies in the base, and then update the environment via a pip install of the GATK python packages in the main image. This would let us make python code changes without having to rebuild the base, but might require a bit of scripting to create a final yml for non-Docker users. I also agree that it would be nice to cut down the Travis time, might be worth taking a look at other strategies to do that—could save everyone a lot of time!. Will try to add the test you suggested sometime tomorrow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-621487662:69,layers,layers,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-621487662,1,['layers'],['layers']
Modifiability,"Thanks for the feedback, @cmnbroad.  @droazen, should I open a ticket for implement the plugin and close this issue? What's about the checking of the quals?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245969652:88,plugin,plugin,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245969652,1,['plugin'],['plugin']
Modifiability,"That's why I am not using in ReadTools and other developmental toolkit the base class from GATK, due to the polluted command line with unused arguments. I think that for give flexibility, some of that arguments should be configurable by extending classes. For example, some tools that does not require reads at all should be able to turn off the read arguments. That will be very useful, although I am not sure how to do it in a proper way without adding more and more interfaces for argument collections. In context case of this PR, I think that adding it does not have any real effect on the GATK codebase, and a lot is gained by downstream projects. For example, if the wrapper script adds another argument that should be parsed in `Main` and documented, the GATK team just add it to its class. If a toolkit has a similar wrapper script, it can also add its own only-doc argument by simply overriding the method...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371822090:221,config,configurable,221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371822090,2,"['config', 'extend']","['configurable', 'extending']"
Modifiability,"The behavior of the GATK3 CombineVariants was very inconsistent and the arguments weren't entirely clear. I also suspect that some operations weren't possible with the arguments given. Rather than port that old broken version, I would advocate for an overhaul or rewrite. @bhanugandham it's going to be a big project to collect requirements and expected behavior for this tool. For example, what should the MQ be for the combined VCF for two different input VCFs with different MQ values? Much of the confusion stemmed from the old ability to merge VCFs containing the same sample. In the case where we take one genotype for each sample name (e.g. the old ` -genotypeMergeOptions PRIORITIZE`) then I believe the old behavior was wrong in some cases, taking the filter status from an input VCF at random. We also need to clarify `FilteredRecordMergeType` options, e.g. https://github.com/broadinstitute/gsa-unstable/issues/935",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/17#issuecomment-430229167:263,rewrite,rewrite,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/17#issuecomment-430229167,1,['rewrite'],['rewrite']
Modifiability,"The task here is to simply move the code while changing as little as possible, and then validate that. Once that's done, we can do whatever refactoring/changes we want to VQSR, or replace it completely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236014525:140,refactor,refactoring,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236014525,1,['refactor'],['refactoring']
Modifiability,"There were a few issues with this case. First, the data source was not constructed 100% correctly. The config file is correct. . The index file is for the tar.gz version of the source data and not for the uncompressed version that they're using. The index should correspond to the source data in the file referenced by the config file itself (not a zipped or otherwise transformed version). Secondly, the source `tsv` data file has the header line for the table commented out. The Xsv codec is aware of leading hash marks as comments and will ignore any such lines. Because of this, the leading hash in the table header is ignored and the file cannot be properly parsed. The fix is simple - just remove the leading hash from the table header (the preceding line with the two hash marks is correctly interpreted as a file header because of the leading hashes acting as comments). Lastly, even if the user fixed the file they would still need to index it with`IndexFeatureFile`. At some point the code underlying this in `HTSJDK` was broken such that no Xsv files can currently be indexed. I have submitted a pull request in `HTSJDK` (https://github.com/samtools/htsjdk/pull/1429) for this and have another ready to go in GATK (#6224) that includes a test for this case so this reversion cannot happen again.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-545186183:103,config,config,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-545186183,2,['config'],['config']
Modifiability,"This seems like a consequence of the fact that we use `java.nio.file.Path`for a lot of things in gatk. This requires a custom `java.nio.file.spi.FileSystemProvider` to be available for each type of path you want to be able to resolve. Spark native uses `org.apache.hadoop.fs.Path` for a lot of things. It's seems likely that that maprfs provides a hadoop file system plugin, which many spark applications can consume, but it's unlikely that it also provides a java.nio.file.Path implementation. ; ; I don't think we'd be able to implement a provider for maprfs ourselves. We don't have any systems with maprfs and don't have the bandwidth to take it on right now. Implementing a file system provider isn't a terribly complicated project, but it's not a trivial one either. However, there's an implementation for hadoop here https://github.com/damiencarol/jsr203-hadoop which is sufficient for what gatk does. If maprfs provides a hadoop file system, it would probably not be too difficult to take that project as a template and modify it to use the maprfs implementation. . I think the only things you'd have to implement for the spark tools to work are the basic Path operations that support the simple operations like `Paths.get()`,`Files.exists()`, and `Path.resolve()`. (although that's not a complete list. . If you are interested in writing a plugin like that, you can add it to the gatk class path at runtime. We might also be open to packaging such a plugin with the gatk if there was wide demand for it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-350070555:367,plugin,plugin,367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-350070555,3,['plugin'],['plugin']
Modifiability,"We've filed a ticket with github support -- however, the branch has been cleared to merge in its current state, as it's had more than enough reviews. We can file tickets to improve/refactor once it's in master.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3945#issuecomment-351092625:181,refactor,refactor,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3945#issuecomment-351092625,1,['refactor'],['refactor']
Modifiability,"Well, that explains that, sort of. The code snippet you're providing looks like it ought to do what you say it does (i.e., the mates have to be paired, not unmapped, mapped to the same contig, and have a difference in their start positions that is at least `mateTooDistantLength`). . But there are two problems with this:. 1) This filter's behavior is unexpected wrt HaplotypeCaller. It seems to me that an inclusive filter (i.e., process only paired-end mappings whose TLEN falls within a specified range) would be more usable. That would imply a filter implementation that accepts a pair of integers, but the expected behavior would be more obvious and in line with GATK's other range-limited parameterizations (e.g., `MappingQualityReadFilter` comes immediately to mind). 2) I can't tell from where I sit, but the code snippet looks correct only if `getStart()` and `getMateStart()` return a zero-based start position of each mate relative to the start of the strand to which the mate is mapped. If the code is just computing the difference between POS for the mates, the computation is incorrect for forward + reverse-complement (Illumina-style) pairs. In addition, computing TLEN requires not only that you consider the orientation of the individual mate mappings, but also that you make an arbitrary decision about how to handle soft-clipped reads. I hate to say this, but I think this parameter needs some attention. Its potential utility with HaplotypeCaller seems evident to me (i.e., it would be good to be able to exclude outliers with unreasonable TLENs) but its implementation and frugal documentation make it unusable in practice.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1103199220:695,parameteriz,parameterizations,695,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1103199220,1,['parameteriz'],['parameterizations']
Modifiability,"While we normally don't recommend ignoring that wrapper, this seems like a good reason to do so. . The wrapper is pretty simple, most of what it's doing is some munging of the input to allow it to be more standardized in several different gatk use cases. The only thing I can think of that you would want to be sure to copy is that it sets a number of properties. . We set these spark `--conf` properties with the wrapper. I don't actually know how important some of them are anymore. If it works without them then you're probably good.; ```; ""spark.kryoserializer.buffer.max"" : ""512m"",; ""spark.driver.maxResultSize"" : ""0"",; ""spark.driver.userClassPathFirst"" : ""false"",; ""spark.io.compression.codec"" : ""lzf"",; ""spark.executor.memoryOverhead"" : ""600"",; ""spark.driver.extraJavaOptions"" : EXTRA_JAVA_OPTIONS_SPARK,; ""spark.executor.extraJavaOptions"" : EXTRA_JAVA_OPTIONS_SPARK; ```. These are htsjdk properties we want to set for spark. ; ```; EXTRA_JAVA_OPTIONS_SPARK= ""-DGATK_STACKTRACE_ON_USER_EXCEPTION=true "" \; ""-Dsamjdk.use_async_io_read_samtools=false "" \; ""-Dsamjdk.use_async_io_write_samtools=false "" \; ""-Dsamjdk.use_async_io_write_tribble=false "" \; ""-Dsamjdk.compression_level=2 ""; ```. If you can get this value into your spark environment variables it prevents and anying warning output. `SUPPRESS_GCLOUD_CREDS_WARNING=true`. Let us know how it works for you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6198#issuecomment-539073054:1251,variab,variables,1251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6198#issuecomment-539073054,1,['variab'],['variables']
Modifiability,"Yeah, I don't like these new interface methods -- they make `GATKRead` significantly worse. We should cache `isUnmapped`, etc. in the adapter to accomplish the same thing, as @lbergelson suggests. Not that hard, and we can just unconditionally invalidate the cached values (using `Boolean` fields set to null) whenever the read is mutated in any way in order to simplify the logic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282:134,adapt,adapter,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282,1,['adapt'],['adapter']
Modifiability,"Yeah, it would be useful (see https://github.com/broadinstitute/gatk/issues/2582). Not sure if/when we'll ever get around to the Barclay changes though. Another simple option that wouldn't require Barclay changes would be to implement it as just another (plugin descriptor) command line argument that could be sued alongside `--read-filter`'. So if you wanted a `ReadNameFilter` and an inverted `ReadLengthFilter`, the syntax would be:. `--read-filter ReadNameFilter --invert-read-filter ReadLengthReadFilter`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6005#issuecomment-502231306:255,plugin,plugin,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6005#issuecomment-502231306,1,['plugin'],['plugin']
Modifiability,"Yeah, the workaround was simply to add the library jar to the classpath and not try to compile them together. I created the issue to soon, Sorry. . As for the NIO library, it is for AWS S3. We are adapting this one https://github.com/Upplication/Amazon-S3-FileSystem-NIO2 to meet our needs. We didn't like the way it handles s3 endpoints because AWS EMR Spark clusters don't support s3 uri's with that particular syntax. Our version modifies it to support normal s3 uri's without endpoints, instead setting the endpoint with a configuration parameter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431:197,adapt,adapting,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431,2,"['adapt', 'config']","['adapting', 'configuration']"
Modifiability,"c 5 12:51:17 2017 -0500. Updates to handle SAM header changes from sl_wgs_acnv_headers and updates to mb_gcnv_python_kernel. commit d02d04df684a2820308a1d1c2bfda4b7d1c5f05e; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Mon Nov 13 12:52:33 2017 -0500. Added CLIs and WDL for python gCNV pipeline. commit 66ed74b68375d43514ef84658e7a6c771ed9053c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Nov 15 01:50:03 2017 -0500. Polished code, ready for review; ; gCNV computational kernel (initial release); ; renaming gammas_s to psi_s to uniformity (sample-specific unexplained variance); ; renamed determine_ploidy_and_depth.py to cohort_determine_ploidy_and_depth.py; finite-temperature forward-backward algorithm; in the ploidy model, replaced alpha_j (NB over-dispersion) with psi_j (unexplained variance) for uniformity. Also, added the possibility of sample-specific unexplained variance in the germline contig ploidy model; ; updated I/O routines and CLIs according to team discussion; ; updated I/O routines and CLIs according to team discussion; ; changed the output layout of the ploidy determination tool; refactored parts of io.py; upped the version to 0.3 as it is not backwards compatible anymore; ; case ploidy determination tool from a given ploidy model; major code cleanup and refactoring of I/O module; refactoring of common CLI script snippets; ; removed all ""targets""; some code cleanup; ; pad flat class bitmask w/ a given padding value in the hybrid q_c_expectation_mode; option to disable annealing and keep the temperature fixed; ; bugfix in finite-temperature forward-backward; further refactoring of model I/O; ; the option to take a previously trained model as starting point in cohort CLI; the option to take previous calls as a starting point in cohort CLI; ; option to save and load adamax moments; ; import/export adamax bias correction tensor; ; refactoring related to fancy opt I/O; added average ploidy column to read depth; updated docs of hybrid ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:11083,refactor,refactored,11083,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['refactor'],['refactored']
Modifiability,"d be in a separate commit that we can just delete at the end, but that can get unwieldy if the files in the commit need to change as we go along. Alternatively you could isolate them into a separate directory. They should either be disabled or made dependent on a test method (see the `@Test` annotation properties `enabled` and `dependsOn`) that is easily toggled so they can be run locally, but don't run on the CI server. Otherwise the CI server build will always fail. In general, its really helpful to have the first commit in the PR contain the completely unmodified GATK3 source files. It makes it much easier for the reviewer to see what changed for the port. I noticed that you have 2 new plugins included in this. I'm not sure if that was suggested by someone on the GATK team (I'm wondering if we want to go down that path...) but I can tell you that the existing plugins required an enormous amount of test development and review iteration. If we do decide to make them plugins, I think it would be a good idea to do so in a separate PR. Also, if we choose to make an AbstractPlugin base class, we may want that to live in the Barclay repo. As @magicdgs points out, master already has your previous commits, so you should start by rebasing on that. Ideally, the branch would have the following commits before we start the first review cycle:. 1. A single commit containing the unmodified GATK3 source (unmodified with the exception that if a file is renamed for GATK4, its helpful to rename the GATK3 version in this commit so it's easy to compare in the next commit). This commit doesn't have to compile or run - its just to make the review process easier for us, and will be deleted at some point. I can help with how to get this into your branch if you like.; 2. Your modified GATK3 tests in a single commit. This will also be removed before merge.; 3. A single commit with all of your ""minimal"" changes for the port, including the real, new tests. This should compile, and tests should",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352:1888,plugin,plugins,1888,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352,1,['plugin'],['plugins']
Modifiability,"dGraph only allows a single annotation/track. I'm not sure if the track definition line is intended to hold any metadata other than display parameters, either? https://genome.ucsc.edu/goldenPath/help/bedgraph.html. As for the unmarked column header line, the reason I decided this would be useful in the CNV TSV formats is that it's very easy to throw the table into a pandas or R dataframe for quick analysis, where you can then use the column names to manipulate the table. Typically, pandas/R TSV loading methods let you specify the `@` comment character to strip the SAM header (although we recently ran into some trouble with this in https://github.com/broadinstitute/gatk/pull/581). Note that we *require* a single unmarked column header, which is easy enough to skip (in the case you don't want to use it) if you know it's there. On the other hand, one could argue that if we store the type of each column in the metadata, then any analysis code should technically use that to parse the table (rather than letting pandas/R automatically infer the type of each column). So a marked column header line would make quick analyses a bit more difficult (as users would need to write parsing code), but could encourage more careful downstream code practices. @SHuang-Broad Just to be clear, the way I originally used ""annotation"" refers to any quantity that could be represented by a single type in a column (not in the sense of variant annotation). If string types are allowed, this is indeed pretty flexible! All I care about extracting is the common functionality related to the fact that we have locatable columns. I think the concerns you raise about e.g. SV representation in VCF are a separate matter, but happy to discuss further. I think once we decide what the header needs to be able to represent and what it should look like, this problem is mostly solved. There may be some things to decide about e.g. representation of doubles, NaNs, etc. but I don't think we need to be too rigid here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329:1644,flexible,flexible,1644,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329,1,['flexible'],['flexible']
Modifiability,"er and -consolidate; 2. With --bypass-feature-reader; 3. With --consolidate without --bypass-feature-reader (This ended up on a node with 384gb.) The other ran on 256GB nodes. . Test 2 ran the fastest with the lowest memory requirements (Wall clock 76 hours); Test 1 ran slower and required more memory 40-50% of 256GB (Wall Clock 94 hours); Test 3 ran initially faster with less memory than test 1 but by batch 65 it was using 75% of 384 GB. This job has not finished and appears stuck on importing batch 65. So the consolidate option appears to have a memory leak or using just requiring too much memory. The -consolidate option was the culprit. So rerunning chr1-3 with just the --bypass-feature-reader option (test2) ran fine without lots of memory being used. Below is the time output from chr1. The output shows the Maximum resident set size (kbytes): **2630440**. Using GATK jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar defined in environment variable GATK_LOCAL_JAR; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx200g -Xms16g -jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace; Command being timed: ""gatk --java-options -Xmx200g -Xms16g GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace""; User time (seconds): 270716.45; System time (seconds): 1723.34; Percent of CPU this job go",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687:1474,variab,variable,1474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687,1,['variab'],['variable']
Modifiability,lizer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/wgs.hg38/pipelines/hc/cram.test/GRCh38_full_analysis_set_plus_decoy_hla.fa.gz --spark-master yarn; 2019-01-09 13:35:04 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:05 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 13:35:09.640 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:35:09.799 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:11.507 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.508 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:35:11.508 INFO CountReadsSpark - For support and documentat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:1703,config,configuration,1703,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['config'],['configuration']
Modifiability,"ly braces can wait for a separate pass (we will want to do those in this PR though). If you're not sure what to include or not just ask. I like the idea of keeping the GATK3 tests working as we go along. We should make a clear distinction between the old and new tests though. Ideally the GATK3 tests would be in a separate commit that we can just delete at the end, but that can get unwieldy if the files in the commit need to change as we go along. Alternatively you could isolate them into a separate directory. They should either be disabled or made dependent on a test method (see the `@Test` annotation properties `enabled` and `dependsOn`) that is easily toggled so they can be run locally, but don't run on the CI server. Otherwise the CI server build will always fail. In general, its really helpful to have the first commit in the PR contain the completely unmodified GATK3 source files. It makes it much easier for the reviewer to see what changed for the port. I noticed that you have 2 new plugins included in this. I'm not sure if that was suggested by someone on the GATK team (I'm wondering if we want to go down that path...) but I can tell you that the existing plugins required an enormous amount of test development and review iteration. If we do decide to make them plugins, I think it would be a good idea to do so in a separate PR. Also, if we choose to make an AbstractPlugin base class, we may want that to live in the Barclay repo. As @magicdgs points out, master already has your previous commits, so you should start by rebasing on that. Ideally, the branch would have the following commits before we start the first review cycle:. 1. A single commit containing the unmodified GATK3 source (unmodified with the exception that if a file is renamed for GATK4, its helpful to rename the GATK3 version in this commit so it's easy to compare in the next commit). This commit doesn't have to compile or run - its just to make the review process easier for us, and will be delete",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352:1604,plugin,plugins,1604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352,1,['plugin'],['plugins']
Modifiability,"ms that we will want to run the filter with more stringent parameters, as higher base error rates are causing homs to leak past the filter, which in turn affects the fit of the allele-fraction model (which only attempts to model hets) by biasing normal segments towards unbalanced, and 2) we now want to run ModelSegments separately on the normal to allow for the filtering of germline events. So we want to be more stringent with low-coverage normals without affecting our high-coverage tumors. For example, here's some hg38 NovaSeq FFPE WGS data from a ~40x normal:. ![download](https://user-images.githubusercontent.com/11076296/43977946-9bd0a1bc-9cb3-11e8-9d7f-016a99c1c173.png). Compare to an hg19 TCGA WGS ~40x normal:. ![download 1](https://user-images.githubusercontent.com/11076296/43978051-f8820770-9cb3-11e8-8e16-13b51792614f.png). The hom-ref tail in the first plot is much fatter and clearly leaks into the het cloud. Also curious is that the het cloud is far less binomial (or even beta-binomial---note also the absence of the tail extending to the origin). I am still not sure why the incoming data looks different. There are several confounding factors: NovaSeq vs. HiSeq, hg38 vs. hg19, AF > 2% gnomAD sites vs. AF > 10% 1000G sites, FFPE vs. frozen, etc. I have not seen enough examples/combinations to be able to say which are the most important factors. Changing the genotyping/filtering strategy can get around this change in the data without a corresponding change in the allele-fraction model for now, but getting the data to look as good as possible upstream would be even better. Another thought: would be nice if the strategy was easily compatible with an eventual implementation of multi-sample segmentation, which would require that the same sites are used in both the tumor and the normal. We would want to strike a balance between maximizing the number of sites and including questionable sites from the normal. Will add more details later. @davidbenjamin @LeeTL1220 @eit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3915#issuecomment-412189218:1597,extend,extending,1597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3915#issuecomment-412189218,1,['extend'],['extending']
Modifiability,"n Tools; #; # Only update this environment if there is a *VERY* good reason to do so!; # If the build is broken but could be fixed by doing something else, then do that thing instead.; # Ensuring the correct environment for canonical (or otherwise reasonable) usage of our standard Docker takes precedence over edge cases.; # If you break the environment, you are responsible for fixing it and also owe the last developer who left this in a reasonable state a beverage of their choice.; # (This may be yourself, and you'll appreciate that beverage while you tinker with dependencies!); #; # When changing dependencies or versions in this file, check to see if the ""supportedPythonPackages"" DataProvider; # used by the testGATKPythonEnvironmentPackagePresent test in PythonEnvironmentIntegrationTest needs to be updated; # to reflect the changes.; #; name: gatk; channels:; # if channels other than conda-forge are added and the channel order is changed (note that conda channel_priority is currently set to flexible),; # verify that key dependencies are installed from the correct channel and compiled against MKL; - conda-forge; - defaults; dependencies:. # core python dependencies; - conda-forge::python=3.6.10 # do not update; - pip=20.0.2 # specifying channel may cause a warning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely nec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:1270,flexible,flexible,1270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,1,['flexible'],['flexible']
Modifiability,"nd other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge::keras=2.2.4 # updated from pip-installed 2.2.0, which caused various conflicts/clobbers of conda-installed packages; # conda-installed 2.2.4 appears to be the most recent version with a consistent API and without conflicts/clobbers; # if you wish to update, note that versions of conda-forge::keras after 2.2.5; # undesirably set the environment variable KERAS_BACKEND = theano by default; - defaults::intel-openmp=2019.4; - conda-forge::scikit-learn=0.22.2; - conda-forge::matplotlib=3.2.1; - conda-forge::pandas=1.0.3. # core R dependencies; these should only be used for plotting and do not take precedence over core python dependencies!; - r-base=3.6.2; - r-data.table=1.12.8; - r-dplyr=0.8.5; - r-getopt=1.20.3; - r-ggplot2=3.3.0; - r-gplots=3.0.3; - r-gsalib=2.1; - r-optparse=1.6.4. # other python dependencies; these should be removed after functionality is moved into Java code; - biopython=1.76; - pyvcf=0.6.8; - bioconda::pysam=0.15.3 # using older conda-installed versions may result in libcrypto / openssl bugs. # pip installs should be avoided, as pip may not respect the dependencies found by the conda solver; - pip:; - gatkPythonPackageArchive.zip; ```. It seems to successfully create the environment. I'd still recommend updating the information on your README.md and the file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:3013,variab,variable,3013,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,1,['variab'],['variable']
Modifiability,"nd the channel order is changed (note that conda channel_priority is currently set to flexible),; # verify that key dependencies are installed from the correct channel and compiled against MKL; - conda-forge; - defaults; dependencies:. # core python dependencies; - conda-forge::python=3.6.10 # do not update; - pip=20.0.2 # specifying channel may cause a warning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge::keras=2.2.4 # updated from pip-installed 2.2.0, which caused various conflicts/clobbers of conda-installed packages; # conda-installed 2.2.4 appears to be the most recent version with a consistent API and without conflicts/clobbers; # if you wish to update, note that versions of conda-forge::keras after 2.2.5; # undesirably set the environment variable KERAS_BACKEND = theano by default; - defaults::intel-openmp=2019.4; - conda-forge::scikit-learn=0.22.2; - conda-forge::matplotlib=3.2.1; - conda-forge::pandas=1.0.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:2181,config,config,2181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,1,['config'],['config']
Modifiability,"ntervals of two jobs, and whether separating the jobs would impact calls. In this example, GenotypeGVCFs would run over 1:1050-1150. For example, if we had a multi-NT variant that spanned 1148-1052, we'd want that called correctly no matter what intervals were used for the jobs. I tried using running GenomicsDBImport with -L over a small region, or I ran SelectVariants on the gVCF first (which behaves a little differently), and then used that subset gVCF as input to GenomicsDBImport, where GenomicsDBImport is given the entire contig as the interval. The resulting workspaces will be slightly different, with the latter containing information over a wider region (GenomicsDBIport truncates start/end of the input records to just the target interval). . So if either of these workspaces is passed to GenotypeGVCFs, using --only-output-calls-starting-in-intervals and -L 1:1050-1150:. I think any upstream padding doesnt matter. If you have a multi-nucleotide polymorphism that starts upstream of 1050 but spans 1050, this job wouldnt be responsible for calling that. The prior job, which has an interval set upstream of this one should call it. I think GenomicsDbImport's behavior is fine here. If you have a multi-NT variant that starts within 1050-1150, but extends outside (i.e. deletion or insertion starting at 1148), this could be a problem. The GenomicsDB workspace created with the interval 1:1050-1150 lacks the information to score that, right? The workspace created using the more permissive SelectVariants->GenomicsDBImport contains that downstream information and presumably would make the same call as if GenotypeGVCFs was given the intact chromosome as input, right?. However, it seems that if I simply create the workspace with a reasonably padded interval (adding 1kb should be more than enough for Illumina, right?), and then run GenotypeGVCFs with the original, unpassed interval, then the resulting workspace should contain all available information and GenotypeGVCFs should be",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221558244:1317,polymorphi,polymorphism,1317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221558244,1,['polymorphi'],['polymorphism']
Modifiability,"o go ahead and add this option, I would probably keep the directory structure of the GermlineCNVCaller output the same (i.e., with folders named ""SAMPLE_#""), and just check the sample_name.txt files at the PostprocessGermlineCNVCalls step. I don't think this should require GermlineCNVCaller code changes, right?. 4) We may require additional code at the WDL level if we want to both switch over to primarily using sample names but also get rid of bundling (i.e., by passing only the calls for each sample when needed). Locally, you can always just search all output for directories containing the appropriate sample_name.txt. But on the cloud, you'd want to make sure that the postprocessing step for a particular sample gets only its corresponding directories, which would have to happen at the WDL level; the check against sample_name.txt at the tool level would just be a formality. I can foresee headaches with globbing and funky sample names. I'm not sure I understand your point about extending PostprocessGermlineCNVCalls to run on all samples. The point of that tool is to take results from all genomic shards for a single sample and stitch them together, right? Even if we extend this to run on a batch of multiple samples (which would just be moving the loop over samples at the WDL level to some lower level, i.e., Java or python), we still need to see all shards for those samples. Perhaps I'm misunderstanding---can you clarify?. @mwalker174 can we once and for all clearly document the issue with the transpose? Perhaps by pointing to specific WGS runs that have issues with call caching? I think being able to pinpoint the exact issue will help us identify the right solution---whether that be choosing an appropriate bundling scheme, taking advantage of #5781 to reduce the number of shards, batching during the postprocessing step, removing unnecessary outputs, etc. Recall that we'd like to be able to use the same WDL locally (when you have easy access to all GermlineCNVCaller re",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765:2180,extend,extending,2180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765,1,['extend'],['extending']
Modifiability,"o the right processing methods in a single pass over the RDD. Reply by @SHuang-Broad. > I tried to fix it in this PR, but that seems to be a big task,; and probably is impossible to achieve in a single pass,; because currently each class of contig ends up producing a different type of object; (3 general classes: simple -> SimpleNovelAdjacency, complex -> ComplexVariantCanonicalRepresentation, and unknown -> SAM records of the contigs); and a groupBy() operation is necessary in the middle using these objects as keys; due to the fact that different contigs may produce the same variant; So what I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:1548,inherit,inherit,1548,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030,1,['inherit'],['inherit']
Modifiability,"ode:. -`BaseRecalibratorSpark` is the standalone BQSR tool, and calls into the `BaseRecalibratorSparkFn` (which is also called from `ReadsPipelineSpark`). -`ApplyBQSRSpark` is the standalone ApplyBQSR tool, and calls into the `ApplyBQSRSparkFn` (also called from `ReadsPipelineSpark`). -Integration tests for the above are in `BaseRecalibratorSparkIntegrationTest` and `ApplyBQSRSparkIntegrationTest`. -Almost all other changes in the branch are related to the BQSR engine refactoring, which I summarize below:; - We pulled out the guts of the walker `BaseRecalibrator` tool, combined it with all of the code from the former `RecalibrationEngine` class (now deleted) to make a new `BaseRecalibrationEngine` class under `utils/recalibration`.; - We stripped out all copies of the code in `BaseRecalibrationEngine` from the walker, dataflow, and spark versions of BQSR, and modified them to call into `BaseRecalibrationEngine`.; - We moved all auxiliary classes needed by the `BaseRecalibrationEngine` (eg., the covariates, etc.) into `utils/recalibration`.; - We refactored the argument collections. Now there is a single shared `RecalibrationArgumentCollection` that contains **only** the parameters for the `BaseRecalibrationEngine` itself, and this argument collection is exposed by all 3 versions of the tool. Input/output arguments have been removed from this argument collection and put into the individual implementations of BQSR, since they vary between the walker, dataflow, and spark versions of the tool. This eliminates awkward problems such as having both a `knownSites` argument AND a `BQSRKnownVariants` exposed at the same time, with only 1 of them usable for a given version of a tool. The dataflow-only `BaseRecalibrationArgumentCollection` has been deleted completely as no longer needed.; - We tweaked the names of some tool arguments to enforce consistency between the 3 versions of the tool as well as the rest of hellbender (eg., output arg for BQSR is now a more standard `-O`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073:1113,refactor,refactored,1113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073,1,['refactor'],['refactored']
Modifiability,"olders named ""SAMPLE_#""), and just check the sample_name.txt files at the PostprocessGermlineCNVCalls step. I don't think this should require GermlineCNVCaller code changes, right?. 4) We may require additional code at the WDL level if we want to both switch over to primarily using sample names but also get rid of bundling (i.e., by passing only the calls for each sample when needed). Locally, you can always just search all output for directories containing the appropriate sample_name.txt. But on the cloud, you'd want to make sure that the postprocessing step for a particular sample gets only its corresponding directories, which would have to happen at the WDL level; the check against sample_name.txt at the tool level would just be a formality. I can foresee headaches with globbing and funky sample names. I'm not sure I understand your point about extending PostprocessGermlineCNVCalls to run on all samples. The point of that tool is to take results from all genomic shards for a single sample and stitch them together, right? Even if we extend this to run on a batch of multiple samples (which would just be moving the loop over samples at the WDL level to some lower level, i.e., Java or python), we still need to see all shards for those samples. Perhaps I'm misunderstanding---can you clarify?. @mwalker174 can we once and for all clearly document the issue with the transpose? Perhaps by pointing to specific WGS runs that have issues with call caching? I think being able to pinpoint the exact issue will help us identify the right solution---whether that be choosing an appropriate bundling scheme, taking advantage of #5781 to reduce the number of shards, batching during the postprocessing step, removing unnecessary outputs, etc. Recall that we'd like to be able to use the same WDL locally (when you have easy access to all GermlineCNVCaller results from all genomic shards) and in the cloud, with minimal duplication of output from bundling when running locally, if possible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765:2371,extend,extend,2371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765,1,['extend'],['extend']
Modifiability,"ong. We should make a clear distinction between the old and new tests though. Ideally the GATK3 tests would be in a separate commit that we can just delete at the end, but that can get unwieldy if the files in the commit need to change as we go along. Alternatively you could isolate them into a separate directory. They should either be disabled or made dependent on a test method (see the `@Test` annotation properties `enabled` and `dependsOn`) that is easily toggled so they can be run locally, but don't run on the CI server. Otherwise the CI server build will always fail. In general, its really helpful to have the first commit in the PR contain the completely unmodified GATK3 source files. It makes it much easier for the reviewer to see what changed for the port. I noticed that you have 2 new plugins included in this. I'm not sure if that was suggested by someone on the GATK team (I'm wondering if we want to go down that path...) but I can tell you that the existing plugins required an enormous amount of test development and review iteration. If we do decide to make them plugins, I think it would be a good idea to do so in a separate PR. Also, if we choose to make an AbstractPlugin base class, we may want that to live in the Barclay repo. As @magicdgs points out, master already has your previous commits, so you should start by rebasing on that. Ideally, the branch would have the following commits before we start the first review cycle:. 1. A single commit containing the unmodified GATK3 source (unmodified with the exception that if a file is renamed for GATK4, its helpful to rename the GATK3 version in this commit so it's easy to compare in the next commit). This commit doesn't have to compile or run - its just to make the review process easier for us, and will be deleted at some point. I can help with how to get this into your branch if you like.; 2. Your modified GATK3 tests in a single commit. This will also be removed before merge.; 3. A single commit with all o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352:1781,plugin,plugins,1781,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352,1,['plugin'],['plugins']
Modifiability,"ould like to address are similar to yours, with some inclussions. * Regarding NIO support, I would go to remove completely `File` support. If API users need to use the `File` abstraction, they should convert to a `java.nio.Path` using the `toPath` method.; * In addition, I would like that HTTP/S and FTP is handled also with NIO. For HTTP/S, I am working in a simple `FileSystemProvider` that should be good enough for using in combination with HTSJDK ([jsr203-http](https://github.com/magicDGS/jsr203-http)), and I can speed up the development there for needs in HTSJDK; for FTP, maybe [ftp-fs](https://github.com/robtimus/ftp-fs) can be used or a simple implementation can be derived from the HTTP/S implementation (without credentials). This will remove the special handling of HTTP/S and FTP paths in HTSJDK in favor of a consistent and pluggable manner.; * Interfaces for the data types are great, and maybe it will be good to have codec interfaces for both encoding and decoding. For example, I am missing encoders in tribble (an attempt in https://github.com/samtools/htsjdk/pull/822 for writing support).; * For VCF, I would like to have a less diploid-centric interface and design, or at least a way of configure the catching of genotype-related attributes. Currently there are methods for homozygotes/heterozygotes that aren't really useful for triploids or even VCFs without variation (for example, in Pool-Seq data).; * Modular design for artifacts: thus, a project with only SAM/BAM requirements will require only `htsjdk-sam`, and if they also want CRAM support, `htsjdk-cram`. See https://github.com/samtools/htsjdk/issues/896 for more info about it.; * Common license for all HTSJDK, or at least for each module. This will be good for taking into account legal concerns when including the library, because now there is a mixture depending on the files that are used. This is what is coming to my mind now. Maybe I added something else in https://github.com/samtools/htsjdk/issues/520",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-363390940:1281,config,configure,1281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-363390940,1,['config'],['configure']
Modifiability,"ouldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, especially given the simplicity of writing a new walker type in GATK4. For all of these reasons we don't want `AssemblyRegionWalker` to inherit from a more abstract traversal type -- it's just going to be its own standalone thing, so that it can evolve freely without affecting anyone else. For `SlidingWindowWalker`, which we still want to merge, I recommend making the traversal do **exactly** what you want for your use case, as clearly and simply as possible, without worrying about serving as a base class for other traversals. Ping me once you're happy with it, and I'll re-review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:2073,inherit,inheritance,2073,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,4,"['evolve', 'inherit']","['evolve', 'inherit', 'inheritance', 'inheriting']"
Modifiability,"provements that were discovered while reviewing the variants ; (https://github.com/SHuang-Broad/GATK-SV-callset-regressionTest/tree/master/Evaluation/Analysis/masterVSfeature/notes.xlsx); The implemented fixes are:; * for removing the hard-coded/explicit mentioning of ""chr"" in non-canonical versions, it is now fixed in 5eff782e4d582d516004fba2cee7535d984b1540; * for contigs whose alignments paint ambiguous picture, i.e. multiple alignment configurations offer equally good explanation:; 	1. if only one configuration has all alignment with MQ above a specified threshold, it is favored; this is implemented in ecc31f5fbec4e524b401fc9474a3a1b7ab08c561; 	2. if one configuration has alignment to non-canonical chromosome that explains the contig better than would-be-event-inducing mappings to canonical chromosomes, the canonical mappings are saved but the better non-canonical mappings are saved as SA tag as in SAM spec, and the VCF record produced is annotated accordingly; this is implemented in 65cdb523a2f9fa2026334713fed45381d76ffc82; * fixed a bug where sometimes an assembly contig as several alignments, only one of which has non-mediocre MQ but at the sametime this alignment contains a large gap, such contigs were previously incorrectly filtered away, they are now salvaged by commit b6b2f197b112981e00efd9d415f010c024d31b36. So, for the FN variants (FN in the sense that they are captured in the stable version of our interpretation tool but now goes missing in the experimental interpretation tool); that were curated in the above-mentioned review, only the following ones are not salvaged, with plans or comments attached. ```; asm012854:tig00000	missing	classified as ""incomplete""; fixable by finishing the last TODO in AssemblyContigAlignmentSignatureClassifier (same problem as face by group represented by asm002398:tig00001); asm014580:tig00018	missing	classified as ""incomplete""; fixable by finishing the last TODO in AssemblyContigAlignmentSignatureClassifier (same problem ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-370923522:863,config,configuration,863,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-370923522,1,['config'],['configuration']
Modifiability,"sh Babadi <mehrtash@broadinstitute.org>; Date: Wed Nov 15 01:50:03 2017 -0500. Polished code, ready for review; ; gCNV computational kernel (initial release); ; renaming gammas_s to psi_s to uniformity (sample-specific unexplained variance); ; renamed determine_ploidy_and_depth.py to cohort_determine_ploidy_and_depth.py; finite-temperature forward-backward algorithm; in the ploidy model, replaced alpha_j (NB over-dispersion) with psi_j (unexplained variance) for uniformity. Also, added the possibility of sample-specific unexplained variance in the germline contig ploidy model; ; updated I/O routines and CLIs according to team discussion; ; updated I/O routines and CLIs according to team discussion; ; changed the output layout of the ploidy determination tool; refactored parts of io.py; upped the version to 0.3 as it is not backwards compatible anymore; ; case ploidy determination tool from a given ploidy model; major code cleanup and refactoring of I/O module; refactoring of common CLI script snippets; ; removed all ""targets""; some code cleanup; ; pad flat class bitmask w/ a given padding value in the hybrid q_c_expectation_mode; option to disable annealing and keep the temperature fixed; ; bugfix in finite-temperature forward-backward; further refactoring of model I/O; ; the option to take a previously trained model as starting point in cohort CLI; the option to take previous calls as a starting point in cohort CLI; ; option to save and load adamax moments; ; import/export adamax bias correction tensor; ; refactoring related to fancy opt I/O; added average ploidy column to read depth; updated docs of hybrid inference; ; modeling intervals can span multiple contigs now; ploidy can change; across contigs with no issue; ; save/load adamax state to .npy instead of .tsv for speed; ; part 1 of doc updates; ; part 2 of doc updates; ; part 3 of doc updates; ; part 4 of doc updates; ; bumped version to 0.5; readme; ; update readme; ; last minute stylistic doc updates.; ````",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:11261,refactor,refactoring,11261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,4,['refactor'],['refactoring']
Modifiability,ter yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 2019-01-07 11:33:18 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:19 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 11:33:24.377 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:33:24.549 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:33:26.271 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.272 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 11:33:26.272 INFO CountReadsSpark - For support and documentat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:1963,config,configuration,1963,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['config'],['configuration']
Modifiability,thanks for the review @kcibul. I made some changes accordingly. re: PrepareCallset file of sample names. That would be nice! It would make this workflow simpler and it also simplifies the access requirements for PrepareCallset. re: Dockstore. We actually ruled this out because Terra says that the definition of a method configuration can change automatically if its updated in dockstore. Which can be useful but it adds a security risk since a compromised Dockstore can change the definition of the production AoU extraction WDL which runs with highly elevated permissions. We already have a script that creates method configurations from github so I can probably add something a little hacky to resolve relative imports to the raw github file that it refers to.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7242#issuecomment-846494686:321,config,configuration,321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7242#issuecomment-846494686,2,['config'],"['configuration', 'configurations']"
Modifiability,tools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; 17:39:18.382 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 17:39:18.825 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 17:39:18.857 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/username/libgkl_compression3681606702485397808.so; 17:39:19.218 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 17:39:19.218 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 17:39:19.218 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:39:19.219 INFO PathSeqPipelineSpark - Executing as username@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 17:39:19.220 INFO PathSeqPipelineSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11; 17:39:19.220 INFO PathSeqPipelineSpark - ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:2348,variab,variables,2348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"tor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 2019-01-07 11:33:18 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:19 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 11:33:24.377 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:33:24.549 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:33:26.271 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.272 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 11:33:26.272 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:33:26.272 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 11:33:26.273 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 11:33:26.273 INFO CountReadsSpark - Start Date/Time: January 7, 2019 11:33:24 AM EST; 11:33:26.273 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.273 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:2355,variab,variables,2355,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"ts header (either proactively or on demand), but transparently to ; application code that is running against the SAMRecord API.; This would allow SAM headers to be transmitted out-of-band in a way that ; depends on the execution environment. Depending on the environment, ; this might be done by proactive broadcast, or you could think of the ; header tag as a promise to retrieve the header if/when it is needed. ; The size and complexity of the header tag might also depend on the ; execution environment. If the execution environment only supports a ; small finite number of headers, the header tag could be a small integer, ; or in a different execution environment it could be; a unique hash of the header or something like that. Memory footprint in ; the receiver is minimized because many SAMRecords can all share the same ; header object.; This requires more work to support in each execution environment, but it ; seems like it could be efficient and allows application code written to ; operate on SAMRecords to be portable; across different execution environments without having to contend with ; the possible presence of headerless SAMRecords. -Bob. On 9/17/15 4:28 PM, droazen wrote:. > @davidadamsphd https://github.com/davidadamsphd, @lbergelson ; > https://github.com/lbergelson, and myself met for an hour or two ; > just now to discuss this issue, and after reviewing all the options I ; > think we were convinced by the following argument:; > ; > The |SAMRecord| class currently allows its header to be set to null, ; > so if there are cases where the class won't function properly or can ; > enter into an inconsistent state when a header is not present these ; > should be treated as bugs and patched, and we should add unit tests to ; > htsjdk to prove that headerless |SAMRecords| function properly. Then ; > in hellbender we can freely use headerless |SAMRecords| everywhere, ; > only restoring the header to the record when writing out the final bam ; > (since our bam writers",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:2594,portab,portable,2594,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518,1,['portab'],['portable']
Modifiability,"ub.com> wrote:; > ; > @SHuang-Broad commented on this pull request.; > ; > In src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/prototype/CpxVariantDetector.java:; > ; > > + this.tigWithInsMappings = new AssemblyContigWithFineTunedAlignments(contig, tigWithInsMappings.insertionMappings);; > +; > + this.basicInfo = new BasicInfo(contig);; > +; > + annotate(refSequenceDictionary);; > + }; > +; > + private static List<AlignmentInterval> deOverlapAlignments(final List<AlignmentInterval> originalAlignments,; > + final SAMSequenceDictionary refSequenceDictionary) {; > + final List<AlignmentInterval> result = new ArrayList<>(originalAlignments.size());; > + final Iterator<AlignmentInterval> iterator = originalAlignments.iterator();; > + AlignmentInterval one = iterator.next();; > + while (iterator.hasNext()) {; > + final AlignmentInterval two = iterator.next();; > + // TODO: 11/5/17 an edge case is possible where the best configuration contains two alignments,; > + // one of which contains a large gap, and since the gap split happens after the configuration scoring,; > I agree it is backwards. But...; > ; > The reason was that the (naive) alignment configuration scoring module rightnow uses MQ and AS (aligner score) for picking the ""best"" configuration (i.e. sub-list of the alignments given by aligner), which would be technically wrong if we were to split the gap and to simply grab the originating alignment's values.; > ; > This is especially true for AS, whose recomputing takes more time, and code, and forces us to know how AS are computed in the aligner so that there's no bias in computing the scores of naive alignments vs gap-split alignments (may not matter in practice, but still takes more code to compute).; > ; > Lots of the code in the discovery stage was devoted actually to alignment related acrobatics and edge cases so that the breakpoints we could resolve are as accurate as possible.; > I've kept in mind your wisdom that different aligners may ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-350618009:1509,config,configuration,1509,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-350618009,2,['config'],['configuration']
Modifiability,"windows in my implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract methods for the three parameters (`getWindowSize()`, `getWindowStep()` and `getWindowPadding()`, and implement `ReadWindowWalker` as a extension of this interface setting `getWindowStep()` to return `getWindowSize()` and `requiresReads()` to true. I can do this once the PR #1567 is accepted and generate the two interfaces (to be sure that the integration with the HC engine is working as expected with the changes), or just implement the `SlidingWindowWalker` and you can include it in the HC PR, or update afterwards to avoid redundancy in the code. What do you thi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:1629,extend,extend,1629,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775,1,['extend'],['extend']
Performance," 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3688baab{/,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4fe2dd02{/api,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@726a8729{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1a2724d3{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://scc-hadoop.bu.edu:4041; 2019-01-07 11:33:27 INFO SparkContext:54 - Added JAR file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar at spark://scc-hadoop.bu.edu:46828/jars/gatk-package-4.0.12.0-spark.jar with timestamp 1546878807984; 2019-01-07 11:33:28 INFO GoogleHadoopFileSystemBase:607 - GHFS version: 1.6.3-hadoop2; 2019-01-07 11:33:29 WARN DomainSocketFactory:117 - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-07 11:33:30 INFO Client:54 - Requesting a new application from cluster with 21 NodeManagers; 2019-01-07 11:33:30 INFO Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-07 11:33:30 INFO Client:54 - Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-07 11:33:30 INFO Client:54 - Setting up container launch context for our AM; 2019-01-07 11:33:30 INFO Client:54 - Setting up the launch environment for our AM container; 2019-01-07 11:33:30 INFO Client:54 - Preparing resources for our AM container; 2019-01-07 11:33:30 INFO HadoopFSDelegationTokenProvider:54 - getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1883879239_1, ugi=farrell@AD.BU.EDU (auth:KERBEROS)]]; 2019-01-07 11:33:30 INFO DFSClient:1023 - Created HDFS_DELEGATION_TOKEN token 11334 for farrell on ha-hdfs",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:10619,load,loaded,10619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['load'],['loaded']
Performance," 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@16b64a03{/,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1584c019{/api,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5817f1ca{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2b395581{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://scc-hadoop.bu.edu:4041; 2019-01-09 13:35:12 INFO SparkContext:54 - Added JAR file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar at spark://scc-hadoop.bu.edu:42689/jars/gatk-package-4.0.12.0-spark.jar with timestamp 1547058912934; 2019-01-09 13:35:13 INFO GoogleHadoopFileSystemBase:607 - GHFS version: 1.6.3-hadoop2; 2019-01-09 13:35:13 WARN DomainSocketFactory:117 - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-09 13:35:14 INFO Client:54 - Requesting a new application from cluster with 21 NodeManagers; 2019-01-09 13:35:14 INFO Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-09 13:35:14 INFO Client:54 - Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-09 13:35:14 INFO Client:54 - Setting up container launch context for our AM; 2019-01-09 13:35:14 INFO Client:54 - Setting up the launch environment for our AM container; 2019-01-09 13:35:14 INFO Client:54 - Preparing resources for our AM container; 2019-01-09 13:35:14 INFO HadoopFSDelegationTokenProvider:54 - getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-682487019_1, ugi=farrell@AD.BU.EDU (auth:KERBEROS)]]; 2019-01-09 13:35:14 INFO DFSClient:1023 - Created HDFS_DELEGATION_TOKEN token 11353 for farrell on ha-hdfs:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:10359,load,loaded,10359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['load'],['loaded']
Performance," accounting for 1) sample-specific depth (which determines the means of the negative-binomial distributions), 2) multiplicative contig-specific bias (which is mild, at least for WGS), and 3) additive sample-contig-specific mosaicism or bias (note that the above genotype priors imply that mosaicism/bias on top of a baseline of CN = 2 is the only deviation allowed for the autosomes, which is somewhat restrictive but greatly aids convergence). I put together a pure PyMC3 prototype that seems to work relatively well. Here are the per-contig coverage histograms (unfiltered bins in blue, bins retained after filtering in red, and negative-binomial fit in green) and a heatmap of per-contig ploidy probabilities. Both the panel (first 20) and case (remaining) samples are shown:. ![prototype-result](https://user-images.githubusercontent.com/11076296/37938642-e9fbd804-312c-11e8-8a6c-02ea4e4fa704.png). Although the prototype model is clearly a good fit to the filtered data, some care in choosing the optimizer and its learning parameters is required to achieve convergence to the correct solution. This is because the problem is inherently multimodal and thus there are many local minima. I found that using AdaMax with a naive strategy of warm restarts (to help kick us out of local minima) worked decently; we can achieve convergence in <10 minutes for 60 samples x 24 contigs x 250 count bins:. ![elbo](https://user-images.githubusercontent.com/11076296/37938658-fc176f12-312c-11e8-89e2-40c68e0f9953.png). I expect that @mbabadi's annealing implementation in the gcnvkernel package will handle the local minima much better. The course of action needed to implement this model should be as follows:. 1) Alter Java code to emit per-contig histograms. Change python code to consume histograms, perform filtering, and fit using the above model (or some variation).; 2) Choose learning parameters appropriate with annealing and check that results are still good.; 3) Update gCNV model to consume the d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271:2495,optimiz,optimizer,2495,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271,1,['optimiz'],['optimizer']
Performance," acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:37 INFO yarn.Client: Submitting application application_1507856833944_0003 to ResourceManager; 17/10/13 18:11:37 INFO impl.YarnClientImpl: Submitted application application_1507856833944_0003; 17/10/13 18:11:37 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1507856833944_0003 and attemptId None; 17/10/13 18:11:38 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:38 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: root.users.hdfs; 	 start time: 1507889497661; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088/proxy/application_1507856833944_0003/; 	 user: hdfs; 17/10/13 18:11:39 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:40 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:41 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 17/10/13 18:11:41 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> mg, PROXY_URI_BASES -> http://mg:8088/proxy/application_1507856833944_0003), /proxy/application_1507856833944_0003; 17/10/13 18:11:41 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 17/10/13 18:11:41 INFO yarn.C",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:11718,queue,queue,11718,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['queue'],['queue']
Performance," normals with 5M bins each, CombineReadCounts took ~1 min, CreatePanelOfNormals (with no QC) took ~4.5 minutes (although ~1 minute of this is writing target weights, which I haven't added to the new version yet) and generated a 2.7GB PoN, and NormalizeSomaticReadCounts took ~8 minutes (~7.5 minutes of which was spent composing/writing results, thanks to overhead from ReadCountCollection). In comparison, the new CreateReadCountPanelOfNormals took ~1 minute (which includes combining read-count files, which takes ~30s of I/O) and generated a 520MB PoN, and DenoiseReadCounts took ~30s (~10s of which was composing/writing results, as we are still forced to generate two ReadCountCollections). Resulting PTN and TN copy ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actuall",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1105,perform,performed,1105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['perform'],['performed']
Performance," remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoise",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:2269,perform,performed,2269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['perform'],['performed']
Performance," the last developer who left this in a reasonable state a beverage of their choice.; # (This may be yourself, and you'll appreciate that beverage while you tinker with dependencies!); #; # When changing dependencies or versions in this file, check to see if the ""supportedPythonPackages"" DataProvider; # used by the testGATKPythonEnvironmentPackagePresent test in PythonEnvironmentIntegrationTest needs to be updated; # to reflect the changes.; #; name: gatk; channels:; # if channels other than conda-forge are added and the channel order is changed (note that conda channel_priority is currently set to flexible),; # verify that key dependencies are installed from the correct channel and compiled against MKL; - conda-forge; - defaults; dependencies:. # core python dependencies; - conda-forge::python=3.6.10 # do not update; - pip=20.0.2 # specifying channel may cause a warning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:1632,perform,performance,1632,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,1,['perform'],['performance']
Performance," this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one to set the maximum number of values allowed per chunk, so that heap usage can be controlled, but the downside is that this translates into a corresponding limit on the number of columns (i.e., intervals). On the other hand, you could theoretically crank this number up to Integer.MAX_VALUE, as long as you set -Xmx high enough... In practice, it's very unlikely that we'll need to go to bins smaller than a read length.); - [ ] <s>Check that CreatePanelOfNormals works correctly on Spark cluster.</s> Implement Randomized SVD, which should give better performance on large matrices. See https://arxiv.org/pdf/1007.5510.pdf and https://research.fb.com/fast-randomized-svd/. For now, I'll require that the coverage matrix can fit in RAM, but more sophisticated versions of the algorithm could be implemented in the future.; - [ ] Update methods doc. Note that some of the CNV section is out of date and incorrect. In particular, we have been taking in PCOV as input to CreatePanelOfNormals for some time now, but the doc states that we take integer read counts. This already yields different results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:3383,perform,performance,3383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['perform'],['performance']
Performance,", et al. Nature. 2022 Jul;607(7920):732-740. doi: 10.1038/s41586-022-04965-x. Epub 2022 Jul 20.PMID: 35859178. On page 69+ of this pdf, they describe the problem and how they cleverly worked around it. ; ; https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-022-04965-x/MediaObjects/41586_2022_4965_MOESM1_ESM.pdf. _It should be noted that running GATK out of the box will cause every job to read the entire; gVCF index file (.tbi) for each of the 150,119 samples. The average size of the index files is ; 4.15MB, so each job would have to read 4.15*150,126 = 623GB of data on top of the actual; gVCF slice data. For 60,000 jobs, this would amount to 623GB*60,000 = 37PB or 25.2GB/sec; of additional read overhead if the jobs are run on 20,000 cores in 17 days. This read; overhead will definitely prevent 20,000 cores from being used simultaneously. However,; this problem was avoided by pre-processing the .tbi files and modifying the software; reading the gVCF files from the central storage in a similar fashion as we did for GraphTyper; and the CRAM index files (.crai)._. This explains why chr1 requires more memory than chr22 despite running on the same number of samples. The larger chr1 tbi index is the source of the memory problem. The Decode solution is too limit the reading of the tbi index to the part that indexes the scattered region. There is a long pause at the beginning of the running GenotypeGVCFs which I never understood. GATK must be the reading of all the sample's gvcfs tbi into memory during that pause. So the reblocking of the gvcfs above reduced the memory foot print by decreasing the tbi size. Decode reduced it by chopping up the index so for each scattered region, GATK could only read a small subset of the index needed for that region. The combination of reblocking and chopping up the tbi would help with the memory requirements even more. However, it is clear that GATK's present reading of the full tbi is not scalable given the memory requirements.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579:2214,scalab,scalable,2214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579,1,['scalab'],['scalable']
Performance,"/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/resources/sv_classifier_test_data.json; src/test/resources/sv_features_test_data.json",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:3093,perform,performance,3093,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,1,['perform'],['performance']
Performance,0.3 35000 101621.1; 14:56:26.027 INFO ProgressMeter - 1:5856032 0.5 55000 105867.6; ...; 19:37:05.295 INFO ProgressMeter - GL000209.1:48811 281.2 30739000 109323.8; 19:37:15.543 INFO ProgressMeter - GL000224.1:65537 281.3 30758000 109324.9; 19:37:25.847 INFO ProgressMeter - GL000248.1:21736 281.5 30768000 109293.8; 19:37:25.906 INFO FilterMutectCalls - Finished pass 0 through the variants; 19:50:04.590 INFO FilterMutectCalls - Shutting down engine; [9 January 2020 7:50:04 PM] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 294.19 minutes.; Runtime.totalMemory()=14966849536; java.lang.IllegalArgumentException: Values in probability array sum to a negative number NaN; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:731); 	at org.broadinstitute.hellbender.utils.MathUtils.normalizeSumToOne(MathUtils.java:731); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.performEMIteration(SomaticClusteringModel.java:336); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:306); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:158); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:159); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runC,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341:4476,perform,performEMIteration,4476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341,1,['perform'],['performEMIteration']
Performance,"0/13 18:11:40 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:41 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 17/10/13 18:11:41 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> mg, PROXY_URI_BASES -> http://mg:8088/proxy/application_1507856833944_0003), /proxy/application_1507856833944_0003; 17/10/13 18:11:41 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 17/10/13 18:11:41 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:42 INFO yarn.Client: Application report for application_1507856833944_0003 (state: RUNNING); 17/10/13 18:11:42 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: 10.131.101.145; 	 ApplicationMaster RPC port: 0; 	 queue: root.users.hdfs; 	 start time: 1507889497661; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088/proxy/application_1507856833944_0003/; 	 user: hdfs; 17/10/13 18:11:42 INFO cluster.YarnClientSchedulerBackend: Application application_1507856833944_0003 has started running.; 17/10/13 18:11:42 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44818.; 17/10/13 18:11:42 INFO netty.NettyBlockTransferService: Server created on 10.131.101.159:44818; 17/10/13 18:11:42 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 17/10/13 18:11:42 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.131.101.159, 44818, None); 17/10/13 18:11:42 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.131.101.159:44818 with 366.3 MB RAM, BlockManagerId(driver, 10.131.101.159, 44818, None); 17/10/13 18:11:42 INF",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:13009,queue,queue,13009,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['queue'],['queue']
Performance,"01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM models, as well. @LeeTL1220 @mbabadi @davidbenjamin I'd be interested to hear your thoughts, if you have any.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:1522,perform,performs,1522,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666,1,['perform'],['performs']
Performance,"1-07 11:33:52 INFO YarnClientSchedulerBackend:54 - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> scc-hsn1.scc.bu.edu, PROXY_URI_BASES -> https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0153), /proxy/application_1542127286896_0153; 2019-01-07 11:33:52 INFO JettyUtils:54 - Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 2019-01-07 11:33:52 INFO Client:54 - Application report for application_1542127286896_0153 (state: ACCEPTED); 2019-01-07 11:33:53 INFO YarnSchedulerBackend$YarnSchedulerEndpoint:54 - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 2019-01-07 11:33:53 INFO Client:54 - Application report for application_1542127286896_0153 (state: RUNNING); 2019-01-07 11:33:53 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: 192.168.18.193; ApplicationMaster RPC port: 0; queue: default; start time: 1546878818531; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0153/; user: farrell; 2019-01-07 11:33:53 INFO YarnClientSchedulerBackend:54 - Application application_1542127286896_0153 has started running.; 2019-01-07 11:33:53 INFO Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45270.; 2019-01-07 11:33:53 INFO NettyBlockTransferService:54 - Server created on scc-hadoop.bu.edu:45270; 2019-01-07 11:33:53 INFO BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 2019-01-07 11:33:53 INFO BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, scc-hadoop.bu.edu, 45270, None); 2019-01-07 11:33:53 INFO BlockManagerMasterEndpoint:54 - Registering block manager scc-hadoop.bu.edu:45270 with 408.6 MB RAM, BlockManagerId(driver, scc-hadoop.bu.edu, 45270, None); 2019-01-07 11:33:53 INFO BlockManagerMaster:54 - Register",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:15896,queue,queue,15896,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['queue'],['queue']
Performance,"1-09 13:35:32 INFO Client:54 - Application report for application_1542127286896_0166 (state: ACCEPTED); 2019-01-09 13:35:33 INFO YarnClientSchedulerBackend:54 - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> scc-hsn1.scc.bu.edu, PROXY_URI_BASES -> https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0166), /proxy/application_1542127286896_0166; 2019-01-09 13:35:33 INFO JettyUtils:54 - Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 2019-01-09 13:35:33 INFO YarnSchedulerBackend$YarnSchedulerEndpoint:54 - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 2019-01-09 13:35:33 INFO Client:54 - Application report for application_1542127286896_0166 (state: RUNNING); 2019-01-09 13:35:33 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: 192.168.18.195; ApplicationMaster RPC port: 0; queue: default; start time: 1547058922320; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0166/; user: farrell; 2019-01-09 13:35:33 INFO YarnClientSchedulerBackend:54 - Application application_1542127286896_0166 has started running.; 2019-01-09 13:35:33 INFO Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43627.; 2019-01-09 13:35:33 INFO NettyBlockTransferService:54 - Server created on scc-hadoop.bu.edu:43627; 2019-01-09 13:35:33 INFO BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 2019-01-09 13:35:33 INFO BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, scc-hadoop.bu.edu, 43627, None); 2019-01-09 13:35:33 INFO BlockManagerMasterEndpoint:54 - Registering block manager scc-hadoop.bu.edu:43627 with 372.6 MB RAM, BlockManagerId(driver, scc-hadoop.bu.edu, 43627, None); 2019-01-09 13:35:33 INFO BlockManagerMaster:54 - Register",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:15195,queue,queue,15195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['queue'],['queue']
Performance,"4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like it’s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so it’s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the task data - or use a Spark accumulator), and then seeing if there's some way to make it more uniform.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:2454,optimiz,optimizing,2454,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884,2,"['cache', 'optimiz']","['cached', 'optimizing']"
Performance,; callable 1.0; ```; results in FilterMutectCalls exception; ```; java.lang.IllegalArgumentException: logValues must be non-infinite and non-NAN; at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeFromLogToLinearSpace(NaturalLogUtils.java:27); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.posteriorProbabilityOfError(Mutect2FilteringEngine.java:93); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:140); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSomaticVariant(SomaticClusteringModel.java:146); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.performEMIteration(SomaticClusteringModel.java:345); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:330); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:153); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:165); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLin,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7276#issuecomment-1293969047:1769,perform,performEMIteration,1769,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7276#issuecomment-1293969047,1,['perform'],['performEMIteration']
Performance,"> At the very least we should add a unit test that generates the evidenceIndexBySampleIndex cache, then calls marginalize() (both types) and asserts that we have emptied the cache. I would do the same for appendEvidence() and addMissingAlleles(). It's simpler than this because allele operations such as `marginalize()` and `addMissingAlleles` don't modify the evidence list. While they require care with the likelihoods arrays they don't require anything at all from the evidence-to-index caches. As I mentioned above, I left the cache updating in `appendEvidence` as it was because it was so simple. I will try to write the test for removing evidence tomorrow. Tempting to try tonight, but I'm trying to accept the reality that working until 2 am is a bad idea.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6593#issuecomment-633180869:92,cache,cache,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6593#issuecomment-633180869,4,['cache'],"['cache', 'caches']"
Performance,"> Just curious, why no last modified checks? Was it to keep the code simpler?. Mostly because I couldn't readily think of a scenario where I would actually want this to call cache, but I could easily imagine call caching leading to undesired clobbering of previously generated results. We can certainly revisit this decision if it turns out we're using the script in ways where we really would want call caching.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8002#issuecomment-1227739990:174,cache,cache,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8002#issuecomment-1227739990,1,['cache'],['cache']
Performance,"@EdwardDixon I did not know that! In that case master does already require AVX. If it only impacts this tool and we provide sufficient warning and instructions, I think the single intel-optimized conda environment will be so much easier to test and maintain. Users who don't have AVX can simply install an older tensorflow in their environment, but GATK doesn't need to worry about it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429142837:186,optimiz,optimized,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429142837,1,['optimiz'],['optimized']
Performance,"@akiezun @droazen @lbergelson Found the issue with FTZ being cleared. The FTZ setting only applies to the thread where FTZ is set. When running tests in gradle/testng, each test is run in a new thread. However, the pairHMM native library is only loaded for the first HaplotypeCaller test, since code in `VectorLoglessPairHMM.java` prevents the library from being loaded more than once in the same JVM. This means only the first test uses FTZ. A code change that loads the pairHMM native library for each test resolves the issue, and all `HaplotypeCallerIntegrationTest` tests pass. Another option to explore is setting FTZ in `main`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701:246,load,loaded,246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701,3,['load'],"['loaded', 'loads']"
Performance,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:79,cache,cached,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307,2,"['cache', 'perform']","['cached', 'performance']"
Performance,"@akiezun thanks. I think this is ready for review now. It would be good to merge something that works, even if there are future performance and usability improvements we can do later.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219460978:128,perform,performance,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219460978,1,['perform'],['performance']
Performance,"@bbimber @mlathara Here is a pretty good article for optimizing the GenomicsDBImport [https://gatk.broadinstitute.org/hc/en-us/articles/360056138571-GDBI-usage-and-performance-guidelines] There is some advice about handling many small contigs that may be useful. . To troubleshoot the GenomicsDBImport high memory issue my script have, I reran the script on chr1 to narrow down the source of the high memory issue. These are running on reblocked gvcfs. . 1. Without --bypass-feature-reader and -consolidate; 2. With --bypass-feature-reader; 3. With --consolidate without --bypass-feature-reader (This ended up on a node with 384gb.) The other ran on 256GB nodes. . Test 2 ran the fastest with the lowest memory requirements (Wall clock 76 hours); Test 1 ran slower and required more memory 40-50% of 256GB (Wall Clock 94 hours); Test 3 ran initially faster with less memory than test 1 but by batch 65 it was using 75% of 384 GB. This job has not finished and appears stuck on importing batch 65. So the consolidate option appears to have a memory leak or using just requiring too much memory. The -consolidate option was the culprit. So rerunning chr1-3 with just the --bypass-feature-reader option (test2) ran fine without lots of memory being used. Below is the time output from chr1. The output shows the Maximum resident set size (kbytes): **2630440**. Using GATK jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar defined in environment variable GATK_LOCAL_JAR; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx200g -Xms16g -jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reade",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687:53,optimiz,optimizing,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687,2,"['optimiz', 'perform']","['optimizing', 'performance-guidelines']"
Performance,"@davidbenjamin, et al. I have two recommendations:. 1) Though I prefer to work symbolically and through proofs, it might be nice to first expand on the validation by proof in the JavaDoc - including for the specific function's header - and anywhere else where necessary across the GATK code, just for sanity's sake, and for tying things together neatly and properly. This process of always going through the mathematical steps alerts me when I code that I have not missed anything. . 2) When dealing with multiple levels of transformations, it probably would be good to formulate a collection of complete set of simple tests. Since like you mentioned {phased} is a subset (⊂) of {unphased}, then the paths of phased genotypes one works with would also be ideal to test on. Does this function have any validation tests confirming the correct likelihoods, which would be performed for both phased and unphased genotypes? These can be generated tests, if original files do not exists. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221:869,perform,performed,869,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221,1,['perform'],['performed']
Performance,"@droazen @cmnbroad @mbabadi I generally agree with the sentiments expressed in #4127, except that I think it's OK to require a conda environment (or even use of the Docker) for these particular tools. How we should validate this requirement is another question. We can discuss more with @vdauwera. @stefandiederich Hopefully once you get the conda environment set up you will be able to run the tools. We would definitely appreciate any feedback you might be able to provide. Note that the gCNV model is relatively sophisticated, so there may be some parameters (which control the priors for the model as well as how inference is performed) that you will need to adjust for your data. Depending on the number of intervals/bins you are using and your memory constraints, you may also need to scatter across multiple GermlineCNVCaller runs; see how things are done in the WDLs here: https://github.com/broadinstitute/gatk/tree/master/scripts/cnv_wdl/germline. As you noted, this pipeline is still in beta. We are currently running several evaluations and hope to soon release some Best Practices recommendations for the aforementioned parameter values that should work well for various data types generated at the Broad. We will also have some blog or forum posts that explain the new CNV pipelines in more detail coming soon---stay tuned!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357034364:630,perform,performed,630,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357034364,2,"['perform', 'tune']","['performed', 'tuned']"
Performance,"@droazen I hacked one of the TrainVariantAnnotationsModelIntegrationTest cases to run in your Docker (only necessary because it seems like `gradlew test --tests *TrainVariantAnnotationsModelIntegrationTest` doesn't recognize tests that use a `DataProvider`, but perhaps I did something wrong). Here are the differences:. ```; (gatk) root@a87e0994889e:/repo# h5diff -v /repo/src/test/resources/large/org/broadinstitute/hellbender/tools/walkers/vqsr/scalable/train/expected/extract.nonAS.snpIndel.posUn.train.snpIndel.posOnly.IF.snp.trainingScores.hdf5 /repo/extract.nonAS.snpIndel.posUn.train.snpIndel.posOnly.IF.snp.trainingScores.hdf5. file1 file2; ---------------------------------------; x x / ; x x /data ; x x /data/scores . group : </> and </>; 0 differences found; group : </data> and </data>; 0 differences found; dataset: </data/scores> and </data/scores>; size: [445] [445]; position scores scores difference ; ------------------------------------------------------------; [ 60 ] -0.419202 -0.419202 5.55112e-17 ; 1 differences found; ```. Looks pretty negligible to me! :stuck_out_tongue_closed_eyes: Probably a result of the native code being called by the python/ML packages used in these tools; even minor changes in the compilers across Ubuntu versions might introduce differences like these. A quick fix might be to replace all system calls to `h5diff` in these tests with `h5diff --use-system-epsilon`; seems to do the trick here. But if that doesn't fix all test cases, then perhaps you can relax things with `h5diff -p EPSILON`, where `EPSILON` is a relative threshold. Probably OK to pick something like `1E-6`. OK if I leave it to you to try this or otherwise check the rest of the cases?. Sorry for the inconvenience! I think the exact-match test worked as intended here, but I probably could've put in better messaging originally. Unfortunately, it's a bit awkward to grab the output of system commands. And thanks for dealing with conda again (a necessary evil, unless we want ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848796931:448,scalab,scalable,448,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848796931,1,['scalab'],['scalable']
Performance,"@droazen I'm not sure this is an improvement. We want the fundamental unit of spark tool to be the transform, not the cli wrapper around it. If we do this then we're pushing more of the contract of the transform outside of itself, i.e. see the newly duplicated bqsr code. I think that it was a deliberate decision to lift all reads into the initial rdd and then filter them in the transforms to what was needed by that transform. This is paying some performance cost in multi-stage pipelines which will potentially apply the same filters over and over again, but it simplifies the code because the filters can be baked into the transform and the pipeline writer doesn't have to think about them. It would be nice if we had a mechanism for adding metadata to an RDD so we can say ""this is a sorted RDD filtered with X,Y,and Z filters"", so we could intelligently avoid re-filtering.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856:450,perform,performance,450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856,1,['perform'],['performance']
Performance,"@droazen, I will address this in #2041. As you suggested this when I implemented `LocusWalker`, I would like to have some idea about why `DownsamplingMethod` is used as a parameter in the constructor. I think that this is misleading, because independently of the method for downsampling the one that is used by `SamplePartitioner` is a `ReservoirDownsampler` (if downsampling is performed), so API users could think that they are performing a different downsampling in LIBS that the actual one. I will keep the constructor in the PR, but I would like some feedback for this either here or in #2041.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006:379,perform,performed,379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006,2,['perform'],"['performed', 'performing']"
Performance,"@jamesemery Great, thanks for checking. Could you do a review pass on this when you get a chance? It's not clear that the approach taken here of sending the owner config file around is what we want....it seems like instead we need a way to load the owner config from the launcher script itself.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188:240,load,load,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188,1,['load'],['load']
Performance,"@jean-philippe-martin I think we should do the comparison in https://github.com/broadinstitute/gatk/issues/995 before porting the ApplyBQSR optimizations, actually. If it turns out that we decide to go with the simpler broadcast approach we'd then need to figure out how the dataflow ApplyBQSR changes fit in. So it probably makes sense to spin out a separate ticket for the ApplyBQSR changes and close this one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/970#issuecomment-148758446:140,optimiz,optimizations,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/970#issuecomment-148758446,1,['optimiz'],['optimizations']
Performance,"@jjfarrell Glad you found that article useful!. In general, `--consolidate` will be memory and time intensive. It's not intuitive, but as you already figured out if `--consolidate` is enabled, we do it on the very last batch. If you only have on the order of a few hundred batches total, not having specified consolidate shouldn't affect read performance much. The only other thing that would help scale here would be to break up your intervals so that larger contigs are split up into multiple regions. Less memory required and you can throw more cores at it (if you have them). What sort of performance did you see on `GenotypeGVCFs` or `SelectVariants`? That could be the other issue with these large intervals.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252834003:343,perform,performance,343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252834003,2,['perform'],['performance']
Performance,"@kdatta Looks like the integration tests passed on travis after clearing the cache! Once you address comments, squash, and rebase onto the latest gatk master the unit tests should pass as well, since you just need the TestNG fix that got merged into master. This means we can merge this today in all likelihood!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829:77,cache,cache,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829,1,['cache'],['cache']
Performance,"@kgururaj As I start to think about upgrading exome joint calling to use GenomicsDBImport the 100 interval threshold seems like it might be problematic. I've been working with WGS data, so I don't have much intuition for benchmarking with missing data. Is there any performance downside to running over larger intervals that include missing data? For example, if we want to scatter the exome 50 ways, each subset of the exome interval list will have ~4000 intervals, but the GVCFs won't have data outside those intervals. Does it make sense to pass to GenomicsDBImport a single interval encompassing all of those?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-409956462:266,perform,performance,266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-409956462,1,['perform'],['performance']
Performance,"@laserson the `SAMRecord` vs. Google `Read` is a loooooong story.; The super-short version:; We had a bunch of utilities written for `SAMRecord` that @droazen refactored over months to take the GATKRead interface. As it happens, the SAM spec and the GA4GH spec are not 100% compatible. So, it's not possible to losslessly convert from A -> B -> A (where A is `SAMRecord` or Google `Read`). The cases where it doesn't work are edge cases, but they exist. Second, @jean-philippe-martin found that converting to Google `Read` was fairly expensive. Between those two points, I think we're probably better off with SAM-backed reads. (Also, right now the Google `Read` is serialized via JSON, so it's not that small anyway.). @tomwhite and @jean-philippe-martin, I think adding the header back will be fine for us engineers working on the engine, but it will make for a poorer user experience for newcomers and Comp Bios to burden them with having to care about what happens with shuffles (when they just want to prototype something). . That said, I think this is probably the best approach we have at our disposal. If we do, we need to do an excellent job of throwing errors if users try to perform actions that would require the header. The error message should explain what really happened and ideally point to some documentation we write explaining the stripping of the header and how to fix it. If this error occurs, it needs to be simple for anyone to fix it. @droazen @lbergelson, what do you two think? (also @laserson, do you have any ideas or thoughts on the header since we're probably stuck with `SAMRecord`?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025:1186,perform,perform,1186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025,1,['perform'],['perform']
Performance,"@lbergelson Do you have an opinion on the best way to pip install the gcnvkernel python package and dependencies for Travis testing? I've verified that the pip install works within a basic conda environment with python=3.6. We'll need to load this environment both for unit/integration tests as well as WDL tests. As long as this is the only python environment we need, I think we can simply use the base environment in the Docker. If more environments are required (e.g., for @lucidtronix), then maybe we'll need to be more clever for unit/integration tests, but we can still load them manually in the scripts that kick off the WDL tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-348073948:238,load,load,238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-348073948,2,['load'],['load']
Performance,"@lbergelson I was referring more to the middle part of the StackOverflow by Daniel Chapman - specifically the [4.1 The ObjectStreamClass Class](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a5082) and [4.6 Stream Unique Identifiers](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a4100):. _If not specified by the class, the value returned is a hash computed from the class's name, interfaces, methods, and fields using the Secure Hash Algorithm (SHA) as defined by the National Institute of Standards._. Now when I look at the `java.io.ObjectStreamClass.java` file for 64-bit JDK7 and JDK8 - from src.zip - both have the same code for the following parts after performing a `diff` - I didn't list all of the lines of code since they are quite long:. ```; public long getSerialVersionUID() {; // REMIND: synchronize instead of relying on volatile?; if (suid == null) {; suid = AccessController.doPrivileged(; new PrivilegedAction<Long>() {; public Long run() {; return computeDefaultSUID(cl);; }; }; );; }; return suid.longValue();; }; ... private static long computeDefaultSUID(Class<?> cl) {; ...very long code which can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:721,perform,performing,721,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['perform'],['performing']
Performance,"@lbergelson everything I know I learned from there:; http://stackoverflow.com/questions/28939166/error-submitting-a-cloud-dataflow-job. Mine was also in the 4MB range, I switched to loading that file at the worker instead of the client and it worked. So the size limit is probably somewhere between 3 and 4MB.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/595#issuecomment-114594863:182,load,loading,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/595#issuecomment-114594863,1,['load'],['loading']
Performance,@lbergelson thank you for the comment and sorry for my bit late response. I excluded the dependency to the jsr203-s3a and tested that both local- and spark-gatk can access s3a files by dynamically loading it. I also added a new directory `scripts/s3a` for documentation and simple tests for s3a demonstration.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-665484597:197,load,loading,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-665484597,1,['load'],['loading']
Performance,"@lucidtronix @mbabadi @samuelklee I think the best solution would be to establish a single, common Python environment, with a single set of dependencies, that all GATK Python tools depend on. We would establish a single docker image that has all of these dependencies pip installed, and could also include a conda env for the GATK environment for users who don't want to use the docker image. If we could do that, it would eliminate the need load per-tool conda environments. From what I've seen so far based on existing branches, the two environments we need (gCNV and CNN-VQSR) don't look that far apart in terms of dependencies. gCNV is using Theano, and CNN Tensorflow, but the rest looks [pretty close](https://docs.google.com/a/broadinstitute.org/spreadsheets/d/1RV7--uBQ0ctlXzMH09cmr0VimpZYIU68DdxJzE60y-c/edit?usp=sharing). So a strawman proposal for the main components for a common environment would be:. Python 3.6; Numpy >= 1.13.1; Scipy 1.0.0; Theano .0.9.0; Tensorflow 1.4.0; Pymc3 3.1; Keras 2.1.1. Can you all chime on on whether you think we can converge in a single environment ? If so, it would greatly simplify things, and we can start with getting a docker image built for running travis tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3692#issuecomment-348188451:442,load,load,442,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3692#issuecomment-348188451,1,['load'],['load']
Performance,"@magicDGS Sorry for the delayed reply, I had to see what direction the `HaplotypeCaller` branch would take before I could answer your post above. In order to get the `HaplotypeCaller` performance up to acceptable levels we've had to make some changes to the traversal that have caused it to diverge quite a bit from the idea of a `SlidingWindowWalker` in this branch. Also, the way `SlidingWindowWalker` handles the `intervalsForTraversal` (using them to select fixed-size windows) is not compatible with what the `HaplotypeCaller` currently requires. As a result, I recommend that we merge your `SlidingWindowWalker` in as a separate traversal rather than trying to reconcile it with the `HaplotypeCaller` branch and mutate it into something that might not be as useful to you. Fortunately, walkers in GATK4 are simple enough that it's perfectly fine to have several similar-but-subtly-different walker types, provided they all serve actual use cases. I'll",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-204134447:184,perform,performance,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-204134447,1,['perform'],['performance']
Performance,"@magicDGS The HaplotypeCaller traversal has undergone some changes in the past few weeks to improve performance and bring the output of the tool closer to GATK3. There is now an `AssemblyRegionWalker` that divides the intervals into active and inactive regions, in a greatly simplified version of the GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:100,perform,performance,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,1,['perform'],['performance']
Performance,"@mbabadi Ah, well file-based I/O would be the simplest option of all, of course, and should definitely be considered as a candidate solution to this ticket, particularly if you've already tried it and found the performance penalty to be minimal for your use case (@cmnbroad take note). The division of labor you describe between Java and Python sounds great, by the way -- exactly the sort of approach I was hoping you'd implement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3698#issuecomment-337319853:211,perform,performance,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3698#issuecomment-337319853,1,['perform'],['performance']
Performance,"@mbabadi I've updated my PR to use miniconda3. @mbabadi @lucidtronix @samuelklee I think we should aim for tools that at least run out-of-box, without depending on any out-of-band configuration other than the conda env. On top of that we can provide guidance/configs for users on how to enable further optimizations, like g++. Does that sound like an achievable goal ?. As for the docker, we're going to have strike the right balance between image bloat and performance(including test performance). I think we're around 4+ gig now, and counting. Before the Python integration we were at 1.9G, and trying to find ways to reduce it. So lets see where we wind up but keep that in mind. Finally, we need to find a way to install the (GATK) python package(s) without depending on access to the GATK repo. Right now I think the gCNV branch has a ""pip install from source"" added to the conda env .yml. That will work on the docker at the moment (and thus on travis), but that won't work for non-docker users how don't have source/repo access. Also, one of the proposals to reduce the size of the docker is to remove the repo clone that is currently there. My proposal is that we change the gradle build to create an archive/zip of the python source (this would include the VQSR-CNN package code as well as gCNV kernel). We can then copy that on to the docker image, and pip-install it from the copy. That would retain the ability to always run travis tests based on the code in the repo, and also keep the nightly docker image in sync. We'll also have deliver the archive as an artifact somehow (perhaps including PyPi) for non-docker users.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277:302,optimiz,optimizations,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277,3,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"@meganshand Here's a quick example:. ![image](https://user-images.githubusercontent.com/11076296/158385742-20a3303b-d8ce-4335-b42f-622da9bfa8d3.png); ![image](https://user-images.githubusercontent.com/11076296/158385777-6174f8b8-7abb-4b31-92d1-11cc8064854b.png). Note that the malaria data used was pretty small: chr1-2 training (~20k positive training/truth variants, ~50k negative training variants; note also that the threshold for determining negative training was not tuned---a threshold corresponding to a 98% truth sensitivity was arbitrarily chosen), chr3 validation (~50k variants), and chr4-6 test (~150k variants). The LL score is calculated from a validation set held out from the training/truth positives used to train the model, while the F1 score is calculated using ""orthogonal truth"" positives/negatives determined using 3 families of ~30 trios each. However, there's some arbitrariness in how we define the boundary for the latter positives/negatives, and hence some arbitrariness in the F1 score itself. But I'd expect using gold-standard GIAB truth would be more straightforward. Not sure how much we can conclude, but that the validation and test F1s are similar and that the validation LL score isn't *too* far off are encouraging. That said, there is a pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian con",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:473,tune,tuned---a,473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,1,['tune'],['tuned---a']
Performance,"@mlathara I think we're talking a bit in circles. The main use case I foresee for a generic split/merge tool would be to allow parallelized processing. I cant say there wouldnt be other uses I'm not seeing now (in the VCF world, SelectVariants is an extremely useful tool), but i dont have a specific use-case for GenomicsDB subsetting today beyond this. . I would point out this rapidly gets into specifics and quirks of any one user's infrastructure. I dont actually mind copying the GenomicsDB workspace prior to appending to it, because processing occurs on shared lustre space, while our permanent data lives on other disk space. Therefore we would probably do a copy no matter what. I agree you dont want to develop our one person's infrastructure. . The only aspect that gives me pause on your plan regarding split jobs is that GATK doesnt provide the scheduler. Sure there used to be queue and I gather GATK pushes WIDL/Cromwell (unless this changed), but we never used these. If GATK is not trying to provide the scheduler (which is better), does this really just look like: . 1) kick off X independent jobs for GenomicsDB/append; 2) each job specifies the interval(s) on which to operate; 3) Each job has no knowledge of the other jobs; 4) each job writes it's output to the same workspace; 5) Presumably there is something in place so jobs can run concurrently. This must be the new feature?. I imagine this could work. It does obligate one to have/use some kind of shared disk space, which we can handle, but could be a negative for some.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641469405:892,queue,queue,892,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641469405,2,"['concurren', 'queue']","['concurrently', 'queue']"
Performance,"@mlathara The nodes have considerably more (256 or so). is there any rule or thumb or guidance on expected memory needs based on number of gVCFs and/or type of input (WES vs WGS)?. I do think you might be onto something though. Out default cluster submission code takes our slurm job memory request, subtracts only a few GB and passes the remainder to -Xmx/Xms. I will update to leave more buffer as you suggest. Our cluster happens to be undergoing maintenance this week, so this particular job was killed. I'll update the GATK version, add --genomicsdb-shared-posixfs-optimizations, and adjust the memory. One other thing: i noticed GenomicsDBImport is not nearly as verbose in logging as typical GATK tools. Is that expected, or a symptom of whatever problem we're having?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-656259475:570,optimiz,optimizations,570,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-656259475,1,['optimiz'],['optimizations']
Performance,@nh13 I wrote a test for your branch (its very simple it just reruns the gvcf mode tests with --disable-optimizations enabled) that should work for your branch. Its in the branch je_addTestForDisableOptimizations. Since you submitted this PR from your own clone of the GATK I cannot push this onto the branch as it stands. Would you be able to copy it into this branch?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7125#issuecomment-793077846:104,optimiz,optimizations,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7125#issuecomment-793077846,1,['optimiz'],['optimizations']
Performance,"@samuelklee DRAGEN STRE model doesn't actually make any alterations to the smith waterman parameters or how they work, it just works by adjusting the indel gap penalties that are used for the PairHMM. At one point we were concerned about SW parameters being different with dragen but as it turns out the biggest visible effect of the SW parameters on the output (the alignment we perform after haplotypes discovery) is irrelevant since they don't realign their reads internally. We kept the default gatk alignment behavior and thus the SW parameters that are used (for dangling head recovery which I believe are the old arguments) still match. As far as unifying the parameters I suspect it could be done though one wonders if there aren't risks where the different contexts in which we use the parameters will not perform as well with a unified set. Speculation on my part though. I agree with David that we should be cautious about making changes that will affect the HaplotypeCaller before November. . I support including an argument in any case (possibly multiple) to include the SW parameters. I would actually advocate we read these files in as tables of parameters where you simply point to on the command line to configure new parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993:380,perform,perform,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993,2,['perform'],['perform']
Performance,"@samuelklee I'd say lets leave 2.1 base image up there for now, and yes on the cache clearing. Once tests pass with the cache cleared it should be good to merge. Feel free to squash and rebase if you like.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408453109:79,cache,cache,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408453109,2,['cache'],['cache']
Performance,"@samuelklee It wasn't just a rebase, it was a complete rewrite because the old code had since become completely entangled with DRAGEN code. But I did it! Everything is passing, the code is dramatically simpler, and it's even a bit faster. I have done my best to make a coherent commit history. I would recommend reviewing one commit at a time in side-by-side diff mode. Note that some commits rip out old code and replace it with pseudocode, deferring the new code to a later commit. Other commits tell a story of what all the different caches meant in order to motivate the simpification of later commits. The baroqueness of the old code was motivated by three considerations:; * cache-friendliness -- traversing all arrays by incrementing the innermost index, reads. This is absolutely essential.; * flattening 3D arrays into 1D arrays. This was a premature optimization.; * Precomputing addition operations -- this was misguided. The DRAGEN code relied on these caches in a rather complex way, which fortunately turned out not to be necessary and which could be dramatically simplified. My notes on tracking all the variables from the parent genotype calculator down to the DRAGEN calculator are in this google doc: https://docs.google.com/document/d/1v6s57mUAwfj38nL3VdktjA059kYBkJfokq18IDy79E8/edit?usp=sharing. Good luck and don't hesitate to ask me to explain anything.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1023647476:537,cache,caches,537,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1023647476,4,"['cache', 'optimiz']","['cache-friendliness', 'caches', 'optimization']"
Performance,"@samuelklee Now it's back to you. I agreed with and implemented all of your suggestions. `GenotypeIndexCalculator`, `GenotypeAlleleCounts`, `GenotypeLikelihoodsCalculator` and `GenotypeLikelihoodsCalculator` (renamed to `GenotypesCache`) now have clearly-defined roles. A lot of premature optimization is gone.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1068695779:289,optimiz,optimization,289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1068695779,1,['optimiz'],['optimization']
Performance,"@samuelklee The module can now save and load everything, including the state of the optimizer. This allows to making interesting inference pipelines. Here's a decent strategy for obtaining the global optimum (it works flawlessly on simulated data every time):. - In the first pass, one disables annealing and obtains the variational parameters in a thermal state. The temperature needs to be _high enough_ to allow most/all local minima to merge, though, not too high to allow copy numbers to travel too far away from baseline copy numbers. If this occurs, one must anneal very slowly in the next stage (see below). The results are checkpointed once converged. - In the second pass, one makes another call to the CLI tool, this time w/ annealing enabled (starting from the same temperature) and starting from the checkpointed thermal results (model params, posteriors, adam(ax) state). The annealing rate must be slow enough to prevent thermal fluctuations from getting quenched (i.e. the evolution must be quasi-isothermal). One must look for a steady and linear rise of ELBO, such that when the annealing protocol ends, SNR quickly drops to values below 1. In both runs, the learning rate must be very small (in the rate 0.01-0.05) such that we wouldn't have to worry about controlling stochastic noise. Adam(ax) quickly adjusts its moment estimates and compensates for the small learning rate, so this doesn't increase the training time significantly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-347369020:40,load,load,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-347369020,2,"['load', 'optimiz']","['load', 'optimizer']"
Performance,"@takutosato had a good suggestion: to stratify to low-complexity regions in the high-confidence regions. Not sure how many variants are there, but will take a look. EDIT: looks like it's ~5k / ~54k on chr22 in CHM. More generally, I think that defining the appropriate loss function for optimization to set ""default"" parameter values obviously has no unique answer. The problem is also made a little more complicated by our current strategy of sensitive calling + non-trivial filtering. But it would be great to come up with some hard constraints (e.g., we never want runtime/cost to exceed X, we always want to maintain Y metrics in these regions on these samples) and general procedures, then apply them as equitably as possible across all method/parameter changes. Also generally, I'm a bit wary of focusing too hard on the high-confidence regions, as this might lead to overfitting or could understate the potential of method/parameter changes in more difficult regions. But probably we'll have to downweight the loss or do more manual checks in low-confidence regions until we improve truth resources there. One naive question, just want to double check: is it correct that the overall scaling of each set of SW parameters is inconsequential? E.g., if I multiply each by a constant, should I expect the same results? I would expect this to be the case (unless my hazy recollection of the details of SW scoring is off) and simple experiments bear this out, but I'm not sure if there are some edge cases or idiosyncrasies in our implementation or use of the scores that I might be missing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714570055:287,optimiz,optimization,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714570055,1,['optimiz'],['optimization']
Performance,"@tedsharpe I've addressed some of your comments here -- all the simpler stuff plus:. - I now only make distal targets for split reads with one supplementary alignment. We can make a ticket to handle more complex cases at some point.; - I renamed the concept of strand in the `EvidenceTargetLink` and related classes -- I'm now calling it `evidenceUpstreamOfBreakpoint`.; - I canonicalize `EvidenceTargetLinks` and only create them when the source is upstream of the target. This allowed me to get rid of the de-duplication code, so thanks for the suggestion. It seemed tricky to me to try to cluster these links during the initial pass over the reads while at the same time keeping track of coherent evidence. In my testing it doesn't seem like it is slow to run over the `EvidenceRDD` again to do this, but we could think about trying to change this sometime if we're looking for optimizations. . Want to take another look?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328300806:881,optimiz,optimizations,881,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328300806,1,['optimiz'],['optimizations']
Performance,"@tomwhite I'm looking into the performance issues now with the new code path -- it brings the output much closer to GATK3, but clearly needs some profiling work. Can you tell me what kind of difference you saw in the runtime on Spark? Eg., was it on the order of 20-30%, or was it worse than that?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3533#issuecomment-330905564:31,perform,performance,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3533#issuecomment-330905564,1,['perform'],['performance']
Performance,"@tomwhite To clarify, I think that the caller of `ensureCapacity()`, namely `GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables()`, also needs to be synchronized in order to avoid some unlikely but still-possible races. Given this, I think that we should consider whether `ThreadLocal` might be a better option here. It's not 100% clear to me whether a `ThreadLocal` `get()` call is cheaper than a synchronized method call, but some casual googling suggests that it might be. If we're going to end up entering a synchronized method on every single call to `GenotypeLikelihoodCalculators.getInstance()`, we might want to do some research into whether `ThreadLocal` + no synchronization would be faster, since I believe that this is a performance-sensitive section of code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422171244:746,perform,performance-sensitive,746,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422171244,1,['perform'],['performance-sensitive']
Performance,@tomwhite and/or @laserson should have a look at this and give JP high-level feedback on his approach to optimization.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/987#issuecomment-146905889:105,optimiz,optimization,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987#issuecomment-146905889,1,['optimiz'],['optimization']
Performance,@wujh2017 Great! Let us know if you have any more feedback. Please be aware that both DetermineGermlineContigPloidy and GermlineCNVCaller are still in beta. There are some parameters that may need to be tuned appropriately for your data. We are currently running evaluations and will release some recommendations that we find suitable for data generated at the Broad.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4457#issuecomment-369254401:203,tune,tuned,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4457#issuecomment-369254401,1,['tune'],['tuned']
Performance,"After more experimentation, one issue I was running into with the ApproxKernSeg method was failure on small and ""epidemic"" events. This is because 1) the segment cost function used in that paper is extensive (growing with the number of points in a segment), and 2) binary segmentation is a global, greedy algorithm. These both cause long events to be preferred over short events, and thus the first changepoints found (and retained after applying the penalty) may not include those for small, obvious events. For example, see performance on this simulated data, which includes events of size 10, 20, 30, and 40 within 100,000 points at S/N ratio 3:1 in addition to sine waves of various frequency at S/N ratio 1:2 (to roughly simulate GC waves). Changepoints arising from the sine waves will be found first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:526,perform,performance,526,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045,1,['perform'],['performance']
Performance,"Aha, after clearing the travis cache for the PR build it passed! Merging",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5194#issuecomment-422499733:31,cache,cache,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5194#issuecomment-422499733,1,['cache'],['cache']
Performance,Are we interested in writing some definitive guide on how to tune the `af-of-alleles-not-in-resource` parameter for different contexts?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4745#issuecomment-387218068:61,tune,tune,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4745#issuecomment-387218068,1,['tune'],['tune']
Performance,"As we discussed on Slack, this will fix the NaNs, but I'm not convinced that we should allow the single-contig use case without at least a warning. The ploidy step will essentially perform no inference, since I think the per-contig bias and ploidy factors will cancel out with the way the likelihood is written---it will simply return the prior, and all samples will be guaranteed to have ploidy = 2. @asmirnov239 is going to do some more testing to make sure we understand this right and perhaps add a warning/documentation. The current likelihood is a bit confusing (I tried to address some of these issues in the unmerged ploidy-model update), but in any case, the problem is degenerate and it's hard to define appropriate behavior without additional priors and model structure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6613#issuecomment-631693589:181,perform,perform,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6613#issuecomment-631693589,1,['perform'],['perform']
Performance,"Can you please test this change with the `HaplotypeCaller` in protected and make sure nothing changes? In particular, can you run `HaplotypeCallerIntegrationTest` and `HaplotypeCallerEngineUnitTest` and make sure they pass with this change? This PR makes me a little nervous given the centrality of the classes touched, even though the optimization itself is simple enough...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216595202:336,optimiz,optimization,336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216595202,1,['optimiz'],['optimization']
Performance,Could I have some feedback about the efficiency of the Fisher's Exact Test implemented here? I'm planning to use it in other context where the performance could be reduced and I think that this is a good opportunity to have some information about it. Thanks in advance!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-267036486:143,perform,performance,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-267036486,1,['perform'],['performance']
Performance,"Even if we had default methods, `Locatable` should be simple (like `Comparable`) and shouldn't be polluted with every possible operation you might want to perform on an interval.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/305#issuecomment-79198026:155,perform,perform,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/305#issuecomment-79198026,1,['perform'],['perform']
Performance,"Executive summary: . My main concern is that the amount of unsupported code is continuing to grow. Adding this PR would bring the total to about ~5k lines of WDL, Java, and test code. In comparison, the amount of corresponding supported CNV code clocks in around ~33k---this includes all of gCNV, as well! Development time has also been non-negligible and dates back to pre-4.0 release. Another concern is that the number of users of this unsupported code is also growing. In fact, it seems like we are actively pointing users to it. This seems unsustainable going forward. Finally, I don't think we have satisfactorily demonstrated which of the functions accomplished by this code (format conversion, post-hoc blacklisting, germline/""CNLOH"" tagging and imputation) are necessary or cannot be performed by existing code or more streamlined and principled methods. (Some of these functions, such as IGV conversion, are already performed by existing code.) Of those functions, I think format conversion is the only one we should retain from this code in an unsupported fashion. So if this PR introduces a useful GISTIC conversion, no harm in merging that. This all sounds like a decision for the new tech lead! @mwalker174 any thoughts? . More detailed responses follow:. > Users are already using this branch and giving me positive feedback (definitely more positive than adjusting num_changepoints_penalty_factor). I suggest merging mostly for practical reasons. It buys us more time to put in a principled solution. And this workflow is clearly marked as an unsupported prototype anyway (as are the GATK CLIs). I want to emphasize that this whole workflow is not a long-term solution. In other words, I would like to get this in and then focus on a supported solution. While it's great that users are giving positive feedback, I refer you to CellBender team's manifesto at https://github.com/broadinstitute/CellBender/commit/28f02f8dbd716aff922bb8da1e56da29347b245b. Can these users help us definitiv",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199:793,perform,performed,793,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199,2,['perform'],['performed']
Performance,"Fine but this is clearly premature optimization. How about a class called Intervals or intervalutils for this sort of random; utility ?. On Thursday, February 18, 2016, droazen notifications@github.com wrote:. > @akiezun https://github.com/akiezun What will actually happen is that; > someone will need that functionality months from now, forget that it; > already exists (embedded in some random tool), and re-implement it. It; > should be moved back now before this is allowed to happen.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185886728. ## . Sent from Gmail Mobile",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185888065:35,optimiz,optimization,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185888065,1,['optimiz'],['optimization']
Performance,"For record keeping, as the comments and replies may be buried in the many commits. ------------; ### On the problem of too many splits of RDD and performance concerns. Initial comment by @cwhelan :; > I'm starting to really not like this approach of splitting up the RDD into lots of smaller RDDs for later processing. It seems inefficient to me: it launches tons of different Spark stages each of which has a bunch of overhead. Perhaps not in this PR, but I think it would be better to classify the contigs on the fly and dispatch them to the right processing methods in a single pass over the RDD. Reply by @SHuang-Broad. > I tried to fix it in this PR, but that seems to be a big task,; and probably is impossible to achieve in a single pass,; because currently each class of contig ends up producing a different type of object; (3 general classes: simple -> SimpleNovelAdjacency, complex -> ComplexVariantCanonicalRepresentation, and unknown -> SAM records of the contigs); and a groupBy() operation is necessary in the middle using these objects as keys; due to the fact that different contigs may produce the same variant; So what I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:146,perform,performance,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030,1,['perform'],['performance']
Performance,"Great! And yes, LL will be optimized separately for SNPs and INDELs. How about this for a first workflow to target?. 1) Run ExtractVariantAnnotations on a training set of chromosomes. You can keep training/truth labels as in Best Practices, for now.; 2) Run TrainVariantAnnotationsModel on that. We'll use the truth scores generated here for any sensitivity conversions---i.e., we'll be calibrating scores only to the truth sites that are contained in the training set of chromosomes.; 3) Use the trained model to run a single shard of ScoreVariantAnnotations on a validation set of chromosomes.; 4) Run some variation of the above script on the resulting outputs to determine SNP and INDEL score thresholds for optimizing the corresponding LL scores. We can also add some code to the script to use the truth scores from step 2 to convert these score thresholds into truth-sensitivity thresholds.; 5) Provide these truth-sensitivity thresholds to ScoreVariantAnnotations and use them to hard filter. Evaluate on a test set of chromosomes. If all looks good, we can later move steps 3-4 into the train tool and automate the passing of sensitivities in 5 via outputs in the model directory. This will let us keep the basic interface of ScoreVariantAnnotations the same, but we'll have to add a few basic parameters to TrainVariantAnnotationsModel to control the train/validation split. So I think all this branch is missing is step 5---we'll simply need to add command-line parameters for the SNP/INDEL sensitivity thresholds and then do the hard filtering in the VCF writing method highlighted above. Do you think you can handle implementing that in this branch, and then the rest at the WDL level? I can help with the python script for the LL stuff (or anything else), if needed. Not sure if you got a chance to check out what your collaborators are doing in the methods you're looking to compare against, but it would be good to understand if this basic scheme for train/validation/test splitting can",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084:27,optimiz,optimized,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084,2,['optimiz'],"['optimized', 'optimizing']"
Performance,HaplotypeCaller does not resume from where it stopped. If you need to perform the same task again restart the whole task using the very same commandline.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7454#issuecomment-918232900:70,perform,perform,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454#issuecomment-918232900,1,['perform'],['perform']
Performance,"Hey @mwalker174,. Setting `--host-min-identity 20` lowered the `READS_AFTER_PREALIGNED_HOST_FILTER` to 131467, which is closer to what I expected (albeit still 12% of the reads) so thanks for the suggestion. I'm still unsure why the preferred approach is to use this fairly arbitrary metric that doesn't incorporate both mates instead of leveraging the known alignments from STAR, which does. Given what I've learned today, I think the better approach (for my use case) would be to filter the unique mappers, the multi-mappers and the chimeric reads (which in my data set represent 97.5% of the reads) and then apply the QUALITY_AND_COMPLEXITY_FILTER and the DUPLICATE_READS_FILTER. Would you agree?. To put myself in your shoes, I would guess that PathSeq is designed for general purpose use cases (which is probably `--is-host-aligned false`) and that performing such a filtering would require specific handling for each supported aligner, which would be a lot of work. Moreover, my use case with short paired-end reads is also probably not common. So I understand why you use the existing approach but are there any reasons that I'm missing as to why you'd suggest to stick with the basic approach for my use case instead of the one that I proposed above?. Best, Welles",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6687#issuecomment-652524772:854,perform,performing,854,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6687#issuecomment-652524772,1,['perform'],['performing']
Performance,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:159,load,loading,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857,2,"['load', 'perform']","['loading', 'performed']"
Performance,"Hi, I am encountering a similar error attempting to run `GenotypeGVCFs` in `gatk v4.1.2.0`. It runs very briefly and writes a handful of variants from a single scaffold to the output file but then exits with `java.lang.ArrayIndexOutOfBoundsException` (see below). I have also tried adding the `-L` flag and an interval list, which performs similarly but outputs variants from a different scaffold. Any idea why this is happening or what I can do to overcome this problem? I have run `GenomicsDBImport` and `GenotypeGVCFs` successfully in the past (same version, same computer) on a different dataset, so I'm not sure what about this data is causing the problem. Any guidance is much appreciated!. Thanks,; Jessie. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /nfs/data1/jsalt/3RAD/colinus_virginianus_13May2017_V3Fw6_newchrom.fasta -V gendb://odont_cyr_8_snp_db -O odont_cyr_8_snp_db.vcf; 14:59:47.866 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 03, 2020 2:59:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:59:59.674 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:59:59.675 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 14:59:59.675 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:00:09.686 INFO GenotypeGVCFs - Executing as jsalt@mustard on Linux v3.10.0-957.1.3.el7.x86_64 amd64; 15:00:09.686 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 15:00:09.687 INFO GenotypeGVCFs - Start Date/Time: F",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640:331,perform,performs,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640,1,['perform'],['performs']
Performance,"How much does count collection cost at the desired bin size? How does this compare to bincov? Perhaps we could eliminate one of these steps if redundant. Note that the read counts are read once and stored in memory, so unless this takes a significant amount of time, then indexing is probably not the highest priority here (although I agree it would be nice to have in general). One related issue, as you mention, is file localization---since each shard only operates on a portion of the counts in each sample, it is a bit wasteful to localize the whole file. But how much does file localization cost? I can't imagine that it is the lowest hanging fruit. One of the more important issues, which you also mention, is optimizing parameters for inference. This includes not only the minimum number of epochs for training, but also things like the learning rate, annealing schedule, iterations per epoch, conditions for epoch convergence, etc. I'll be talking about how to tune these inference parameters---as well as other things in the pipeline---at the next BSV meeting. Let's brainstorm more things to try and prioritize them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5288#issuecomment-427562932:716,optimiz,optimizing,716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5288#issuecomment-427562932,2,"['optimiz', 'tune']","['optimizing', 'tune']"
Performance,"I can't reproduce this yet. I tried downloading the jar, unzipping it, and running the example command you gave, but I can't reproduce what you're seeing. I modified it for my local files:; ```; java -jar gatk-package-4.2.5.0-local.jar \; GenotypeGVCFs \; -R /Users/louisb/Workspace/gatk/src/test/resources/large/Homo_sapiens_assembly19.fasta.gz \; --variant gendb:///Users/louisb/Workspace/gatk/output \; -O out.vcf \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 \; --max-alternate-alleles 6 \; --force-output-intervals 20 \; -L 20 \; --only-output-calls-starting-in-intervals \; --genomicsdb-shared-posixfs-optimizations; ```; It runs to completion on my machine. ; My md5sum matches yours so that's not the problem. It's not clear to me what's going on here. Are the previous releases working on your cluster still?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042010522:627,optimiz,optimizations,627,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042010522,1,['optimiz'],['optimizations']
Performance,"I created a panel of normals from 90 WGS TCGA samples with 250bp (~11.5M) bins, which took **~57 minutes** total and produced an **11GB PoN** (this file includes all of the input read counts---which take up 20GB as a combined TSV file and a whopping 63GB as individual TSV files---as well as the eigenvectors, filtering results, etc.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:961,optimiz,optimize,961,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503,1,['optimiz'],['optimize']
Performance,"I did a quick scalability ministudy - this seems to scale well up to 12 cores and then diminishes due to Amdahl's law I think that's fine. There is no diminished runtime due to OMP overhead when on 1 core. Note that our cluster on which I wan these was not empty (a few of the 48 cores were in use) and do this is just a ballpark estimate of scalability, in particular 24 was worse than 12 probably due to interference. Based on this I think OMP is a good idea and it's going to work on 1 CPU too. limited to 1 OMP thread, using 10GB of RAM. ```; real 2m15.621s; user 3m17.269s; Total compute time in PairHMM computeLogLikelihoods() : 50.964700625000006; ```. ---. limited to 1 OMP thread, using 32GB of RAM . ```; real 1m46.597s; user 3m17.363s; Total compute time in PairHMM computeLogLikelihoods() : 45.797104454; ```. limited to 2 OMP threads, using 32GB of RAM. ```; real 1m26.310s; user 3m24.636s; Total compute time in PairHMM computeLogLikelihoods() : 23.790980359000002; ```. limited to 4 OMP threads, using 32GB of RAM. ```; real 1m15.298s; user 3m29.834s; Total compute time in PairHMM computeLogLikelihoods() : 11.332445694; ```. limited to 6 OMP threads, using 32GB of RAM. ```; real 1m14.015s; user 3m20.876s; Total compute time in PairHMM computeLogLikelihoods() : 7.862075811; ```. limited to 12 OMP threads, using 32GB of RAM. ```; real 1m6.370s ; user 3m42.340s; Total compute time in PairHMM computeLogLikelihoods() : 4.585800097; ```. limited to 24 OMP threads, using 32GB of RAM (clearly, OMP hits the limit here). ```; real 1m8.779s; user 4m15.489s; Total compute time in PairHMM computeLogLikelihoods() : 3.047581173; ```. limited to 48 OMP threads, using 32GB of RAM (worse than 12 threads). ```; real 1m11.535s; user 6m26.100s; Total compute time in PairHMM computeLogLikelihoods() : 4.112299148; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1800#issuecomment-218810496:14,scalab,scalability,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1800#issuecomment-218810496,2,['scalab'],['scalability']
Performance,"I haven't understood how multi-allele model exactly works in the old GATK, so can't comment on why it does not perform well. In general, I am supportive of making the new model the default going forward. However:. > when we remove the other models. I would suggest retaining the old model if possible. As I said on the method meeting, the old model takes the full power of population information (by full, I mean under the Wright-Fisher and HWE assumptions, you can't derive a more powerful model in theory). My understanding is that David's current model isn't. This is fine as long as the information from sequence data overwhelms the population information, which is usually true for highCov data. However, when data is thin, the population information will play a more important role. Without thorough evaluations in multiple scenarios, it is not clear when the loss of population information in the new model starts to matter. It would be good to keep the old model as a reference point, at least for biallelic SNPs, until we have more comparison.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242810127:111,perform,perform,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242810127,1,['perform'],['perform']
Performance,"I ran `FindBreakpointEvidenceSpark` and did some high-level checks to see if there are any opportunities for performance improvements. (cc @tedsharpe @cwhelan). This is the command line I ran. (Earlier I had run more executors with smaller memory settings, but the job didn't complete then.); ; ```bash; ./gatk-launch FindBreakpointEvidenceSpark \; -I hdfs:///user/$USER/broad-svdev-test-data/data/NA12878_PCR-_30X.bam \; -O hdfs:///user/$USER/broad-svdev-test-data/assembly \; --exclusionIntervals hdfs:///user/$USER/broad-svdev-test-data/reference/GRCh37.kill.intervals \; --kmersToIgnore hdfs:///user/$USER/broad-svdev-test-data/reference/Homo_sapiens_assembly38.dups \; -- \; --sparkRunner SPARK --sparkMaster yarn-client --sparkSubmitCommand spark2-submit\; --driver-memory 16G \; --num-executors 5 \; --executor-cores 7 \; --executor-memory 25G; ```. What does FindBreakpointEvidenceSpark do, from the perspective of Spark?. * [runTool] filter out secondary and supplementary alignments; * [getMappedQNamesSet] filter out duplicate reads, reads that failed vendor checks, unmapped reads; * Job 0 [ReadMetadata] mapPartitions to find partition stats; * Job 1 [getIntervals] filter and multiple map partitions to find breakpoint intervals ; * Job 2 [removeHighCoverageIntervals] mapPartitionsToPair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you want",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:109,perform,performance,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884,1,['perform'],['performance']
Performance,"I see. . Yes. That's what I'm planning on (except that `AssemblyContigAlignmentsConfigPicker` is upstream of this unit), and here's the thought for why:; * I'd try to place the alignment picking step in a single place as much as possible, this makes improvements to the alignment picking/filtering step easier; * the size-based filter can be tuned, even by an CLI argument, this would affect the number of segments in the CPX logic, and the alt_arrangment annotations, and the simple variants re-interpreted by `CpxVariantReInterpreterSpark`, but it won't affect the alt haplotype sequence, which IMO is what really is important. ; * I'm developing a downstream variant filter, which hopefully can cut down the false-positives. And for the question of ""why 2 instead of 1"", I think what you are suggesting is to change; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 2;; if (one.getSizeOnRead() >= MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; to; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 1;; if (one.getSizeOnRead() > MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; Am i right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353:342,tune,tuned,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353,1,['tune'],['tuned']
Performance,"I talked to comms and we agreed that a ""mitochondria-mode"" argument to Mutect2 was the right balance of clarity (you're really running Mutect2 not a wrapper) and simplicity (you don't need a laundry list of arguments to change which mode you're in if you just want to run with optimized defaults). . @ldgauthier @davidbenjamin @takutosato @rcmajovski Could you please take another look? Removing the wrapper tools has cleaned up the code so there are fewer changes now. I also changed TLOD to LOD in this version, but I'm happy to take that out and have that be future work if anyone is worried about it being a breaking change.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-428195077:277,optimiz,optimized,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-428195077,1,['optimiz'],['optimized']
Performance,"I tried clearing my caches and rebuilding, but I resolve everything. I noticed that our artifactory website looks much different today than it did yesterday. I wonder if it was down temporarily for an update. Maybe try again now? Unless they put it behind the firewall which would be a disaster... can you access https://artifactory.broadinstitute.org/?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641:20,cache,caches,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641,1,['cache'],['caches']
Performance,"I was looking `ReadWindowWalker` and I found it very interesting for iterate over windows, but although it is similar I still think that is not solving the same problem as the `SlidingWindowWalker` for two reasons:; 1. `ReadWindowWalker` requires reads to construct the `ReadWindow`, and it is not general for both reads and variants. My main idea behind the `SlidingWindowWalker` was to perform operation over windows along the genome (or requested intervals) for any kind of source provided to the walker.; 2. On the other hand, the approach to generate the sliding windows in my implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:388,perform,perform,388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775,1,['perform'],['perform']
Performance,"I was mistaken about this not being faster - I was using a counting function that Spark can optimise by pulling onto the map side so that the records don't go through the shuffle. I changed this to simply dump the processed reads so they have to go through the shuffle, and I got the following timings when processing a 121GB BAM file.; - With shuffle: 27 min; - No shuffle (two scans over input): 24.7 min (8% saving); - No shuffle (one scan over input): 17 min (37% saving). The version that does two scans is faster, but not hugely so. Removing a scan is possible, but requires the use of a sequence dictionary to find the end points of contigs. I've done this in the latest version of my branch (https://github.com/broadinstitute/gatk/compare/tw_overlap_partitioner), but there are more edge cases to test. Before I do this, however, it would be worth trying this approach with the Haplotype Caller to see if it works, and if it is appreciably faster. If the number of reads is filtered significantly so only a fraction go through the shuffle, then the performance gains will be smaller, and may not in fact be worth the increase in code complexity. @droazen, what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-249590040:1057,perform,performance,1057,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-249590040,1,['perform'],['performance']
Performance,"I've implemented the Gaussian-kernel binary-segmentation algorithm from this paper: https://hal.inria.fr/hal-01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM m",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:274,perform,performance,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666,1,['perform'],['performance']
Performance,"If these events were indeed not CNLOH, as we discussed, then I don't think we should merge this. Perhaps we should take a step back and answer definitively whether simply blacklisting common germline regions is enough to replicate/obviate most of the postprocessing. Should be straightforward to run an evaluation with and without blacklisting---and hopefully our truth data accurately reflects whether blacklisting is desirable. If tagging/filtering rare germline is still a concern, then I'd say the next step is to see whether simply changing segmentation parameters to artificially decrease resolution and/or simple length-based filtering suffices. Finally, simple filtering based on CR-AF as described above could be implemented. If the normal is available, we can make IS_NORMAL calls simply based on the overlap of the ModelSegments posteriors (with corresponding qualities). If not, then some heuristic determination of the normal state from the tumor alone as in Marton's caller could be performed. This would combine the IS_NORMAL calling and filtering steps into one simple tool. The output could be a tagged/filtered ModelSegments .seg file and the corresponding VCF.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-458551250:997,perform,performed,997,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-458551250,1,['perform'],['performed']
Performance,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:1391,load,load,1391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495,1,['load'],['load']
Performance,"Interesting. Sorry this is causing so much trouble. From one of your above comments I wasn't clear if the solution using `--conf 'spark.submit.deployMode=cluster'` work correctly or not. . Is it possible that it's correct behavior for it to fail with the linkage error? According to the [mapr doc](https://maprdocs.mapr.com/52/DevelopmentGuide/c-loading-mapr-native-library.html) that command causes it to expect the application to load the library itself, but GATK by default doesn't have a copy of MAPR and won't load it on it's own. Have you included the mapr library somehow into the gatk jar? Or is it provided to spark some other way? I don't really know how maprfs works and how it interacts with hadoop paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350315653:346,load,loading-mapr-native-library,346,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350315653,3,['load'],"['load', 'loading-mapr-native-library']"
Performance,It looks like it made the tests substantially slower.... I'm not totally clear on why. Maybe because it has to re-optimize code every time it restarts the jvm.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6093#issuecomment-521372663:114,optimiz,optimize,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6093#issuecomment-521372663,1,['optimiz'],['optimize']
Performance,Jar on Maven central updated - please clear any cached jars,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-295584988:48,cache,cached,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-295584988,1,['cache'],['cached']
Performance,"Java implementation of segmentation is now in the sl_wgs_segmentation dev branch, with a few simple unit tests. I'll expand on these and add tests for denoising in the future, but for now we have a working revised pipeline up through segmentation. The CLI is simply named ModelSegments (since my thinking is that it could eventually replace ACNV). I ran it on some old denoised exomes. Runtime is <10s, comparable to CBS. Here's a particularly noisy exome:. CBS found 1398 segments:; ![cbs](https://user-images.githubusercontent.com/11076296/30165095-cdf6251a-93ac-11e7-91fb-dcc8f48fe07f.png). Kernel segmentation with a penalty given by a = 1, b = 0 found 1018 segments:; ![kern](https://user-images.githubusercontent.com/11076296/30165106-dbbe0b40-93ac-11e7-99ec-5d58d8417d8b.png). Kernel segmentation with a penalty given by a = b = 1 (which is probably a reasonable default penalty, at least based on asymptotic theoretical arguments) reduced this to 270 segments :; ![kern-smooth](https://user-images.githubusercontent.com/11076296/30165113-e2b545a8-93ac-11e7-97a9-a692e43ebbdf.png). The number of segments can similarly be controlled in WGS. WGS runtime is ~7min for 250bp bins, ~30s of which is TSV reading, and there is one more spot in my implementation that could stand a bit of optimization, which might bring the runtime down. In contrast, I kicked off CBS 45 minutes ago, and it's still running... @LeeTL1220 this is probably ready to hand off to you for some WDL writing and preliminary evaluation. ; Although I can't guarantee that there aren't bugs, I ran about ~80 exomes with no problem. We can talk later today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936:1289,optimiz,optimization,1289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936,1,['optimiz'],['optimization']
Performance,Just adding a note here that `FeatureCache` should eventually be refactored to use the simplified Interval class (when it exists) to track cache boundaries and compute overlap.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630:139,cache,cache,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630,1,['cache'],['cache']
Performance,"Not sure what your `GenotypeGVCFs` command was, but did you use the `--genomicsdb-shared-posixfs-optimizations` option? This option is available for the import too and may improve your performance.; ```; --genomicsdb-shared-posixfs-optimizations <Boolean>; Allow for optimizations to improve the usability and performance for shared Posix; Filesystems(e.g. NFS, Lustre). If set, file level locking is disabled and file system; writes are minimized. Default value: false. Possible values: {true, false} ; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879779166:97,optimiz,optimizations,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879779166,5,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"Note that GATK3 uniquified the `GATKCommandLine` lines using a scheme like this:. ```; ##GATKCommandLine.SelectVariants.2=<ID=SelectVariants,Version=3.4-228-g2497091,Date=""Tue Jan 05 13:48:45 EST 2016"",Epoch=1452019725506,CommandLineOptions=""analysis_type=SelectVariants input_file=[] showFullBamList=false read_buffer_size=null phone_home=AWS gatk_key=null tag=NA read_filter=[] disable_read_filter=[] intervals=[3:113005755-195507036] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/humgen/1kg/reference/human_g1k_v37.fasta nonDeterministicRandomSeed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 static_quantized_quals=null round_down_quantized=false disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=true never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 reference_window_stop=0 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=variant source=/humgen/gsa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834:962,perform,performanceLog,962,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834,1,['perform'],['performanceLog']
Performance,"Now, it seems like calling `contaminationDownsampling` right after `retainEvidence` could cause problems if both methods remove reads. However, one might correctly point out that although the cache invalidation I mentioned is not handled systematically, the method `removeEvidenceByIndex` _does_ have some code to update the evidence by sample and the evidence index map. It's possible that this code is totally fine and that this lead is a dead end. However, the code looks like it could be simpler and it's tough to parse. For example, try to track the `to` variable, which determines the determination of the outer `for` loop:. ```; for (int etrIndex = 1, to = nextIndexToRemove, from = to + 1; to < newEvidenceCount; etrIndex++, from++) {; if (etrIndex < evidencesToRemove.length) {; nextIndexToRemove = evidencesToRemove[etrIndex];; evidenceIndex.remove(evidences.get(nextIndexToRemove));; } else {; nextIndexToRemove = oldEvidenceCount;; }; for (; from < nextIndexToRemove; from++) {; final EVIDENCE evidence = evidences.get(from);; evidences.set(to, evidence);; evidenceIndex.put(evidence, to++);; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625030697:192,cache,cache,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625030697,1,['cache'],['cache']
Performance,"Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:4560,perform,perform,4560,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['perform'],['perform']
Performance,"One note that might be useful (or known already to the team): simply calling `cache()` doesn't cause any action. It seems that one might need to force the computation to be done on the RDD (e.g. `count()`), for caching to work, if the predicate depends on the results of computation. (ref last comment in #1877)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1811#issuecomment-225204823:78,cache,cache,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1811#issuecomment-225204823,1,['cache'],['cache']
Performance,"Pair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like it’s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so it’s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:2301,perform,performing,2301,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884,1,['perform'],['performing']
Performance,"Rationale for engine changes:; This tool opens a large number of feature files (TSVs, not VariantContexts) and iterates over them simultaneously. No querying, just a single pass through each.; Issue 1: When a feature file lives in the cloud, it takes unacceptably long (several seconds, typically) to initialize it. A few seconds doesn't seem like a long time, but when there are large numbers of feature files to open, it adds up. This is caused by a large number of codecs (mostly the vcf-processing codecs) opening and reading the first few bytes of the file in the canDecode method. To avoid this I've reversed the order in which we test each codec, checking first if it produces the correct subtype of Feature, and only then calling canDecode. If you don't know what specific subtype you need, you can just ask for any Feature by passing Feature.class. It's much faster that way.; Issue 2: Each open feature source soaks up a huge amount of memory. That's because text-based feature reading is optimized for VCFs, which can have enormously long lines. So huge buffers are allocated. The problem is compounded for cloud-based feature files for which we allocate a large cloud prefetch buffer. (Though that feature can be turned off, which helps a little.) But the biggest memory hog is the TabixReader, which always reads in the index, regardless of whether it's used or not. Tabix indices are very large. To avoid this, I've created a smaller, simpler FeatureReader subclass called a TextFeatureReader that loads the index only when necessary. The revisions allow the new tool to run using an order of magnitude less memory. Faster, too.; Issue 3: The code in FeatureDataSource that creates a FeatureReader is brittle, and tests for various subclasses. To allow use of the new TextFeatureReader, I added a FeatureReaderFactory interface that allows one to ask the codec for an appropriate FeatureReader.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1284340770:999,optimiz,optimized,999,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1284340770,2,"['load', 'optimiz']","['loads', 'optimized']"
Performance,"Regarding the non-Docker integration tests failing earlier today, I think this was because the R packages were added to the Travis cache in #3101. @cmnbroad cleared the cache to see if we could reproduce a compiler error introduced in #3934 on Travis (for the record, we could reproduce it on my local Ubuntu machine and gsa5, but not on Travis). This removed the cached getopt dependency, which then caused tests to fail. See #4246.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4209#issuecomment-359999441:131,cache,cache,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4209#issuecomment-359999441,3,['cache'],"['cache', 'cached']"
Performance,STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 2019-01-07 11:33:18 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:19 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 11:33:24.377 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:33:24.549 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:33:26.271 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.272 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 11:33:26.272 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:33:26.272 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 11:33:26.273 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:2210,load,load,2210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['load'],['load']
Performance,"Sorry, just saw this now. We still don't have a simple solution for training models without pysam. We can probably do something similar to what we do with inference, but I think the current priority is to improve inference throughput so it will probably be a little while before we get to re-writing the training code. If people feel we should re-prioritize please let me know.; I have installed the conda environment on the same OSX version, without seeing this issue.; Which gcc version are you using @mwalker174 ? ; My `gcc -v` output is:; ```; Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1; Apple LLVM version 8.0.0 (clang-800.0.42.1); Target: x86_64-apple-darwin15.6.0; Thread model: posix; InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193:223,throughput,throughput,223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193,1,['throughput'],['throughput']
Performance,"Thanks @davidbenjamin for the feedback and sorry for the slow response. We have been working on improving PairHMM by adding AVX-512 (#3615) and FPGA (#2725) implementations. . We are also adding AVX2 (#3701) and AVX-512 (future PR) Smith-Waterman, which will improve the performance of Mutect2. We have the data above and will provide benchmarking results of your Mutect2 command with these improvements.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2562#issuecomment-338732871:271,perform,performance,271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2562#issuecomment-338732871,1,['perform'],['performance']
Performance,"Thanks @davidbenjamin, I can try that out. Any other parameters or modes that you feel might be gating any of these metrics/optimizations, which should be explored jointly with the SW parameters?. I guess the same question applies for `linked-de-bruijn-graph`, which is currently marked as experimental: what would be the procedure/criteria for changing the default behavior? Hopefully, we can answer this question for the case of a binary parameter before tackling 12 parameters! In general, I'm interested in establishing clear processes so it's easier for anybody to propose improvements. If there's no clear answer just yet, I'm happy to stop at exposing these parameters, perhaps consolidating defaults to one of the current sets if that is not too disagreeable (which is just slightly more complicated than a binary decision). Don't want to rabbit hole if there's no need. Hopefully, at the least, the blog-like documentation above will provide useful pointers to anyone that might want to tackle similar efforts in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715407619:124,optimiz,optimizations,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715407619,1,['optimiz'],['optimizations']
Performance,Thanks @gbggrant! @droazen @ldgauthier I just pushed a branch that resolves the error by simplifying the evidence-to-index cache.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-626448066:123,cache,cache,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-626448066,1,['cache'],['cache']
Performance,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:292,load,load,292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791,4,['load'],['load']
Performance,"Thanks for chiming in!. @davidbenjamin can you give a little more detail on the kind of merging operations you'd need?. @tedsharpe I believe bedGraph only allows a single annotation/track. I'm not sure if the track definition line is intended to hold any metadata other than display parameters, either? https://genome.ucsc.edu/goldenPath/help/bedgraph.html. As for the unmarked column header line, the reason I decided this would be useful in the CNV TSV formats is that it's very easy to throw the table into a pandas or R dataframe for quick analysis, where you can then use the column names to manipulate the table. Typically, pandas/R TSV loading methods let you specify the `@` comment character to strip the SAM header (although we recently ran into some trouble with this in https://github.com/broadinstitute/gatk/pull/581). Note that we *require* a single unmarked column header, which is easy enough to skip (in the case you don't want to use it) if you know it's there. On the other hand, one could argue that if we store the type of each column in the metadata, then any analysis code should technically use that to parse the table (rather than letting pandas/R automatically infer the type of each column). So a marked column header line would make quick analyses a bit more difficult (as users would need to write parsing code), but could encourage more careful downstream code practices. @SHuang-Broad Just to be clear, the way I originally used ""annotation"" refers to any quantity that could be represented by a single type in a column (not in the sense of variant annotation). If string types are allowed, this is indeed pretty flexible! All I care about extracting is the common functionality related to the fact that we have locatable columns. I think the concerns you raise about e.g. SV representation in VCF are a separate matter, but happy to discuss further. I think once we decide what the header needs to be able to represent and what it should look like, this problem is most",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329:643,load,loading,643,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329,1,['load'],['loading']
Performance,"Thanks for these questions, @tfenne, and glad you are experimenting with the workflow. Several Broad-internal groups are running various WGS and WES analyses and are seeing encouraging performance, so I’m looking forward to hearing feedback from you as well. However, I am currently indisposed and may be out for the next month or so, but @mwalker174 (who has now taken over from me as CNV tech lead, with a focus on the germline workflows) and @asmirnov239 should be able to point you in the right direction and respond to you in more detail. In the meantime, you may find some pointers in various forum posts or issues here on GitHub. We’re looking into improving documentation processes for runtime/requirements GATK-wide, which should help with this sort of thing in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6166#issuecomment-532343246:185,perform,performance,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6166#issuecomment-532343246,1,['perform'],['performance']
Performance,"Thanks, @cmnbroad!. - You're right about gatkbase-2.1.0, that image is coming from #5026, which needs some more work. We can delete it for the time being if you think it'll cause confusion.; - Correct, I think the import statement for `reshape` in BQSR.R was always incorrect/extraneous. `reshape2` is the correct dependency for `ggplot2` (which is itself imported), and `reshape` is not explicitly used in BQSR.R. So to recap: I removed the installation of this unnecessary package, but failed to remove an unnecessary import statement since it was in an untested code path, which was then caught when users tried to run the tool. Investigation of this issue then revealed that `ggplot2` was not installed correctly in the current base image, due to a completely unrelated dependency issue.; - Good call on clearing the Travis cache. Not actually sure how to do that, do I just delete the cache at https://travis-ci.org/broadinstitute/gatk/caches for this particular branch?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408447924:828,cache,cache,828,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408447924,3,['cache'],"['cache', 'caches']"
Performance,"Thanks, @droazen. While I understand the effects of the funding landscape on academic resources, it seems to me this is a full capitulation of the GATK developer team given a serious bug, especially in light of the fact that the team seems to have enough resources to continue working on Mutect3. Mutect2 has been one of the best performing variant callers of the last years and is a major reason for the Broad's good reputation in the oncology bioinformatics field. GATK and Mutect2 are used by hundreds of institutions in clinical practice, affecting thousands of real patients' lives. Almost all of these institutions are likely to use clinical WES assays due to cost reasons and will thus have been directly affected by this issue _for the last three years_. Also, almost all of these institutions will never learn of this bug since they likely trusted in the developers to have proper functional regression tests in place. If this is indeed the best the Broad can do as an institution, then I will take your offer of providing a build of Mutect2 4.1.8.1 with the log4j vulnerability patched out - thank you. The one thing that I am asking for in addition (for the sake of the overall oncology bioinformatics community), however, is that you conduct a best effort to notify organizations (universities, hospitals, and biotechs/pharmaceuticals that you know are using Mutect2) and best-practise workflow owners (Nextflow, Snakemake, WDL, CWL etc. that include Mutect2) of the forced downgrade. Also, I think it makes sense to include a very prominent warning into the Mutect2 READMEs and GATK best practice documentations and guides. I know that this is work, too, but with success comes responsibility, and I can just hope that providing proper warnings uses less developer bandwidth than applying binary search to find out which of these [10 commits between 4.1.8.1 and 4.1.9.0 that are touching variant filtering (see below)](https://github.com/broadinstitute/gatk/compare/4.1.8.1...4.1.9.0) bro",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1535909226:330,perform,performing,330,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1535909226,1,['perform'],['performing']
Performance,"That's correct, @akiezun.; However, it's not just stripping out that code. There are a ton of optimizations that can then be made to the code to simplify it afterwards. These classes were made very bulky to accommodate the indel calibration, and we should really remove that bulk. I can help with that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152047427:94,optimiz,optimizations,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152047427,1,['optimiz'],['optimizations']
Performance,"The advantage of using SLF4J is that it is a general facade, so it makes simpler to change for one logging system to other if the bound is implemented. For the most common logging systems (log4j, jul, JLC, etc.), there are this implementation and even no-op logging. One of the nice things from slf4j is that it allows to use the logging format set by the software to every library dependency, controlling the verbosity of other libraries too. . After having a look to the gradle dependencies, it seems that ADAM and Spark use slf4j. This will allow better integration with the two libraries: now the `slf4-jdk` is completely removed, and I don't know if this will blow up at some point if some of the ADAM/Spark classes try to load them. In addition, it will make GATK4 more general. Regarding features, I'm not using more that what log4j is providing, but I'm quite familiar with logback and I have a bias to use it if possible, but the GATK framework as it is implemented now ""force"" to use log4j. But anyway, I'm happy also with using log4j and I was only suggesting this for make GATK4 more general (and to come back in my work to logback, but that is just personal taste). @lbergelson, feel free to close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054:728,load,load,728,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054,1,['load'],['load']
Performance,"VD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one to set the maximum number of values allowed per chunk, so that heap usage can be controlled, but the downside is that this translates into a corresponding limit o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1915,perform,performed,1915,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['perform'],['performed']
Performance,"We decided to remove the ""conversion"" to AllelicCapseg output from ModelSegments, since this was an ill defined procedure. The models used by AllelicCapseg and AllelicCNV/ModelSegments are simply different, so it's not possible to define a unique conversion between their model parameters. Compounding this, we also had difficulty finding up-to-date documentation about the models used by various versions of both AllelicCapseg and ABSOLUTE. That said, some of this removed functionality can be found in unsupported WDLs at https://github.com/broadinstitute/gatk/tree/master/scripts/unsupported/combine_tracks_postprocessing_cnv (specifically, see the PrototypeACSConversion task in combine_tracks.wdl). These scripts also attempt to perform rudimentary filtering of germline events found in the matched normal; see first link below for some additional caveats. Note that we cannot really answer further questions or otherwise support these scripts (and it's possible that the experimental/beta GATK tools used in the WDLs may be removed in the future), and the developer responsible for them has moved on from the Broad---use them at your own risk. See also https://gatkforums.broadinstitute.org/gatk/discussion/comment/59467 https://github.com/broadinstitute/gatk/pull/5450 https://github.com/broadinstitute/gatk/issues/5804 for additional context.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6685#issuecomment-652407603:734,perform,perform,734,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6685#issuecomment-652407603,1,['perform'],['perform']
Performance,"We don't yet have good regression tests for Spark that run on a cluster and are separate from the jenkins performance tests. https://github.com/broadinstitute/gatk/issues/2298 will satisfy part of the requirements for this ticket once it's done (by catching the most basic regressions before merge), but there's also a need for larger-scale correctness tests whose status is clearly visible on github.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713:106,perform,performance,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713,1,['perform'],['performance']
Performance,"We have discussed this and I have shown @lbergelson the error of his ways ;). Admittedly I'm still working on improving the presentation of content on the website -- but user feedback suggests they find the current site far superior to the old wiki. Also, I hate wikis. Also also, Louis was mostly complaining about the dev zone and queue docs, which do suck.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151957099:333,queue,queue,333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151957099,1,['queue'],['queue']
Performance,"We're going to release the new model in 3.7 and have users test-drive that for quite a bit before we move to release 4.0, so we should be able to get some feedback on performance in the wild before we need to make any final decisions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589:167,perform,performance,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589,1,['perform'],['performance']
Performance,"Yeah, I don't like these new interface methods -- they make `GATKRead` significantly worse. We should cache `isUnmapped`, etc. in the adapter to accomplish the same thing, as @lbergelson suggests. Not that hard, and we can just unconditionally invalidate the cached values (using `Boolean` fields set to null) whenever the read is mutated in any way in order to simplify the logic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282:102,cache,cache,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282,2,['cache'],"['cache', 'cached']"
Performance,"Yes, that is easy to do. The guidance on how much memory to leave wasnt clear on the docs. Is there a rule of thumb on how much we should leave? We can do whatever makes sense. the command is something like:. ```. /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; -Djava.io.tmpdir=/mnt/scratch/prime-seq/tmp.5E76utMagnGenomicsDB_Append_Merge_2020-11-04_09-17-56-Job1 \; -Xmx104g \; -Xms104g \; -Xss2m \; -jar /home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar GenomicsDBImport \; -V <Repeated 183 times for gVCFs> \ ; --genomicsdb-update-workspace-path /home/exacloud/gscratch/prime-seq/workDir/9a2611e8-0112-1039-8c80-f8f3fc869aa5/Job1.work/WGS_Nov_1300.gdb \; --batch-size 50 \; --consolidate \; --genomicsdb-shared-posixfs-optimizations. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-724293565:742,optimiz,optimizations,742,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-724293565,1,['optimiz'],['optimizations']
Performance,"adoc for any class in that package. The integration test runs `com.sun.tools.javadoc.Main.execute` and asserts that the output code is zero, which does not yield a useful error message. In order to produce something more meaningful I hacked the test to output the entire `stdout` and `stderr` as follows:. ```; final StringWriter out = new StringWriter();; final PrintWriter err = new PrintWriter(out);. final int result = com.sun.tools.javadoc.Main.execute(""program"", err, err, err, ""doclet"",docArgList.toArray(new String[] {}));; err.flush(); // probably not needed; String message = out.toString(); // message contains the entire stdout and stderr of the call to execute; Assert.assertEquals(result, 0, message);; ```. The output is about 2000 lines, but a lot of it is clearly innocuous. Removing lines such as; * `2022-08-16T00:09:07.2336106Z [parsing completed 1ms]`; * `2022-08-16T00:09:07.4456202Z [loading ZipFileIndexFileObject[/jars/gatk-package-4.2.6.1-56-gad9a538-SNAPSHOT-test.jar(org/broadinstitute/hellbender/tools/walkers/variantutils/ReblockGVCFIntegrationTest.class)]]`; * `2022-08-16T00:09:07.4459732Z [loading RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalIntervalArgumentCollection.java]]`; * `2022-08-16T00:09:07.4462012Z [parsing started RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalReferenceInputArgumentCollection.java]]`; * 2022-08-16T00:09:07.2322755Z [loading ZipFileObject[/gatk/gatk-package-unspecified-SNAPSHOT-local.jar(htsjdk/samtools/SAMSequenceDictionary.class)]]. brings it down to 353 lines, the majority of which look like . ```2022-08-16T00:09:07.4435974Z src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java:479: error: cannot find symbol; 2022-08-16T00:09:07.4436105Z @VisibleForTesting; ```. Here's that 353-line file:. [log-no-parsing-loading.txt](https://github.com/broadinstitute/gatk/files/9355026/log-no-parsing-loading.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488:1306,load,loading,1306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488,4,['load'],['loading']
Performance,"avadoc.doclet](https://docs.oracle.com/en/java/javase/11/docs/api/jdk.javadoc/jdk/javadoc/doclet/package-summary.html). The javadoc tools in `org.broadinstitute.hellbender.utils.help` may need to be re-written (and it's not clear if it's possible to support Java 8 and Java 11 simultaneously).; * Travis build. Getting this to build and test on Java 11 in addition to the current builds may be fairly involved as the matrix is already quite complicated. (The current PR just changes Java 8 to Java 11 for testing purposes - we'd need a way of getting both to run.). The vast majority of tests are passing on Java 11, the following are failing:; * Missing `TwoBitRecord` (from ADAM); * `ReferenceMultiSparkSourceUnitTest`; * `ImpreciseVariantDetectorUnitTest`; * `SVVCFWriterUnitTest`; * `DiscoverVariantsFromContigAlignmentsSAMSparkIntegrationTest`; * `StructuralVariationDiscoveryPipelineSparkIntegrationTest`; * `SvDiscoverFromLocalAssemblyContigAlignmentsSparkIntegrationTest`; * `java.lang.NoSuchMethodError: java.nio.ByteBuffer.clear()Ljava/nio/ByteBuffer;`; * `SeekableByteChannelPrefetcherTest`; * `GatherVcfsCloudIntegrationTest`; * `Could not serialize lambda`; * `ExampleAssemblyRegionWalkerSparkIntegrationTest`; * `PileupSparkIntegrationTest`; * Native HMM library code caused the tests to crash on my Mac:; ```; Running Test: Test method testLikelihoodsFromHaplotypes[0](org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM@6282d367, true)(org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest); dyld: lazy symbol binding failed: can't resolve symbol __ZN13shacc_pairhmm9calculateERNS_5BatchE in /private/var/folders/cj/wyp4zgw17vj4m9qdmddvmcc80000gn/T/libgkl_pairhmm13775554937319419112.dylib because dependent dylib #1 could not be loaded; dyld: can't resolve symbol __ZN13shacc_pairhmm9calculateERNS_5BatchE in /private/var/folders/cj/wyp4zgw17vj4m9qdmddvmcc80000gn/T/libgkl_pairhmm13775554937319419112.dylib because dependent dylib #1 could not be loaded; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359:2536,load,loaded,2536,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359,2,['load'],['loaded']
Performance,"c/user/farrell/.sparkStaging/application_1542127286896_0153/__spark_libs__7473738539612638927.zip; 2019-01-07 11:33:38 INFO Client:54 - Uploading resource file:/tmp/spark-1ac79f09-1a36-4668-92d9-0739775f98ed/__spark_conf__4147634812449814799.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0153/__spark_conf__.zip; 2019-01-07 11:33:38 INFO SecurityManager:54 - Changing view acls to: farrell; 2019-01-07 11:33:38 INFO SecurityManager:54 - Changing modify acls to: farrell; 2019-01-07 11:33:38 INFO SecurityManager:54 - Changing view acls groups to:; 2019-01-07 11:33:38 INFO SecurityManager:54 - Changing modify acls groups to:; 2019-01-07 11:33:38 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-07 11:33:38 INFO Client:54 - Submitting application application_1542127286896_0153 to ResourceManager; 2019-01-07 11:33:38 INFO YarnClientImpl:251 - Submitted application application_1542127286896_0153; 2019-01-07 11:33:38 INFO SchedulerExtensionServices:54 - Starting Yarn extension services with app application_1542127286896_0153 and attemptId None; 2019-01-07 11:33:39 INFO Client:54 - Application report for application_1542127286896_0153 (state: ACCEPTED); 2019-01-07 11:33:39 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1546878818531; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0153/; user: farrell; 2019-01-07 11:33:40 INFO Client:54 - Application report for application_1542127286896_0153 (state: ACCEPTED); 2019-01-07 11:33:41 INFO Client:54 - Application report for application_1542127286896_0153 (state: ACCEPTED); 2019-01-07 11:33:42 INFO Client:54 - Applic",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:13432,queue,queue,13432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['queue'],['queue']
Performance,"c/user/farrell/.sparkStaging/application_1542127286896_0166/__spark_libs__7821719163562430010.zip; 2019-01-09 13:35:22 INFO Client:54 - Uploading resource file:/tmp/spark-69cc5c72-eff6-4259-8b3b-12fa6f8c42b0/__spark_conf__4520928824604875683.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0166/__spark_conf__.zip; 2019-01-09 13:35:22 INFO SecurityManager:54 - Changing view acls to: farrell; 2019-01-09 13:35:22 INFO SecurityManager:54 - Changing modify acls to: farrell; 2019-01-09 13:35:22 INFO SecurityManager:54 - Changing view acls groups to:; 2019-01-09 13:35:22 INFO SecurityManager:54 - Changing modify acls groups to:; 2019-01-09 13:35:22 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-09 13:35:22 INFO Client:54 - Submitting application application_1542127286896_0166 to ResourceManager; 2019-01-09 13:35:22 INFO YarnClientImpl:251 - Submitted application application_1542127286896_0166; 2019-01-09 13:35:22 INFO SchedulerExtensionServices:54 - Starting Yarn extension services with app application_1542127286896_0166 and attemptId None; 2019-01-09 13:35:23 INFO Client:54 - Application report for application_1542127286896_0166 (state: ACCEPTED); 2019-01-09 13:35:23 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1547058922320; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0166/; user: farrell; 2019-01-09 13:35:24 INFO Client:54 - Application report for application_1542127286896_0166 (state: ACCEPTED); 2019-01-09 13:35:25 INFO Client:54 - Application report for application_1542127286896_0166 (state: ACCEPTED); 2019-01-09 13:35:26 INFO Client:54 - Applic",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:13171,queue,queue,13171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['queue'],['queue']
Performance,"ch can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorData",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:2222,perform,performing,2222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['perform'],['performing']
Performance,"created for the TH evaluation:. Here is the 100% tumor run through ModelSegments, which yields 130 segments:; ![T modeled](https://user-images.githubusercontent.com/11076296/76560309-43d2dd80-6477-11ea-87d2-75d430d220a2.png). Here is the 75% tumor + 25% normal mixture run through ModelSegments, which yields 83 segments:; ![N-25-T-75 modeled](https://user-images.githubusercontent.com/11076296/76558609-fd2fb400-6473-11ea-9bd3-293a66eb3e4e.png). Here is the 25% tumor + 75% normal mixture run through ModelSegments, which yields 50 segments:; ![N-75-T-25 modeled](https://user-images.githubusercontent.com/11076296/76560439-85638880-6477-11ea-8d8d-f0f9a11d70a6.png). We can compare against the new workflow, in which we run SegmentJointSamples to jointly segment on the two mixtures. This yields a joint-sample segmentation with 162 segments, which can be passed as an additional input to individual ModelSegments runs on the two mixtures. It is used as the initial segmentation for both runs, after which the usual modelling and smoothing steps are performed. For the 75% tumor + 25% normal mixture, this yields 122 segments (up from 83):; ![N-25-T-75-SJS modeled](https://user-images.githubusercontent.com/11076296/76558618-015bd180-6474-11ea-996a-48d39770149b.png). For the 25% tumor + 75% normal mixture, this yields 105 segments (up from 50):; ![N-75-T-25-SJS modeled](https://user-images.githubusercontent.com/11076296/76560726-34a05f80-6478-11ea-9027-a54726c46b9e.png). One could imagine that smoothing could be disabled (so that all samples retain the common segmentation after modeling) or made more aggressive (so that private events don't get inadvertently introduced into other samples due to noise, perhaps), depending on the use case. It looks like the joint segmentation allows some additional events to be resolved, although I haven't done any rigorous evaluations. We could probably cook up some evaluations using simulated toy data or in silico mixtures, but there's really no reaso",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598386823:1187,perform,performed,1187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598386823,1,['perform'],['performed']
Performance,"d I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:2278,perform,performed,2278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['perform'],['performed']
Performance,"d positives and negatives and use the existing code for extracting labels, but this will require a bit of engineering and be more trouble than it's worth. There are other options---see https://ir.cwi.nl/pub/30479, for example. We might want to experiment with the LL score discussed there (see https://www.aaai.org/Papers/ICML/2003/ICML03-060.pdf for the original paper---although note that despite the paper's high citation count, I'm not sure what the canonical name for this metric actually is, but it doesn't appear to be ""LL score""---perhaps someone else knows or has better Google-fu and can figure it out) before moving on to their methods for estimating F1. Doing a literature search for other discussions of optimizing F1 or other metrics in the context of positive-unlabeled learning might be worthwhile, but I think most methods will probably involve some sort of estimation of the base rate in unlabeled data. I think we may have to add some mechanism for holding out a validation set during training if we want to automatically tune thresholds in a rigorous fashion. Shouldn't be too bad---we can just have the training tool randomly mask out a set of the truth and pass the mask to the scoring tool (or maybe just determine the threshold in the training tool, if we are running in positive/negative mode and have access to unlabeled data)---but does add a couple of parameters to the tool interfaces. This also adds additional dependence on the quality of the truth resources. I think an implicit assumption in any use of the truth---even just thresholding/calibrating by sensitivity---is that it is a random sample; however, I'm not sure how true this is in actual use. For example, in malaria, it looks like we may have to resort to using a callset that has been very conservatively filtered as truth, which will bias us towards high scores and the peaks of the positive distribution. Perhaps we can also experiment with just treating training/truth on an equal footing (I think the d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241:1316,tune,tune,1316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241,1,['tune'],['tune']
Performance,"d try to do it, as that is a relatively expensive resource to create. For example, some very naive hard filtering (red) of the histogram yields a peak that is easily fit by a negative binomial (green)---even a Poisson fit does not appear to bias the depth estimates, and certainly does not result in incorrect ploidy estimates:. ![masked_fit](https://user-images.githubusercontent.com/11076296/37863641-827a6e8a-2f37-11e8-83d5-cb4af32a898b.png). (Incidentally, it is helpful to plot on a log scale when checking the fit of these distributions.). This strategy also gives us a way to ignore low-level mosaicism or large germline events, which filtering on mappability may not address:. ![mosaic](https://user-images.githubusercontent.com/11076296/37863649-d0ac378c-2f37-11e8-8e98-45e1fa9a3d7a.png). So let's try to encapsulate changes to the ploidy tool. I agree that the histogram creation can be easily done on the Java side, to save on intermediate file writing. We can probably just cap the maximum bin to `k` and pass a samples x contig TSV where each entry is a vector with `k + 1` elements. I agree that there is still a lot of important work to be done in exploring our best practices for coverage collection, and I know that you have been interested in improving them for a while. Ultimately, we may want to consider incorporating mappability or other informative metadata, as we've discussed. However, this will require some non-trivial investment in method/tool development time. Since our preliminary evaluations show that even with the current, naive strategies the tool is performing reasonably well, I am prioritizing cutting a release and improving/automating the evaluations. As we discussed, this will both allow users to start using the tool (which will hopefully result in useful feedback) and establish a baseline for us. This will ultimately provide the necessary foundation for future exploratory work and method development---which always takes more time than we think it will!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375881040:2153,perform,performing,2153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375881040,1,['perform'],['performing']
Performance,"done with my review. small edits, 1 bug, and a few performance questions. I'd like to see a simple perf run of HC with and without those changes. All those streams in math-heavy tight loops make me concerned a bit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231096260:51,perform,performance,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231096260,1,['perform'],['performance']
Performance,"e GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, esp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:1359,tune,tuned,1359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,2,"['perform', 'tune']","['performance', 'tuned']"
Performance,"e forceValidOutput=false filter_reads_with_N_cigar=false filter_mismatching_base_and_quals=false filter_bases_not_stored=false"">; ##GATKCommandLine.SelectVariants=<ID=SelectVariants,Version=3.4-228-g2497091,Date=""Tue Jan 05 13:29:45 EST 2016"",Epoch=1452018585661,CommandLineOptions=""analysis_type=SelectVariants input_file=[] showFullBamList=false read_buffer_size=null phone_home=AWS gatk_key=null tag=NA read_filter=[] disable_read_filter=[] intervals=null excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/humgen/1kg/reference/human_g1k_v37.fasta nonDeterministicRandomSeed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 static_quantized_quals=null round_down_quantized=false disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=false never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 reference_window_stop=0 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=var",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834:4069,perform,performanceLog,4069,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834,1,['perform'],['performanceLog']
Performance,"e to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one to set the maximum number of values allowed per chunk, so that heap usage can be controlled, but the downside is that this translates into a corresponding limit on the number of columns (i.e., intervals). On the other hand, you could theoretically crank this number up to Integer.MAX_VALUE, as long as you set -Xmx high enough... In practice, it's very unlikely that we'll need to go to bins smaller than a read length.); - [ ] <s>Check that CreatePanelOfNormals works correctly on Spark cluster.</s> Implement Randomized SVD, which should give bet",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:2412,perform,performing,2412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['perform'],['performing']
Performance,"e up 20GB as a combined TSV file and a whopping 63GB as individual TSV files---as well as the eigenvectors, filtering results, etc.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if it is not already overkill. To go bigger by more than an order of magnitude, we'll have to go out of core, which will require more substantial changes to the code. But since the real culprit responsible ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:1167,perform,performing,1167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503,1,['perform'],['performing']
Performance,"ed to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot re",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:3795,bottleneck,bottleneck,3795,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['bottleneck'],['bottleneck']
Performance,"eed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than trying to figure out where the old one is failing would be best.; - [x] For small bins (250bp), the copy-ratio model is currently a bit memory intensive, since it stores an outlier indicator boolean for every data point (it gets by with -Xmx12g for 100 iterations at 250bp). There is no easy away around storing this at the GibbsSampler level (although we could make some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we cou",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:6616,perform,performing,6616,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['perform'],['performing']
Performance,enFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; 18/04/24 17:42:11 INFO ShutdownHookManager: Shutdown hook called; 18/04/24 17:42:11 INFO ShutdownHookManager: Deleting directory /tmp/username/spark-99d4cb79-5c44-425b-8f72-9476e7fd884c; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:46039,concurren,concurrent,46039,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['concurren'],['concurrent']
Performance,"erent results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - [x] <s>Investigate the effect of keeping duplicates. I am still not sure why we do this, and it may have a more drastic impact on WGS data.</s> Turns out we don't keep duplicates for WGS; see #3367.; - [ ] Check that GC-bias-correction+PCA and PCA-only perform comparably, even at small bin sizes (~300bp). From what I've seen, this is true for larger bin sizes (~3kbp), so explicit GC-bias correction may not be necessary. (That is, even at these (purportedly) large bin sizes, the effect of the read-based GC-bias correction is obvious for those samples where it is important. However, the end result is not very different from PCA-only denoising with no GC-bias correction performed.); - [x] <s>Check that changing CBS alpha parameter sufficiently reduces hypersegmentation.</s> <s>Looks like the hybrid p-value calculation in DNACopy is not accurate enough to handle WGS-size data. (Also, it's relatively slow, taking ~30 minutes on ~10M intervals.) Even if I set alpha to 0, I still get a ridiculous number of segments! So I think it's finally time to scrap CBS. I'll look into other R segmentation packages that might give us a quick drop-in solution, but we may want to roll our own simple algorithm (which we will scrap anyway once the coverage model is in for somatic).</s> I've implemented a fast kernel-segmentation method that seems very promising, see below.; - [ ] Investigate performance vs. CGA ReCapSeg pipeline on THCA samples.; - [ ] Investigate concordance with Genome STRiP.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:5096,perform,performed,5096,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,2,['perform'],"['performance', 'performed']"
Performance,"events are not mosaic CNLOH, then we should clean up all mention of CNLOH in this code. Either way, can we quantify the level of improvement gained by filtering such events in a reproducible evaluation? If so, let's bring that into gatk-evaluation. Finally, there are many more options available to change the segmentation and/or resolution than the single one you mentioned. If the users you are working with can clearly specify their analysis goals in terms of resolution, then it might be possible to sidestep the problem entirely without adding more unsupported code. This would also buy us more time to put in a principled solution, without the risk of unsupported code getting entrenched in their workflows. > There are definitely events that get missed without the germline tagging, so this is an improvement over blacklisting alone. And while I have seen erroneous germline tagging (i.e. false calling a segment germline), it was only ever due to really noisy data (e.g. a bad PoN) or a poorly tuned segment caller. This is encouraging. This means that a straightforward approach to germline filtering, such as simply identifying overlapping posteriors as mentioned above, should work well. Prototyping this approach shouldn't take long at all, especially when the matched normal is guaranteed to be available, as it is in this workflow (tumor-only would require some work to identify the normal state, as mentioned previously). I'd rather just roll that, evaluate it, and merge it instead. Key here is that we sidestep the deficiencies of the current CR-only caller, which also shares the blame for this ""CNLOH"" issue (since these events aren't called in the normal and don't become candidates for tagging, as currently implemented). > And this would be a possible ""better solution"" Shall I file an issue for this? This could also allow us to obviate the TagGermline tool, which is fine by me. I've already expanded the scope of https://github.com/broadinstitute/gatk/issues/4115 to include t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199:3501,tune,tuned,3501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199,1,['tune'],['tuned']
Performance,"flow. We can add a single BAM case workflow or expand the matched-pair workflow to handle this, depending on the discussion at https://github.com/broadinstitute/gatk/issues/3657.; - WES/WGS is toggled by providing an optional target-file input.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] O",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:1544,perform,performed,1544,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['perform'],['performed']
Performance,"fter which the usual modelling and smoothing steps are performed. For the 75% tumor + 25% normal mixture, this yields 122 segments (up from 83):; ![N-25-T-75-SJS modeled](https://user-images.githubusercontent.com/11076296/76558618-015bd180-6474-11ea-996a-48d39770149b.png). For the 25% tumor + 75% normal mixture, this yields 105 segments (up from 50):; ![N-75-T-25-SJS modeled](https://user-images.githubusercontent.com/11076296/76560726-34a05f80-6478-11ea-9027-a54726c46b9e.png). One could imagine that smoothing could be disabled (so that all samples retain the common segmentation after modeling) or made more aggressive (so that private events don't get inadvertently introduced into other samples due to noise, perhaps), depending on the use case. It looks like the joint segmentation allows some additional events to be resolved, although I haven't done any rigorous evaluations. We could probably cook up some evaluations using simulated toy data or in silico mixtures, but there's really no reason why this shouldn't work decently well, especially if the kernel-segmentation method works well on a single sample for your data. It would also be interesting to understand at which point changing segmentation parameters on a single sample can no longer yield the same performance as joint segmentation on a fixed number of samples; however, this is probably a function of various S/N ratios, and it might not be easy to characterize this behavior outside of toy data. The segmentation parameter space is big enough to make this unwieldy even for toy data, too. Perhaps we can get some feedback from test users---not only on performance, but also on the structure of the new workflow. It might also be worth gauging whether a new WDL is warranted. Otherwise, we just need to add some unit tests for correctness of the multisample-segmentation backend class, integration tests for plumbing of the new tool, and perhaps address some of the issues mentioned above. Then I'd say this is good to go.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598386823:2407,perform,performance,2407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598386823,2,['perform'],['performance']
Performance,"ges.githubusercontent.com/11076296/29582016-23fbc6a6-8749-11e7-951e-f618e8489a0b.png). ![4](https://user-images.githubusercontent.com/11076296/29582044-3eb20a1e-8749-11e7-84a0-3734bad15e1f.png). ![5](https://user-images.githubusercontent.com/11076296/29582047-410ac490-8749-11e7-8a98-b2098cf1b5ea.png); 4) For each of these cost functions, find (up to) the _C<sub>max</sub>_ most significant local minima. The problem of finding local minima of a noisy function can be solved by using topological persistence (e.g., https://people.mpi-inf.mpg.de/~weinkauf/notes/persistence1d.html and http://www2.iap.fr/users/sousbie/web/html/indexd3dd.html?post/Persistence-and-simplification). A straightforward watershed algorithm can sort all local minima by persistence in linear time after an initial sort of the data.; 5) These sets of local minima from all window sizes together provide the pool of candidate changepoints (some of which may overlap exactly or approximately). We perform backwards selection using the global segmentation cost. That is, we compute the global segmentation cost given all the candidate changepoints, calculate the cost change for removing each of the changepoints individually, remove the changepoint with the minimum cost change, and repeat. This gives the global cost as a function of the number of changepoints _C_.; 6) Add a penalty _a C + b C log(N / C)_ to the global cost and find the minimum to determine the number of changepoints. For the above simulated data, _a = 2_ and _b = 2_ works well, recovering all of the changepoints in the above example with no false positives:; ![6](https://user-images.githubusercontent.com/11076296/29582517-fbd604be-874a-11e7-8ef7-7bd727f65dcb.png). ![7](https://user-images.githubusercontent.com/11076296/29582518-fddf015c-874a-11e7-89e4-87250d2a52ab.png). In contrast, CBS produces two false positives (around the third and seventh of the true changepoints):. ![8](https://user-images.githubusercontent.com/11076296/29582545-18875126-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:2312,perform,perform,2312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586,1,['perform'],['perform']
Performance,"gion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlying craziness); 2. Change the ActiveRegion traversal so that it respects the dcov value (could be hard -- the LIBS downsampling process discards reads on-the-fly from previous loci when moving to a new locus, but an active region involves data for multiple loci. The potential performance win for the HC is huge, though, if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads in memory is actually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in Locu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:4890,perform,performance,4890,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['perform'],['performance']
Performance,"gion.java:139); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:287); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:314); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:802); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.forceFlush(AbstractNioChannel.java:319); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:637); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:748); 18/04/24 17:42:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:42:11 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:42:11 INFO BlockManager: BlockManager stopped; 18/04/24 17:42:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:42:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:42:11 INFO SparkContext: Successfully stopped SparkContext; 17:42:11.053 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:42:11 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=866648064; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:38177,concurren,concurrent,38177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"given that we only have easy access to scores for positive truth---and hence, no false positives, which precludes calculation of precision and F1. I *think* we could pass a VCF for a sample with gold-standard positives and negatives and use the existing code for extracting labels, but this will require a bit of engineering and be more trouble than it's worth. There are other options---see https://ir.cwi.nl/pub/30479, for example. We might want to experiment with the LL score discussed there (see https://www.aaai.org/Papers/ICML/2003/ICML03-060.pdf for the original paper---although note that despite the paper's high citation count, I'm not sure what the canonical name for this metric actually is, but it doesn't appear to be ""LL score""---perhaps someone else knows or has better Google-fu and can figure it out) before moving on to their methods for estimating F1. Doing a literature search for other discussions of optimizing F1 or other metrics in the context of positive-unlabeled learning might be worthwhile, but I think most methods will probably involve some sort of estimation of the base rate in unlabeled data. I think we may have to add some mechanism for holding out a validation set during training if we want to automatically tune thresholds in a rigorous fashion. Shouldn't be too bad---we can just have the training tool randomly mask out a set of the truth and pass the mask to the scoring tool (or maybe just determine the threshold in the training tool, if we are running in positive/negative mode and have access to unlabeled data)---but does add a couple of parameters to the tool interfaces. This also adds additional dependence on the quality of the truth resources. I think an implicit assumption in any use of the truth---even just thresholding/calibrating by sensitivity---is that it is a random sample; however, I'm not sure how true this is in actual use. For example, in malaria, it looks like we may have to resort to using a callset that has been very conservat",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241:992,optimiz,optimizing,992,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241,1,['optimiz'],['optimizing']
Performance,"hat's happening. We wouldn't expect gatk4 haplotype caller to be that much slower. . It looks like they're running beta2 which is kind of old as well. Can you ask them what exact version they're using?. Can you ask if they have the log (stdout + stderr) for the gatk4 non-spark run? I can't tell what pairhmm they're actually running with and the logs would help with that. . Can you also find out what sort of hardware they're running on? Specifically, is it an intel machine with support for AVX?. A good setting for` --nativePairHmmThreads` is probably 4-8, you won't see any improvement after that. I also noticed that they're setting -XX:+UseParallelGC -XX:ParallelGCThreads=32 for the gatk3. They would be better off setting it to 2-4 threads. Performance gets worse beyond that typically from what I've seen. They can set the same thing for gatk4 using`--javaOptions ' -XX:+UseParallelGC -XX:ParallelGCThreads=4'`. Their spark configuration looks wrong in a number of ways which is probably a big part of why they're not seeing any improvement. In general you want executors with ~4-8 cores and at least 4g of memory per core. I don't know how much memory their nodes have, and I don't know if they're running with autoscaling turned on, but I suspect they're only allocating 1 executor on 1 node and then it's thrashing memory because it's trying to run 32 threads at once. Spark tuning for haplotype caller is going to be complicated though and I don't know how to do it will yet, we will be revisiting it in the next quarter probably. They're also running withs spark 2.1.0, we currently require spark 2.0.2 which is an unfortunately specific version, we're planning on upgrading to spark 2.2.+ in the next quarter. . You should make it clear to them that the results will not be the same between 3, 4, and 4-spark yet and that 4 is in rapid state of flux and has known performance issues that we're planning on working soon. Even so though, that slowdown they're seeing is bizarrely large.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332879964:1917,perform,performance,1917,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332879964,1,['perform'],['performance']
Performance,"he RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2016-02-14 19:04:19 WARN MetricsSystem:71 - Using default name DAGScheduler for source because spark.app.id is not set.; MinimalVariant -- interval(1:737406-737411), snp(false), indel(true); 19:04:24.266 INFO PrintVCFSpark - Shutting down engine; [February 14, 2016 7:04:24 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark done. Elapsed time: 0.14 minutes.; Runtime.totalMemory()=90177536; ```. This seems to have the ability to load a VCF as a JavaRDD. Let me know if this is what you were looking for, and sorry if I am assuming this solves the problem. Thanks and hope you're keeping warm :); Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:2182,load,load,2182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857,2,['load'],['load']
Performance,"he per-contig coverage histograms (unfiltered bins in blue, bins retained after filtering in red, and negative-binomial fit in green) and a heatmap of per-contig ploidy probabilities. Both the panel (first 20) and case (remaining) samples are shown:. ![prototype-result](https://user-images.githubusercontent.com/11076296/37938642-e9fbd804-312c-11e8-8a6c-02ea4e4fa704.png). Although the prototype model is clearly a good fit to the filtered data, some care in choosing the optimizer and its learning parameters is required to achieve convergence to the correct solution. This is because the problem is inherently multimodal and thus there are many local minima. I found that using AdaMax with a naive strategy of warm restarts (to help kick us out of local minima) worked decently; we can achieve convergence in <10 minutes for 60 samples x 24 contigs x 250 count bins:. ![elbo](https://user-images.githubusercontent.com/11076296/37938658-fc176f12-312c-11e8-89e2-40c68e0f9953.png). I expect that @mbabadi's annealing implementation in the gcnvkernel package will handle the local minima much better. The course of action needed to implement this model should be as follows:. 1) Alter Java code to emit per-contig histograms. Change python code to consume histograms, perform filtering, and fit using the above model (or some variation).; 2) Choose learning parameters appropriate with annealing and check that results are still good.; 3) Update gCNV model to consume the depth emitted by this model properly, if necessary, and rerun evaluations. Other improvements enabled by mappability filtering (as discussed in #4558) or coverage collection can follow this initial model revision. In the meantime, we will continue the first round of evaluations using the old ploidy model, spot checking genotype calls as necessary. This will allow us to tune gCNV parameters (which will hopefully be largely unaffected by any changes to the ploidy model). How does this sound, @ldgauthier @mbabadi @asmirnov239?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271:3289,perform,perform,3289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271,2,"['perform', 'tune']","['perform', 'tune']"
Performance,"imation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than trying to figure out where the old one is failing would be best.; - [x] For small bins (250bp), the copy-ratio model is currently a bit memory intensive, since it stores an outlier indicator boolean for every data point (it gets by with -Xmx12g for 100 iterations at 250bp). There is no easy away around storing this at the GibbsSampler level (although we could make some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we could move to a BitSet, but I'm not sure what the performance hit will be. EDIT: It was trivial to switch over to a BitSet, which seems to let us get away with -Xmx8g instead of -Xmx12g. Calling:; - I've ported over the naive `ReCapSegCaller` wholesale. This can take in the output of `ModelSegments`, so we can take advantage of the improved segmentation as before, but we still don't use the modeled minor-allele fractions when making calls. The method for copy-ratio calling is also extremely naive, with hardcoded bounds for identifying the copy-neutral state.; - [ ] @MartonKN is going to work on an improved caller for his next project. This caller should also make simple calls (not full allelic copy number, but just `0`, `+`, `-`), but should also take advantage of the copy-ratio and minor-allele fraction posteriors estimated by `ModelSegments` to generate quality scores. Plotting:; - Other than the allele-fraction model, the limiting factor was the original plotting code (some plotting runs originally to",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:7677,perform,performance,7677,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['perform'],['performance']
Performance,"ld code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old all",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:3504,perform,perform,3504,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['perform'],['perform']
Performance,"ll overlap the locus post-realignment. The end result is that DP for the HaplotypeCaller represents the undownsampled depth of coverage at the locus in question, subject to the hardcoded cap of 1000 and realignment to the haplotypes. In this particular case, the actual depth at locus 1:14464 is 561 (with no downsampling), and the DP value is 546. The difference is likely due to realignment of reads to the haplotypes by the walker. So, we basically have two options:; 1. Change the documentation for the DP annotation to mention that for ActiveRegion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlying craziness); 2. Change the ActiveRegion traversal so that it respects the dcov value (could be hard -- the LIBS downsampling process discards reads on-the-fly from previous loci when moving to a new locus, but an active region involves data for multiple loci. The potential performance win for the HC is huge, though, if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads in memory is actually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:4331,perform,performance,4331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['perform'],['performance']
Performance,loaded for pull request base (`master@d9fd22f`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `12.5%`. ```diff; @@ Coverage Diff @@; ## master #5787 +/- ##; ==========================================; Coverage ? 44.104% ; Complexity ? 19589 ; ==========================================; Files ? 1973 ; Lines ? 147147 ; Branches ? 16215 ; ==========================================; Hits ? 64898 ; Misses ? 77129 ; Partials ? 5120; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5787?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...utils/activityprofile/ActivityProfileUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5787/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9hY3Rpdml0eXByb2ZpbGUvQWN0aXZpdHlQcm9maWxlVW5pdFRlc3QuamF2YQ==) | `0.442% <0%> (ø)` | `1 <0> (?)` | |; | [...ils/optimization/PersistenceOptimizerUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5787/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL29wdGltaXphdGlvbi9QZXJzaXN0ZW5jZU9wdGltaXplclVuaXRUZXN0LmphdmE=) | `2% <0%> (ø)` | `1 <0> (?)` | |; | [...utils/downsampling/DownsamplingMethodUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5787/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9kb3duc2FtcGxpbmcvRG93bnNhbXBsaW5nTWV0aG9kVW5pdFRlc3QuamF2YQ==) | `3.448% <0%> (ø)` | `1 <0> (?)` | |; | [...yper/StandardCallerArgumentCollectionUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5787/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9TdGFuZGFyZENhbGxlckFyZ3VtZW50Q29sbGVjdGlvblVuaXRUZXN0LmphdmE=) | `4.098% <0%> (ø)` | `2 <0> (?)` | |; | [...lbender/utils/mcmc/ParameterizedStateUnitTest.java](https://codecov,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-471963750:1097,optimiz,optimization,1097,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-471963750,1,['optimiz'],['optimization']
Performance,nfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(username); groups with view permissions: Set(); users with modify permissions: Set(username); groups with modify permissions: Set(); 18/04/24 17:39:20 INFO Utils: Successfully started service 'sparkDriver' on port 46576.; 18/04/24 17:39:20 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:39:20 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:39:20 INFO BlockManagerMasterEndpo,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:7078,load,load,7078,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['load'],['load']
Performance,"nsferTo(FileChannelImpl.java:608); at io.netty.channel.DefaultFileRegion.transferTo(DefaultFileRegion.java:139); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:287); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:314); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:802); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.forceFlush(AbstractNioChannel.java:319); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:637); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:748); 18/04/24 17:42:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:42:11 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:42:11 INFO BlockManager: BlockManager stopped; 18/04/24 17:42:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:42:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:42:11 INFO SparkContext: Successfully stopped SparkContext; 17:42:11.053 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:42:11 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=866648064; org.apache.spark.SparkException: Job aborted due to stage failure: Tas",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:38080,concurren,concurrent,38080,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"ntutils` package, which is strange because the PR did not modify the javadoc for any class in that package. The integration test runs `com.sun.tools.javadoc.Main.execute` and asserts that the output code is zero, which does not yield a useful error message. In order to produce something more meaningful I hacked the test to output the entire `stdout` and `stderr` as follows:. ```; final StringWriter out = new StringWriter();; final PrintWriter err = new PrintWriter(out);. final int result = com.sun.tools.javadoc.Main.execute(""program"", err, err, err, ""doclet"",docArgList.toArray(new String[] {}));; err.flush(); // probably not needed; String message = out.toString(); // message contains the entire stdout and stderr of the call to execute; Assert.assertEquals(result, 0, message);; ```. The output is about 2000 lines, but a lot of it is clearly innocuous. Removing lines such as; * `2022-08-16T00:09:07.2336106Z [parsing completed 1ms]`; * `2022-08-16T00:09:07.4456202Z [loading ZipFileIndexFileObject[/jars/gatk-package-4.2.6.1-56-gad9a538-SNAPSHOT-test.jar(org/broadinstitute/hellbender/tools/walkers/variantutils/ReblockGVCFIntegrationTest.class)]]`; * `2022-08-16T00:09:07.4459732Z [loading RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalIntervalArgumentCollection.java]]`; * `2022-08-16T00:09:07.4462012Z [parsing started RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalReferenceInputArgumentCollection.java]]`; * 2022-08-16T00:09:07.2322755Z [loading ZipFileObject[/gatk/gatk-package-unspecified-SNAPSHOT-local.jar(htsjdk/samtools/SAMSequenceDictionary.class)]]. brings it down to 353 lines, the majority of which look like . ```2022-08-16T00:09:07.4435974Z src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java:479: error: cannot find symbol; 2022-08-16T00:09:07.4436105Z @VisibleForTesting; ```. Here's that 353-line file:. [log-no-parsing-loading.txt](https://",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488:1090,load,loading,1090,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488,1,['load'],['loading']
Performance,"on appears to have a memory leak or using just requiring too much memory. The -consolidate option was the culprit. So rerunning chr1-3 with just the --bypass-feature-reader option (test2) ran fine without lots of memory being used. Below is the time output from chr1. The output shows the Maximum resident set size (kbytes): **2630440**. Using GATK jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar defined in environment variable GATK_LOCAL_JAR; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx200g -Xms16g -jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace; Command being timed: ""gatk --java-options -Xmx200g -Xms16g GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace""; User time (seconds): 270716.45; System time (seconds): 1723.34; Percent of CPU this job got: 99%; Elapsed (wall clock) time (h:mm:ss or m:ss): 76:08:24; Average shared text size (kbytes): 0; Average unshared data size (kbytes): 0; Average stack size (kbytes): 0; Average total size (kbytes): 0; Maximum resident set size (kbytes): 2630440; Average resident set size (kbytes): 0; Major (requiring I/O) page faults: 5; Minor (reclaiming a frame) page faults: 206030721; Voluntary context switches: 11129822; Involuntary context switches: 176522; Swaps: 0; File system inputs: 627981312; File system outputs: 466730160; Socke",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687:1912,optimiz,optimizations,1912,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687,1,['optimiz'],['optimizations']
Performance,"ore modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code instead.; - [x] In the meantime, we can try to speed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density est",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:5690,bottleneck,bottleneck,5690,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['bottleneck'],['bottleneck']
Performance,"ot sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage if there is no data for known indels. I will propose very soon two PRs with fully functional tools (without the n-way out feature for indel-realignment), and trying to add simple integration tests with the data already available on the repository and running with GATK3.8-1. If that is OK for you, I will proceed with this approach.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:1619,perform,performed,1619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115,1,['perform'],['performed']
Performance,"ound first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether any given point is a suitable changepoint; using the data locally surrounding the point typically suffices. With these modifications, ApproxKernSeg can handle both scenarios:; ![wave-kern](https://user-images.githubusercontent.com/11076296/29322762-a679dba6-81ac-11e7-9360-083a4e1da398.png); ![wave-kern-small-waves](https://user-images.githubusercontent.com/11076296/29322801-dad82010-81ac-11e7-8238-e057b0072e1b.png). This local window approach is still linear in time, so runtime is still ~1s for the above (about ~10x faster than CBS). One issue still remains, which is that even this improved approach tends to find directly adjacent possible changepoints around a true changepoint before moving on to another true changepoint. We can probably clean this up with some simple postprocessing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:1964,perform,perform,1964,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045,1,['perform'],['perform']
Performance,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:12 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:31 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.24, executor 1, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:44322 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:41:49 INFO BlockManagerInfo: Added broadcast_0",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:29089,concurren,concurrent,29089,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:34 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:53 INFO TaskSetManager: Lost task 1.1 in stage 2.0 (TID 6) on xx.xx.xx.24, executor 1: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn'",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:31788,concurren,concurrent,31788,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 18/04/24 17:40:52 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 01:33 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:40:52 INFO TaskSetMana",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:25902,concurren,concurrent,25902,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:42:02 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 117.869179 s; 18/04/24 17:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/24 17:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:42:02 ERROR TransportRequestHandler: Error sending result StreamResponse{",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:35526,concurren,concurrent,35526,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.forea,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:40722,concurren,concurrent,40722,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"pled solution. And this workflow is clearly marked as an unsupported prototype anyway (as are the GATK CLIs). I want to emphasize that this whole workflow is not a long-term solution. In other words, I would like to get this in and then focus on a supported solution. Two comments: ; > If these events were indeed not CNLOH, as we discussed, then I don't think we should merge this. Perhaps we should take a step back and answer definitively whether simply blacklisting common germline regions is enough to replicate/obviate most of the postprocessing. Should be straightforward to run an evaluation with and without blacklisting---and hopefully our truth data accurately reflects whether blacklisting is desirable. There are definitely events that get missed without the germline tagging, so this is an improvement over blacklisting alone. And while I have seen erroneous germline tagging (i.e. false calling a segment germline), it was only ever due to really noisy data (e.g. a bad PoN) or a poorly tuned segment caller. I am pretty sure that most common germline regions are being blacklisted already. The hotspots addressed in this PR (faux-CNLoH) could be added, but I think we will find new areas and a few of these areas were rather big. I have users that are actively using this from the branch, for reasons other than the faux-CNLoH pruning. Results are improving without an appreciable hit to sensitivity, which we got when using parameters like num_changepoints_penalty_factor. As a compromise, I can always default the CNLoH piece to `false`, since there are other useful changes on this branch. (Users did not have as strong an opinion about the faux-CNLoH pruning, since GISTIC does not use MAF and ABSOLUTE requires a manual review). > simple filtering based on CR-AF as described above could be implemented. If the normal is available, we can make IS_NORMAL calls simply based on the overlap of the ModelSegments posteriors (with corresponding qualities). If not, then some heuristic ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461258874:1512,tune,tuned,1512,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461258874,1,['tune'],['tuned']
Performance,"r definitively whether simply blacklisting common germline regions is enough to replicate/obviate most of the postprocessing. Should be straightforward to run an evaluation with and without blacklisting---and hopefully our truth data accurately reflects whether blacklisting is desirable. There are definitely events that get missed without the germline tagging, so this is an improvement over blacklisting alone. And while I have seen erroneous germline tagging (i.e. false calling a segment germline), it was only ever due to really noisy data (e.g. a bad PoN) or a poorly tuned segment caller. I am pretty sure that most common germline regions are being blacklisted already. The hotspots addressed in this PR (faux-CNLoH) could be added, but I think we will find new areas and a few of these areas were rather big. I have users that are actively using this from the branch, for reasons other than the faux-CNLoH pruning. Results are improving without an appreciable hit to sensitivity, which we got when using parameters like num_changepoints_penalty_factor. As a compromise, I can always default the CNLoH piece to `false`, since there are other useful changes on this branch. (Users did not have as strong an opinion about the faux-CNLoH pruning, since GISTIC does not use MAF and ABSOLUTE requires a manual review). > simple filtering based on CR-AF as described above could be implemented. If the normal is available, we can make IS_NORMAL calls simply based on the overlap of the ModelSegments posteriors (with corresponding qualities). If not, then some heuristic determination of the normal state from the tumor alone as in Marton's caller could be performed. This would combine the IS_NORMAL calling and filtering steps into one simple tool. The output could be a tagged/filtered ModelSegments .seg file and the corresponding VCF. And this would be a possible ""better solution"" Shall I file an issue for this? This could also allow us to obviate the TagGermline tool, which is fine by me.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461258874:2597,perform,performed,2597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461258874,1,['perform'],['performed']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:07 INFO TaskSetManager:54 - Starting task 1.1 in stage 0.0 (TID 3, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:07 WARN TaskSetManager:66 - Lost task 3.0 in stage 0.0 (TID 2, scc-q21.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 4924320, span 190238, expected MD5 8a9ef2f91a78ffdc56561ece832e9f5d; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at sc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:25072,concurren,concurrent,25072,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:08 INFO TaskSetManager:54 - Starting task 3.1 in stage 0.0 (TID 4, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:08 INFO TaskSetManager:54 - Lost task 1.1 in stage 0.0 (TID 3) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 1]; 2019-01-07 11:34:09 INFO TaskSetManager:54 - Starting task 9.0 in stage 0.0 (TID 5, scc-q12.scc.bu.edu, executor 2, partition 9, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:09 WARN TaskSetManager:66 - Lost task 2.0 in stage 0.0 (TID 0, scc-q12.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f; a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:26807,concurren,concurrent,26807,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:09 INFO TaskSetManager:54 - Starting task 1.2 in stage 0.0 (TID 6, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:09 INFO TaskSetManager:54 - Lost task 3.1 in stage 0.0 (TID 4) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 4924320, span 190238, expected MD5 8a9ef2f91a78ffdc56561ece832e9f5d) [duplicate 1]; 2019-01-07 11:34:10 INFO TaskSetManager:54 - Starting task 2.1 in stage 0.0 (TID 7, scc-q12.scc.bu.edu, executor 2, partition 2, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:10 WARN TaskSetManager:66 - Lost task 9.0 in stage 0.0 (TID 5, scc-q12.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 3, start 97885291, span 192458, expected MD5 ef90368731b6e0be845bc82cd92b0c6a; at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:28998,concurren,concurrent,28998,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:10 INFO TaskSetManager:54 - Starting task 3.2 in stage 0.0 (TID 8, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:10 INFO TaskSetManager:54 - Lost task 1.2 in stage 0.0 (TID 6) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 2]; 2019-01-07 11:34:11 INFO TaskSetManager:54 - Starting task 1.3 in stage 0.0 (TID 9, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:11 INFO TaskSetManager:54 - Lost task 3.2 in stage 0.0 (TID 8) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 4924320, span 190238, expected MD5 8a9ef2f91a78ffdc56561ece832e9f5d) [",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:31187,concurren,concurrent,31187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:49 INFO TaskSetManager:54 - Starting task 7.0 in stage 0.0 (TID 3, scc-q01.scc.bu.edu, executor 1, partition 7, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:49 WARN TaskSetManager:66 - Lost task 2.0 in stage 0.0 (TID 1, scc-q01.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:24359,concurren,concurrent,24359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:50 INFO TaskSetManager:54 - Starting task 1.1 in stage 0.0 (TID 4, scc-q20.scc.bu.edu, executor 2, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:50 WARN TaskSetManager:66 - Lost task 4.0 in stage 0.0 (TID 2, scc-q20.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at s",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:26096,concurren,concurrent,26096,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:51 INFO TaskSetManager:54 - Starting task 4.1 in stage 0.0 (TID 5, scc-q20.scc.bu.edu, executor 2, partition 4, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:51 INFO TaskSetManager:54 - Lost task 1.1 in stage 0.0 (TID 4) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 1]; 2019-01-09 13:35:52 INFO TaskSetManager:54 - Starting task 2.1 in stage 0.0 (TID 6, scc-q01.scc.bu.edu, executor 1, partition 2, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:52 WARN TaskSetManager:66 - Lost task 7.0 in stage 0.0 (TID 3, scc-q01.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 2, start 131325815, span 181534, expected MD5 c240a972d49aa89fb57dae94d1d90d36; a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:27832,concurren,concurrent,27832,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:52 INFO TaskSetManager:54 - Starting task 1.2 in stage 0.0 (TID 7, scc-q20.scc.bu.edu, executor 2, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:52 INFO TaskSetManager:54 - Lost task 4.1 in stage 0.0 (TID 5) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683) [duplicate 1]; 2019-01-09 13:35:53 INFO TaskSetManager:54 - Starting task 7.1 in stage 0.0 (TID 8, scc-q01.scc.bu.edu, executor 1, partition 7, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:53 INFO TaskSetManager:54 - Lost task 2.1 in stage 0.0 (TID 6) on scc-q01.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:30023,concurren,concurrent,30023,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-01-07 11:34:12 INFO DAGScheduler:54 - Job 0 failed: count at CountReadsSpark.java:80, took 9.419880 s; 2019-01-07 11:34:12 INFO AbstractConnector:318 - Stopped Spark@f1d88ea{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-01-07 11:34:12 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-01-07 11:34:12 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-01-07 11:34:12 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-01-07 11:34:12 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:34977,concurren,concurrent,34977,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-01-09 13:35:56 INFO DAGScheduler:54 - Job 0 failed: count at CountReadsSpark.java:80, took 12.691336 s; 2019-01-09 13:35:56 INFO AbstractConnector:318 - Stopped Spark@22fda322{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-01-09 13:35:56 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-01-09 13:35:56 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-01-09 13:35:56 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-01-09 13:35:56 INFO MapOutputTrackerMasterEndpoint:54 - MapOutput",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:34727,concurren,concurrent,34727,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:38193,concurren,concurrent,38193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['concurren'],['concurrent']
Performance,r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-1ac79f09-1a36-4668-92d9-0739775f98ed; 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-ed279998-3783-4f41-8fe5-f44a4fac3ee4; ```. CountReads runs fine..... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar CountReads --input HG04302.alt_bwamem_GRCh3,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:43088,concurren,concurrent,43088,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-69cc5c72-eff6-4259-8b3b-12fa6f8c42b0; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-0bd07e00-4f6d-43bd-b9d2-b1999376c72b; ```. Just to verify, the non-spark version still runs fine with the compressed fasta.... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference GRCh38_full_analysis_set_plus_decoy_hla.fa.gz; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar CountReads --input H",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:42840,concurren,concurrent,42840,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"s doc. Note that some of the CNV section is out of date and incorrect. In particular, we have been taking in PCOV as input to CreatePanelOfNormals for some time now, but the doc states that we take integer read counts. This already yields different results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - [x] <s>Investigate the effect of keeping duplicates. I am still not sure why we do this, and it may have a more drastic impact on WGS data.</s> Turns out we don't keep duplicates for WGS; see #3367.; - [ ] Check that GC-bias-correction+PCA and PCA-only perform comparably, even at small bin sizes (~300bp). From what I've seen, this is true for larger bin sizes (~3kbp), so explicit GC-bias correction may not be necessary. (That is, even at these (purportedly) large bin sizes, the effect of the read-based GC-bias correction is obvious for those samples where it is important. However, the end result is not very different from PCA-only denoising with no GC-bias correction performed.); - [x] <s>Check that changing CBS alpha parameter sufficiently reduces hypersegmentation.</s> <s>Looks like the hybrid p-value calculation in DNACopy is not accurate enough to handle WGS-size data. (Also, it's relatively slow, taking ~30 minutes on ~10M intervals.) Even if I set alpha to 0, I still get a ridiculous number of segments! So I think it's finally time to scrap CBS. I'll look into other R segmentation packages that might give us a quick drop-in solution, but we may want to roll our own simple algorithm (which we will scrap anyway once the coverage",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:4673,perform,perform,4673,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['perform'],['perform']
Performance,"s method than our previous probabilistic approaches. Even SNP segmentation will be much cheaper. > What is the name of this approach? ""KernSeg""?. Not sure...I couldn't find an R package, although an R/C implementation is mentioned in the paper. But the python implementation is straightforward and a pure Java implementation should not be so bad. There are some cythonized numpy methods that my python implementation used, but I think equivalent implementations of these methods should be relatively fast in pure Java as well. > What variant of the algorithm did you implement? the paper lists several. I implemented what they call ApproxKSeg. It's an approximate version that combines binary segmentation with the low-rank approximation to the Gaussian kernel. > I haven't read the paper in detail yet, but is it possible to choose a conservatively large number of possible break points and then filter bad break points, possibly based on the rapid decline of the change point probability? i.e. does the algorithm naturally produce change point probabilities?. Yes, you can oversegment and then choose which breakpoints to retain. However, there are no proper changepoint probabilities, only changepoint costs. Adding a penalty term based on the number of changepoints seems to perform relatively well in simple tests, but one could certainly devise other ways to filter changepoints (some of which could yield probabilities, if you are willing to assume a probabilistic model). I think we should just think of this as a fast, heuristic, non-parametric method for finding breakpoints in multidimensional data. > Is it possible to throw in additional change points incrementally, without doing extra work, until a certain criterion is met? (see above). The version I implemented adds changepoints via binary segmentation. The time complexity required to split a segment is linear in the number of points contained in the segment, although some care must be taken in the implementation to ensure this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321140715:2775,perform,perform,2775,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321140715,1,['perform'],['perform']
Performance,"sh Babadi <mehrtash@broadinstitute.org>; Date: Wed Nov 15 01:50:03 2017 -0500. Polished code, ready for review; ; gCNV computational kernel (initial release); ; renaming gammas_s to psi_s to uniformity (sample-specific unexplained variance); ; renamed determine_ploidy_and_depth.py to cohort_determine_ploidy_and_depth.py; finite-temperature forward-backward algorithm; in the ploidy model, replaced alpha_j (NB over-dispersion) with psi_j (unexplained variance) for uniformity. Also, added the possibility of sample-specific unexplained variance in the germline contig ploidy model; ; updated I/O routines and CLIs according to team discussion; ; updated I/O routines and CLIs according to team discussion; ; changed the output layout of the ploidy determination tool; refactored parts of io.py; upped the version to 0.3 as it is not backwards compatible anymore; ; case ploidy determination tool from a given ploidy model; major code cleanup and refactoring of I/O module; refactoring of common CLI script snippets; ; removed all ""targets""; some code cleanup; ; pad flat class bitmask w/ a given padding value in the hybrid q_c_expectation_mode; option to disable annealing and keep the temperature fixed; ; bugfix in finite-temperature forward-backward; further refactoring of model I/O; ; the option to take a previously trained model as starting point in cohort CLI; the option to take previous calls as a starting point in cohort CLI; ; option to save and load adamax moments; ; import/export adamax bias correction tensor; ; refactoring related to fancy opt I/O; added average ploidy column to read depth; updated docs of hybrid inference; ; modeling intervals can span multiple contigs now; ploidy can change; across contigs with no issue; ; save/load adamax state to .npy instead of .tsv for speed; ; part 1 of doc updates; ; part 2 of doc updates; ; part 3 of doc updates; ; part 4 of doc updates; ; bumped version to 0.5; readme; ; update readme; ; last minute stylistic doc updates.; ````",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:11775,load,load,11775,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,2,['load'],['load']
Performance,"simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than trying to figure out where the old one is failing would be best.; - [x] For small bins (250bp), the copy-ratio model is currently a bit memory intensive, since it stores an outlier indicator boolean for every data point (it gets by with -Xmx12g for 100 iterations at 250bp). There is no easy away around storing this at the GibbsSampler level (although we could make some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we could move to a BitSet, but I'm not sure what the performance hit will be. EDIT: It was trivial to switch",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:6745,perform,perform,6745,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['perform'],['perform']
Performance,"ta used was pretty small: chr1-2 training (~20k positive training/truth variants, ~50k negative training variants; note also that the threshold for determining negative training was not tuned---a threshold corresponding to a 98% truth sensitivity was arbitrarily chosen), chr3 validation (~50k variants), and chr4-6 test (~150k variants). The LL score is calculated from a validation set held out from the training/truth positives used to train the model, while the F1 score is calculated using ""orthogonal truth"" positives/negatives determined using 3 families of ~30 trios each. However, there's some arbitrariness in how we define the boundary for the latter positives/negatives, and hence some arbitrariness in the F1 score itself. But I'd expect using gold-standard GIAB truth would be more straightforward. Not sure how much we can conclude, but that the validation and test F1s are similar and that the validation LL score isn't *too* far off are encouraging. That said, there is a pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian consistency, white = reference. The second plot shows the training/truth positives used to train the model and to calculate the LL score in the validation shard. The third plot shows the ""orthogonal truth"" positives/negatives used to calculate F1. So we can see that the difficulty in deri",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:1307,optimiz,optimizing,1307,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,1,['optimiz'],['optimizing']
Performance,"taProgramGroup.java. And `6. ` from above. ---; ## FilterLongReadAlignmentsSAMSpark. 1. In the one-line summary, I'm not clear on what is meant by ""Filters"". Based on the result file, seems like it collects metrics on each contig alignment.; 2. ; 3. If metrics, then DiagnosticsAndQCProgramGroup.java. And `6. ` from above. ---; ## FindBadGenomicKmersSpark. 1. The term ""copy number"" should be reserved in reference to CNV analyses. So instead, how about:; Identify sequence contexts that occur at high frequency in a reference; 2. Please define a kmer. If only a reference fasta is required (as listed under Inputs) great. But if the tool also depends on a FAI index and DICT dictionary, please do include them. Also, it would be good to provide an example of how such information is used in SV discovery, e.g. ""the resulting file can be given to FindBreakpointEvidenceSpark, which will then ignore such sequence contexts during analysis."" Also would be good to mention that the default kmer size (--k-size 51) is optimized for human if indeed this is the case.; 3. ReferenceProgramGroup.java. And `6. ` from above. ---; ## FindBreakpointEvidenceSpark. 1. Assembles and aligns contigs of genomic breakpoint regions associated with structural variants ; 2. Overview and Notes could use finessing but let's leave this for next year. One thing to include is a reference to FermiLite for those seeking more information. A publication would be best. And `6. ` from above. ---; ## StructuralVariationDiscoveryPipelineSpark. 1. Runs the structural variant discovery workflow on a single sample in Spark ; 2. Fyi we sanction a ""Caveats"" section, which is likely more appropriate for the PE expectation and the fact that low coverage data less than 30x will give suboptimal results. Also, should mention this workflow is meant only for WGS. Or is it the case one case use exome data? Second note on BwaMemIndexImageCreator could be consolidated with the same under Inputs. Same with third note. And `6. ` from",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3948#issuecomment-351467451:3315,optimiz,optimized,3315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3948#issuecomment-351467451,1,['optimiz'],['optimized']
Performance,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:12 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:31 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.24, executor 1, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:44322 (size:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:29005,concurren,concurrent,29005,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:34 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:53 INFO TaskSetManager: Lost task 1.1 in stage 2.0 (TID 6) on xx.xx.xx.24, executor 1: o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:31704,concurren,concurrent,31704,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 18/04/24 17:40:52 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 01:33 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_00",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:25818,concurren,concurrent,25818,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:42:02 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 117.869179 s; 18/04/24 17:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/24 17:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:35442,concurren,concurrent,35442,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:40638,concurren,concurrent,40638,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"tprocessGermlineCNVCalls, so that external dictionaries provided via `--sequence-dictionary` do not override those in the count files, and perhaps fail if one is provided for any of the tools (I don’t recall exactly how VCF indexing is triggered by providing one, as seems to be indicated by the tutorial, but hopefully we can disallow external dictionaries while still taking advantage of the relevant engine features for VCF writing). EDIT: Went digging in Slack to try to remind myself of the context of these changes, and found the following PR comment from 1/7 (although it seems to have mysteriously disappeared from GitHub):. > Just so I understand, are we allowing overriding of the sequence dictionary in the shards (and skipping the consistency check) by allowing the parameter --sequence-dictionary to be specified? If so, we might want to document. Otherwise, I'd be inclined to enforce using the sequence dictionary in the shards (and ensuring the consistency check across shards is performed) by changing the null check in getBestAvailableSequenceDictionary to a check that the dictionary has not been set via the command line. EDIT^2: I think I misremembered the details of how #6330 hooked up the sequence dictionary and how getBestAvailableSequenceDictionary in GATKTool works (which probably explains why that comment was deleted...). Now that I actually go back and look, the `--sequence-dictionary` is not hooked up at all, so there is no change to revert in point 4!. Note that after all of this, it will *still* be possible to get into trouble at the gCNV step if you make funky shards (e.g., you could have shard 1 contain intervals from chr1 and chr3, and shard 2 contain intervals from chr2). I don't think it is possible to check for this case early, but you would still fail at PostprocessGermlineCNVCalls as above. Of course, all of these possibilities can be avoided by simply using the WDL, but it will be good to harden checks for those still working at the command line",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-719576249:3068,perform,performed,3068,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-719576249,1,['perform'],['performed']
Performance,ue -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/wgs.hg38/pipelines/hc/cram.test/GRCh38_full_analysis_set_plus_decoy_hla.fa.gz --spark-master yarn; 2019-01-09 13:35:04 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:05 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 13:35:09.640 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:35:09.799 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:11.507 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.508 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:35:11.508 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:35:11.508 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:35:11.508 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:1950,load,load,1950,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['load'],['load']
Performance,"ut.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be for",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:1806,optimiz,optimizations,1806,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['optimiz'],['optimizations']
Performance,"utput from chr1. The output shows the Maximum resident set size (kbytes): **2630440**. Using GATK jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar defined in environment variable GATK_LOCAL_JAR; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx200g -Xms16g -jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace; Command being timed: ""gatk --java-options -Xmx200g -Xms16g GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace""; User time (seconds): 270716.45; System time (seconds): 1723.34; Percent of CPU this job got: 99%; Elapsed (wall clock) time (h:mm:ss or m:ss): 76:08:24; Average shared text size (kbytes): 0; Average unshared data size (kbytes): 0; Average stack size (kbytes): 0; Average total size (kbytes): 0; Maximum resident set size (kbytes): 2630440; Average resident set size (kbytes): 0; Major (requiring I/O) page faults: 5; Minor (reclaiming a frame) page faults: 206030721; Voluntary context switches: 11129822; Involuntary context switches: 176522; Swaps: 0; File system inputs: 627981312; File system outputs: 466730160; Socket messages sent: 0; Socket messages received: 0; Signals delivered: 0; Page size (bytes): 4096; Exit status: 0. ```. So using the import on reblocked gvcfs using --bypass-feature-reader was the fastest way to import our 3500 gVCFs and minimize memory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687:2253,optimiz,optimizations,2253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687,1,['optimiz'],['optimizations']
Performance,"which includes combining read-count files, which takes ~30s of I/O) and generated a 520MB PoN, and DenoiseReadCounts took ~30s (~10s of which was composing/writing results, as we are still forced to generate two ReadCountCollections). Resulting PTN and TN copy ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 wr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1555,perform,perform,1555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['perform'],['perform']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:07 INFO TaskSetManager:54 - Starting task 1.1 in stage 0.0 (TID 3, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:07 WARN TaskSetManager:66 - Lost task 3.0 in stage 0.0 (TID 2, scc-q21.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 4924320, span 190238, expected MD5 8a9ef2f91a78ffdc56561ece832e9f5d; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:24988,concurren,concurrent,24988,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:08 INFO TaskSetManager:54 - Starting task 3.1 in stage 0.0 (TID 4, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:08 INFO TaskSetManager:54 - Lost task 1.1 in stage 0.0 (TID 3) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 1]; 2019-01-07 11:34:09 INFO TaskSetManager:54 - Starting task 9.0 in stage 0.0 (TID 5, scc-q12.scc.bu.edu, executor 2, partition 9, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:09 WARN TaskSetManager:66 - Lost task 2.0 in stage 0.0 (TID 0, scc-q12.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:26723,concurren,concurrent,26723,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:09 INFO TaskSetManager:54 - Starting task 1.2 in stage 0.0 (TID 6, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:09 INFO TaskSetManager:54 - Lost task 3.1 in stage 0.0 (TID 4) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 4924320, span 190238, expected MD5 8a9ef2f91a78ffdc56561ece832e9f5d) [duplicate 1]; 2019-01-07 11:34:10 INFO TaskSetManager:54 - Starting task 2.1 in stage 0.0 (TID 7, scc-q12.scc.bu.edu, executor 2, partition 2, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:10 WARN TaskSetManager:66 - Lost task 9.0 in stage 0.0 (TID 5, scc-q12.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:28914,concurren,concurrent,28914,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:10 INFO TaskSetManager:54 - Starting task 3.2 in stage 0.0 (TID 8, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:10 INFO TaskSetManager:54 - Lost task 1.2 in stage 0.0 (TID 6) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 2]; 2019-01-07 11:34:11 INFO TaskSetManager:54 - Starting task 1.3 in stage 0.0 (TID 9, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:11 INFO TaskSetManager:54 - Lost task 3.2 in stage 0.0 (TID 8) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequenc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:31103,concurren,concurrent,31103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:49 INFO TaskSetManager:54 - Starting task 7.0 in stage 0.0 (TID 3, scc-q01.scc.bu.edu, executor 1, partition 7, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:49 WARN TaskSetManager:66 - Lost task 2.0 in stage 0.0 (TID 1, scc-q01.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterato",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:24275,concurren,concurrent,24275,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:50 INFO TaskSetManager:54 - Starting task 1.1 in stage 0.0 (TID 4, scc-q20.scc.bu.edu, executor 2, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:50 WARN TaskSetManager:66 - Lost task 4.0 in stage 0.0 (TID 2, scc-q20.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:26012,concurren,concurrent,26012,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:51 INFO TaskSetManager:54 - Starting task 4.1 in stage 0.0 (TID 5, scc-q20.scc.bu.edu, executor 2, partition 4, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:51 INFO TaskSetManager:54 - Lost task 1.1 in stage 0.0 (TID 4) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 1]; 2019-01-09 13:35:52 INFO TaskSetManager:54 - Starting task 2.1 in stage 0.0 (TID 6, scc-q01.scc.bu.edu, executor 1, partition 2, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:52 WARN TaskSetManager:66 - Lost task 7.0 in stage 0.0 (TID 3, scc-q01.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:27748,concurren,concurrent,27748,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:52 INFO TaskSetManager:54 - Starting task 1.2 in stage 0.0 (TID 7, scc-q20.scc.bu.edu, executor 2, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:52 INFO TaskSetManager:54 - Lost task 4.1 in stage 0.0 (TID 5) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683) [duplicate 1]; 2019-01-09 13:35:53 INFO TaskSetManager:54 - Starting task 7.1 in stage 0.0 (TID 8, scc-q01.scc.bu.edu, executor 1, partition 7, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:53 INFO TaskSetManager:54 - Lost task 2.1 in stage 0.0 (TID 6) on scc-q01.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequenc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:29939,concurren,concurrent,29939,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-01-07 11:34:12 INFO DAGScheduler:54 - Job 0 failed: count at CountReadsSpark.java:80, took 9.419880 s; 2019-01-07 11:34:12 INFO AbstractConnector:318 - Stopped Spark@f1d88ea{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-01-07 11:34:12 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-01-07 11:34:12 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-01-07 11:34:12 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:34893,concurren,concurrent,34893,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-01-09 13:35:56 INFO DAGScheduler:54 - Job 0 failed: count at CountReadsSpark.java:80, took 12.691336 s; 2019-01-09 13:35:56 INFO AbstractConnector:318 - Stopped Spark@22fda322{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-01-09 13:35:56 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-01-09 13:35:56 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-01-09 13:35:56 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:34643,concurren,concurrent,34643,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Opti,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:38109,concurren,concurrent,38109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['concurren'],['concurrent']
Performance,xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-1ac79f09-1a36-4668-92d9-0739775f98ed; 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-ed279998-3783-4f41-8fe5-f44a4fac3ee4; ```. CountReads runs fine..... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg/gatk/4.0.12.0/ins,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:43004,concurren,concurrent,43004,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-69cc5c72-eff6-4259-8b3b-12fa6f8c42b0; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-0bd07e00-4f6d-43bd-b9d2-b1999376c72b; ```. Just to verify, the non-spark version still runs fine with the compressed fasta.... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference GRCh38_full_analysis_set_plus_decoy_hla.fa.gz; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /shar",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:42756,concurren,concurrent,42756,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"xtensive (growing with the number of points in a segment), and 2) binary segmentation is a global, greedy algorithm. These both cause long events to be preferred over short events, and thus the first changepoints found (and retained after applying the penalty) may not include those for small, obvious events. For example, see performance on this simulated data, which includes events of size 10, 20, 30, and 40 within 100,000 points at S/N ratio 3:1 in addition to sine waves of various frequency at S/N ratio 1:2 (to roughly simulate GC waves). Changepoints arising from the sine waves will be found first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether any given point is a suitable changepoint; using the data locally surrounding the point typically suffices. With these modifications, ApproxKernSeg can handle both scenarios:; ![wave-kern](https://us",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:1162,tune,tune,1162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045,1,['tune'],['tune']
Performance,"y ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1812,perform,perform,1812,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['perform'],['perform']
Performance,"your recommendation is still to copy the workspace prior to merging/appending to it, then the distributed processing still means copying the original, and to my thinking copying each contig's folder into a new workspace, vs. copying each contig into the same workspace is basically the same overhead. We also tend to keep the long-lived copy on our warm storage, with processing happening on our cluster's lustre filesystem. . 2) Again, i dont think it's necessarily right to assume every job will operate on the same set of intervals. We generally would use the same pattern, but there are legitimate cases in which different intervals/job would better match the cluster's availability. If we're appending a limited number of samples and our cluster is busy, we might want to scatter using more intervals/job since each job would finish fairly quickly and the practical reality is fewer total jobs would complete quicker. if we are performing an operation that requires a lot of time/job (like creating a new workspace or appending a lot of samples), we might do one job/contig. It's also worth pointing out that macaque has 1000s of small unplaced contigs, and therefore we almost never do a simple 1:1 job:contig scheme.; ; 3) When I was originally thinking about how to scatter/gather the creation of a combined gVCF, the overhead of re-merging was huge. There was zero point in taking the per-contig gVCFs and concat/bgzipping a new one, just to split it again. When I started down this road, my idea was to make a folder holding each gVCF, and a top-level JSON file to map contig->filepath, so code could intelligently work with these. The latter essentially describes the structure of a GenomicsDB workspace. Unlike concatenating gVCFS, the overhead of moving directories around is practically zero. Sure, I could make a folder of GenomicsDB workspaces, but if I'm already moving them, what's the point in not merging? . I could understand that is the workspace lived on a shared filesystem and",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-640881049:1038,perform,performing,1038,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-640881049,1,['perform'],['performing']
Safety, at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41555,abort,abortStage,41555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['abort'],['abortStage']
Safety," can hopefully rely on per-bin bias modeling to at least partially account for mappability in gCNV calling (and we certainly wouldn't want to filter out a significant fraction of the genome, in any case). Do we agree?. To answer your first question, the criterion for choosing the peak is quite hacky at the moment, but I found that filtering low-count bins to first check for the presence of a high peak and then falling back to the peak at zero works perfectly fine in practice. . We can certainly try to do something smarter, since, as you say, bin filtering may be desirable---even if we implement mappability filtering---to remove large germline events (it's true that the ""example"" I showed above is indeed from the PAR-like region on X, as you point out, but this is roughly how a large arm-level event would appear even after mappability filtering.) Although the model I fit above, which is simply a sparse mixture of NBs with regularly-spaced means (modulo some sample-specific and contig-specific jitter), could conceivably capture such events as well, we want to avoid models where a single NB might try to capture two or more peaks. Also, just to clarify, the weird mosaic examples are the bottom two plots out of the four above---you can see the shifted (non-X, in one of the examples) single peaks. However, it's interesting that the PARs are still showing up in XY---I'm pretty sure I used the blacklist you provided, although I will double check. Did that only include the ""official"" PARs, or also the additional ones you found?. In any case, are we comfortable calling in those regions (here I'm talking about gCNV, not ploidy)? As I show above, I don't think we need mappability to nail the baseline ploidy. Can we then rely on the per-bin bias to account for these regions in gCNV (pinning them back to the correct CN) without mappability filtering? And with mappability filtering, how substantial is the hit to coverage in these regions? Should we blacklist them for the time bein",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375923639:1327,avoid,avoid,1327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375923639,1,['avoid'],['avoid']
Safety," first step to correctly identify the issue. So it seems a bit premature to even prototype a method, much less merge it. I think this PR, as is, muddies the waters quite a bit. For example, it introduces a new Record class that denotes this type of ""CNLOH"" with a `C`. If we want to merge this, I suggest that we first correctly identify the issue. If these events are not mosaic CNLOH, then we should clean up all mention of CNLOH in this code. Either way, can we quantify the level of improvement gained by filtering such events in a reproducible evaluation? If so, let's bring that into gatk-evaluation. Finally, there are many more options available to change the segmentation and/or resolution than the single one you mentioned. If the users you are working with can clearly specify their analysis goals in terms of resolution, then it might be possible to sidestep the problem entirely without adding more unsupported code. This would also buy us more time to put in a principled solution, without the risk of unsupported code getting entrenched in their workflows. > There are definitely events that get missed without the germline tagging, so this is an improvement over blacklisting alone. And while I have seen erroneous germline tagging (i.e. false calling a segment germline), it was only ever due to really noisy data (e.g. a bad PoN) or a poorly tuned segment caller. This is encouraging. This means that a straightforward approach to germline filtering, such as simply identifying overlapping posteriors as mentioned above, should work well. Prototyping this approach shouldn't take long at all, especially when the matched normal is guaranteed to be available, as it is in this workflow (tumor-only would require some work to identify the normal state, as mentioned previously). I'd rather just roll that, evaluate it, and merge it instead. Key here is that we sidestep the deficiencies of the current CR-only caller, which also shares the blame for this ""CNLOH"" issue (since these ev",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199:3149,risk,risk,3149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199,1,['risk'],['risk']
Safety," the count likelihood is actually misspecified there. As an example, consider trying to fit a Poisson to data that is actually zero-inflated Poisson---fitting the histogram will actually result in a more robust estimate for the mean. Another benefit is that truncated data (as we have here) is straightforwardly handled in an unbiased way. In the special case of complete, trivially-binned data, the full, unbinned likelihood is recovered. I think this sort of histogram fitting is pretty standard in the astro/particle community. We can certainly change up the model to include strictly quantized + free-floating states as you describe (rather than the ""fuzzily quantized"" states I use here), but I just wanted to avoid having another level of mixtures/logsumexps for this quick prototype. However, note that modeling mosaicism on the autosomes is desirable, but there we also want the strong diploid prior to nail down the depth and per-contig bias. So we will have to be a little careful about how we introduce free-floating states. Also, since I was not using gcnvkernel, I had to integrate out all discrete parameters. It may be that we can write down a nice model with discrete parameters if we use your inference framework. Finally, I did not further bin the counts here (or rather, the bin size is 1), which already yields the maximum information, but I did use a maximum-count cutoff. If we use the same cutoff for all samples, this allows us to simply pass a non-ragged matrix from Java (with dimensions of samples x contigs x maximum count) as a TSV. However, we may run into trouble if we hit a case sample with very high depth. So some sort of sparse representation of the histogram might indeed be desirable, but I think it should be an exact representation of the full histogram. This would require us to sync up code to emit and consume the representation in both Java and python, so I'd like to avoid it if possible---I think I'd prefer just emitting the ragged matrix, in that case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522:2356,avoid,avoid,2356,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522,1,['avoid'],['avoid']
Safety," why this is happening or what I can do to overcome this problem? I have run `GenomicsDBImport` and `GenotypeGVCFs` successfully in the past (same version, same computer) on a different dataset, so I'm not sure what about this data is causing the problem. Any guidance is much appreciated!. Thanks,; Jessie. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /nfs/data1/jsalt/3RAD/colinus_virginianus_13May2017_V3Fw6_newchrom.fasta -V gendb://odont_cyr_8_snp_db -O odont_cyr_8_snp_db.vcf; 14:59:47.866 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 03, 2020 2:59:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:59:59.674 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:59:59.675 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 14:59:59.675 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:00:09.686 INFO GenotypeGVCFs - Executing as jsalt@mustard on Linux v3.10.0-957.1.3.el7.x86_64 amd64; 15:00:09.686 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 15:00:09.687 INFO GenotypeGVCFs - Start Date/Time: February 3, 2020 2:59:47 PM CST; 15:00:09.687 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:00:09.687 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:00:09.688 INFO GenotypeGVCFs - HTSJDK Version: 2.19.0; 15:00:09.688 INFO GenotypeGVCFs - Picard Version: 2.19.0; 15:00:09.689 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEV",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640:1411,detect,detect,1411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640,1,['detect'],['detect']
Safety,"#### Hiding / deprecating tools and their docs. @samuelklee To add to @sooheelee's answer, if there are any tools that you definitely want gone and already have a replacement for, I would encourage you to kill them off (ie delete from the code) before the 4.0 launch. While we're still in beta we can remove anything at the drop of a hat. Once 4.0 is out, we'll have a deprecation policy (exact details TBD) that will allow us to prune unwanted tools over time, but it will be less trivial. And as Soo Hee said, everything that's in the current code release MUST be documented. We used to hide tools/docs in the past and it caused us more headaches than not. . That being said, as part of that TBD deprecation policy it will probably make sense to make a ""Deprecated"" program group where tools go to die. If there are tools you plan to kill but don't want to do it before 4.0 is released for whatever reason, you could put them there. Documentation standards can be less stringent for tools in that bucket. To be clear I think the deprecation group name should be generic, ie not named to match any particular use case or functionality. That will help us avoid seeing deprecation buckets proliferate for each variant class/ use case. Does that sound like a reasonable compromise?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346189138:1155,avoid,avoid,1155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346189138,1,['avoid'],['avoid']
Safety,"01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM models, as well. @LeeTL1220 @mbabadi @davidbenjamin I'd be interested to hear your thoughts, if you have any.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:1263,recover,recovered,1263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666,1,['recover'],['recovered']
Safety,"019-01-07 11:34:12 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-01-07 11:34:12 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-01-07 11:34:12 INFO MemoryStore:54 - MemoryStore cleared; 2019-01-07 11:34:12 INFO BlockManager:54 - BlockManager stopped; 2019-01-07 11:34:12 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-01-07 11:34:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-01-07 11:34:12 INFO SparkContext:54 - Successfully stopped SparkContext; 11:34:12.605 INFO CountReadsSpark - Shutting down engine; [January 7, 2019 11:34:12 AM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.80 minutes.; Runtime.totalMemory()=1003487232; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 9, scc-q21.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:36682,abort,aborted,36682,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['abort'],['aborted']
Safety,"1. @nalinigans It's a very reasonable question. It's true, the --avoid-nio flag is technically redundant. You can recreate it with a combination of other flags. I added it because ; a) I didn't realize that was the when I started adding it. ; b) The combination of flags was kind of complicated so it was helpful to have something that gave you clear instructions about what you needed to enable. I think we could merge them, although I think there is one sanity check we do even when -bypass-feature-reader is turned on, that we need to turn off. I basically added ""something that works for Megan's project right now."" . 2. Yes, the various cases were getting complicated and I had a bug when -V was enabled so I just disabled it as an option. It would make sense to add -V support for azure files. I just didn't do it because I was in a rush and I figured it was better to disable it than to have it potentially be wrong. . 3. Yeah, that's the error I saw. It's definitely better than nothing. It would be great if it could be propagated back up to the java layer as a Java exception though. It currently ends the program with SIGABORT I think which doesn't play that nicely with various reporting and retry mechanisms. No super high priority, but nice if you have the cycles.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8632#issuecomment-1865021020:65,avoid,avoid-nio,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8632#issuecomment-1865021020,3,"['avoid', 'redund', 'sanity check']","['avoid-nio', 'redundant', 'sanity check']"
Safety,"2019-01-09 13:35:56 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-01-09 13:35:56 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-01-09 13:35:56 INFO MemoryStore:54 - MemoryStore cleared; 2019-01-09 13:35:56 INFO BlockManager:54 - BlockManager stopped; 2019-01-09 13:35:56 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-01-09 13:35:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-01-09 13:35:56 INFO SparkContext:54 - Successfully stopped SparkContext; 13:35:56.383 INFO CountReadsSpark - Shutting down engine; [January 9, 2019 1:35:56 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.78 minutes.; Runtime.totalMemory()=1009254400; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 11, scc-q20.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonf",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:36433,abort,aborted,36433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['abort'],['aborted']
Safety,"4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like it’s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so it’s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the task data - or use a Spark accumulator), and then seeing if there's some way to make it more uniform.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:2599,avoid,avoid,2599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884,1,['avoid'],['avoid']
Safety,"> Hi Kevin, our team would like to get this merged into `ah_var_store` soon per VS-1254. I'm aware of only a handful of outstanding issues:; > ; > * The failing PGEN tests. I'm happy to help here in any way I can though right now I don't have a sense of what could be causing this beyond the platform differences you suggested.; > ; > * The `10` vs `10.0` change we discussed recently to avoid division by zero.; > ; > * We'll want to merge / rebase from `ah_var_store` and then build a new GATK Docker image which would be entered into `GetToolVersions` in `GvsUtils.wdl`. I'm happy to take on building this image once the merge / rebase is ready. Hi Miguel, sorry about the delay. I'm working on the failing tests issue this afternoon. I know what the issue is, so I just have to implement a fix, which I think should be fairly simple. Once I have that ready and have confirmed the tests aren't failing anymore, I'll do the rebase and then let you know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8708#issuecomment-2004517723:388,avoid,avoid,388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708#issuecomment-2004517723,1,['avoid'],['avoid']
Safety,"@DuyDN This is a known issue in BQSR -- see https://github.com/broadinstitute/gatk/issues/6242. Sorry for the inconvenience! We hope to be able to develop a fix within the next several months. The fact that you ran into this error indicates that there may not actually be any usable reads in that particular read group -- they were likely all filtered out by one of the BQSR filters, which filter out malformed, low mapping quality, unmapped, and secondary alignments. You could likely avoid the error by filtering out that read group using the `ReadGroupBlackListReadFilter` in GATK while running ApplyBQSR (`--read-filter ReadGroupBlackListReadFilter`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7549#issuecomment-963494490:486,avoid,avoid,486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7549#issuecomment-963494490,1,['avoid'],['avoid']
Safety,"@adaykin The difference between the chr1, chr2, etc. convention and the 1, 2, etc. convention is more than just a difference in naming. Different versions of the human reference use different naming schemes. For example the b37 reference uses 1, 2, etc., while the hg38 reference uses chr1, chr2, etc. For this reason, it is not safe to simply translate the contig names on-the-fly. You need to do a proper liftover from one reference to another using a tool such as `LiftoverVcf`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7538#issuecomment-963507876:329,safe,safe,329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7538#issuecomment-963507876,1,['safe'],['safe']
Safety,"@akiezun I think there's a case to be made for having the ability to separate closure of resources from generation of final output on success, even if it's not currently needed -- I'd be ok with keeping both methods provided we can settle on the right names to avoid confusion, and provided we update the docs to make it clear when each method should be overridden. @lbergelson's suggestion of `onTraversalSuccess()` and `cleanup()` seems reasonable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212629504:261,avoid,avoid,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212629504,1,['avoid'],['avoid']
Safety,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:210,avoid,avoid,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307,1,['avoid'],['avoid']
Safety,"@asmirnov239 I've borrowed the CopyNumberTestUtils class from #7889 into which you moved the method for detecting deltas in the doubles. I'm going to merge this PR once tests pass, so just be aware of this when rebasing your branch if you make any further changes. We might consider adding a simple test of the test method itself. I'll let you do it in your branch, or we can file an issue and tackle it after everything is merged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1165717972:104,detect,detecting,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1165717972,1,['detect'],['detecting']
Safety,"@bhanugandham @fleharty this issue touches upon our discussion of https://gatkforums.broadinstitute.org/gatk/discussion/24335/loh-detection-using-gatk4s-somatic-cnv-workflow. We might consider just a simple modification of the genotyping step (e.g., keeping all ROHs longer than a hard threshold) to start, which would probably cover the most common use cases with minimal effort. Can use 100% HCC1143 in tumor-only mode as an initial test, but it would be good to collect other examples.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3915#issuecomment-531833700:130,detect,detection-using-,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3915#issuecomment-531833700,1,['detect'],['detection-using-']
Safety,"@cmnbroad How about we detect the common case of filters composed using only AND, and use simplified output in that case, and revert back to the complex output when filters are composed in more complex ways? That would resolve the problem in practice, since (as far as I know) all of the filters we actually use are composed using only AND.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3520#issuecomment-367072562:23,detect,detect,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3520#issuecomment-367072562,1,['detect'],['detect']
Safety,"@cmnbroad I understand that I could have retained a bunch of single-use text files, but it seemed like the more permutations one adds, the less it makes sense to have a separate, very redundant, static text file to check each scenario. There's a ton of VariantContext-related tests that parse the output VCF to test some feature as opposed to checking in a bunch of VCF text files.... While I'll grant the 4th test case I added (where we pass chr 2) isnt especially compelling over just testing chr 1, one could argue more breadth is a good thing here. if you want clarity, pulling that VariantEval report parsing code into a method called extractUniqueContigsFromEvalReport(), or simply adding a comment line, supports this goal. Anyway, I'm checking in slightly clarified version of this now, simply to get tests running. If you respond to the above, maybe we go with that. In the interest of time, I'll stage and check in the version which restores the text files and goes that route.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7238#issuecomment-831459741:184,redund,redundant,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7238#issuecomment-831459741,1,['redund'],['redundant']
Safety,"@cwhelan . Thanks for the review!; I've incorporated most of your review suggestions, with the fowling exception because I need to think about what need to be done to make less review rounds. > This logic does more than detect variants, though.. it also annotates existing variants with the imprecise evidence. I'm also a little hesitant to move this all into its own separate class -- we really should be moving towards a model where we look at all three sources of evidence (breakpoint assemblies, imprecise evidence clusters, and coverage) simultaneously for eg @mwalker174 's work, and splitting handling of imprecise evidence into its own class seems like a step in the wrong direction. I agree. That's what I'm thinking about for complex inversions as well. So what about the following in this particular PR:. 1. move `StructuralVariationDiscoveryPipelineSpark.makeEvidenceLinkTree()` into `ImpreciseVariantDetector`;; 2. drop `ImpreciseVariantDetector.detectImpreciseVariantsAndAddReadAnnotations()` considering it really only delegates to `processEvidenceTargetLinks()`; 3. rename `ImpreciseVariantDetector` as `EvidenceTargetLinkHandler`; 4. reduce the work of `DiscoverVariantsFromContigAlignmentsSAMSpark.discoverVariantsAndWriteVCF()` into detecting only simple variants based on assemblies and name it `discoverSimpleVariants()`; 5. let `StructuralVariationDiscoveryPipelineSpark` call into `EvidenceTargetLinkHandler.processEvidenceTargetLinks()` to get back VariantContexts, then write VCF . `processEvidenceTargetLinks()` really does two things at the moment: annotation on breakpoints and call imprecise deletions; preferably, we should go the all-evidence-at-the-same-time approach and decouple the two but I am trying to not mess with it right now. If you agree, I'll implement it in a separate commit.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3934#issuecomment-357345426:220,detect,detect,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3934#issuecomment-357345426,3,['detect'],"['detect', 'detectImpreciseVariantsAndAddReadAnnotations', 'detecting']"
Safety,@davidbenjamin Could you take a look at this? @TedBrookings thinks it might be as simple as changing the check to allow 0 length reads when initializing the pairHmm. Neither of us are sure that that's a great solution though. . It seems like if you have no read bases you can't do any useful calculation. Should there be an earlier check in mutect that avoids assembling a region if there aren't any reads with bases?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-465193131:353,avoid,avoids,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-465193131,1,['avoid'],['avoids']
Safety,@davidbenjamin I think that this issue will be addressed by the AFCalculator refactoring one way or another (e.g. by lifting up the max-alt-allele restrictions or simply avoid adding the NON-REF allele before calling the AFCalculator).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1858#issuecomment-221770394:170,avoid,avoid,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1858#issuecomment-221770394,1,['avoid'],['avoid']
Safety,"@davidbenjamin I tried and this time its a different error. ; ```; 14:55:53.232 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/shollizeck/clustering.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 09, 2020 2:55:53 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:55:53.432 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.433 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.4.1-6-g6bb31a7-SNAPSHOT; 14:55:53.433 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:55:53.433 INFO FilterMutectCalls - Executing as shollizeck@stpr-res-compute02.unix.petermac.org.au on Linux v3.10.0-1062.4.3.el7.x86_64 amd64; 14:55:53.433 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_232-b09; 14:55:53.434 INFO FilterMutectCalls - Start Date/Time: 9 January 2020 2:55:53 PM; 14:55:53.434 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.434 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.434 INFO FilterMutectCalls - HTSJDK Version: 2.21.0; 14:55:53.435 INFO FilterMutectCalls - Picard Version: 2.21.2; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:55:53.435 INFO FilterMutectCalls - Deflater: IntelDeflater; 14:55:53.435 INFO FilterMutectCalls - Inflater: IntelInflater; 14:55:53.435 INFO FilterMutectCalls - GCS max retries/reopens: 20; 14:55:53.435 INFO FilterMutectCalls - Requester pays: disabled; 14:55:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341:357,detect,detect,357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341,1,['detect'],['detect']
Safety,"@droazen I'm not sure this is an improvement. We want the fundamental unit of spark tool to be the transform, not the cli wrapper around it. If we do this then we're pushing more of the contract of the transform outside of itself, i.e. see the newly duplicated bqsr code. I think that it was a deliberate decision to lift all reads into the initial rdd and then filter them in the transforms to what was needed by that transform. This is paying some performance cost in multi-stage pipelines which will potentially apply the same filters over and over again, but it simplifies the code because the filters can be baked into the transform and the pipeline writer doesn't have to think about them. It would be nice if we had a mechanism for adding metadata to an RDD so we can say ""this is a sorted RDD filtered with X,Y,and Z filters"", so we could intelligently avoid re-filtering.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856:861,avoid,avoid,861,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856,1,['avoid'],['avoid']
Safety,"@frank-y-liu Something has gone slightly wrong in this branch git-wise -- it looks like you've duplicated some commits from master, and the merge commits in your history imply that your git workflow needs some tweaking. In general, you want to always `rebase` rather than `merge` or `pull` (and avoid mixing the two, which can cause problems), since `rebase` produces a much cleaner history. Since you're working in a fork, you should create an ""upstream"" remote if you haven't already:. `git remote add upstream https://github.com/broadinstitute/gatk.git`. Then when you're working in a branch to which you've made one or more commits, and you want to update your branch with the latest changes from our master, do this:. `git fetch upstream`; `git rebase -i upstream/master`. This will bring up a screen allowing you to ""squash"" (combine) your work into a single commit that is suitable for merging into our master branch. If you do this with the current version of your branch, and select ""squash"" for all but the top commit, I believe you'll succeed in repairing your git history. . Note that after each `rebase`, when you want to push your changes to github you'll need to do a `git push -f` instead of a simple `git push`, since `rebase` changes your commit history. Try it out and let me know how it goes!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1776#issuecomment-215474210:295,avoid,avoid,295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1776#issuecomment-215474210,1,['avoid'],['avoid']
Safety,"@jamesemery I will gladly review. If I understand the code change it seems like there was already basically the right logic to avoid this _but_ the code was neglecting to put the force calling alleles in a representation consistent with the output VCF. And the fix is simply to compute `forcedAlleles = AssemblyBasedCallerUtils.getAllelesConsistentWithGivenAlleles(givenAlleles, vc)` a bit upstream of where we were doing so previously. If I've got that right, this PR gets my :thumbsup:.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7740#issuecomment-1081342841:127,avoid,avoid,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7740#issuecomment-1081342841,1,['avoid'],['avoid']
Safety,"@jean-philippe-martin Can you comment on this error with your thoughts? Despite now doing a channel reopen on `UnknownHostException` in our fork of the NIO library, all reopens are failing, which implies that this error can't be recovered from via a simple retry. Could there be something wrong in our authentication setup?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412906931:229,recover,recovered,229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412906931,1,['recover'],['recovered']
Safety,"@lbergelson you beat me because I was stuck trying to actually run a Picard tool in the integration test. (For future reference, that needs a workaround because the test running adds the ERROR level logging to all command lines and Barclay can't parse that for Picard tools for some reason.). The big reason I was using this instead of IntervalListTools is because the Picard version creates a terrible output file structure that I was having trouble capturing with a simple glob in WDL. I agree that the functionality here is largely redundant, but it was helping me get my workflow working faster at the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435894196:535,redund,redundant,535,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435894196,1,['redund'],['redundant']
Safety,"@marchoeppner `MarkDuplicatesGATK` was removed because it had fallen out-of-date with respect to the version in Picard, and as an unmaintained tool was in our view not safe for use, and was causing confusion for our users. The loss of CRAM support is an unfortunate side effect of its removal. We've been doing a lot of work on our parallel version of `MarkDuplicates`, however, which is called `MarkDuplicatesSpark`. This version is fully up-to-date with respect to the Picard version, can run much faster than the Picard version when multiple cores or multiple machines are available, and will fully support CRAM in the future. CRAM support in that tool will come as a side effect of our migration to the new Disq library (https://github.com/disq-bio/disq), which is scheduled to happen within the next few months. In the meantime, I'd suggest continuing to request the Picard community to add CRAM support to their version. It's likely not a lot of work, and may simply require passing the reference through to the reader class, which could be a ~1 line change!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5218#issuecomment-424882567:168,safe,safe,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5218#issuecomment-424882567,1,['safe'],['safe']
Safety,"@mlathara Our primary use case involves calling variants on a constantly growing large dataset of WGS and WXS data. As you probably realize, the CombineGVCFs/GenomicsDBImport step is incredibly time consuming, and scatter/gather is pretty much essential to make these operations work in any halfway reasonable period of time. I have off-hand heard people from the broad mention large WXS datasets, and keep in mind we're working mostly w/ WGS. Regarding processing: our main downstream use right now is GenotypeGVCFs, and yes we expect to run that scatter/gather as well. I agree that in principle we could maintain these data as a folder of workspaces. In fact that was my original plan before I realized the GenomicsDB workspace already is essentially a folder of per-contig folders. The reason I like the solution of copying around the folders is b/c our end product is in an official file format that tools understand how to use. . A related point, before we decided to try GenomicsDB, my plan was to create a scheme (""file format"") that would allow our code to better operate on a folder of per-contig CombinedGVCF file. I would probably have written out a top-level JSON file that served the same purpose as the JSON files in a GenomicsDB workspace. As noted above, GenomicsDB is essentially already doing this for me. To the question about usage and support: perhaps that ways to think about this would be interval-based split and merge tools for GenomicsDB workspaces? This would obscure the internal structure of the workspace from the user (even if they basically just to folder copying). The split tool should be really simple and not have many caveats. The merge tool could have a lot of limits on what kind of workspaces can or cannot be merged. Perhaps it could do sanity checking on the JSON files to make sure they're compatible, and then copy the folders into this new merged workspace?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-635338868:1779,sanity check,sanity checking,1779,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-635338868,1,['sanity check'],['sanity checking']
Safety,"@pieterlukasse Yeah - I'm planning on updating some of the Funcotator core to be more permissive for input data types and to fix a few long-standing bugs, but have been unable to do so because of other high-priority tasks (as @lbergelson said). The output formats are pretty well-established, so I don't think there's any risk in writing an additional parser. However if you simply want to view the outputs, you can render the annotations in `MAF` format and that will produce a `MAF` (TSV) file that is much more easily viewed / parsed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8154#issuecomment-1379018376:322,risk,risk,322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8154#issuecomment-1379018376,1,['risk'],['risk']
Safety,"@ronlevine I know this a port from gatk3, but I think theres a bit of refactoring that can be done. It seems like it's more complicated than it needs to be. Could you take a look and see? In particular I'm not sure why things get converted to a bitset, it looks like you should just be able to derive the indecies directly and avoid creating a bitset. If I'm missing some detail and it can't be simplified let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049:327,avoid,avoid,327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049,1,['avoid'],['avoid']
Safety,"@samuelklee DRAGEN STRE model doesn't actually make any alterations to the smith waterman parameters or how they work, it just works by adjusting the indel gap penalties that are used for the PairHMM. At one point we were concerned about SW parameters being different with dragen but as it turns out the biggest visible effect of the SW parameters on the output (the alignment we perform after haplotypes discovery) is irrelevant since they don't realign their reads internally. We kept the default gatk alignment behavior and thus the SW parameters that are used (for dangling head recovery which I believe are the old arguments) still match. As far as unifying the parameters I suspect it could be done though one wonders if there aren't risks where the different contexts in which we use the parameters will not perform as well with a unified set. Speculation on my part though. I agree with David that we should be cautious about making changes that will affect the HaplotypeCaller before November. . I support including an argument in any case (possibly multiple) to include the SW parameters. I would actually advocate we read these files in as tables of parameters where you simply point to on the command line to configure new parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993:583,recover,recovery,583,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993,2,"['recover', 'risk']","['recovery', 'risks']"
Safety,"@samuelklee I think it's right for what we're doing. We mount the test data as `/testdata` and then create a symlink from src/test/resources to /testdata to provide it to the test files. It seems to work.; ; I'm not clear what they get more of the other way around. More tests? Are they using our create docker script? Or our travis file? Or something else? I think we might just be able to just directly mount test data to src/test/resources and avoid the symlink, but I probably had a reason when I set it up that way... I think this is a non-issue unless they can provide more information.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3730#issuecomment-339439156:447,avoid,avoid,447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3730#issuecomment-339439156,1,['avoid'],['avoid']
Safety,"@shuaiwang2 Hi, we don't currently support indexes that long. We use a bai index for bams and tabix for vcf which only support up to 512 M. You need to use a CSI index for references that large but we don't support writing those. (Reading them is weird, I think we can read BAM csi indexes but not VCF ones). . It might be possible to work around this issue by setting `--create-output-variant-index false`, although downstream gatk tools would need an index if you're sharding them. Otherwise I recommend splitting your chromosomes into two separate parts and calling on the split chromosomes. Splitting along a long region of N's should be a safe way to avoid missing any useful calls. (The telemere might be a good spot unless you have a T2T reference.). . We should probably improve that error message to make it clear what the problem is.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8192#issuecomment-1422828609:644,safe,safe,644,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192#issuecomment-1422828609,2,"['avoid', 'safe']","['avoid', 'safe']"
Safety,"@tomwhite To clarify, I think that the caller of `ensureCapacity()`, namely `GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables()`, also needs to be synchronized in order to avoid some unlikely but still-possible races. Given this, I think that we should consider whether `ThreadLocal` might be a better option here. It's not 100% clear to me whether a `ThreadLocal` `get()` call is cheaper than a synchronized method call, but some casual googling suggests that it might be. If we're going to end up entering a synchronized method on every single call to `GenotypeLikelihoodCalculators.getInstance()`, we might want to do some research into whether `ThreadLocal` + no synchronization would be faster, since I believe that this is a performance-sensitive section of code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422171244:187,avoid,avoid,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422171244,1,['avoid'],['avoid']
Safety,"Additional feedback from the user for the mutect2 workflow. > ""Of note, it is really difficult and not really 'user-friendly' to have to predict disc space and runtime for Funcotator, which seem to depend (based on calculations you copied above from other Functotator workflows) on outputs of Mutect2 (eg vcf sizes), when here Mutect and Funcotator and bundled together. So I cannot see output of Mutect to predict values for Funcotator - especially not when I get to run this over hundreds of samples. It is also pricey to have jobs failing because of this. It would be much better to have these variables encoded, so that the algorithm uses Mutect outputs to predict memory etc. that it will need to run Funcotator downstream. If this is really how things work (and this is my current understanding), I really do not know how to estimate this for many samples without 'trial and error' that is both costly and it will take extremely long time....""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6680#issuecomment-651354230:137,predict,predict,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6680#issuecomment-651354230,3,['predict'],['predict']
Safety,"Are the errors below part of this, when starting BwaSpark with spark-submit?; I activated ""--disable-sequence-dictionary-validation true"", but that doesn't help. It is very unclear, why a BAM is not recognized as a BAM file. I have tried all kinds of ways to make sure that it is a BAM and not a SAM file.; The documentation for BwaSpark also says ""BAM/SAM/CRAM file containing reads"", so if SAM files are really not possible, that should probably be changed.; ...; Even on verbosity DEBUG, the comments are not at all helpful to get at the problem.; E.g. ""Cannot retrieve file pointers within SAM text files.""; Is that a general statement about SAM files? Or does it only say, that in this specific SAM file (which is actually a BAM file), file pointers cannot be found?; What pointers are meant exactly?; How could this be fixed?. ```; ""SamReaderFactory	Unable to detect file format from input URL or stream, assuming SAM format.""; Which URL?; Which stream?; Why would this happen? What could be the error?; The SAM/BAM distinction seems very unclear. It would be more helpful, if some specific missing aspect (e.g. not queryname sorted) would be clearly declared as the culprit.; ...; 00:29 DEBUG: [kryo] Write: SAMFileHeader{VN=1.5, SO=queryname}; ...; WARNING	2018-01-16 02:11:25	SamReaderFactory	Unable to detect file format from input URL or stream, assuming SAM format.; ...; java.lang.UnsupportedOperationException: Cannot retrieve file pointers within SAM text files.; 	at htsjdk.samtools.SAMTextReader.getFilePointerSpanningReads(SAMTextReader.java:185); ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4131#issuecomment-357838062:866,detect,detect,866,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4131#issuecomment-357838062,2,['detect'],['detect']
Safety,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:373,unsafe,unsafe,373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661,1,['unsafe'],['unsafe']
Safety,"As we discussed, it's possible that these are simply common germline CNVs that are being median-normalized out in CR by the PoN. Let's investigate the sample-median-normalized counts in some of the questionable regions, along with the per-bin medians in the PoN. I do not think a gCNV run is necessary (it will probably be a bit expensive, anyway). More generally, I think a better approach to germline tagging would be to avoid the caller entirely. Let's take the ModelSegments output for a normal, and then tag ModelSegments segments in the tumor that sufficiently overlap any normal segment in CR-AF-genomic space (where we have some freedom to define the overlap criteria). Essentially, let's just try to highlight differences between the tumor and normal in CR-AF space. This would rescue events in the normal that may be further amplified or deleted in the tumor. Subsequently, simple filtering of these events would be less misleading than imputation. I do not think such tagging should be implemented in Java, if we can avoid it. Rather, a relatively simple python script that runs through each tumor segment and checks for overlaps would suffice. This script could output a tagged/filtered ModelSegments result, as well as do the conversion step for downstream tools. This also obviates the need for the Java code for combining segment breakpoints and additional CNV collection classes in the current post-processing tools. What do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-442911810:423,avoid,avoid,423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-442911810,2,['avoid'],['avoid']
Safety,"First-pass review complete -- back to @tomwhite. Many of my suggestions center around pushing arguments and functionality up into `GATKSparkTool` as much as possible, even if they're not applicable to every tool, as we ideally want to spare tool authors from having to manually manage these low-level Spark parameters when they don't want/need to, and we also want to enforce consistency across tools and avoid duplicated boilerplate code. At the same time, there should be clear mechanisms for tools to override the defaults when they have to (eg., overridable methods in `GATKSparkTool`), as I'm not sure whether tools like BQSR are going to be happy with the new 128 MB default input split size.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1432#issuecomment-172100907:405,avoid,avoid,405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1432#issuecomment-172100907,1,['avoid'],['avoid']
Safety,"For a quick analysis, I made a serialized versions of DBSNP (13572728 variants from `dbsnp_135.b37.excluding_sites_after_129.vcf`), size of VCF on disk 2175071049 bytes (2.0G). (all false postive probs are predicted, it'd be easy to measure it too). Map keys are contig names. ```; Map of String->BloomFilter with 0.001 false positive prob = 27320529 bytes (26M); Map of String->BloomFilter with 0.0004 false positive prob = 30943001 bytes (30M); Map of String->BloomFilter with 0.0001 false positive prob = 36423625 bytes (35M); Map of String->BloomFilter with 0.00004 false positive prob = 40046089 bytes (38M); Map of String->BloomFilter with 0.00001 false positive prob = 45526681 bytes (43M); Map of String->BloomFilter with 0.000001 false positive prob = 54629745 bytes (52M); Map of String->int[] of positions = 60790452 bytes (58M); List<GATKVariant> made just like the one in spark BQSR = 366463957 bytes (349M); ```. Variants from dbSNP cover 0.004 of the genome (15195436 bases of 3101804739) so if we want reasonable precision (number of false positives over all reported hits), say 0.9 precision (of 10 hits only 1 can be false) we need (1-0.9) x 0.004 false positive prob = 0.0004. For 0.99 precision (of 100 hits only 1 can be false) we need (1-0.99) x 0.004 false postive prob = 0.00004. These are approximations of course. Given these numbers, I conclude that, for now, exploring BloomFilters does not seem to make sense (too little saving and too many complications with using a probabilistic data structure - eg we'd need to use it too for the walker BQSR). It does make sense however to explore alternatives to the list of GATKVariants because it's very big when serialized (maybe Kryo does a better job but it's still a big object). A simple alternative like sorted int[] may be sufficient and has attractive properties (trivial to implement and understand, O(log) lookups, 0% false positives, small size when serialized).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1407#issuecomment-203727133:206,predict,predicted,206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1407#issuecomment-203727133,1,['predict'],['predicted']
Safety,"For reporting the number of reads that fails each of the filters, the composed filter could be changed by a `CountingReadFilter`; using the `getSummaryLine()` method will provide the number of reads failing each of the components. Developers could have in their tools a field with the `WellFormedReadFilter` and call a new method for reporting the summary, to log a warning/debug line. For exploding depending on the tool, maybe an advance/hidden argument can be added to the filter (something like `--failOnMalformed`) to throw an exception if true; developers might add a default filter with this value equals to true if they want to enforce by default this behaviour. I think that this a simpler idea for allow the developer to choose, and give some flexibility to the user to change the behaviour as its own risk (they can disable all filters anyway, which is also risky).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699:812,risk,risk,812,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699,2,['risk'],"['risk', 'risky']"
Safety,Hello @bbimber thank you for the response. I would recommend using the read filters (in your case `-rf MappingQualityReadFilter --minimum-mapping-quality ##` to achieve the same functionality as the `-mmq` argument from GATK3. When porting over the tool we tried to push as much functionality from obscure arguments into the existing filtering framework as possible and `-mmq` was one of the ones that was redundant as it was a simple filter placed on the reads before counting them which the existing filtering code was able to handle. I will add some lines to the documentation clarifying this for users in the future.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6617#issuecomment-634752928:406,redund,redundant,406,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6617#issuecomment-634752928,1,['redund'],['redundant']
Safety,"Hi mwalker174,; I tried both command lines. As lbergelson predicted, the one with --spark-runner LOCAL produces the same error as before (see below, could you explain me why?), while the one with --spark-runner SPARK runs smoothly. . Is this option ok with running a master-workers system? Can I use this option safely with . I have now a different issue with PathSeqPipelineSpark? As I tried, the first error solved, but I have another issue (I think it's better to open a new thred for that, since it is about an input file not found). Thank you! . ```; -bash-4.1$ ../../../gatk PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt --spark-runner SPARK; Using GATK jar /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar; Running:; /home/int/eva/username/bin/spark-2.2.0-bin-hadoop2.7//bin/spark-submit --master spark://xx.xx.xx.xx:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:58,predict,predicted,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,"['predict', 'safe']","['predicted', 'safely']"
Safety,"How much does count collection cost at the desired bin size? How does this compare to bincov? Perhaps we could eliminate one of these steps if redundant. Note that the read counts are read once and stored in memory, so unless this takes a significant amount of time, then indexing is probably not the highest priority here (although I agree it would be nice to have in general). One related issue, as you mention, is file localization---since each shard only operates on a portion of the counts in each sample, it is a bit wasteful to localize the whole file. But how much does file localization cost? I can't imagine that it is the lowest hanging fruit. One of the more important issues, which you also mention, is optimizing parameters for inference. This includes not only the minimum number of epochs for training, but also things like the learning rate, annealing schedule, iterations per epoch, conditions for epoch convergence, etc. I'll be talking about how to tune these inference parameters---as well as other things in the pipeline---at the next BSV meeting. Let's brainstorm more things to try and prioritize them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5288#issuecomment-427562932:143,redund,redundant,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5288#issuecomment-427562932,1,['redund'],['redundant']
Safety,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:322,avoid,avoid,322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208,1,['avoid'],['avoid']
Safety,"I have a good feeling about numerical instability from this point forward because:. * My terminology was lazy. It's not really ""numerical instability,"" which is a deep and frightening topic, but rather just plain old finite precision, which is not nearly so hydra-headed a problem.; * I learned the general rule for avoiding finite precision problems with a qual score, which is: always calculate probabilities of alleles being absent. Previously I was calculating the probability that samples had an allele and subtracting (in log space) that from 1. The problem with that is that for very good GQs this probability is so closed to 1 that quals can become infinite. In this PR we add up the probabilities of genotypes that don't have the allele, which is small but non-zero and everything works fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434769078:316,avoid,avoiding,316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434769078,1,['avoid'],['avoiding']
Safety,"I like the idea of the modified regexes, that seems like the best balance of usability and flexibility/power. I'd rather avoid having a slew of new special-cased arguments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/588#issuecomment-309815640:121,avoid,avoid,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/588#issuecomment-309815640,1,['avoid'],['avoid']
Safety,"I think it triggers in certain situations where a firewall is blocking the connection. If the internet is simply unreachable it doesn't happen, so I don't know what the exact error case is. It happened consistently for people inside Intel's firewall or vpn. . An option to disable gcs support isn't a bad idea, it's kind of a hack though, it would be better if we could understand and avoid triggering the problem. If we could only initialize GCS support when we are sure that we actually are accessing files from google that could be a useful, but it doesn't seem like there's any single point we can plug into to detect that, it would have to be spread over everything that uses paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141:385,avoid,avoid,385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141,2,"['avoid', 'detect']","['avoid', 'detect']"
Safety,"I'd also be hesitant to break the previous expectation that IntervalArgumentCollection contains a non-empty list of intervals. If I understand correctly (and apologies if not, I'm glancing at the repo between paternity-leave duties and am quite sleep deprived!), all calling code would have to add an explicit check that the new option isn't enabled or risk failing ungracefully downstream. For CNV code, this might be as simple as changing the validation method `CopyNumberArgumentValidationUtils.validateIntervalArgumentCollection`, but I wouldn't generally expect it to be so straightforward to add such checks throughout the codebase. I also agree with @lbergelson that the expected behavior might not be immediately clear and that perhaps this could be addressed in the scattering step---seems like shards could just be limited to regions that cover the resource at the outset. Consider also an older comment at https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435588845 about whether or not we should just use the equivalent Picard tool (horrible glob aside).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540740687:353,risk,risk,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540740687,1,['risk'],['risk']
Safety,"I've implemented the Gaussian-kernel binary-segmentation algorithm from this paper: https://hal.inria.fr/hal-01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM m",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:696,recover,recover,696,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666,1,['recover'],['recover']
Safety,"In GATK4, the way to make a tool multithreaded is to implement it as a Spark tool. All Spark tools can be trivially parallelized across multiple threads using the local runner, and across a cluster using spark-submit or gcloud. . We wanted to avoid the complexities of implementing our own map/reduce framework, as was done in previous versions of the GATK, and instead rely on a standard, third-party framework to keep the GATK4 engine as simple as possible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273206164:243,avoid,avoid,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273206164,1,['avoid'],['avoid']
Safety,"It seems to me the `Header definition line` encompasses the information given by the `VCF Field` so this latter is redundant. . It would definitely be useful to categorize INFO (cohort) versus FORMAT (SAMPLE) level annotations. I'm not clear on the significance of the `Type` nor `Category` fields. `Type` might be the groupings, e.g. HaplotypeCaller standard annotations versus Mutect2 standard annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-344423143:115,redund,redundant,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-344423143,1,['redund'],['redundant']
Safety,"Just to make sure I understand the issue---will this cause technical problems in the Firecloud environment, or is it more of a style issue?. If the latter, one reason I prefer the use of optional file inputs to trigger tool-level ""modes"" when possible is that it propagates more naturally from the tool level. For example, let's consider a tool that can operate in either tumor-only or matched-pair mode. It is natural at the tool level to make the tumor a required input and the normal optional. The other options are quite awkward: 1) make both inputs required and switch between using the normal or not with a flag (in which case it is very easy for the user to shoot themselves in the foot if they forget to set the flag right, and we'd have to pass a dummy normal every time we want to run tumor only if we don't actually have a pair), 2) leave the normal as optional but add a flag anyway, which would be redundant and require an additional validation (i.e., if the flag is set to matched mode but we don't have a normal, we should fail early), or 3) write separate tools for each mode with the corresponding required inputs. If we accept that optional file input is the way to handle such a scenario at the tool level but not at the workflow level, then we will simply run into the same problems at the workflow level. I'm sure there are more complex scenarios when triggering on file presence/absence doesn't uniquely specify a workflow, in which case flags are a must. But for simple scenarios, I'm not sure why we shouldn't take advantage of the ability to specify optional file inputs in WDL (actually, I'm not sure how else we are supposed to use them?). However, if this is a problem for Firecloud, then I'd like to understand why---and what possible solutions there might be.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334046444:911,redund,redundant,911,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334046444,1,['redund'],['redundant']
Safety,"Looks like this failed on travis. I think given that given the lateness of the hour (release wise), we might want to take the original change that removes the libgcc-ng dependency, since that passed on travis, and rely on the simple workarounds for osx, which we'll have to convey out-of-band. Anything that requires changing the docker image seems risky at this point, not to mention that the image is already at 5.2 gig, which is way over our desired target. @samuelklee Any thoughts on this ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4087#issuecomment-356131086:349,risk,risky,349,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4087#issuecomment-356131086,1,['risk'],['risky']
Safety,MapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41458,abort,abortStage,41458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['abort'],['abortStage']
Safety,"OK - PedigreeValidationType is now set in the constructor and is final. This does not separate the two intertwined codepaths around PedigreeFile vs. FounderIds, but that was a pre-existing problem. It doesnt doesnt change the pre-existing weirdness around the timing of setting pedigreeFile and/or founderIds within GATKAnnotationPluginDescriptor, where PedigreeAnnotation gets special treatment. I dont think this makes that situation any worse. if you still have concerns on this proposal, I actually think I could make our code work if you simply exposed a protected getPedigreeFile() method on PedigreeAnnotation. I can make the SampleDB instance in my code without needed to share code here. It seemed useful to expose some of that code to avoid duplication, but if it's going to over-complicate we can remove it. Also: that one test failure seems potentially unrelated (https://travis-ci.com/github/broadinstitute/gatk/jobs/510624560)? A compile issue with javadoc?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853986169:745,avoid,avoid,745,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853986169,1,['avoid'],['avoid']
Safety,"OK, the first test run I tried was with 1kb bins and *no additional normals*. Coverage takes about an hour to collect per BAM and ploidy inference takes about 10 minutes. A few things:. 1) Looks like we are concordant with the truth CN on X for all but 3/40 of the samples. The GQs for these discordant calls are low (~3, 23, and 25 compared with ~400 for most of the others). 2) However, we are striking out on over half of the samples on Y. We mostly call 1 copy when the truth calls 0. Mehrtash thinks this is because a) I didn't mask out any PARs or otherwise troublesome regions on Y and b) I didn't include any other normals. I'll try rerunning with a mask first, then with other normals, and then with both. Hopefully this should clear up with just the mask. 3) There are a few samples where we strike out because the truth calls 2 copies on Y and we call 1. Mehrtash pointed out that this is most likely because the prior table we put together assumes Y can have at most 1 copy. So hopefully these are trivially recovered once we relax this. 4) The GQs are weirdly high on 1, X, and Y compared to the rest of the autosomes. @ldgauthier any idea why this might be? If there's no reason, then something funny is going on within the tool. I haven't gotten a chance to plot any of the counts data yet, either, which may make things more obvious. I'll do this today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-364234449:1020,recover,recovered,1020,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-364234449,1,['recover'],['recovered']
Safety,"OK. However, don't forget that the denoising model is fit independently in each block. So introducing too many blocks could cause overfitting, in a sense. Also, you want to make sure that you have enough bins in each block to learn the model. 10k seems safe, but I'd spot check results first if you want to go down to 1k.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-391071615:253,safe,safe,253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-391071615,1,['safe'],['safe']
Safety,"One of our goals for alpha (https://github.com/broadinstitute/gatk/issues/961) is actually to wrap `spark-submit` and its many options to make it easier to run hellbender tools on spark. We want users to be able to type a simple command like `./hellbender ToolName [toolArgs] --sparkMaster X`, and have hellbender figure out whether to invoke `spark-submit` or `gcloud dataproc` on their behalf, and provide sensible defaults for all relevant spark options. . Perhaps there is a way in `SparkCommandLineProgram` to detect whether an option has already been set externally, and allow the default to be overridden if it has been?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1070#issuecomment-152538633:515,detect,detect,515,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1070#issuecomment-152538633,1,['detect'],['detect']
Safety,"Rationale for engine changes:; This tool opens a large number of feature files (TSVs, not VariantContexts) and iterates over them simultaneously. No querying, just a single pass through each.; Issue 1: When a feature file lives in the cloud, it takes unacceptably long (several seconds, typically) to initialize it. A few seconds doesn't seem like a long time, but when there are large numbers of feature files to open, it adds up. This is caused by a large number of codecs (mostly the vcf-processing codecs) opening and reading the first few bytes of the file in the canDecode method. To avoid this I've reversed the order in which we test each codec, checking first if it produces the correct subtype of Feature, and only then calling canDecode. If you don't know what specific subtype you need, you can just ask for any Feature by passing Feature.class. It's much faster that way.; Issue 2: Each open feature source soaks up a huge amount of memory. That's because text-based feature reading is optimized for VCFs, which can have enormously long lines. So huge buffers are allocated. The problem is compounded for cloud-based feature files for which we allocate a large cloud prefetch buffer. (Though that feature can be turned off, which helps a little.) But the biggest memory hog is the TabixReader, which always reads in the index, regardless of whether it's used or not. Tabix indices are very large. To avoid this, I've created a smaller, simpler FeatureReader subclass called a TextFeatureReader that loads the index only when necessary. The revisions allow the new tool to run using an order of magnitude less memory. Faster, too.; Issue 3: The code in FeatureDataSource that creates a FeatureReader is brittle, and tests for various subclasses. To allow use of the new TextFeatureReader, I added a FeatureReaderFactory interface that allows one to ask the codec for an appropriate FeatureReader.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1284340770:590,avoid,avoid,590,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1284340770,2,['avoid'],['avoid']
Safety,Seems like something like https://github.com/broadinstitute/gatk/issues/4794 could be avoided if we rewrote this. It seems like a pretty simple rewrite too...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4535#issuecomment-391044481:86,avoid,avoided,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4535#issuecomment-391044481,1,['avoid'],['avoided']
Safety,"Since this touches a lot of files, I'll categorize changes here:; 1. Convenience Script Changes; - bug fixes for running on Linux; scripts/sv/manage_sv_pipeline.sh; - detect number of preemptible workers to choose NUM_EXECUTORS correctly; scripts/sv/run_whole_pipeline.sh; - allow sanity_checks.sh to run from outside GATK_DIR, exit correctly on error; scripts/sv/sanity_checks.sh. 2. Minor changes to existing utils to support new filter; - allow construction of IntHistogram.CDF from known cdfFractions and nCounts; src/main/java/org/broadinstitute/hellbender/tools/spark/utils/IntHistogram.java; - in constructor, coverage is passed as a float; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/ReadMetadata.java; - add xgboost maven repository for gradle; build.gradle. 3. Significant changes to existing code to support/invoke new filter; - add arguments for XGBoostEvidenceFilter, changes for scaling density filter by coverage; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/StructuralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hel",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:167,detect,detect,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,1,['detect'],['detect']
Safety,"Thanks a lot for all your feedback about this @lbergelson and @droazen. From my side this could be close now, although it may be useful to have some of this information in the Wiki to avoid confusion. Thank you very much again!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273732039:184,avoid,avoid,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273732039,1,['avoid'],['avoid']
Safety,"Thanks for helping me to understand why you didn't mark the metadata. This may seem like quibbling, but I'd suggest that we mark the metadata with a comment character, and let the pandas/R users remove it. They'll notice if they forget to do that, because the columns won't be named as expected, and they'll have to fix it up. Whereas the risk for automated programs is that they'll simply delete the first row, which might be real data if the file has been reordered for some reason, or if the tool implementing the standard is non-compliant. The resulting bugs will be subtle, and might easily go undetected. Building in behavior to delete lines starting with ""CHROM\t"" seems odd and fraught with peril in a way that building in behavior to strip comments, doesn't. That's my take, anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480956381:339,risk,risk,339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480956381,1,['risk'],['risk']
Safety,"Thanks for your feedback, @cmnbroad. In my case, I think that `IntegrationTestSpec` is a good way of avoid complicated code to test tool results, but it is true that it have some problems (one that I had was the usage for testing programs where the outputs are determined by a prefix in the command line, but with different suffixes). I think, from the API user point of view, that a class like `IntegrationTestSpec` to facilitate program output testing (including user exceptions) will be nice for developing purposes. Nevertheless, this is just a convenience that I asked for here, but I can try to solve the issues with the `BaseTest` instead. By the way, I would love to have this interface in GATK at least for now, because several of my tools rely on the `IntegrationTestSpecs` for development...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889:101,avoid,avoid,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889,1,['avoid'],['avoid']
Safety,"The `Poisson` arises because we want to our model to generate the *occurrences*, assuming that each *count bin* provides equal weight---rather than the counts themselves. As usual, modeling each bin as Poisson is close enough to modeling all bins as multinomial for our purposes. If we directly use the NB likelihood and simply weight the count likelihood by occurrences, occurrences in the peak will strongly affect the result, adversely so if the count likelihood is actually misspecified there. As an example, consider trying to fit a Poisson to data that is actually zero-inflated Poisson---fitting the histogram will actually result in a more robust estimate for the mean. Another benefit is that truncated data (as we have here) is straightforwardly handled in an unbiased way. In the special case of complete, trivially-binned data, the full, unbinned likelihood is recovered. I think this sort of histogram fitting is pretty standard in the astro/particle community. We can certainly change up the model to include strictly quantized + free-floating states as you describe (rather than the ""fuzzily quantized"" states I use here), but I just wanted to avoid having another level of mixtures/logsumexps for this quick prototype. However, note that modeling mosaicism on the autosomes is desirable, but there we also want the strong diploid prior to nail down the depth and per-contig bias. So we will have to be a little careful about how we introduce free-floating states. Also, since I was not using gcnvkernel, I had to integrate out all discrete parameters. It may be that we can write down a nice model with discrete parameters if we use your inference framework. Finally, I did not further bin the counts here (or rather, the bin size is 1), which already yields the maximum information, but I did use a maximum-count cutoff. If we use the same cutoff for all samples, this allows us to simply pass a non-ragged matrix from Java (with dimensions of samples x contigs x maximum count) as a ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522:873,recover,recovered,873,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522,1,['recover'],['recovered']
Safety,"The sequences of 150,119 genomes in the UK Biobank.](https://pubmed.ncbi.nlm.nih.gov/35859178/; Halldorsson BV, et al. Nature. 2022 Jul;607(7920):732-740. doi: 10.1038/s41586-022-04965-x. Epub 2022 Jul 20.PMID: 35859178. On page 69+ of this pdf, they describe the problem and how they cleverly worked around it. ; ; https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-022-04965-x/MediaObjects/41586_2022_4965_MOESM1_ESM.pdf. _It should be noted that running GATK out of the box will cause every job to read the entire; gVCF index file (.tbi) for each of the 150,119 samples. The average size of the index files is ; 4.15MB, so each job would have to read 4.15*150,126 = 623GB of data on top of the actual; gVCF slice data. For 60,000 jobs, this would amount to 623GB*60,000 = 37PB or 25.2GB/sec; of additional read overhead if the jobs are run on 20,000 cores in 17 days. This read; overhead will definitely prevent 20,000 cores from being used simultaneously. However,; this problem was avoided by pre-processing the .tbi files and modifying the software; reading the gVCF files from the central storage in a similar fashion as we did for GraphTyper; and the CRAM index files (.crai)._. This explains why chr1 requires more memory than chr22 despite running on the same number of samples. The larger chr1 tbi index is the source of the memory problem. The Decode solution is too limit the reading of the tbi index to the part that indexes the scattered region. There is a long pause at the beginning of the running GenotypeGVCFs which I never understood. GATK must be the reading of all the sample's gvcfs tbi into memory during that pause. So the reblocking of the gvcfs above reduced the memory foot print by decreasing the tbi size. Decode reduced it by chopping up the index so for each scattered region, GATK could only read a small subset of the index needed for that region. The combination of reblocking and chopping up the tbi would help with the memory requirements even more. Ho",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579:1142,avoid,avoided,1142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579,1,['avoid'],['avoided']
Safety,"The test failures in the branch build are clearly related to the recent travis key migration. The PR build (which is the one we care about) passes, so this should be safe to merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7393#issuecomment-953279491:166,safe,safe,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7393#issuecomment-953279491,1,['safe'],['safe']
Safety,"There have been a few instances like this though. This one was obviously accidental, but things like the correct spelling of @magicDGS actual name seem like reasonable things to be able to include in the source. Also, testing non-ascii characters seems like something that is going to be increasingly common as we support new versions of the spec so it seems like we should learn to avoid this problem...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5936#issuecomment-492761095:383,avoid,avoid,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5936#issuecomment-492761095,1,['avoid'],['avoid']
Safety,"This task is to take the training data generated in issue #3092 and learn something from it, for example a regression model that predicts a distribution of artifactual read fractions. Using the learned model in filtering is a separate issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2973#issuecomment-307680993:129,predict,predicts,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2973#issuecomment-307680993,1,['predict'],['predicts']
Safety,"To add some commentary to why this is happening: It looks like multiple threads are hitting this line simultaneously and based on the overload of `ArrayList.add()` this error could be triggered by multiple calls to `ensureCapacityInternal()` inside the add method:; ```; final List<ReadsPathDataSource> readSources = new ArrayList<>(threads);; final ThreadLocal<ReadsPathDataSource> threadReadSource = ThreadLocal.withInitial(; () -> {; final ReadsPathDataSource result = new ReadsPathDataSource(readArguments.getReadPaths(), factory);; readSources.add(result);; return result;; });; ```; The fix should be simple you just have to make sure ti synchronize the initialization or swap out the readSources object to one that is itself thread safe. @vruano",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899732721:739,safe,safe,739,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899732721,1,['safe'],['safe']
Safety,"Updated plan. ----------; ## Small improvements in new interpretation tool; ; - [x] Output bam instead of sam for assembly alignments; - [x] Instead of creating directory, new interpretation tool writes files (behavior consistent with current interpretation tool); - [x] Prefix with sample name for output files' names; - [x] Add `INSLEN` annotation when there's `INSSEQ`; - [x] Clarify the boundary between `AlignedContig` and `AssemblyContigWithFineTunedAlignments`; - [x] Increase test coverage for `AssemblyContigAlignmentsConfigPicker`; ; ----------; ## Consolidate logic, bump test coverage and update how variants are represented. ### consolidate logic; When initially prototyped, there's redundancy in logic for simple variants, now it's time to consolidate. - [x] `AssemblyContigWithFineTunedAlignments`; - [x] `hasIncompletePicture()`. - [x] `AssemblyContigAlignmentSignatureClassifier`; - [x] Don't make so many splits; - [x] Reduce `RawTypes` into fewer cases; ; - [x] `ChimericAlignment`; - [x] update documentation; - [x] implement a `getCoordinateSortedRefSpans()`, and use in `BreakpointsInference`; - [x] `isNeitherSimpleTranslocationNorIncompletePicture()`; - [x] `extractSimpleChimera()`. ### bump test coverage; Once code above is consolidated, bump test coverage, particularly for the classes above and the following poorly-covered classes; - [x] `ChimericAlignment`; - [x] `isForwardStrandRepresentation()`; - [x] `splitPairStrongEnoughEvidenceForCA()` ; - [x] `parseOneContig()` (needs testing because we need it for simple-re-interpretation for CPX variants) Note that `nextAlignmentMayBeInsertion()` is currently broken in the sense that when using this to filter out alignments whose ref span is contained by another, check if the two alignments involved are head/tail. - [x] `BreakpointsInference` & `BreakpointComplications`. - [x] `NovelAdjacencyAndAltHaplotype`; - [x] `toSimpleOrBNDTypes()`. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence`; - [x] serialization ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021:696,redund,redundancy,696,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021,1,['redund'],['redundancy']
Safety,"We can add this as an option, but I think there are a few arguments against outright replacement (which may have led to this design decision in the first place):. 1) I don't believe any CNV tools take in sample name as input at the GATK command line, by design. We instead take the BAM basename as the entity ID during the CollectCounts task; this ID is then passed along at the WDL level and is only used to construct the filenames of output files. This obviates the need for tools like GetSampleName and avoids issues with parsing funky sample names at the command line, handling/passing special characters at all levels (WDL, Java, python), etc. (The implicit assumption is that the BAM basename is more likely to be well formed.). 2) I would argue that specifying a sample index is relatively user friendly, especially if we are typically running on all samples. In that case, all you need to know is the total number of samples and that we use zero indexing, and then you simply iterate over all indices. (If you want to run on a single, particular sample, then perhaps using the sample name might be more friendly, but I'd argue that this use case is not typical.). 3) If you want to go ahead and add this option, I would probably keep the directory structure of the GermlineCNVCaller output the same (i.e., with folders named ""SAMPLE_#""), and just check the sample_name.txt files at the PostprocessGermlineCNVCalls step. I don't think this should require GermlineCNVCaller code changes, right?. 4) We may require additional code at the WDL level if we want to both switch over to primarily using sample names but also get rid of bundling (i.e., by passing only the calls for each sample when needed). Locally, you can always just search all output for directories containing the appropriate sample_name.txt. But on the cloud, you'd want to make sure that the postprocessing step for a particular sample gets only its corresponding directories, which would have to happen at the WDL level; the c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765:506,avoid,avoids,506,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765,1,['avoid'],['avoids']
Safety,"We can certainly modify IntervalListTools to make the behavior more intuitive (e.g. add a new mode for `INTERVAL_COUNT_WITH_OVREFLOW`), but I'm not sure I understand the issue. We're just trying to avoid calling the GATK command if the scatter is going to be a noop?. The GATK SplitIntervals has slightly different behavior that's helpful in some cases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6502#issuecomment-599618889:198,avoid,avoid,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6502#issuecomment-599618889,1,['avoid'],['avoid']
Safety,"We can implement this, but first can you explain why your `canDecode()` methods can't unambiguously detect your file formats? The VCF/BCF codecs use a magic value at the start of the file to detect the format -- are your codecs guessing as to the intended format? Is there no way to be sure what the format is?. If we have multiple codecs able to decode a file, and only one produces `Features` that match the type parameter of the `FeatureInput`, would it solve your problem if we selected that codec rather than blow up? This would be a bit simpler/easier than a new annotation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163297503:100,detect,detect,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163297503,2,['detect'],['detect']
Safety,"We decided to remove the ""conversion"" to AllelicCapseg output from ModelSegments, since this was an ill defined procedure. The models used by AllelicCapseg and AllelicCNV/ModelSegments are simply different, so it's not possible to define a unique conversion between their model parameters. Compounding this, we also had difficulty finding up-to-date documentation about the models used by various versions of both AllelicCapseg and ABSOLUTE. That said, some of this removed functionality can be found in unsupported WDLs at https://github.com/broadinstitute/gatk/tree/master/scripts/unsupported/combine_tracks_postprocessing_cnv (specifically, see the PrototypeACSConversion task in combine_tracks.wdl). These scripts also attempt to perform rudimentary filtering of germline events found in the matched normal; see first link below for some additional caveats. Note that we cannot really answer further questions or otherwise support these scripts (and it's possible that the experimental/beta GATK tools used in the WDLs may be removed in the future), and the developer responsible for them has moved on from the Broad---use them at your own risk. See also https://gatkforums.broadinstitute.org/gatk/discussion/comment/59467 https://github.com/broadinstitute/gatk/pull/5450 https://github.com/broadinstitute/gatk/issues/5804 for additional context.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6685#issuecomment-652407603:1144,risk,risk,1144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6685#issuecomment-652407603,1,['risk'],['risk']
Safety,"We decided to replace SparkGenomeReadCounts with a relatively simple ReadWalker to avoid various bugs we were running into (some of which were due to Hadoop-BAM). We found that these bugs gave rise to a relatively high failure rate---roughly 1 in 50 TCGA BAMs. Like any ReadWalker, you can specify custom read filters using GATK engine arguments such as `--disable-default-read-filters` and `--read-filter ...` However, because we count fragment centers (rather than read starts, as in SparkGenomeReadCounts), disabling filters which check that reads are properly paired may lead to unexpected behavior. In principle, we could write a similar ReadWalkerSpark version of the tool. However, our experience running the tool showed that CollectFragmentCounts was already faster than SparkGenomeReadCounts in Spark local mode, sometimes by a factor of ~5 (and, more importantly, it didn't run into Hadoop-BAM failures). We may do some more careful profiling and roll a ReadWalkerSpark version in the future, but these aren't too high priority at the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4185#issuecomment-358660583:83,avoid,avoid,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4185#issuecomment-358660583,1,['avoid'],['avoid']
Safety,"Yes. We should expect that PCR error shouldn't affect the base-quality, so two high quality, disagreeing bases are an indication of a PCR error, while one low-quality base, and one high quality base that have differing qualities looks more like a sequencing error. We might be able to obtain a data-driven model for that using the overlapping bases themselves (over monomorphic sites). The only problem is that this is only true when the reads haven't been processed by Consensus calling....but if we have a good model for consensus calling within haplotype caller we could avoid doing that upfront and simply deal with everything within haplotype caller. **That** would be ideal!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400815571:574,avoid,avoid,574,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400815571,1,['avoid'],['avoid']
Safety,"aps pushing a fresh branch to this repo might make it a little easier for us to check it out for review---again, not a big deal, so I'll leave it up to you. 2) We try to adhere to the Google style guide https://google.github.io/styleguide/javaguide.html, so the review may yield a lot of seemingly minor and nitpicky change requests. Don't take these personally---the goal is just to make the code base as uniform and easy to maintain as possible! If you prefer, I'm sure we can find a GATK developer to take a quick once over of your branch and make these minor changes. 3) Since the new tool borrows so heavily from CollectAllelicCounts, I think it might be worth consolidating shared code and reducing code duplication---again, with the goal of making future maintenance more straightforward. I'll try to identify some places this can be done during my review. Again, we can make these changes on our end during the once over, or you can address them after the review (or we could also do this on our end in a separate PR after this one goes in). 4) In the near future, I think we should finally make the effort to replace both GetPileupSummaries and CollectAllelicCounts with this new tool. As mentioned in our email thread, @davidbenjamin and I discussed this long ago, e.g. https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926. From a methods perspective, we'd simply need expand the current functionality of your tool to also report the reference allele and do some quick sanity checks to make sure that the differences in count definition and read filtering don't have any undesired downstream effects. However, as we also discussed, this will come with some additional overhead---we'll need to update documentation, workshop slides, tutorials, WDLs, and make sure that any changes in output formats are clearly highlighted in the release notes. I'll leave this effort to @davidbenjamin and @mwalker174. Thanks again for doing this. Let us know how you'd like to proceed!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6543#issuecomment-610462293:1851,sanity check,sanity checks,1851,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6543#issuecomment-610462293,1,['sanity check'],['sanity checks']
Safety,"dk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f) [duplicate 1]; 2019-01-07 11:34:12 INFO TaskSetManager:54 - Starting task 3.3 in stage 0.0 (TID 11, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:12 INFO TaskSetManager:54 - Lost task 1.3 in stage 0.0 (TID 9) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 3]; 2019-01-07 11:34:12 ERROR TaskSetManager:70 - Task 1 in stage 0.0 failed 4 times; aborting job; 2019-01-07 11:34:12 INFO YarnScheduler:54 - Cancelling stage 0; 2019-01-07 11:34:12 INFO YarnScheduler:54 - Stage 0 was cancelled; 2019-01-07 11:34:12 INFO DAGScheduler:54 - ResultStage 0 (count at CountReadsSpark.java:80) failed in 9.293 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 9, scc-q21.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:33466,abort,aborted,33466,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['abort'],['aborted']
Safety,"e deleted, and its use `support = range(lo, hi)` should become `support = new IndexRange(lo, hi)`. Then `IntStream.of(support).mapToDouble(___).toArray()` becomes `range.mapToDouble(___)` and `apply(promote(support), ___)` becomes `support.mapToDouble(___)`. * `final double relErr = 1 + pow(10, -7)` should become a `static` constant. * The steps; ```java; final double maxD1 = arrayMax(d1);; final double[] d2 = apply(apply(d1, d -> d - maxD1), d -> exp(d));; final double sumD2 = sum(d2);; return apply(d2, d -> d/sumD2);; ```; inside `dnHyper` are just a home-brewed way to normalize the log-space array `d1`. Let's go throught the steps: 1) find the max. 2) subtract the max -- subtracting a constant in log-space is equivalent to dividing by a constant in real space, and since we're normalizing in the end this constant is arbitrary. It's done for numerical stability. 3) exponentiate to get an unnormalized real-space array. 4) find the sum. 5) divide by the sum to get the normalized result. The log-10 version of this is `MathUtils::normalizeFromLog10ToLinearSpace(d1)`. You could either calculate `d1` in log-10 space or convert it, replacing the above line with `return MathUtils.normalizeFromLog10ToLinearSpace(MathUtils.applyToArrayInPlace(d1, MathUtils::logToLog10))`. The latter option is simpler. * Import static should be avoided except to escape horrible clutter, which is not the case here. * The second argument of `Utils.validateArg(condition, calculated string expression)` should become `Utils.validateArg(condition, () -> calculated string expression)`. In the first version, the expression is calculated *even if* the condition is satisfied, whereas in the second it is only calculated as needed. It's not critical here but it's a good habit to get into. PS I am a zealot of the Boy Scout Rule (always leave the camp site cleaner than you found it). It is a great way to get familiar with a large code base and a great way to make the code more readable for the next person.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155:1960,avoid,avoided,1960,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155,1,['avoid'],['avoided']
Safety,"e or directory)) [duplicate 1]; 02:34 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.25, executor 6, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:45142 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:42:02 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 8) on xx.xx.xx.xx, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 18/04/24 17:42:02 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:42:02 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 117.782 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:33857,abort,aborted,33857,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['abort'],['aborted']
Safety,"e to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should co",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:4036,detect,detection,4036,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['detect'],['detection']
Safety,ead.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41797,abort,abortStage,41797,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['abort'],['abortStage']
Safety,ed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 static_quantized_quals=null round_down_quantized=false disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=false never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 reference_window_stop=0 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=variant source=/humgen/gsa-hpprojects/dev/gauthier/AS_GTEx/GTEx_AS/GTEx_AS.recal.multialleleics.AS.recalibrated.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=/humgen/gsa-hpprojects/dev/gauthier/AS_GTEx/GTEx_AS/GTEx_AS.recal.multialleleics.AS.recalibrated.mixed.vcf sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] exclude_sample_expressions=[] selectexpressions=[] invertselect=false excludeNonVariants=false excludeFiltered=false preserveAlleles=false removeUnusedAlternates=false restrictAllelesTo=ALL keepOriginalAC=false ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834:4392,unsafe,unsafe,4392,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834,1,['unsafe'],['unsafe']
Safety,ed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 static_quantized_quals=null round_down_quantized=false disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=true never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 reference_window_stop=0 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=variant source=/humgen/gsa-hpprojects/dev/gauthier/AS_GTEx/GTEx_AS/GTEx_AS.recal.multialleleics.AS.recalibrated.mixed.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=/humgen/gsa-hpprojects/dev/gauthier/AS_GTEx/VQSR.AStest.input.vcf sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] exclude_sample_expressions=[] selectexpressions=[] invertselect=false excludeNonVariants=false excludeFiltered=false preserveAlleles=false removeUnusedAlternates=false restrictAllelesTo=ALL keepOriginalAC=false keepOriginalDP=false mendelianViola,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834:1285,unsafe,unsafe,1285,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834,1,['unsafe'],['unsafe']
Safety,"es. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active region traversals, and disallow the -dcov argument for active region walkers. Downsampling would then by controlled solely by the new argument to set the max # of reads per active region.; - Clarifying the meaning of the per-locus DP annotation for the HC given things like realignment of reads to haplotypes and region-based downsampling. ---. Eventually it was agreed that this would never be fixed in Classic GATK but should be taken into account in designing Hellbender's downsampling.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:6173,avoid,avoid,6173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['avoid'],['avoid']
Safety,"executor 1: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 02:34 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.25, executor 6, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:45142 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:42:02 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 8) on xx.xx.xx.xx, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 18/04/24 17:42:02 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:42:02 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 117.782 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.ca",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:33597,abort,aborting,33597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['abort'],['aborting']
Safety,"implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract methods for the three parameters (`getWindowSize()`, `getWindowStep()` and `getWindowPadding()`, and implement `ReadWindowWalker` as a extension of this interface setting `getWindowStep()` to return `getWindowSize()` and `requiresReads()` to true. I can do this once the PR #1567 is accepted and generate the two interfaces (to be sure that the integration with the HC engine is working as expected with the changes), or just implement the `SlidingWindowWalker` and you can include it in the HC PR, or update afterwards to avoid redundancy in the code. What do you think, @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:2524,avoid,avoid,2524,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775,2,"['avoid', 'redund']","['avoid', 'redundancy']"
Safety,"in the count files, and perhaps fail if one is provided for any of the tools (I don’t recall exactly how VCF indexing is triggered by providing one, as seems to be indicated by the tutorial, but hopefully we can disallow external dictionaries while still taking advantage of the relevant engine features for VCF writing). EDIT: Went digging in Slack to try to remind myself of the context of these changes, and found the following PR comment from 1/7 (although it seems to have mysteriously disappeared from GitHub):. > Just so I understand, are we allowing overriding of the sequence dictionary in the shards (and skipping the consistency check) by allowing the parameter --sequence-dictionary to be specified? If so, we might want to document. Otherwise, I'd be inclined to enforce using the sequence dictionary in the shards (and ensuring the consistency check across shards is performed) by changing the null check in getBestAvailableSequenceDictionary to a check that the dictionary has not been set via the command line. EDIT^2: I think I misremembered the details of how #6330 hooked up the sequence dictionary and how getBestAvailableSequenceDictionary in GATKTool works (which probably explains why that comment was deleted...). Now that I actually go back and look, the `--sequence-dictionary` is not hooked up at all, so there is no change to revert in point 4!. Note that after all of this, it will *still* be possible to get into trouble at the gCNV step if you make funky shards (e.g., you could have shard 1 contain intervals from chr1 and chr3, and shard 2 contain intervals from chr2). I don't think it is possible to check for this case early, but you would still fail at PostprocessGermlineCNVCalls as above. Of course, all of these possibilities can be avoided by simply using the WDL, but it will be good to harden checks for those still working at the command line. @ldgauthier @droazen @mwalker174 what do you think? Happy to review later, but OK if I pass this off to you all?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-719576249:3960,avoid,avoided,3960,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-719576249,1,['avoid'],['avoided']
Safety,"k.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683) [duplicate 2]; 2019-01-09 13:35:56 INFO TaskSetManager:54 - Starting task 4.3 in stage 0.0 (TID 12, scc-q20.scc.bu.edu, executor 2, partition 4, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:56 INFO TaskSetManager:54 - Lost task 1.3 in stage 0.0 (TID 11) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 3]; 2019-01-09 13:35:56 ERROR TaskSetManager:70 - Task 1 in stage 0.0 failed 4 times; aborting job; 2019-01-09 13:35:56 INFO YarnScheduler:54 - Cancelling stage 0; 2019-01-09 13:35:56 INFO YarnScheduler:54 - Stage 0 was cancelled; 2019-01-09 13:35:56 INFO DAGScheduler:54 - ResultStage 0 (count at CountReadsSpark.java:80) failed in 12.543 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 11, scc-q20.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:33215,abort,aborted,33215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['abort'],['aborted']
Safety,"m.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.nextMatePosition.position == null ? 8 : 0 #mate_unmapped. so it looks like the doc supports the existing code. Should I submit this as an issue? . Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:1749,redund,redundant,1749,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033,1,['redund'],['redundant']
Safety,"nd other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge::keras=2.2.4 # updated from pip-installed 2.2.0, which caused various conflicts/clobbers of conda-installed packages; # conda-installed 2.2.4 appears to be the most recent version with a consistent API and without conflicts/clobbers; # if you wish to update, note that versions of conda-forge::keras after 2.2.5; # undesirably set the environment variable KERAS_BACKEND = theano by default; - defaults::intel-openmp=2019.4; - conda-forge::scikit-learn=0.22.2; - conda-forge::matplotlib=3.2.1; - conda-forge::pandas=1.0.3. # core R dependencies; these should only be used for plotting and do not take precedence over core python dependencies!; - r-base=3.6.2; - r-data.table=1.12.8; - r-dplyr=0.8.5; - r-getopt=1.20.3; - r-ggplot2=3.3.0; - r-gplots=3.0.3; - r-gsalib=2.1; - r-optparse=1.6.4. # other python dependencies; these should be removed after functionality is moved into Java code; - biopython=1.76; - pyvcf=0.6.8; - bioconda::pysam=0.15.3 # using older conda-installed versions may result in libcrypto / openssl bugs. # pip installs should be avoided, as pip may not respect the dependencies found by the conda solver; - pip:; - gatkPythonPackageArchive.zip; ```. It seems to successfully create the environment. I'd still recommend updating the information on your README.md and the file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:3717,avoid,avoided,3717,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,1,['avoid'],['avoided']
Safety,"nt bin* provides equal weight---rather than the counts themselves. As usual, modeling each bin as Poisson is close enough to modeling all bins as multinomial for our purposes. If we directly use the NB likelihood and simply weight the count likelihood by occurrences, occurrences in the peak will strongly affect the result, adversely so if the count likelihood is actually misspecified there. As an example, consider trying to fit a Poisson to data that is actually zero-inflated Poisson---fitting the histogram will actually result in a more robust estimate for the mean. Another benefit is that truncated data (as we have here) is straightforwardly handled in an unbiased way. In the special case of complete, trivially-binned data, the full, unbinned likelihood is recovered. I think this sort of histogram fitting is pretty standard in the astro/particle community. We can certainly change up the model to include strictly quantized + free-floating states as you describe (rather than the ""fuzzily quantized"" states I use here), but I just wanted to avoid having another level of mixtures/logsumexps for this quick prototype. However, note that modeling mosaicism on the autosomes is desirable, but there we also want the strong diploid prior to nail down the depth and per-contig bias. So we will have to be a little careful about how we introduce free-floating states. Also, since I was not using gcnvkernel, I had to integrate out all discrete parameters. It may be that we can write down a nice model with discrete parameters if we use your inference framework. Finally, I did not further bin the counts here (or rather, the bin size is 1), which already yields the maximum information, but I did use a maximum-count cutoff. If we use the same cutoff for all samples, this allows us to simply pass a non-ragged matrix from Java (with dimensions of samples x contigs x maximum count) as a TSV. However, we may run into trouble if we hit a case sample with very high depth. So some sort of spa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522:1159,avoid,avoid,1159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522,1,['avoid'],['avoid']
Safety,"o.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:748); 18/04/24 17:42:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:42:11 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:42:11 INFO BlockManager: BlockManager stopped; 18/04/24 17:42:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:42:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:42:11 INFO SparkContext: Successfully stopped SparkContext; 17:42:11.053 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:42:11 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=866648064; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:39053,abort,aborted,39053,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['abort'],['aborted']
Safety,or.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:38516,abort,abortStage,38516,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['abort'],['abortStage']
Safety,"ore refinements to the method and have settled on the following procedure:. Assume we have _N_ data points:; ![1](https://user-images.githubusercontent.com/11076296/29580954-bea24562-8745-11e7-9c8c-d68504ba31da.png); To find segments, we:; 1) Select _C<sub>max</sub>_, the maximum number of changepoints to discover. In practice, _C<sub>max</sub> = 100_ per chromosome should more than suffice.; 2) Select a kernel (linear for sensitivity to changes in the distribution mean, Gaussian with a specified variance _σ<sup>2</sup>_ for multimodal data, etc.) and a subsample of _p_ points to approximate it using SVD.; 3) Select window sizes _w<sub>j</sub>_ for which to compute local costs at each point. To be precise, we compute the cost of a changepoint at the point with index _i_, assuming adjacent segments containing the points with indices _[i - w<sub>j</sub> + 1, i]_ and _[i + 1, i + w<sub>j</sub>]_. Selecting a minimum window size and then doubling up to relevant length scales (noting that longer window lengths allow for more subtle changepoints to be detected) works well in practice. For example, here are what the cost functions look like for window sizes of 8, 16, 32, and 64:. ![2](https://user-images.githubusercontent.com/11076296/29582011-210d37b8-8749-11e7-9383-0c657232347e.png). ![3](https://user-images.githubusercontent.com/11076296/29582016-23fbc6a6-8749-11e7-951e-f618e8489a0b.png). ![4](https://user-images.githubusercontent.com/11076296/29582044-3eb20a1e-8749-11e7-84a0-3734bad15e1f.png). ![5](https://user-images.githubusercontent.com/11076296/29582047-410ac490-8749-11e7-8a98-b2098cf1b5ea.png); 4) For each of these cost functions, find (up to) the _C<sub>max</sub>_ most significant local minima. The problem of finding local minima of a noisy function can be solved by using topological persistence (e.g., https://people.mpi-inf.mpg.de/~weinkauf/notes/persistence1d.html and http://www2.iap.fr/users/sousbie/web/html/indexd3dd.html?post/Persistence-and-simplification). ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:1082,detect,detected,1082,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586,1,['detect'],['detected']
Safety,park.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:38613,abort,abortStage,38613,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['abort'],['abortStage']
Safety,"tes** total and produced an **11GB PoN** (this file includes all of the input read counts---which take up 20GB as a combined TSV file and a whopping 63GB as individual TSV files---as well as the eigenvectors, filtering results, etc.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if it is not already overkill. To go bigger by more than an order of magnitude, we'll have to go out of ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:1066,avoid,avoid,1066,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503,1,['avoid'],['avoid']
Safety,thanks for the review @kcibul. I made some changes accordingly. re: PrepareCallset file of sample names. That would be nice! It would make this workflow simpler and it also simplifies the access requirements for PrepareCallset. re: Dockstore. We actually ruled this out because Terra says that the definition of a method configuration can change automatically if its updated in dockstore. Which can be useful but it adds a security risk since a compromised Dockstore can change the definition of the production AoU extraction WDL which runs with highly elevated permissions. We already have a script that creates method configurations from github so I can probably add something a little hacky to resolve relative imports to the raw github file that it refers to.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7242#issuecomment-846494686:432,risk,risk,432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7242#issuecomment-846494686,1,['risk'],['risk']
Safety,"u.edu, executor 2, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:54 INFO TaskSetManager:54 - Lost task 4.2 in stage 0.0 (TID 9) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683) [duplicate 2]; 2019-01-09 13:35:56 INFO TaskSetManager:54 - Starting task 4.3 in stage 0.0 (TID 12, scc-q20.scc.bu.edu, executor 2, partition 4, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:56 INFO TaskSetManager:54 - Lost task 1.3 in stage 0.0 (TID 11) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 3]; 2019-01-09 13:35:56 ERROR TaskSetManager:70 - Task 1 in stage 0.0 failed 4 times; aborting job; 2019-01-09 13:35:56 INFO YarnScheduler:54 - Cancelling stage 0; 2019-01-09 13:35:56 INFO YarnScheduler:54 - Stage 0 was cancelled; 2019-01-09 13:35:56 INFO DAGScheduler:54 - ResultStage 0 (count at CountReadsSpark.java:80) failed in 12.543 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 11, scc-q20.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:32948,abort,aborting,32948,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['abort'],['aborting']
Safety,"u.edu, executor 2, partition 9, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:11 INFO TaskSetManager:54 - Lost task 2.1 in stage 0.0 (TID 7) on scc-q12.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f) [duplicate 1]; 2019-01-07 11:34:12 INFO TaskSetManager:54 - Starting task 3.3 in stage 0.0 (TID 11, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:12 INFO TaskSetManager:54 - Lost task 1.3 in stage 0.0 (TID 9) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 3]; 2019-01-07 11:34:12 ERROR TaskSetManager:70 - Task 1 in stage 0.0 failed 4 times; aborting job; 2019-01-07 11:34:12 INFO YarnScheduler:54 - Cancelling stage 0; 2019-01-07 11:34:12 INFO YarnScheduler:54 - Stage 0 was cancelled; 2019-01-07 11:34:12 INFO DAGScheduler:54 - ResultStage 0 (count at CountReadsSpark.java:80) failed in 9.293 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 9, scc-q21.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:4",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:33200,abort,aborting,33200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['abort'],['aborting']
Safety,"ually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active region traversals, and disallow the -dcov argument for active region walkers. Downsampling would then by controlled solely by the new argument to set the max # of reads per active region.; - Clarifying the meaning of the per-locus DP annotation for the HC given things like realignment of ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:5690,avoid,avoid,5690,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['avoid'],['avoid']
Safety,"ut.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be for",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:1895,redund,redundant,1895,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['redund'],['redundant']
Safety,"weinkauf/notes/persistence1d.html and http://www2.iap.fr/users/sousbie/web/html/indexd3dd.html?post/Persistence-and-simplification). A straightforward watershed algorithm can sort all local minima by persistence in linear time after an initial sort of the data.; 5) These sets of local minima from all window sizes together provide the pool of candidate changepoints (some of which may overlap exactly or approximately). We perform backwards selection using the global segmentation cost. That is, we compute the global segmentation cost given all the candidate changepoints, calculate the cost change for removing each of the changepoints individually, remove the changepoint with the minimum cost change, and repeat. This gives the global cost as a function of the number of changepoints _C_.; 6) Add a penalty _a C + b C log(N / C)_ to the global cost and find the minimum to determine the number of changepoints. For the above simulated data, _a = 2_ and _b = 2_ works well, recovering all of the changepoints in the above example with no false positives:; ![6](https://user-images.githubusercontent.com/11076296/29582517-fbd604be-874a-11e7-8ef7-7bd727f65dcb.png). ![7](https://user-images.githubusercontent.com/11076296/29582518-fddf015c-874a-11e7-89e4-87250d2a52ab.png). In contrast, CBS produces two false positives (around the third and seventh of the true changepoints):. ![8](https://user-images.githubusercontent.com/11076296/29582545-18875126-874b-11e7-9166-9061bb120e43.png). We can change the penalty factor to smooth out less significant segments (which may be due to systematic noise, GC waves, etc.). Setting _a = 10, b = 10_ gives:; ![9](https://user-images.githubusercontent.com/11076296/29582598-515dffe0-874b-11e7-93c4-59422cd43b54.png); ![10](https://user-images.githubusercontent.com/11076296/29583130-0fe5316c-874d-11e7-8504-43618928cf68.png). (Note that the DNAcopy implementation of CBS does not allow for such simple control of the ""false-positive rate,"" as even setting the",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:2866,recover,recovering,2866,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586,1,['recover'],['recovering']
Safety,y(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); at org,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:38855,abort,abortStage,38855,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['abort'],['abortStage']
Security, Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/tools/walkers/vqsr/FilterVariantTranches.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvRmlsdGVyVmFyaWFudFRyYW5jaGVzLmphdmE=) | `92.24% <ø> (ø)` | `42 <0> (ø)` | :arrow_down: |; | [...der/tools/walkers/vqsr/CNNVariantWriteTensors.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OVmFyaWFudFdyaXRlVGVuc29ycy5qYXZh) | `85.71% <100%> (+2.38%)` | `4 <0> (ø)` | :arrow_down: |; | [...hellbender/tools/walkers/vqsr/CNNVariantTrain.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OVmFyaWFudFRyYWluLmphdmE=) | `60% <46.66%> (-20.65%)` | `4 <0> (ø)` | |; | [...lkers/validation/EvaluateInfoFieldConcordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vRXZhbHVhdGVJbmZvRmllbGRDb25jb3JkYW5jZS5qYXZh) | `72.58% <72.58%> (ø)` | `14 <14> (?)` | |; | [...ellbender/tools/walkers/vqsr/CNNScoreVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OU2NvcmVWYXJpYW50cy5qYXZh) | `73.68% <77.14%> (-1.32%)` | `41 <17> (+1)` | |; | [...ools/walkers/validation/InfoConcordanceRecord.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vSW5mb0NvbmNvcmRhbmNlUmVjb3JkLmphdmE=) | `93.93% <93.93%> (ø)` | `8 <8> (?)` | |; | [...n/EvaluateInfoFieldConcordanceIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pul,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5175#issuecomment-423756354:2067,validat,validation,2067,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5175#issuecomment-423756354,1,['validat'],['validation']
Security, `85.71% <100%> (+2.38%)` | `4 <0> (ø)` | :arrow_down: |; | [...hellbender/tools/walkers/vqsr/CNNVariantTrain.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OVmFyaWFudFRyYWluLmphdmE=) | `60% <46.66%> (-20.65%)` | `4 <0> (ø)` | |; | [...lkers/validation/EvaluateInfoFieldConcordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vRXZhbHVhdGVJbmZvRmllbGRDb25jb3JkYW5jZS5qYXZh) | `72.58% <72.58%> (ø)` | `14 <14> (?)` | |; | [...ellbender/tools/walkers/vqsr/CNNScoreVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OU2NvcmVWYXJpYW50cy5qYXZh) | `73.68% <77.14%> (-1.32%)` | `41 <17> (+1)` | |; | [...ools/walkers/validation/InfoConcordanceRecord.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vSW5mb0NvbmNvcmRhbmNlUmVjb3JkLmphdmE=) | `93.93% <93.93%> (ø)` | `8 <8> (?)` | |; | [...n/EvaluateInfoFieldConcordanceIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vRXZhbHVhdGVJbmZvRmllbGRDb25jb3JkYW5jZUludGVncmF0aW9uVGVzdC5qYXZh) | `96% <96%> (ø)` | `3 <3> (?)` | |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (-30%)` | `1% <0%> (-2%)` | |; | [...ithwaterman/SmithWatermanIntelAlignerUnitTest.java](https://codecov.io/gh/bro,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5175#issuecomment-423756354:2692,validat,validation,2692,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5175#issuecomment-423756354,1,['validat'],['validation']
Security," pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian consistency, white = reference. The second plot shows the training/truth positives used to train the model and to calculate the LL score in the validation shard. The third plot shows the ""orthogonal truth"" positives/negatives used to calculate F1. So we can see that the difficulty in deriving F1 as a function of the score along the horizontal axis to give the third plot lies in collapsing the columns in the top plot into a single condition positive or condition negative status. Again, hard to do so without some arbitrariness; I simply came up with some rules to convert various amounts of red, yellow, green, etc. in each column to a red/white/green status. If you're using a single gold-standard sample, this should definitely be more straightforward. In any case, the optimal validation LL score at ~0.02 does appear to line up quite well visually with where one might manually set a threshold. It corresponds pretty well with the transition from the yellow/red/grey junk to the clean green/white sites in the top plot. Here's the same for the test set:. ![image](https://user-images.githubusercontent.com/11076296/158385662-6693a6c9-709c-482f-9a7e-5bb7030b3383.png). Happy to chat more about how you might implement this in your WDL---should be pretty straightforward!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:2782,validat,validation,2782,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,1,['validat'],['validation']
Security," redirect tqdm progress bar to python logger. commit 2e45bd30968b921fae225de3901fb97ece690b0c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:45:49 2017 -0500. more arg related fixes. commit bb89a3bb338d88199881e8aca65f656f2acd7c0a; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:41:20 2017 -0500. arg related bugfixes in WDL, python, and java CLIs. commit 23569787ee2c8cc6c9227a44170cbbd02fe4427f; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 17:21:05 2017 -0500. fixed issue with python boolean argparse (they use weird semantics). commit ae841c9ed4cd9b2ca1ac0e9082d175ff8ea98298; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 16:44:02 2017 -0500. shorter gCNV WDL tests. commit 5466b806e36df16cad2d045be074e7f9afec0957; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 16:38:15 2017 -0500. fixed arg issues in somatic WDL; exposed all missing args to java side; major update to germline WDLs; all optional python args exposed to WDLs as optional args. commit 50cb6fd08de15469a9080cbb27ff30c8b7ee7e21; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:50:45 2017 -0500. missing serialVersionUID. commit 5f0f31eab63b0e6f6105708ded7f86c96c830781; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:35:33 2017 -0500. annotated intervals kebab case; updated germline WDL workflows. commit 29cc6234dbfb8db12559217a650c6ceb170c5797; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:15:28 2017 -0500. cleanup test files. commit 08a35bb4e65eceb735adcd41a91132e9a34d2b66; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 02:50:19 2017 -0500. update WDL scripts. commit 12bcfa192ee6fa6da21239ebf5b513633efe974f; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 02:47:33 2017 -0500. significant updates to GermlineCNVCaller; integration tests for GermlineCNVCaller w",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:5916,expose,exposed,5916,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,2,['expose'],['exposed']
Security," with 25.4 GB RAM, BlockManagerId(7, scc-q14.scc.bu.edu, 36726, None); 18/03/07 20:31:49 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (192.168.18.195:47862) with ID 6; 18/03/07 20:31:49 INFO storage.BlockManagerMasterEndpoint: Registering block manager scc-q11.scc.bu.edu:46002 with 25.4 GB RAM, BlockManagerId(6, scc-q11.scc.bu.edu, 46002, None); 18/03/07 20:31:49 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 246.6 KB, free 8.4 GB); 18/03/07 20:31:50 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.3 KB, free 8.4 GB); 18/03/07 20:31:50 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.48.225.55:41567 (size: 25.3 KB, free: 8.4 GB); 18/03/07 20:31:50 INFO spark.SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:112; 18/03/07 20:31:50 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 7175 for farrell on ha-hdfs:scc; 18/03/07 20:31:50 INFO security.TokenCache: Got dt for hdfs://scc; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:scc, Ident: (HDFS_DELEGATION_TOKEN token 7175 for farrell); 18/03/07 20:31:50 INFO input.FileInputFormat: Total input paths to process : 1; 18/03/07 20:31:51 INFO spark.SparkContext: Starting job: aggregate at FlagStatSpark.java:73; 18/03/07 20:31:51 INFO scheduler.DAGScheduler: Got job 0 (aggregate at FlagStatSpark.java:73) with 629 output partitions; 18/03/07 20:31:51 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (aggregate at FlagStatSpark.java:73); 18/03/07 20:31:51 INFO scheduler.DAGScheduler: Parents of final stage: List(); 18/03/07 20:31:51 INFO scheduler.DAGScheduler: Missing parents: List(); 18/03/07 20:31:51 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at GATKSparkTool.java:220), which has no missing parents; 18/03/07 20:31:51 INFO memory.MemoryStore: Block broadcast_1 stored as values in mem",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371353888:5073,secur,security,5073,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371353888,1,['secur'],['security']
Security,!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CountReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 11:33:26.277 INFO CountReadsSpark - Initializing engine; 11:33:26.277 INFO CountReadsSpark - Done initializing engine; 2019-01-07 11:33:26 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:26 INFO SparkContext:54 - Running Spark version 2.3.0; 2019-01-07 11:33:26 INFO SparkContext:54 - Submitted application: CountReadsSpark; 2019-01-07 11:33:26 INFO SecurityManager:54 - Changing view acls to: farrell; 2019-01-07 11:33:26 INFO SecurityManager:54 - Changing modify acls to: farrell; 2019-01-07 11:33:26 INFO SecurityManager:54 - Changing view acls groups to:; 2019-01-07 11:33:26 INFO SecurityManager:54 - Changing modify acls groups to:; 2019-01-07 11:33:26 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-07 11:33:27 INFO Utils:54 - Successfully started service 'sparkDriver' on port 46828.; 2019-01-07 11:33:27 INFO SparkEnv:54 - Registering MapOutputTracker; 2019-01-07 11:33:27 INFO SparkEnv:54 - Registering BlockManagerMaster; 2019-01-07 11:33:27 INFO BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-07 11:33:27 INFO BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up; 2019-01-07 11:33:27 INFO DiskBlockManager:54 - Created local directory at /tmp/blockmgr-08460386-3abb-4431-ba8d-5b7d41a2a05c; 2019-01-07 11:33:27 INFO MemoryStore:54 - MemoryStore started with capacity 408.6 MB; 2019-01-07 11:33:27 INFO SparkEnv,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:5347,authenticat,authentication,5347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['authenticat'],['authentication']
Security,!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CountReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 13:35:11.512 INFO CountReadsSpark - Initializing engine; 13:35:11.512 INFO CountReadsSpark - Done initializing engine; 2019-01-09 13:35:11 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:11 INFO SparkContext:54 - Running Spark version 2.3.0; 2019-01-09 13:35:11 INFO SparkContext:54 - Submitted application: CountReadsSpark; 2019-01-09 13:35:11 INFO SecurityManager:54 - Changing view acls to: farrell; 2019-01-09 13:35:11 INFO SecurityManager:54 - Changing modify acls to: farrell; 2019-01-09 13:35:11 INFO SecurityManager:54 - Changing view acls groups to:; 2019-01-09 13:35:11 INFO SecurityManager:54 - Changing modify acls groups to:; 2019-01-09 13:35:11 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-09 13:35:12 INFO Utils:54 - Successfully started service 'sparkDriver' on port 42689.; 2019-01-09 13:35:12 INFO SparkEnv:54 - Registering MapOutputTracker; 2019-01-09 13:35:12 INFO SparkEnv:54 - Registering BlockManagerMaster; 2019-01-09 13:35:12 INFO BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-09 13:35:12 INFO BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up; 2019-01-09 13:35:12 INFO DiskBlockManager:54 - Created local directory at /tmp/blockmgr-dd94d6fb-7e3d-4def-a895-6e60f05d7a05; 2019-01-09 13:35:12 INFO MemoryStore:54 - MemoryStore started with capacity 372.6 MB; 2019-01-09 13:35:12 INFO SparkEnv,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:5086,authenticat,authentication,5086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['authenticat'],['authentication']
Security,"/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/resources/sv_classifier_test_data.json; src/test/resources/sv_features_test_data.json",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:3079,validat,validation,3079,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,1,['validat'],['validation']
Security,"9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 18:11:33.871 INFO PrintReadsSpark - Initializing engine; 18:11:33.871 INFO PrintReadsSpark - Done initializing engine; 17/10/13 18:11:33 INFO spark.SparkContext: Running Spark version 2.2.0.cloudera1; 17/10/13 18:11:34 WARN spark.SparkConf: spark.master yarn-client is deprecated in Spark 2.0+, please instead use ""yarn"" with specified deploy mode.; 17/10/13 18:11:34 INFO spark.SparkContext: Submitted application: PrintReadsSpark; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:34 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:34 INFO util.Utils: Successfully started service 'sparkDriver' on port 45754.; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering MapOutputTracker; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering BlockManagerMaster; 17/10/13 18:11:34 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 17/10/13 18:11:34 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 17/10/13 18:11:34 INFO storage.DiskBlockManager: Created local directory at /tmp/hdfs/blockmgr-ea0e0669-2981-4277-80a0-a67eddf1001d; 17/10/13 18:11:34 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering OutputCommitCoordinator; 17/10/13 18:11:34 INFO util.log: Logging initialized @3816ms; 17/10/13 18:11:34 INFO server.Server: jetty-9.3.z-SNAPSHOT; 17/10/13 ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:4300,authenticat,authentication,4300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['authenticat'],['authentication']
Security,"@bensprung So I thought this would be a trivial change. It turns out that encoding the Genotype as something like `1/1` is done way down in the depths of the VCF encoder and isn't exposed in an accessible way. It's going to need a (hopefully simple) change to the underlying htsjdk library to expose that machinery. It shouldn't be hard, it just means it will take a bit longer to get to than I expected.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8160#issuecomment-1397695685:180,expose,exposed,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8160#issuecomment-1397695685,3,"['access', 'expose']","['accessible', 'expose', 'exposed']"
Security,"@cmnbroad : first - would it be possible to kick off travis tests? i refactored this and dont seem to be able to do that. Second, yes, I was trying to reorder and condense the commits but clearly didnt work. I think the problem was trying to put your GATK3 commit first (which would seem to make sense). in any case, I just recreated this, putting a pristine GATK3 first, following a consolidated set of my commits with 1) the limited core changes, 2) the meat of the VariantEval port, and 3) A separate commit with a port of GATK3 VariantEvalIntegrationTest which is useful for validation but should not be merged. To your points:. 1) I substantially cut down the incoming large files, mostly by limiting the intervals of new large VCFs. 2) On the plugin: this was discussed above, and I initially also pointed out this should ultimately go into Barclay. You are actually the one who proposed staging it in GATK. I am not entirely sure I understand the reticence on plugins; however, my goal is to get VariantEval ported by touching as little of it as possible. This is already sucking up a ton of time. I flipped VariantEvalUtils to gather a list of classes from the appropriate package instead of a full-on plugin. That should satisfy that concern?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431913735:579,validat,validation,579,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431913735,1,['validat'],['validation']
Security,@cmnbroad @SHuang-Broad . The cluster uses Kerberos for authentication. This style of pathname works for reading the cram file which is on the hdfs file system. . Using the hadoop shell works fine.... ; hadoop fs -ls hdfs:///project/casa/gcad/adsp.cc; Found 2 items; drwxrwxr-x - zhucc casa 0 2018-04-27 14:59 hdfs:///project/casa/gcad/adsp.cc/cram; drwxrwxr-x - farrell casa 0 2018-05-08 15:21 hdfs:///project/casa/gcad/adsp.cc/sv. When I change this to a local file a similar error occurs. The program runs for 40 plus minutes and then gets the following error. . ```; 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 92.0 in stage 13.0 (TID 68093) in 1108 ms on scc-q01.scc.bu.edu (executor 24) (101/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 101.0 in stage 13.0 (TID 68102) in 1061 ms on scc-q01.scc.bu.edu (executor 6) (102/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 34.0 in stage 13.0 (TID 68035) in 1653 ms on scc-q01.scc.bu.edu (executor 24) (103/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 44.0 in stage 13.0 (TID 68045) in 1553 ms on scc-q07.scc.bu.edu (executor 7) (104/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 63.0 in stage 13.0 (TID 68064) in 1362 ms on scc-q01.scc.bu.edu (executor 24) (105/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 102.0 in stage 13.0 (TID 68103) in 1057 ms on scc-q07.scc.bu.edu (executor 7) (106/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 39.0 in stage 13.0 (TID 68040) in 1604 ms on scc-q06.scc.bu.edu (executor 23) (107/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 5.0 in stage 13.0 (TID 68006) in 2015 ms on scc-q01.scc.bu.edu (executor 24) (108/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 10.0 in stage 13.0 (TID 68011) in 1928 ms on scc-q06.scc.bu.edu (executor 23) (109/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 15.0 in stage 13.0 (TID 68016) in 1865 ms,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-494014590:56,authenticat,authentication,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-494014590,1,['authenticat'],['authentication']
Security,"@cmnbroad After thinking about this I went ahead and created VariantEvalEngine. Doing this in one PR will simplify some of the sticking points around what is a final change vs. what it expected to be fixed later. With this change, the goal is to strip most logic from VariantEval into the engine. This engine can be constructed with a VariantEvalArgumentCollection, and any kind of GATKTool as the owner. I tried to minimize the amount of context the VariantEvalEngine needed to hang on to. This means all the child classes have visibility on the VariantEvalEngine, but are no longer directly exposed to either the walker class or the argument collection. . All the logic around gathering the arguments to form DrivingVariants is moved to a static method in VariantEvalEngine. . I also rebased and fixed conflicts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-750428516:593,expose,exposed,593,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-750428516,1,['expose'],['exposed']
Security,"@cmnbroad OK - what about this proposal? I just added a protected getter and fixed the typo in 'annotation'? We could expose a constructor based way to set PedigreeValidationType, but if you dont really want to expose more of the guts of PedigreeAnnotation to subclasses prior to splitting apart founderIds and pedigree, what about keeping this as simple and minimal as possible?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-855970929:118,expose,expose,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-855970929,2,['expose'],['expose']
Security,"@cmnbroad 👍 to adding an advanced command line option for it. . @magicDGS Our goal is to make it unnecessary for normal users to ever need to see a stacktrace. We're definitely not at the point yet where every UserException produces either a) the complete information necessary to debug, or even b) the correct information. We're trying to fix all those cases, but there's a lot of possible failure modes between cloud access, spark, filesystem plugins, etc, so it's going to be an issue for a while. . I don't think it will hurt the user experience to have an extra commandline argument for it. Printing the stacktrace when debug is on isn't a bad idea, but I think it's better to have finer grained control over it. . The other issue is that it's easier to explain to an unsophisticated user how to set an extra commmand line argument rather than trying to get them to set the environment variable which well be helpful for our support team when they're trying to debug someone's problem. (especially since setting the environment variables may be different on a spark cluster than on a local run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394:419,access,access,419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394,1,['access'],['access']
Security,"@davidbenjamin, et al. I have two recommendations:. 1) Though I prefer to work symbolically and through proofs, it might be nice to first expand on the validation by proof in the JavaDoc - including for the specific function's header - and anywhere else where necessary across the GATK code, just for sanity's sake, and for tying things together neatly and properly. This process of always going through the mathematical steps alerts me when I code that I have not missed anything. . 2) When dealing with multiple levels of transformations, it probably would be good to formulate a collection of complete set of simple tests. Since like you mentioned {phased} is a subset (⊂) of {unphased}, then the paths of phased genotypes one works with would also be ideal to test on. Does this function have any validation tests confirming the correct likelihoods, which would be performed for both phased and unphased genotypes? These can be generated tests, if original files do not exists. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221:152,validat,validation,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221,2,['validat'],['validation']
Security,"@droazen @cmnbroad @mbabadi I generally agree with the sentiments expressed in #4127, except that I think it's OK to require a conda environment (or even use of the Docker) for these particular tools. How we should validate this requirement is another question. We can discuss more with @vdauwera. @stefandiederich Hopefully once you get the conda environment set up you will be able to run the tools. We would definitely appreciate any feedback you might be able to provide. Note that the gCNV model is relatively sophisticated, so there may be some parameters (which control the priors for the model as well as how inference is performed) that you will need to adjust for your data. Depending on the number of intervals/bins you are using and your memory constraints, you may also need to scatter across multiple GermlineCNVCaller runs; see how things are done in the WDLs here: https://github.com/broadinstitute/gatk/tree/master/scripts/cnv_wdl/germline. As you noted, this pipeline is still in beta. We are currently running several evaluations and hope to soon release some Best Practices recommendations for the aforementioned parameter values that should work well for various data types generated at the Broad. We will also have some blog or forum posts that explain the new CNV pipelines in more detail coming soon---stay tuned!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357034364:215,validat,validate,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357034364,1,['validat'],['validate']
Security,"@droazen thanks for the quick response! Just to be clear, my concerns were about testing that I didn't somehow screw up the original behavior through the exposure, not just testing that *some* behavior was exposed. But message received---will keep things on the simple side!. Also, please see the plots in #5564 to get an idea of the effect on outputs, if you haven't already. Would appreciate any thoughts you might have on that thread!. Will try to get this done in the next day or two, thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-896328697:206,expose,exposed,206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-896328697,1,['expose'],['exposed']
Security,"@droazen,. Apologies for the delay in getting back to you. Given the nature of our work, it's essential that we address and remove any high and critical vulnerabilities, regardless of their real-world threat level. Ensuring our system remains secure is our top priority. Here is the pull request with the modifications to address the high and critical vulnerabilities: [#8950](https://github.com/broadinstitute/gatk/pull/8950). Please review and let me know if you have any feedback.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-2285999993:201,threat,threat,201,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-2285999993,2,"['secur', 'threat']","['secure', 'threat']"
Security,"@jamesemery and now the overview of the more complex changes:. - `AssemblyResultSet`: the code for adding and removing haplotypes based on pileup alleles has become a `void` method of this class, where it belongs. Here and elsewhere I introduce snappy variable and function named referring to ""good"" and ""bad"" alleles, which I find visually much clearer. The code is basically the same as before but somewhat streamified. I extracted a `makeHaplotypeWithInsertedEvent` method to eliminate some code duplication between GGA and pileup force-calling.; - `HaplotypeCallerEngine` and `Mutect2Engine`: Force-calling alleles are split into biallelic `Events`. Duplicated code for finding all pileup events, then sifting them into good event to force-call and bad events to remove is extracted as `PileupBasedAlleles.goodAndBadPileupEvents`. Computing `allVariationEvents` is much simpler because 1) it now uses `Event` instead of `VariantContext` and 2) `Event` overrides `equals` and `hashCode`.; - `PileupBasedAlleles`: `getPileupVariantContexts` and sorting into good and bad pileup variants has been unified into `goodAndBadPileupEvents()`. It has additionally been somewhat rewritten for conciseness. Also, instead of the somewhat kludgy method of making `VariantContext` with four temporary attributes, then filtering based on those attributes, it calculates the filtering status immediately and uses `Events`. Also fixed the somewhat-misleading use of the word `alt` to mean `SNP`.; - `AssemblyBasedCallerUtils`: `applyPileupEventsAsForcedAlleles`, along with several helper methods that it calls, has been moved into `AssemblyResultResult`, where it is now a void member method.; - `GATKVariantContextUtils` mainly just using `Event` instead of `VariantContext`, which simplifies the code for splitting a `VariantContext` into biallelics. After going through this exercise I realize that it's not actually so much. The diff's bark is worse than its bite. The overwhelming majority of changes are eit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8332#issuecomment-1574175702:980,hash,hashCode,980,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8332#issuecomment-1574175702,1,['hash'],['hashCode']
Security,"@jamesemery sorry to bug on this topic, but I'm hoping to make a push early this year to fully migrate my lab off GATK3 . I looked more closely at the specific annotations we need to migrate. I decided that I will implement our walker, 'DiscvrVariantAnnotator', which is basically a light wrapper around VariantAnnotation. This will make it easier to spike in custom annotations. In that walker, I will override makeVariantAnnotations(). I will make a new marker interface for EngineAwareAnnotation, and test that on all the Annotation classes, and use this to inject FeatureManager. So no core GATK changes needed. I did find one thing I'd like to propose. You probably know PedigreeAnnotation is special-cased in GATK. Annotations that use it have automatic argument validation and have the SampleDB injected. Currently, PedigreeAnnotation is a subclass of InfoFieldAnnotation, so isnt available to GenotypeAnnotations. There doesnt appear to be a solid reason why. I tried to fix that and my best idea is the proposal here: #7041 . The core idea is to convert InfoFieldAnnotation and GenotypeAnnotation to interfaces. This is generally a trivial switch in existing code. With that, it becomes possible for classes that currently extend PedigreeAnnotation (which I switched to no longer extend InfoFieldAnnotation) to simply PedigreeAnnotation and implement InfoFieldAnnotation. This makes it possible for future classes to extend PedigreeAnnotation and implement GenotypeAnnotation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-760424063:561,inject,inject,561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-760424063,3,"['inject', 'validat']","['inject', 'injected', 'validation']"
Security,"@jean-philippe-martin Can you comment on this error with your thoughts? Despite now doing a channel reopen on `UnknownHostException` in our fork of the NIO library, all reopens are failing, which implies that this error can't be recovered from via a simple retry. Could there be something wrong in our authentication setup?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412906931:302,authenticat,authentication,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412906931,1,['authenticat'],['authentication']
Security,@jean-philippe-martin I think we can set up a repro by creating a new github project with a simple travis build that just does an NIO access. I don't think we can reproduce it locally since I'm pretty sure it's a bad interaction with the environment.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5928#issuecomment-516890466:134,access,access,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5928#issuecomment-516890466,1,['access'],['access']
Security,"@jonn-smith Is there a forum post (or other docs) on how to setup a datasource for remote, NIO access? Do we make it clear that this only supports what the GATK supports?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5425#issuecomment-439976455:95,access,access,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5425#issuecomment-439976455,1,['access'],['access']
Security,"@lbergelson I disagree -- it's very clear to me that those tests will trigger Google authentication, just by tracing through the code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2706#issuecomment-300806909:85,authenticat,authentication,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706#issuecomment-300806909,1,['authenticat'],['authentication']
Security,"@lbergelson I was referring more to the middle part of the StackOverflow by Daniel Chapman - specifically the [4.1 The ObjectStreamClass Class](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a5082) and [4.6 Stream Unique Identifiers](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a4100):. _If not specified by the class, the value returned is a hash computed from the class's name, interfaces, methods, and fields using the Secure Hash Algorithm (SHA) as defined by the National Institute of Standards._. Now when I look at the `java.io.ObjectStreamClass.java` file for 64-bit JDK7 and JDK8 - from src.zip - both have the same code for the following parts after performing a `diff` - I didn't list all of the lines of code since they are quite long:. ```; public long getSerialVersionUID() {; // REMIND: synchronize instead of relying on volatile?; if (suid == null) {; suid = AccessController.doPrivileged(; new PrivilegedAction<Long>() {; public Long run() {; return computeDefaultSUID(cl);; }; }; );; }; return suid.longValue();; }; ... private static long computeDefaultSUID(Class<?> cl) {; ...very long code which can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:404,hash,hash,404,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['hash'],['hash']
Security,@lbergelson thank you for the comment and sorry for my bit late response. I excluded the dependency to the jsr203-s3a and tested that both local- and spark-gatk can access s3a files by dynamically loading it. I also added a new directory `scripts/s3a` for documentation and simple tests for s3a demonstration.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-665484597:165,access,access,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-665484597,1,['access'],['access']
Security,"@lbergson's instructions above are good, but somehow they did not work for me. I was able to follow the [application default credentials](https://developers.google.com/identity/protocols/application-default-credentials) instructions, though. Here are the steps I took:. 1. create a new service account on the Google Cloud web page and download the JSON key file.; 2. gcloud auth activate-service-account --key-file ""$PATH_TO_THE_KEY_FILE""; 3. export GOOGLE_APPLICATION_CREDENTIALS=""$PATH_TO_THE_KEY_FILE"". I cleared my credentials first to make sure that the access worked because of the above steps, not because of other credentials. After those steps, gatk was able to run from my desktop and access files using the service account credentials. `gsutil ls` worked as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282900470:559,access,access,559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282900470,2,['access'],['access']
Security,"@magicDGS Yes, I think it would be much simpler if we had one PR with all of the fixes for the validation rules (and related help issues). The extensibility changes we've been discussing should be a separate PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-278330318:95,validat,validation,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-278330318,1,['validat'],['validation']
Security,"@magicDGS, after much discussion with @droazen, we will make this test set even smaller. Also, the current test set will be temporary, to unimpede you, until updates to the main test set can be made (discussion to start next week). @cmnbroad says we need to keep an eye out for coverage in the tests, given the need to make this dataset extremely small. So I removed chr17 from the mini-reference, such that only four small snippets of reference from various chromosomes remained. However, it appears there are only two usable indel realignments going on with GATK3. Note thate these are targeted exome samples with comparatively low coverage. [for_magicDGS.zip](https://github.com/broadinstitute/gatk/files/1820785/for_magicDGS.zip). There are two samples, so as to enable testing nWayOut, and an artificial reference `hg38_Shl01`. The two sites to hone in on are chr11:177568 and chr11:207134. Note also that the BAMs are in an invalid state according to ValidateSamFile. However, GATK3 RealignerTargetCreator and IndelRealigner did not seem to mind. I think these tools should allow processing of BAMs in any validation state. Apologies for the meager state of the data. On the bright side, the data set including the ref is only 23MB and will meet with @droazen's approval in terms of size. . Good luck @magicDGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600:1112,validat,validation,1112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600,1,['validat'],['validation']
Security,"@mbabadi I've updated my PR to use miniconda3. @mbabadi @lucidtronix @samuelklee I think we should aim for tools that at least run out-of-box, without depending on any out-of-band configuration other than the conda env. On top of that we can provide guidance/configs for users on how to enable further optimizations, like g++. Does that sound like an achievable goal ?. As for the docker, we're going to have strike the right balance between image bloat and performance(including test performance). I think we're around 4+ gig now, and counting. Before the Python integration we were at 1.9G, and trying to find ways to reduce it. So lets see where we wind up but keep that in mind. Finally, we need to find a way to install the (GATK) python package(s) without depending on access to the GATK repo. Right now I think the gCNV branch has a ""pip install from source"" added to the conda env .yml. That will work on the docker at the moment (and thus on travis), but that won't work for non-docker users how don't have source/repo access. Also, one of the proposals to reduce the size of the docker is to remove the repo clone that is currently there. My proposal is that we change the gradle build to create an archive/zip of the python source (this would include the VQSR-CNN package code as well as gCNV kernel). We can then copy that on to the docker image, and pip-install it from the copy. That would retain the ability to always run travis tests based on the code in the repo, and also keep the nightly docker image in sync. We'll also have deliver the archive as an artifact somehow (perhaps including PyPi) for non-docker users.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277:775,access,access,775,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277,2,['access'],['access']
Security,"@meganshand Here's a quick example:. ![image](https://user-images.githubusercontent.com/11076296/158385742-20a3303b-d8ce-4335-b42f-622da9bfa8d3.png); ![image](https://user-images.githubusercontent.com/11076296/158385777-6174f8b8-7abb-4b31-92d1-11cc8064854b.png). Note that the malaria data used was pretty small: chr1-2 training (~20k positive training/truth variants, ~50k negative training variants; note also that the threshold for determining negative training was not tuned---a threshold corresponding to a 98% truth sensitivity was arbitrarily chosen), chr3 validation (~50k variants), and chr4-6 test (~150k variants). The LL score is calculated from a validation set held out from the training/truth positives used to train the model, while the F1 score is calculated using ""orthogonal truth"" positives/negatives determined using 3 families of ~30 trios each. However, there's some arbitrariness in how we define the boundary for the latter positives/negatives, and hence some arbitrariness in the F1 score itself. But I'd expect using gold-standard GIAB truth would be more straightforward. Not sure how much we can conclude, but that the validation and test F1s are similar and that the validation LL score isn't *too* far off are encouraging. That said, there is a pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian con",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:564,validat,validation,564,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,2,['validat'],['validation']
Security,"@meganshand There is a warning in the docs for `ReadCoordinateComparator` that it should not be used for bam file output that needs to match the ordering of `SAMRecordCoordinateComparator` exactly, since it sorts all unmapped reads after all mapped reads. `ReadCoordinateComparator` is a comparator for `GATKRead`, and that interface does not allow unmapped reads to have a position. Ie., even if an unmapped `SAMRecord` is assigned the position of its mapped mate, calling `getContig()`/`getStart()` on the unmapped read via the `GATKRead` interface will return null/0. This was done mainly for consistency reasons and to simplify client code. Whenever we need bam file order for reads in GATK4, we operate on SAMRecords directly and use either the `SAMRecordCoordinateComparator` from htsjdk or the `HeaderlessSAMRecordCoordinateComparator` (for headerless Spark reads) that produces the same ordering. I recommend addressing this for this tool via `presorted = false` for now, since the GATK3 version has it set to false as well with the comment: ""**we don't want to assume that reads will be written in order by the manager because in deep, deep pileups it won't work**"". This suggests that even if you were to change the comparator used by this tool to behave like `SAMRecordCoordinateComparator`, you'd still have ordering issues in deep coverage areas. It's worthwhile, though, to open a separate ticket to explore whether `ReadCoordinateComparator` could be changed to exactly match bam file order. Eg., perhaps we could add `getAssignedContig()`, `getAssignedStart()`, etc. methods to `GATKRead` to expose the positions that unmapped reads with mapped mates get assigned for sorting purposes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1853#issuecomment-224668518:1608,expose,expose,1608,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1853#issuecomment-224668518,1,['expose'],['expose']
Security,"@samuelklee I support exposing these parameters via the command line, but I'd be opposed to any consolidation of parameters that changes the HaplotypeCaller output prior to the initial DRAGEN-GATK release in November, as the evaluations in that project are difficult enough as it is. If you want to do an evaluation to find the best set of SW parameters now, that's fine of course -- but we wouldn't be able to actually merge any breaking HaplotypeCaller changes until after the November DRAGEN-GATK release, and we'd also have to check whether the proposed changes affect the functional equivalence of GATK and DRAGEN (we're developing tests now that can check this). If you want to expose the SW parameters on the CLI now, I think 12 arguments is fine. Just give each argument a clear prefix indicating what it applies to (eg., `--read-to-haplotype-mismatch-penalty`). If a user has gotten to the point where they feel the need to mess with the SW parameters, their command line is probably already long and complex as it is, so adding a few additional arguments won't ruin their day.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705081291:684,expose,expose,684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705081291,1,['expose'],['expose']
Security,"@schaluva Could I get access to a bam for that chromosome or some smaller interval that exhibits the bug and the original, non-simplified germline resource VCF? I think the error in filtering has been fixed, so I'm focusing on the first error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098#issuecomment-530090255:22,access,access,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098#issuecomment-530090255,1,['access'],['access']
Security,"Also, MQ filtering results in stochastic coverage dropout. It is likely that low MQ regions significantly overlap across samples, in which case, downstream CNV can learn such biases and correct the coverage. Will test this in validations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375722179:226,validat,validations,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375722179,1,['validat'],['validations']
Security,Another argument against this: the map function of a tool should clearly articulate its inputs in its signature. A map() that takes no parameters and relies on reflection/injection into members for its inputs would be supremely bad design.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76739266:171,inject,injection,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76739266,1,['inject'],['injection']
Security,"Are the errors below part of this, when starting BwaSpark with spark-submit?; I activated ""--disable-sequence-dictionary-validation true"", but that doesn't help. It is very unclear, why a BAM is not recognized as a BAM file. I have tried all kinds of ways to make sure that it is a BAM and not a SAM file.; The documentation for BwaSpark also says ""BAM/SAM/CRAM file containing reads"", so if SAM files are really not possible, that should probably be changed.; ...; Even on verbosity DEBUG, the comments are not at all helpful to get at the problem.; E.g. ""Cannot retrieve file pointers within SAM text files.""; Is that a general statement about SAM files? Or does it only say, that in this specific SAM file (which is actually a BAM file), file pointers cannot be found?; What pointers are meant exactly?; How could this be fixed?. ```; ""SamReaderFactory	Unable to detect file format from input URL or stream, assuming SAM format.""; Which URL?; Which stream?; Why would this happen? What could be the error?; The SAM/BAM distinction seems very unclear. It would be more helpful, if some specific missing aspect (e.g. not queryname sorted) would be clearly declared as the culprit.; ...; 00:29 DEBUG: [kryo] Write: SAMFileHeader{VN=1.5, SO=queryname}; ...; WARNING	2018-01-16 02:11:25	SamReaderFactory	Unable to detect file format from input URL or stream, assuming SAM format.; ...; java.lang.UnsupportedOperationException: Cannot retrieve file pointers within SAM text files.; 	at htsjdk.samtools.SAMTextReader.getFilePointerSpanningReads(SAMTextReader.java:185); ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4131#issuecomment-357838062:121,validat,validation,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4131#issuecomment-357838062,1,['validat'],['validation']
Security,"Back to @meganshand. I put in a simple mitochondrial integration test. Given that our MC3 validation already covers this particular bug I actually don't think it needs a new test for mitochondria. Also, for later, are any of your spike-in bams public (or rather, public + public)? I noticed that the NA12878 truth doesn't have very low AFs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5057#issuecomment-408649991:90,validat,validation,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5057#issuecomment-408649991,1,['validat'],['validation']
Security,"By the way, I thought @vdauwera was opposed to using optional inputs in this way at some point (see #3657). Was that question ever decided? (I'm still of the opinion that they *should* be used in this way, but this is one of the reasons I didn't for this iteration of the WDL.). To be clear, the pair WDL right now does not allow all of the workflow paths (tumor-only, no PoN, etc.) that the new tools make possible. It only allows the one that we will most likely run in production (matched-normal + PoN). We should probably make the WDL a little more flexible to cover the most common use cases, but I'm fine if it doesn't completely expose all of the possible workflow paths---this would probably just make the WDL harder to maintain. Users can write their own WDLs in this case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362696132:636,expose,expose,636,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362696132,1,['expose'],['expose']
Security,"Completing https://github.com/broadinstitute/hellbender/issues/673 will take care of the case of ""missing reference for CRAM"", but we also need to make sure we're handling the case of ""wrong reference"" elegantly (where elegantly means ""throw a `UserException` with a descriptive error message). We want a test case with a reference that is the wrong reference for a CRAM, but has a compatible sequence dictionary (so that it won't be caught by the sequence dictionary validation). Both the wrong and missing reference cases should have a simple integration test that runs, eg., `PrintReads` with `expectedExceptions = UserException.class`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374:468,validat,validation,468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374,1,['validat'],['validation']
Security,FO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(username); groups with view permissions: Set(); users with modify permissions: Set(username); groups with modify permissions: Set(); 18/04/24 17:39:20 INFO Utils: Successfully started service 'sparkDriver' on port 46576.; 18/04/24 17:39:20 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:39:20 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:39:20 INFO DiskBlockManager: Created local directory at /tmp/username/blockmgr-24b23f43-effa-45a6-8539-b9de234fa346; 18/04/24 17:39:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:39:20 INFO SparkEnv: Registering OutputComm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:7602,authenticat,authentication,7602,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['authenticat'],['authentication']
Security,"For repeated operators (whether xIyI or xMyM), I think GATK3 has/had a function to simplify cigars to ""sanitize"" that situation. In terms of desired behavior, we don't want to filter out the reads, we want to transform them to be processable without difficulty. I think xIyD should be considered valid.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/428#issuecomment-95056320:103,sanitiz,sanitize,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/428#issuecomment-95056320,1,['sanitiz'],['sanitize']
Security,"Great! And yes, LL will be optimized separately for SNPs and INDELs. How about this for a first workflow to target?. 1) Run ExtractVariantAnnotations on a training set of chromosomes. You can keep training/truth labels as in Best Practices, for now.; 2) Run TrainVariantAnnotationsModel on that. We'll use the truth scores generated here for any sensitivity conversions---i.e., we'll be calibrating scores only to the truth sites that are contained in the training set of chromosomes.; 3) Use the trained model to run a single shard of ScoreVariantAnnotations on a validation set of chromosomes.; 4) Run some variation of the above script on the resulting outputs to determine SNP and INDEL score thresholds for optimizing the corresponding LL scores. We can also add some code to the script to use the truth scores from step 2 to convert these score thresholds into truth-sensitivity thresholds.; 5) Provide these truth-sensitivity thresholds to ScoreVariantAnnotations and use them to hard filter. Evaluate on a test set of chromosomes. If all looks good, we can later move steps 3-4 into the train tool and automate the passing of sensitivities in 5 via outputs in the model directory. This will let us keep the basic interface of ScoreVariantAnnotations the same, but we'll have to add a few basic parameters to TrainVariantAnnotationsModel to control the train/validation split. So I think all this branch is missing is step 5---we'll simply need to add command-line parameters for the SNP/INDEL sensitivity thresholds and then do the hard filtering in the VCF writing method highlighted above. Do you think you can handle implementing that in this branch, and then the rest at the WDL level? I can help with the python script for the LL stuff (or anything else), if needed. Not sure if you got a chance to check out what your collaborators are doing in the methods you're looking to compare against, but it would be good to understand if this basic scheme for train/validation/test splitting can",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084:565,validat,validation,565,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084,1,['validat'],['validation']
Security,"Here's how these scripts are organized and why they take the form it is now:. How to run; * `manage/project.sh` is the ""executable""; * paths for VCF files (zipped or not) from PacBio callsets on CHM haploids, and Manta's VCF on the mixture should be provided to `manage/project.sh`, and; * paths for two versions of GATK-SV callsets; one is fine, but scripts in the sub-directory `manage` must be modified. Two GATK-SV vcf files are requested because this would allow one to compare if a supposedly improvement would make our raw sensitivity/specificity better or worse, that was the use case [here](https://github.com/SHuang-Broad/GATK-SV-callset-regressionTest), and; * paths to where results are to be stored, one for each GATK-SV callset must be given and ; * path to where to store the results of comparing the two callsets; * several GNU bash utilities are expected, `guniq` and `gsort`, when run on a Mac, as well as `bedtools`. and what to expect; * the scripts checks the VCF files, prints to screen a slew of information that one can pipe, or simplely browse through.; * the scripts also outputs the ID's of variants from each of the two GATK callsets that are ""validated"" by PacBio haploid calls. Misc points:; * watch out for ""duplicated"" records, as sometimes different assembly contigs mapped to the same locations have slightly different alleles (SNP, for example) hence both would be output, but there aren't many such records based on experience; * there are also some variants that we output to the VCFs having size <50 or >50K, both of which are filtered upfront and saved separately.; * The scripts started when we first call insertions, deletions, inversions and small duplications, and back then PacBio call sets on the CHM haploids were not available, so Manta's calls were used as ""reference"", that explains why they are referred to throughout the project",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4406#issuecomment-365730030:1172,validat,validated,1172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4406#issuecomment-365730030,1,['validat'],['validated']
Security,"Here's some old code that uses SamLocusIterator (from tfennel) that AllelicCapseg can adapt for now. From Tim:; ""They key to making this nice and simple is the SamLocusIterator class, which given a BAM file and a list of intervals, will give you pileups at each position in the intervals, filtered how you want them, and even provide convenience methods to access the exact base per read that is piled up at the site etc. The really nice things about doing it this way is that the constructor to SamLocusIterator takes a simple parameter to tell it whether to use an index/query mechanism (similar to what you're doing now) or to just read the BAM serially up until the last interval is reached and output the loci of interest. Running the below with ~100k sites on a standard exome (15GB or so) without using the index took only about 15 minutes."". ```; public void pileup(final File bam, final IntervalList intervals, final int minQ, final File outputFile) {; final int MAX_INTERVALS_FOR_INDEX = 25000; // just a guess, not sure what the right number is. final SamLocusIterator iterator = new SamLocusIterator(new SAMFileReader(bam), intervals, intervals.size() < MAX_INTERVALS_FOR_INDEX);; iterator.setEmitUncoveredLoci(false);; iterator.setQualityScoreCutoff(minQ);. final BufferedWriter out = IoUtil.openFileForBufferedWriting(outputFile); // will automatically gzip if filename ends with .gz; try {; while (iterator.hasNext()) {; final SamLocusIterator.LocusInfo locus = iterator.next();; int a=0, c=0, g=0, t=0;. for (final SamLocusIterator.RecordAndOffset rec : locus.getRecordAndPositions()) {; switch (rec.getReadBase()) {; case 'A' : ++a; break;; case 'C' : ++c; break;; case 'G' : ++g; break;; case 'T' : ++t; break;; }; }. out.append(locus.getSequenceName() + ""\t"" + locus.getPosition() + ""\t"" + a + ""\t"" + c + ""\t"" + g + ""\t"" + t + ""\t"");; }. out.close(); ; }; catch (IOException ioe) { throw new RuntimeIOException(ioe); }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420:357,access,access,357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420,1,['access'],['access']
Security,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:61,access,access,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857,1,['access'],['access']
Security,"Hi Jose,; Your system ulimit setting is too low. Please do this with root at; /etc/security/limits.conf; * soft memlock unlimited; * hard memlock unlimited; * hard nofile 20480; * soft nofile 20480; * hard nproc 40960; * soft nproc 40960; * soft stack unlimited; * hard stack unlimited. Ruzhu; -------------------------------------------; Ruzhu Chen, PhD (845) 433-8426(T/L 293-8426); Email: ruzhuchen@us.ibm.com, Mobile: (845) 337-7238; Sr. Technical Solution Architect, HPC / Genomics & Life Sciences; IBM Systems, 2455 South Road, Poughkeepsie, NY 12601. From:	Jose Sergio Hleap <notifications@github.com>; To:	broadinstitute/gatk <gatk@noreply.github.com>; Cc:	ruzhuchen <ruzhuchen@us.ibm.com>, Mention; <mention@noreply.github.com>; Date:	03/12/2020 11:37 AM; Subject:	[EXTERNAL] Re: [broadinstitute/gatk] Got ""Too many open files""; when use BaseRecalibratorSpark (#5316). Apologies on the poor report. There are no other users in these compute; nodes (I am the tester) and for all intents and purposes the ulimit is; pretty high (hard limit of 8192 max files). I am using GATK version; 4.1.4.1, although it might be the one that has been optimised for IBM; power9 systems by @ruzhuchen. Currently I am waiting for the sys admin to; increase the max files further, but I believe that this is far from ideal.; Here is the (simplified) command:. gatk --java-options ""-Xmx40g; -Djava.library.path=/bio/apps/gatk_4.1.4/gatk-4.1.4.1/libs; -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" Mutect2 -R; Homo_sapiens_assembly38.fa -I illuminaN_hg38.br.recal.bam; --max-mnp-distance 0 -O illuminaN.vcf.gz. May be I am running it wrong?. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-598269062:83,secur,security,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-598269062,1,['secur'],['security']
Security,"Hmm, I started taking a stab at the LL score implementation, but I think it's going to complicate the code quite a bit and add some branching options to the tool interfaces. Compounding this with a change in the use of ""truth"" and ""validation"" terminology, I fear that the resulting differences from the legacy strategy might be a bit much for users to digest!. So I'd want to better understand the cost/benefit before we proceed. How critical is automatic tuning of the hard threshold? And what's the relative importance to method changes that increase AUC (i.e., as opposed to figuring out where on the curve to hard threshold)? Is there a clear path forward for evaluating such a tuning process? @meganshand would be glad to chat more!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065524909:232,validat,validation,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065524909,1,['validat'],['validation']
Security,"Hmm, as we discussed, I'm not sure this will be so straightforward, given that we only have easy access to scores for positive truth---and hence, no false positives, which precludes calculation of precision and F1. I *think* we could pass a VCF for a sample with gold-standard positives and negatives and use the existing code for extracting labels, but this will require a bit of engineering and be more trouble than it's worth. There are other options---see https://ir.cwi.nl/pub/30479, for example. We might want to experiment with the LL score discussed there (see https://www.aaai.org/Papers/ICML/2003/ICML03-060.pdf for the original paper---although note that despite the paper's high citation count, I'm not sure what the canonical name for this metric actually is, but it doesn't appear to be ""LL score""---perhaps someone else knows or has better Google-fu and can figure it out) before moving on to their methods for estimating F1. Doing a literature search for other discussions of optimizing F1 or other metrics in the context of positive-unlabeled learning might be worthwhile, but I think most methods will probably involve some sort of estimation of the base rate in unlabeled data. I think we may have to add some mechanism for holding out a validation set during training if we want to automatically tune thresholds in a rigorous fashion. Shouldn't be too bad---we can just have the training tool randomly mask out a set of the truth and pass the mask to the scoring tool (or maybe just determine the threshold in the training tool, if we are running in positive/negative mode and have access to unlabeled data)---but does add a couple of parameters to the tool interfaces. This also adds additional dependence on the quality of the truth resources. I think an implicit assumption in any use of the truth---even just thresholding/calibrating by sensitivity---is that it is a random sample; however, I'm not sure how true this is in actual use. For example, in malaria, it looks like we",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241:97,access,access,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241,1,['access'],['access']
Security,"I think it triggers in certain situations where a firewall is blocking the connection. If the internet is simply unreachable it doesn't happen, so I don't know what the exact error case is. It happened consistently for people inside Intel's firewall or vpn. . An option to disable gcs support isn't a bad idea, it's kind of a hack though, it would be better if we could understand and avoid triggering the problem. If we could only initialize GCS support when we are sure that we actually are accessing files from google that could be a useful, but it doesn't seem like there's any single point we can plug into to detect that, it would have to be spread over everything that uses paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141:50,firewall,firewall,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141,3,"['access', 'firewall']","['accessing', 'firewall']"
Security,"I think that it is necessary to have a way for downstream projects to override some of the top-level arguments in the base CLP class. For example, the config file is for documentation purposes, but I don't want to expose users to that argument because I will set the defaults programmatically. Another example is the GCS retries, which might not be useful for a software that is not planning to support GCS even if it is already implemented (or does not want to expose). As a downstream developer, for me it is important to being able to configure arguments and expose/hide them to my final users; with the current implementation, my main issue is to have an argument that are irrelevant for the toolkit user and that I get questions about why and how to use them (the most clear example, the config file). If the main problem is to change an interface, a default value for new methods can be added to keep the same behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183:214,expose,expose,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183,3,['expose'],['expose']
Security,"I tried clearing my caches and rebuilding, but I resolve everything. I noticed that our artifactory website looks much different today than it did yesterday. I wonder if it was down temporarily for an update. Maybe try again now? Unless they put it behind the firewall which would be a disaster... can you access https://artifactory.broadinstitute.org/?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641:260,firewall,firewall,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641,2,"['access', 'firewall']","['access', 'firewall']"
Security,I wasn't able to reproduce this exact issue. However at least I can make the error message a bit more user-friendly (submitting #2417 for review). Crucially this will now give the name of the file we're having issues with (thus removing the uncertainty about whether it's the data or the index file we cannot access).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286:309,access,access,309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286,1,['access'],['access']
Security,"I would like to specify what passing a `ReadFilter` to some of my tools means, so maybe passing an `ArgumentCollection` will be simpler than this one, I agree. Although #2085 may solve the issue regarding the `ReadTransformer`/`ReadFilter` ordering, I would like to have in the plugin a way to specify different parameters (maybe some of then hidden before expose to users or advanced in the case of disabling). I will open a new PR for that change, but I will really appreciate if I can get something like that in this and other plugins (if implemented).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983:357,expose,expose,357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983,1,['expose'],['expose']
Security,"I'd also be hesitant to break the previous expectation that IntervalArgumentCollection contains a non-empty list of intervals. If I understand correctly (and apologies if not, I'm glancing at the repo between paternity-leave duties and am quite sleep deprived!), all calling code would have to add an explicit check that the new option isn't enabled or risk failing ungracefully downstream. For CNV code, this might be as simple as changing the validation method `CopyNumberArgumentValidationUtils.validateIntervalArgumentCollection`, but I wouldn't generally expect it to be so straightforward to add such checks throughout the codebase. I also agree with @lbergelson that the expected behavior might not be immediately clear and that perhaps this could be addressed in the scattering step---seems like shards could just be limited to regions that cover the resource at the outset. Consider also an older comment at https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435588845 about whether or not we should just use the equivalent Picard tool (horrible glob aside).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540740687:445,validat,validation,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540740687,2,['validat'],"['validateIntervalArgumentCollection', 'validation']"
Security,"I'm new at this, so I may be missing something, but it looks to me like there is an issue with the conversion code in GenomicsConverter.makeSAMRecord() in com.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:823,validat,validation,823,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033,1,['validat'],['validation']
Security,"In looking at his further, the container header contains a stream offset, and each slice header also contains a global record counter. Both of these need to be updated. Its not clear if its possible to repair these without re-encoding the entire container stream, but if so that should probably be done in a method exposed by htsjdk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2201#issuecomment-324756574:315,expose,exposed,315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2201#issuecomment-324756574,1,['expose'],['exposed']
Security,"It's not clear to me that we want these tools in Gatk4. We deliberately didn't port them because we felt they were unnecessary going forward. . I understand that there are some legitimate use cases that require them: ex low coverage naive variant calling from high ploidy pools which haplotype caller would do poorly on. (Also, do we know that haplotype caller doesn't do well on those sorts of things? Maybe we should consider modifications there if it doesn't?) I'm not sure that supporting that use case is worth the added complexity of maintaining and supporting these tools. Especially since we don't provide a pileup based variant caller as part of gatk4... . @vdauwera Can you comment? . @sooheelee I'm not sure I agree with you that supporting this for mutect 1 is useful. ; A) We don't want to support the use of mutect 1 anymore and would like to encourage people to switch to mutect 2 which I think we now believe is a better variant caller for both snps and indels. ; B) Mutect 1 users are already using gatk3, so they have access to these tools already. Mutect 1 also requires co-cleaning which I believe is a different but related tool to indel realignment. . For the variant review issue, we have thoughts on implementing a much better solution for variant review by creating an assembly plugin for igv.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988:1036,access,access,1036,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988,1,['access'],['access']
Security,"Just to make sure I understand the issue---will this cause technical problems in the Firecloud environment, or is it more of a style issue?. If the latter, one reason I prefer the use of optional file inputs to trigger tool-level ""modes"" when possible is that it propagates more naturally from the tool level. For example, let's consider a tool that can operate in either tumor-only or matched-pair mode. It is natural at the tool level to make the tumor a required input and the normal optional. The other options are quite awkward: 1) make both inputs required and switch between using the normal or not with a flag (in which case it is very easy for the user to shoot themselves in the foot if they forget to set the flag right, and we'd have to pass a dummy normal every time we want to run tumor only if we don't actually have a pair), 2) leave the normal as optional but add a flag anyway, which would be redundant and require an additional validation (i.e., if the flag is set to matched mode but we don't have a normal, we should fail early), or 3) write separate tools for each mode with the corresponding required inputs. If we accept that optional file input is the way to handle such a scenario at the tool level but not at the workflow level, then we will simply run into the same problems at the workflow level. I'm sure there are more complex scenarios when triggering on file presence/absence doesn't uniquely specify a workflow, in which case flags are a must. But for simple scenarios, I'm not sure why we shouldn't take advantage of the ability to specify optional file inputs in WDL (actually, I'm not sure how else we are supposed to use them?). However, if this is a problem for Firecloud, then I'd like to understand why---and what possible solutions there might be.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334046444:947,validat,validation,947,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334046444,1,['validat'],['validation']
Security,"Maybe I misunderstand the underlying model, but if some Pedigree annotations only need to know which samples are founders (ExcessHet ?) , and some need to know the full relationships (PossibleDeNovo), then I'm suggesting we change the class hierarchy to reflect that:. PedigreeAnnotation; |--TrioAnnotation; |----PossibleDeNovo; |--ExcessHet (assuming ExcessHet only needs founders...); ... Then the plugin could deterministically validate whether the user has provided sufficient args for the set of requested annotations; and if so, propagate them accordingly. A TrioAnnotation could only be populated (from the command line at least) from a file, whereas the others could be populated from either a file or just a set of IDs. I think it would simplify the annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463372550:431,validat,validate,431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463372550,1,['validat'],['validate']
Security,"Need some guidance here. The CompareSAMs tool was not propagating the validation stringency. I have a fix for that, but that alone doesn't fix the compareBAMFiles test in BaseRecalibrationIntegrationTest.java, since that uses SamAssertionUtils.assertSamsEqual, which also doesn't propagate (or accept) a validation stringency. Changing SamAssertionUtils to use either SILENT or LENIENT does fix the integration test, and all the other tests pass, but it seems like a relaxing of the stringency, and I'm not sure it should be necessary to the BQSR test. If relaxing the stringency for BQSR test _IS_ the right path, one possibility is to add a new method to SamAssertionUtils that accepts a validation stringency argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266:70,validat,validation,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266,3,['validat'],['validation']
Security,"No problem – I can see how this would be challenging to review – maybe it’s not even practical – if you decide it’s best just not to use it that’s OK - working on these issues has been a valuable learning experience for me. . I sent an archive with the new validations and the diffs between the old and new bams to akiezun. Although in many cases the files shared types of errors, I had to look at each file individually to take into account the particular errors in each file and how to fix them without (to the best of my knowledge) interfering with the purpose of the test. I did write a python script to use where necessary for converting multiple unpaired reads in a file to single reads, and I used bash scripts to call the picard tools to convert multiple files at a time from bam to sam for editing and then back again after they were modified. In some cases I had to modify the values in test output files to match the values produced by the test using the modified bams/sams, or just capture the new output files and use them to replace the old with the new. In two cases where file format errors appeared to be necessary but the filename did not indicate this, I renamed the files to make this clear. From: Louis Bergelson ; Sent: Thursday, August 20, 2015 2:13 PM; To: broadinstitute/hellbender ; Cc: nenewell ; Subject: Re: [hellbender] Issue 569 - bam and sam file cleanup. (#809). @nenewell Sorry this has been sitting. We've been trying to figure out how to review this one. Could you describe how you made the changes? Did you script it or go through by hand and modify them all?. —; Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051:257,validat,validations,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051,1,['validat'],['validations']
Security,"OK - PedigreeValidationType is now set in the constructor and is final. This does not separate the two intertwined codepaths around PedigreeFile vs. FounderIds, but that was a pre-existing problem. It doesnt doesnt change the pre-existing weirdness around the timing of setting pedigreeFile and/or founderIds within GATKAnnotationPluginDescriptor, where PedigreeAnnotation gets special treatment. I dont think this makes that situation any worse. if you still have concerns on this proposal, I actually think I could make our code work if you simply exposed a protected getPedigreeFile() method on PedigreeAnnotation. I can make the SampleDB instance in my code without needed to share code here. It seemed useful to expose some of that code to avoid duplication, but if it's going to over-complicate we can remove it. Also: that one test failure seems potentially unrelated (https://travis-ci.com/github/broadinstitute/gatk/jobs/510624560)? A compile issue with javadoc?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853986169:550,expose,exposed,550,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853986169,2,['expose'],"['expose', 'exposed']"
Security,"OK, thanks @droazen. I'll go ahead and expose all of them in a branch for now. For my own education, perhaps @jamesemery or @vruano can comment---does turning on DRAGEN sidestep all or some subset of the code paths where the above 3 sets of parameters come into play?. For what it's worth, now that I'm looking at short variants in malaria as a ""novice"" HC/M2 user, the command line options are quite daunting! Many of them are not well documented and it's not always clear which options might interact with each other. Perhaps once the evaluations are in place, it might be worth doing some model ablation to see if we can come up with a more minimal set of options (including the consolidation of the parameters under discussion, if possible).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705096593:39,expose,expose,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705096593,1,['expose'],['expose']
Security,"OK, that's reasonable. I'll dig into the other test changes. I can answer a few:. - Regarding passing the VariantWalker: I agree that's not an improvement by itself, but I would argue it's not that much different than it was. My plan is to pass a VariantEvalContext object, which would obscure any need to have knowledge of the walker. In an attempt to keep this PR simpler, I didnt complete that work. I do expect to make a second PR in relatively short order, once we get this resolved. - With respect to testEvalTrackWithoutGenotypesWithSampleFields and the different reference: I think the issue is that the old version (master GATK branch) didnt validate as strictly. When switching to MultiVariantWalkerGroupedOnStart, the reference is required, and the tool will error if the contigs dont match. VariantEval on the master branch didnt really need the reference for anything, and was apparently more permissive if it didnt line up. It probably preferentially grabbed the dictionary from the VCF header. I will look into those other questions",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-744698072:651,validat,validate,651,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-744698072,1,['validat'],['validate']
Security,"Ok -- caveat for all -- objects in bucket that are accessed via simple API Key need to have: User:allUsers:reader ACL permissions. if you need more complex access control, we'll have to support the ""secretFile"" attribute in `gcloud dataproc` -- not just the apiKey.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-228425738:51,access,accessed,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-228425738,2,['access'],"['access', 'accessed']"
Security,Please make this option hidden if it's only being kept for testing purposes (and document clearly that that is the case). Users should not have access to options that are not expected to have value.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2231#issuecomment-316842338:144,access,access,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2231#issuecomment-316842338,1,['access'],['access']
Security,"Returning false instead of throwing when data is missing in the `GoogleGenomicsReadToGATKReadAdapter` is misleading -- we don't know the answer to the question the client is asking in such cases, so returning false is not correct behavior. If these fields are actually missing in the underlying reads we really do want to blow up with an exception on access, as it means the read object is not usable by us (and the query that produced these incomplete objects likely needs to be modified). Could you restore the previous versions of these methods (`isSecondaryAlignment()`, `isDuplicate()`, etc.) before I review?. As for the Google read converters, could you open a separate ticket with your description of the inconsistencies so we can decide whether to submit a patch?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/871#issuecomment-136771148:351,access,access,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/871#issuecomment-136771148,1,['access'],['access']
Security,"ScoreVariantAnnotations:. Scores variant calls in a VCF file based on site-level annotations using a previously trained model. TODOs:. - [x] Integration tests. Exact-match tests for (non-exhaustive) configurations given by the Cartesian product of the following options:; * Java Bayesian Gaussian Mixture Model (BGMM) backend vs. python sklearn IsolationForest backend; (BGMM tests to be added once PR for the backend goes in.); * non-allele-specific vs. allele-specific; * SNP-only vs. SNP+INDEL (for both of these options, we use trained models that contain both SNP and INDEL scorers as input) ; - [x] Tool-level docs. Minor TODOs:. - [x] Parameter-level docs.; - [x] Parameter/mode validation.; - [x] Double check or add behavior for handling previously filtered input, clearing present filters, etc. Future work:. - [ ] The `score_samples` method of the sklearn IsolationForest is single-threaded. See (possibly stalled) PR at https://github.com/scikit-learn/scikit-learn/pull/14001 and some workarounds using e.g. `multiprocessing` ibid.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948563:686,validat,validation,686,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948563,1,['validat'],['validation']
Security,"Some comments/questions for the review:; - I'll add a separate ticket to rewrite the integration tests, all of which pass and most of which are disabled since they require access to large files on the broad file system. In the meantime I need to add a couple of small tests to get the coverage back up, and would like to get the CR process started.; - I ported a bunch of support files but need feedback on whether they're in the right location.; - Somewhere I saw something that said GATK no longer supports .ped files ? If not, what should the replacement be in the tests require pedigree input?; - Is it a requirement to support Ploidy > 2 ? The current GATK tool, and thus the HB tool, do not; - I did not port the WalkerTestSpec.disableShadowVCF? Is that needed in Hellbender ?; - Are there other headers I should be applying to the output variant file ?. Command Line Arguments:; - I didn't port the GATK command line argument ""-no_cmd_line_in_header"". Should I ? And if not, should the command line args automatically be propagated to the output vcf file ? I didn't see GATK do this anywhere.; - There was one test that used --variant:dbsnp on the command line but I couldn't find the code that processed that in GATK, not sure what the means on the command line.; - I replaced ""-U LENIENT_VCF_PROCESSING"" with ""--lenient"" (testFileWithoutInfoLineInHeaderWithOverride needs this to pass).; - I replaced ""-L"" with --interval since HB seems to use -L for ""lane"" ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027:172,access,access,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027,1,['access'],['access']
Security,"Thank you @vruano for your diligent review. I've implemented logger classes to encapsulate the metrics classes. Unfortunately the metrics classes must remain public in order to write output using `MetricsUtils.saveMetrics()`, but at least the tools aren't using them directly. There are two logging class groups - one for Filter and one Score. For Filter, there is an interface `PSFilterLogger` that is implemented by a file-logging class `PSFilterFileLogger` and a dummy class `PSFilterEmptyLogger` that does nothing. There are analogous classes for Score, but there is no Empty logger because it's not actually necessary. This adds a lot of new classes (maybe you can think of a better way) but usage has been greatly simplified. As we discussed in person, I don't think there is a faster way to count the reads in Spark. If you wanted to count the reads as they pass through, you would have to use some kind of atomic type that would be slow. Also it may be impossible to account for cases when tasks fail and restart. @lbergelson @droazen In this PR, I wanted to use htsjdk's MetricsFile and MetricBase classes for writing metrics to a file. I notice that these classes are mostly used for picard-related things. Is this the preferred way to do things? They do force you to expose public variables and also use an upper-case naming convention. On the other hand, they are somewhat convenient.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-334308160:1278,expose,expose,1278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-334308160,1,['expose'],['expose']
Security,"Thanx for feedback. I obviously don’t know much if anything about the underlying logic but; have had enough experience to look in unusual places. Have a good weekend. RDB. On Fri, Nov 1, 2019 at 4:24 PM JP Martin <notifications@github.com> wrote:. > @rdbremel <https://github.com/rdbremel> for ""mystery 1"" see issue #5447; > <https://github.com/broadinstitute/gatk/issues/5447>. This should be an; > innocuous warning that it can't initialize the Google Cloud Storage code; > and shouldn't cause a failure unless you try to access paths that start; > with ""gs://"". Going through the Cloud initialization steps described in the; > README should remove the warning (though again, this isn't required if you; > don't need to read files from the cloud).; >; > Mystery 2: For what it's worth, ""GC overhead limit exceeded"" indicates; > that the VM was spending too much time in GC. Running low on memory is a; > possible cause but generating too many small objects or being stuck in an; > infinite loop of allocation/deallocation are others. In the past these have; > been caused by inputs that were malformed in some way. This isn't the place; > for this discussion though, please file a separate issue since it's a; > separate bug.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/6182?email_source=notifications&email_token=ANCR2VHWQ6XDSUQ6KEGISFDQRSM7TA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEC4GNZY#issuecomment-548955879>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ANCR2VEC5ARUEQRTEDGJ3TDQRSM7TANCNFSM4I2MRFQA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548989454:524,access,access,524,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548989454,1,['access'],['access']
Security,"The new pipeline is in a complete state. Nearly all tools and scripts were rewritten, many from scratch. I've tried to minimize interaction with old `tools/exome` code (notably, `ReadCountCollection` and `SegmentUtils`). There are still some improvements that can be made (especially in segment union and the subsequent modeling), but we should be ready to go for a first validation. Some notes:. WDL:; - I've moved the old pipeline to `somatic_old` folders.; - There is now just a matched-pair workflow and a panel workflow. We can add a single BAM case workflow or expand the matched-pair workflow to handle this, depending on the discussion at https://github.com/broadinstitute/gatk/issues/3657.; - WES/WGS is toggled by providing an optional target-file input.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:372,validat,validation,372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['validat'],['validation']
Security,"The task here is to simply move the code while changing as little as possible, and then validate that. Once that's done, we can do whatever refactoring/changes we want to VQSR, or replace it completely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236014525:88,validat,validate,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236014525,1,['validat'],['validate']
Security,"There were a few issues with this case. First, the data source was not constructed 100% correctly. The config file is correct. . The index file is for the tar.gz version of the source data and not for the uncompressed version that they're using. The index should correspond to the source data in the file referenced by the config file itself (not a zipped or otherwise transformed version). Secondly, the source `tsv` data file has the header line for the table commented out. The Xsv codec is aware of leading hash marks as comments and will ignore any such lines. Because of this, the leading hash in the table header is ignored and the file cannot be properly parsed. The fix is simple - just remove the leading hash from the table header (the preceding line with the two hash marks is correctly interpreted as a file header because of the leading hashes acting as comments). Lastly, even if the user fixed the file they would still need to index it with`IndexFeatureFile`. At some point the code underlying this in `HTSJDK` was broken such that no Xsv files can currently be indexed. I have submitted a pull request in `HTSJDK` (https://github.com/samtools/htsjdk/pull/1429) for this and have another ready to go in GATK (#6224) that includes a test for this case so this reversion cannot happen again.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-545186183:511,hash,hash,511,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-545186183,5,['hash'],"['hash', 'hashes']"
Security,"To be clear, this will work perfectly fine as long as you have enough space in /dev/shm---which is typically true everywhere outside of our default Docker container. I'm loath to cripple a tool just because of limitations that are fundamentally elsewhere...let's just address those in the appropriate places. (Furthermore, I'm especially loath to write a plotting tool that takes ~5 minutes to generate a plot!) And yes, while it is not great that data.table forces us to use /dev/shm, I think `fread(""grep ..."")` is relatively standard. If `--shm-size` is indeed not exposed, why doesn't the Google backend scale /dev/shm or other tmpfs space with requested machine memory?. If there really is no other way around it, then all we're doing is filtering out the lines beginning with `@`. We could do this first by calling system commands within R to write to a temporary file, and then reading that back in with fread. This seems hacky to me, but I've confirmed that it works within the Docker. This will solve our immediate problem, but I still think it's worth taking a look at those other limitations elsewhere now as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357337827:568,expose,exposed,568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357337827,1,['expose'],['exposed']
Security,Variants Processed Variants/Minute; 14:55:54.857 INFO FilterMutectCalls - Starting pass 0 through the variants; 14:56:05.368 INFO ProgressMeter - 1:2019484 0.2 16000 91332.9; 14:56:15.521 INFO ProgressMeter - 1:4008750 0.3 35000 101621.1; 14:56:26.027 INFO ProgressMeter - 1:5856032 0.5 55000 105867.6; ...; 19:37:05.295 INFO ProgressMeter - GL000209.1:48811 281.2 30739000 109323.8; 19:37:15.543 INFO ProgressMeter - GL000224.1:65537 281.3 30758000 109324.9; 19:37:25.847 INFO ProgressMeter - GL000248.1:21736 281.5 30768000 109293.8; 19:37:25.906 INFO FilterMutectCalls - Finished pass 0 through the variants; 19:50:04.590 INFO FilterMutectCalls - Shutting down engine; [9 January 2020 7:50:04 PM] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 294.19 minutes.; Runtime.totalMemory()=14966849536; java.lang.IllegalArgumentException: Values in probability array sum to a negative number NaN; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:731); 	at org.broadinstitute.hellbender.utils.MathUtils.normalizeSumToOne(MathUtils.java:731); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.performEMIteration(SomaticClusteringModel.java:336); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:306); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:158); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:159); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.Com,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341:4269,validat,validateArg,4269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341,1,['validat'],['validateArg']
Security,"We've had to do that in other places... On Fri, Jan 12, 2018 at 3:20 PM, samuelklee <notifications@github.com>; wrote:. > To be clear, this will work perfectly fine as long as you have enough; > space in /dev/shm---which is typically true everywhere outside of our; > default Docker container.; >; > I'm loath to cripple a tool just because of limitations that are; > fundamentally elsewhere...let's just address those in the appropriate; > places. (Furthermore, I'm especially loath to write a plotting tool that; > takes ~5 minutes to generate a plot!) And yes, while it is not great that; > data.table forces us to use /dev/shm, I think fread(""grep ..."") is; > relatively standard.; >; > If --shm-size is indeed not exposed, why doesn't the Google backend scale; > /dev/shm or other tmpfs space with requested machine memory?; >; > If there really is no other way around it, then all we're doing is; > filtering out the lines beginning with @. We could do this first by; > calling system commands within R to write to a temporary file, and then; > reading that back in with fread. This seems hacky to me, but I've confirmed; > that it works within the Docker. This will solve our immediate problem, but; > I still think it's worth taking a look at those other limitations elsewhere; > now as well.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357337827>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXkxNvcMcJfIhdlPhdU3vLHTiAVPPSks5tJ76mgaJpZM4RclpR>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357349198:719,expose,exposed,719,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357349198,1,['expose'],['exposed']
Security,"Well, when the `clearItems` call is removed from the `consumeFinalizeItems` else branch, some `HaplotypeCallerSparkIntegrationTest`s [fail](https://api.travis-ci.com/v3/job/173147001/log.txt) because `PushToPullIterator` doesn't call clearItems to reset the state when `consumeFinalizeItems` returns no items, and the next submit is rejected because eoi hasn't been reset. So, since `consumeFinalizeItems` can't reset/mutate the state when there are no items, then we can't assert the precondition that `endOfInput==false` in `submit`. So I'm removing the validation of that from both `ReservoirDownsampler` and `PositionalDownsampler`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5594#issuecomment-457870723:556,validat,validation,556,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5594#issuecomment-457870723,1,['validat'],['validation']
Security,"Your interpretation sounds right, although I wish the language in the SAM spec were clearer - something like ""if 0x1 is unset, fields 0x2, 0x8, 0x20, 0x40, and 0x80 have no meaning and are ignored by the tools"". SAMRecord.IsValid() returns errors not only for 0x8(mate unmapped)/unpaired read, but also for the other four fields, so all of these errors would need to be removed. IsValid() also triggers an error when an unpaired read has RNEXT set, but the spec. ; doesn't appear to exclude this error. No error is triggered for the unpaired/PNEXT case. . So I agree that it looks like IsValid() should be changed to align with the spec. But I can also imagine potential pitfalls of leaving the GenomicsConverter code the way it is. The code adds spurious information to the bam that might cause problems with legacy versions of the tools. I don't know what the plans are for the state of the existing tool distribution once gatk4 is released. Is it possible that bam files produced in the cloud could make their way into gatk3 workflows, maybe via the sharing of bams between groups? If this happens, then unpaired reads processed in the cloud, with their mate unpaired flags set by the converter, could trigger validation errors when they are fed to the legacy tools. Is there a downside to altering the converter code as well as modifying the ; validator (I don't know what altering google packages entails...)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392:1213,validat,validation,1213,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392,2,['validat'],"['validation', 'validator']"
Security,"atic_old` folders.; - There is now just a matched-pair workflow and a panel workflow. We can add a single BAM case workflow or expand the matched-pair workflow to handle this, depending on the discussion at https://github.com/broadinstitute/gatk/issues/3657.; - WES/WGS is toggled by providing an optional target-file input.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalizati",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:1437,expose,exposed,1437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['expose'],['exposed']
Security,"bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [May 5, 2017 5:03:35 PM UTC] Executing as yarn@ip-172-30-0-122 on Linux 4.4.35-33.55.amzn1.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_121-b13; Version: Version:4.alpha.2-252-gf627ed4-SNAPSHOT; 17/05/05 17:03:35 INFO SparkContext: Running Spark version 2.1.0; 17/05/05 17:03:35 INFO SecurityManager: Changing view acls to: yarn,hadoop; 17/05/05 17:03:35 INFO SecurityManager: Changing modify acls to: yarn,hadoop; 17/05/05 17:03:35 INFO SecurityManager: Changing view acls groups to: ; 17/05/05 17:03:35 INFO SecurityManager: Changing modify acls groups to: ; 17/05/05 17:03:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, hadoop); groups with view permissions: Set(); users with modify permissions: Set(yarn, hadoop); groups with modify permissions: Set(); 17/05/05 17:03:35 INFO Utils: Successfully started service 'sparkDriver' on port 42358.; 17/05/05 17:03:35 INFO SparkEnv: Registering MapOutputTracker; 17/05/05 17:03:35 INFO SparkEnv: Registering BlockManagerMaster; 17/05/05 17:03:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 17/05/05 17:03:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 17/05/05 17:03:35 INFO DiskBlockManager: Created local directory at /mnt/yarn/usercache/hadoop/appcache/application_1493961816416_0010/blockmgr-356a706f-2395-4ef6-985a-d3a7d7b01a8a; 17/05/05 17:03:35 INFO DiskBlockManager: Created local directory at /mnt1/yarn/usercache/hadoop/appcache/application_1493961816416_0010/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046:3408,authenticat,authentication,3408,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046,1,['authenticat'],['authentication']
Security,"but for fingerprinting it seems that since it is effectively random-access,; perhaps prefetching will not be worth it?. On Fri, Apr 12, 2019 at 2:32 PM droazen <notifications@github.com> wrote:. > @yfarjoun <https://github.com/yfarjoun> We should sit down at some point; > to discuss the best way to activate the prefetching in Picard. It may be a; > little less trivial than I had thought based on the above, but should still; > be fairly simple.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5882#issuecomment-482678256>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0ptBXdOQ-9HlXMjjpFHI_zp-cQJqks5vgNEzgaJpZM4csje4>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5882#issuecomment-482678782:68,access,access,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5882#issuecomment-482678782,1,['access'],['access']
Security,"c/user/farrell/.sparkStaging/application_1542127286896_0153/__spark_libs__7473738539612638927.zip; 2019-01-07 11:33:38 INFO Client:54 - Uploading resource file:/tmp/spark-1ac79f09-1a36-4668-92d9-0739775f98ed/__spark_conf__4147634812449814799.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0153/__spark_conf__.zip; 2019-01-07 11:33:38 INFO SecurityManager:54 - Changing view acls to: farrell; 2019-01-07 11:33:38 INFO SecurityManager:54 - Changing modify acls to: farrell; 2019-01-07 11:33:38 INFO SecurityManager:54 - Changing view acls groups to:; 2019-01-07 11:33:38 INFO SecurityManager:54 - Changing modify acls groups to:; 2019-01-07 11:33:38 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-07 11:33:38 INFO Client:54 - Submitting application application_1542127286896_0153 to ResourceManager; 2019-01-07 11:33:38 INFO YarnClientImpl:251 - Submitted application application_1542127286896_0153; 2019-01-07 11:33:38 INFO SchedulerExtensionServices:54 - Starting Yarn extension services with app application_1542127286896_0153 and attemptId None; 2019-01-07 11:33:39 INFO Client:54 - Application report for application_1542127286896_0153 (state: ACCEPTED); 2019-01-07 11:33:39 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1546878818531; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0153/; user: farrell; 2019-01-07 11:33:40 INFO Client:54 - Application report for application_1542127286896_0153 (state: ACCEPTED); 2019-01-07 11:33:41 INFO Client:54 - Application report for application_1542127286896_0153 (state: ACCEPTED); 2019-01-07 11:33:42 INFO Client:54 - Applic",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:12578,authenticat,authentication,12578,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['authenticat'],['authentication']
Security,"c/user/farrell/.sparkStaging/application_1542127286896_0166/__spark_libs__7821719163562430010.zip; 2019-01-09 13:35:22 INFO Client:54 - Uploading resource file:/tmp/spark-69cc5c72-eff6-4259-8b3b-12fa6f8c42b0/__spark_conf__4520928824604875683.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0166/__spark_conf__.zip; 2019-01-09 13:35:22 INFO SecurityManager:54 - Changing view acls to: farrell; 2019-01-09 13:35:22 INFO SecurityManager:54 - Changing modify acls to: farrell; 2019-01-09 13:35:22 INFO SecurityManager:54 - Changing view acls groups to:; 2019-01-09 13:35:22 INFO SecurityManager:54 - Changing modify acls groups to:; 2019-01-09 13:35:22 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-09 13:35:22 INFO Client:54 - Submitting application application_1542127286896_0166 to ResourceManager; 2019-01-09 13:35:22 INFO YarnClientImpl:251 - Submitted application application_1542127286896_0166; 2019-01-09 13:35:22 INFO SchedulerExtensionServices:54 - Starting Yarn extension services with app application_1542127286896_0166 and attemptId None; 2019-01-09 13:35:23 INFO Client:54 - Application report for application_1542127286896_0166 (state: ACCEPTED); 2019-01-09 13:35:23 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1547058922320; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0166/; user: farrell; 2019-01-09 13:35:24 INFO Client:54 - Application report for application_1542127286896_0166 (state: ACCEPTED); 2019-01-09 13:35:25 INFO Client:54 - Application report for application_1542127286896_0166 (state: ACCEPTED); 2019-01-09 13:35:26 INFO Client:54 - Applic",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:12317,authenticat,authentication,12317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['authenticat'],['authentication']
Security,"d binding in [jar:file:/mnt/yarn/usercache/hadoop/filecache/37/__spark_libs__6987413740287883326.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 17/05/05 17:03:30 INFO SignalUtils: Registered signal handler for TERM; 17/05/05 17:03:30 INFO SignalUtils: Registered signal handler for HUP; 17/05/05 17:03:30 INFO SignalUtils: Registered signal handler for INT; 17/05/05 17:03:30 INFO ApplicationMaster: Preparing Local resources; 17/05/05 17:03:32 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1493961816416_0010_000002; 17/05/05 17:03:32 INFO SecurityManager: Changing view acls to: yarn,hadoop; 17/05/05 17:03:32 INFO SecurityManager: Changing modify acls to: yarn,hadoop; 17/05/05 17:03:32 INFO SecurityManager: Changing view acls groups to: ; 17/05/05 17:03:32 INFO SecurityManager: Changing modify acls groups to: ; 17/05/05 17:03:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, hadoop); groups with view permissions: Set(); users with modify permissions: Set(yarn, hadoop); groups with modify permissions: Set(); 17/05/05 17:03:32 INFO ApplicationMaster: Starting the user application in a separate Thread; 17/05/05 17:03:32 INFO ApplicationMaster: Waiting for spark context initialization...; [May 5, 2017 5:03:35 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output hdfs:///output2.bam --input hdfs:///chr1.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046:1835,authenticat,authentication,1835,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046,1,['authenticat'],['authentication']
Security,"d positives and negatives and use the existing code for extracting labels, but this will require a bit of engineering and be more trouble than it's worth. There are other options---see https://ir.cwi.nl/pub/30479, for example. We might want to experiment with the LL score discussed there (see https://www.aaai.org/Papers/ICML/2003/ICML03-060.pdf for the original paper---although note that despite the paper's high citation count, I'm not sure what the canonical name for this metric actually is, but it doesn't appear to be ""LL score""---perhaps someone else knows or has better Google-fu and can figure it out) before moving on to their methods for estimating F1. Doing a literature search for other discussions of optimizing F1 or other metrics in the context of positive-unlabeled learning might be worthwhile, but I think most methods will probably involve some sort of estimation of the base rate in unlabeled data. I think we may have to add some mechanism for holding out a validation set during training if we want to automatically tune thresholds in a rigorous fashion. Shouldn't be too bad---we can just have the training tool randomly mask out a set of the truth and pass the mask to the scoring tool (or maybe just determine the threshold in the training tool, if we are running in positive/negative mode and have access to unlabeled data)---but does add a couple of parameters to the tool interfaces. This also adds additional dependence on the quality of the truth resources. I think an implicit assumption in any use of the truth---even just thresholding/calibrating by sensitivity---is that it is a random sample; however, I'm not sure how true this is in actual use. For example, in malaria, it looks like we may have to resort to using a callset that has been very conservatively filtered as truth, which will bias us towards high scores and the peaks of the positive distribution. Perhaps we can also experiment with just treating training/truth on an equal footing (I think the d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241:1257,validat,validation,1257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241,1,['validat'],['validation']
Security,"dian count is significantly away from the main peak (due to the abundance of low mappability bins with small counts). . Also, the second peak of chrX coverage in XY samples that you show above is not a large germline event -- it is simply a low mappable PAR-like region that borrows reads from chrY. Here's how the X coverage distribution looks like on an XY sample after mappability filtering (which removes most of all approximate homologies):; ![chrx](https://user-images.githubusercontent.com/15305869/37867778-54e3d196-2f73-11e8-8345-d8964b39a17e.png). **The second spurious peak is gone and range of NB-like behavior is pretty much perfect. Without mappability filtering, all of the bins on the second mode _will_ show up as CN = 2 events (in fact, if you look at gCNV calls on a typical XY samples, there are tons of CN = 2 calls).**. Most, if not all, of the non-NB-like coverage before/after the main peak in your plots are reads from unmappable regions, many of which show up as real CNV events if we do not filter them (reads in these regions do not follow from the coverage model and we are at the mercy of BWA). I strongly believe Genome STRiP has achieved ~ 99% experimental validation accuracy because of aggressive filtering, not because of a superior model (it's an elementary Gaussian mixture mix). Garbage in, garbage out. Anyhow, I am not comfortable at all with cutting a non-Beta release without taking care of about:. 1. Mappability-based bin/read filtering (for WGS), and; 2. Trying out and evaluating a bait-based coverage collection (for WES), so that the raw coverage distribution is more NB-like to begin with. These are both perfectly achievable goals before May 15. I'd be happy to leave stuff such as different coverage collection strategies (e.g. base call coverage) and fragment-based per-sample GC content estimation for later. These are other areas where significant improvements come from. For the record -- I am working full steam on evaluations, as we discussed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375917669:1391,validat,validation,1391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375917669,1,['validat'],['validation']
Security,"e GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, esp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:1255,expose,expose,1255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,1,['expose'],['expose']
Security,"e PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class type of JNIReadDataHolderClass; * @param haplotypeDataHolderClass class type of JNIHaplotypeDataHolderClass; */; void jniInitializeClassFields(Class<JNIRead",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:1130,expose,expose,1130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864,1,['expose'],['expose']
Security,"e deleted, and its use `support = range(lo, hi)` should become `support = new IndexRange(lo, hi)`. Then `IntStream.of(support).mapToDouble(___).toArray()` becomes `range.mapToDouble(___)` and `apply(promote(support), ___)` becomes `support.mapToDouble(___)`. * `final double relErr = 1 + pow(10, -7)` should become a `static` constant. * The steps; ```java; final double maxD1 = arrayMax(d1);; final double[] d2 = apply(apply(d1, d -> d - maxD1), d -> exp(d));; final double sumD2 = sum(d2);; return apply(d2, d -> d/sumD2);; ```; inside `dnHyper` are just a home-brewed way to normalize the log-space array `d1`. Let's go throught the steps: 1) find the max. 2) subtract the max -- subtracting a constant in log-space is equivalent to dividing by a constant in real space, and since we're normalizing in the end this constant is arbitrary. It's done for numerical stability. 3) exponentiate to get an unnormalized real-space array. 4) find the sum. 5) divide by the sum to get the normalized result. The log-10 version of this is `MathUtils::normalizeFromLog10ToLinearSpace(d1)`. You could either calculate `d1` in log-10 space or convert it, replacing the above line with `return MathUtils.normalizeFromLog10ToLinearSpace(MathUtils.applyToArrayInPlace(d1, MathUtils::logToLog10))`. The latter option is simpler. * Import static should be avoided except to escape horrible clutter, which is not the case here. * The second argument of `Utils.validateArg(condition, calculated string expression)` should become `Utils.validateArg(condition, () -> calculated string expression)`. In the first version, the expression is calculated *even if* the condition is satisfied, whereas in the second it is only calculated as needed. It's not critical here but it's a good habit to get into. PS I am a zealot of the Boy Scout Rule (always leave the camp site cleaner than you found it). It is a great way to get familiar with a large code base and a great way to make the code more readable for the next person.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155:2063,validat,validateArg,2063,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155,2,['validat'],['validateArg']
Security,"en't seen discussed in these threads.; Perhaps it's impractical, but I'll mention it anyway. It seems like ; another approach would be to create (internal to the implementation) a ; ""header tag"" that could be efficiently serialized; and passed as part of the SAMRecord when you need to distribute it. The ; header tag could be used by the receiver to reattach the SAMRecord to ; its header (either proactively or on demand), but transparently to ; application code that is running against the SAMRecord API.; This would allow SAM headers to be transmitted out-of-band in a way that ; depends on the execution environment. Depending on the environment, ; this might be done by proactive broadcast, or you could think of the ; header tag as a promise to retrieve the header if/when it is needed. ; The size and complexity of the header tag might also depend on the ; execution environment. If the execution environment only supports a ; small finite number of headers, the header tag could be a small integer, ; or in a different execution environment it could be; a unique hash of the header or something like that. Memory footprint in ; the receiver is minimized because many SAMRecords can all share the same ; header object.; This requires more work to support in each execution environment, but it ; seems like it could be efficient and allows application code written to ; operate on SAMRecords to be portable; across different execution environments without having to contend with ; the possible presence of headerless SAMRecords. -Bob. On 9/17/15 4:28 PM, droazen wrote:. > @davidadamsphd https://github.com/davidadamsphd, @lbergelson ; > https://github.com/lbergelson, and myself met for an hour or two ; > just now to discuss this issue, and after reviewing all the options I ; > think we were convinced by the following argument:; > ; > The |SAMRecord| class currently allows its header to be set to null, ; > so if there are cases where the class won't function properly or can ; > enter int",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:2261,hash,hash,2261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518,1,['hash'],['hash']
Security,"es.githubusercontent.com/11076296/158385777-6174f8b8-7abb-4b31-92d1-11cc8064854b.png). Note that the malaria data used was pretty small: chr1-2 training (~20k positive training/truth variants, ~50k negative training variants; note also that the threshold for determining negative training was not tuned---a threshold corresponding to a 98% truth sensitivity was arbitrarily chosen), chr3 validation (~50k variants), and chr4-6 test (~150k variants). The LL score is calculated from a validation set held out from the training/truth positives used to train the model, while the F1 score is calculated using ""orthogonal truth"" positives/negatives determined using 3 families of ~30 trios each. However, there's some arbitrariness in how we define the boundary for the latter positives/negatives, and hence some arbitrariness in the F1 score itself. But I'd expect using gold-standard GIAB truth would be more straightforward. Not sure how much we can conclude, but that the validation and test F1s are similar and that the validation LL score isn't *too* far off are encouraging. That said, there is a pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian consistency, white = reference. The second plot shows the training/truth positives used to train the model and to calculate the LL score in the validation shard. The third plot s",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:1148,validat,validation,1148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,2,['validat'],['validation']
Security,"et held out from the training/truth positives used to train the model, while the F1 score is calculated using ""orthogonal truth"" positives/negatives determined using 3 families of ~30 trios each. However, there's some arbitrariness in how we define the boundary for the latter positives/negatives, and hence some arbitrariness in the F1 score itself. But I'd expect using gold-standard GIAB truth would be more straightforward. Not sure how much we can conclude, but that the validation and test F1s are similar and that the validation LL score isn't *too* far off are encouraging. That said, there is a pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian consistency, white = reference. The second plot shows the training/truth positives used to train the model and to calculate the LL score in the validation shard. The third plot shows the ""orthogonal truth"" positives/negatives used to calculate F1. So we can see that the difficulty in deriving F1 as a function of the score along the horizontal axis to give the third plot lies in collapsing the columns in the top plot into a single condition positive or condition negative status. Again, hard to do so without some arbitrariness; I simply came up with some rules to convert various amounts of red, yellow, green, etc. in each column to a red/white/green status. If you're u",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:1659,validat,validation,1659,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,1,['validat'],['validation']
Security,"m.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.nextMatePosition.position == null ? 8 : 0 #mate_unmapped. so it looks like the doc supports the existing code. Should I submit this as an issue? . Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:1375,validat,validation,1375,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033,1,['validat'],['validation']
Security,mail.com>; Date: Tue Dec 12 11:26:31 2017 -0500. disabled some gCNV WDL tests. commit 6d8ca07fef41518b5b157fb9a214d4536c617156; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 10:54:54 2017 -0500. fixed DenoiseReadCountsIntegrationTest files. commit adfbef12f2ab90f93b49a4f786979549648e1f22; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Mon Dec 11 02:22:56 2017 -0500. removed CNV evaluation code from this branch. commit 18c8d31f39a1964474c5d7b12ee8cbfafc4ac9e2; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Sun Dec 10 00:19:58 2017 -0500. GS VCF parser outputs dict for samples instead of list. commit b138be39cd8428342668ee6678079021006f983b; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Sun Dec 10 00:15:19 2017 -0500. renaming. commit eab5c90b74b4eb6bd11acb0fd1e0fa58a3b5b0c7; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 16:23:40 2017 -0500. exposed a global preemptible_attempts to gCNV workflows; set OMP_NUM_THREADS and MKL_NUM_THREADS to the number of requested CPUs. commit ad6fe348d6a7896c169b2b0499e2a4bca34021ad; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 10:21:25 2017 -0500. reverted log level in germline CNV tests. commit d9eb4e504baab834a9efc07cc3479176db2946ce; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 10:20:30 2017 -0500. the proper python environment yml for mkl and open -- leads to orders of magnitude speedup!. commit fea6bf874e0b62262a3b1d239ce4d76792d5c416; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 09:31:43 2017 -0500. revert. commit 456c53f88d01b603f4175d8896a0dac036af03f8; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 08:17:22 2017 -0500. enabled openmp g++ linking in theano. commit e2afef14ddb957f2dbdea76fd783d3bfb8d7a64e; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 08:04:19 2017 -0500. mkl. commit 43e2a65201286161fcd5bfe7dbb21ae888e1,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:2311,expose,exposed,2311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['expose'],['exposed']
Security,mory including 384 MB overhead; 17/10/13 18:11:36 INFO yarn.Client: Setting up container launch context for our AM; 17/10/13 18:11:36 INFO yarn.Client: Setting up the launch environment for our AM container; 17/10/13 18:11:36 INFO yarn.Client: Preparing resources for our AM container; 17/10/13 18:11:37 INFO yarn.Client: Uploading resource file:/tmp/hdfs/spark-c7e5eece-205e-4bce-a69b-4168c9b79045/__spark_conf__2918234914787361986.zip -> hdfs://mg:8020/user/hdfs/.sparkStaging/application_1507856833944_0003/__spark_conf__.zip; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:37 INFO yarn.Client: Submitting application application_1507856833944_0003 to ResourceManager; 17/10/13 18:11:37 INFO impl.YarnClientImpl: Submitted application application_1507856833944_0003; 17/10/13 18:11:37 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1507856833944_0003 and attemptId None; 17/10/13 18:11:38 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:38 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: root.users.hdfs; 	 start time: 1507889497661; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088/proxy/application_1507856833944_0003/; 	 user: hdfs; 17/10/13 18:11:39 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:10902,authenticat,authentication,10902,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['authenticat'],['authentication']
Security,"naging and querying Supplementary alignment; > information from read alignment records:; >; > Some of the things that I think smell:; >; > 1.; >; > Querying: implemented in htsjdk consists in forging artificial; > SAMRecords that contain only the alignment info in the SA tag element... It; > seems to me that it makes more sense to create class to hold this; > information alone (e.g. ReadAlignmentInfo or ReadAlignment); SATagBuilder; > already has defined a private inner class with that in mind ""SARead"" so why; > not flesh it out and make it public.; > 2.; >; > Writing: currently SATagBuilder gets attached to a read, parsing its; > current SA attribute content into SARead instances. It provides the; > possibility adding additional SAM record one by one or clearing the list.; > ... then it actually updates the SA attribute on the original read when a; > method (setTag) is explicitly called.; >; > I don't see the need to attach the SATag Builder to a read... it could; > perfectly be free standing; the same builder could be re-apply to several; > reads for that matter and I don't see any gain in hiding the read SA tag; > setting process,... even if typically this builder output would go to the; > ""SA"" tag, perhaps at some point we would like to also write SA coordinate; > list somewhere else, some other tag name or perhaps an error message... why; > impose this single purpose limitation?; >; > I suggest to drop the notion of a builder for a more general custom; > ReadAlignmentInfo (or whatever name) list. Such list could be making; > reference to a dictionary to validate its elements, prevent duplicates,; > keep the primary SA in the first position... etc.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/3324>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AArTZft11VTCtCHT_xr89kPL7hMFYQyhks5sQNghgaJpZM4Ofpkb>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323:2350,validat,validate,2350,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323,1,['validat'],['validate']
Security,"ode:. -`BaseRecalibratorSpark` is the standalone BQSR tool, and calls into the `BaseRecalibratorSparkFn` (which is also called from `ReadsPipelineSpark`). -`ApplyBQSRSpark` is the standalone ApplyBQSR tool, and calls into the `ApplyBQSRSparkFn` (also called from `ReadsPipelineSpark`). -Integration tests for the above are in `BaseRecalibratorSparkIntegrationTest` and `ApplyBQSRSparkIntegrationTest`. -Almost all other changes in the branch are related to the BQSR engine refactoring, which I summarize below:; - We pulled out the guts of the walker `BaseRecalibrator` tool, combined it with all of the code from the former `RecalibrationEngine` class (now deleted) to make a new `BaseRecalibrationEngine` class under `utils/recalibration`.; - We stripped out all copies of the code in `BaseRecalibrationEngine` from the walker, dataflow, and spark versions of BQSR, and modified them to call into `BaseRecalibrationEngine`.; - We moved all auxiliary classes needed by the `BaseRecalibrationEngine` (eg., the covariates, etc.) into `utils/recalibration`.; - We refactored the argument collections. Now there is a single shared `RecalibrationArgumentCollection` that contains **only** the parameters for the `BaseRecalibrationEngine` itself, and this argument collection is exposed by all 3 versions of the tool. Input/output arguments have been removed from this argument collection and put into the individual implementations of BQSR, since they vary between the walker, dataflow, and spark versions of the tool. This eliminates awkward problems such as having both a `knownSites` argument AND a `BQSRKnownVariants` exposed at the same time, with only 1 of them usable for a given version of a tool. The dataflow-only `BaseRecalibrationArgumentCollection` has been deleted completely as no longer needed.; - We tweaked the names of some tool arguments to enforce consistency between the 3 versions of the tool as well as the rest of hellbender (eg., output arg for BQSR is now a more standard `-O`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073:1325,expose,exposed,1325,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073,2,['expose'],['exposed']
Security,"olders named ""SAMPLE_#""), and just check the sample_name.txt files at the PostprocessGermlineCNVCalls step. I don't think this should require GermlineCNVCaller code changes, right?. 4) We may require additional code at the WDL level if we want to both switch over to primarily using sample names but also get rid of bundling (i.e., by passing only the calls for each sample when needed). Locally, you can always just search all output for directories containing the appropriate sample_name.txt. But on the cloud, you'd want to make sure that the postprocessing step for a particular sample gets only its corresponding directories, which would have to happen at the WDL level; the check against sample_name.txt at the tool level would just be a formality. I can foresee headaches with globbing and funky sample names. I'm not sure I understand your point about extending PostprocessGermlineCNVCalls to run on all samples. The point of that tool is to take results from all genomic shards for a single sample and stitch them together, right? Even if we extend this to run on a batch of multiple samples (which would just be moving the loop over samples at the WDL level to some lower level, i.e., Java or python), we still need to see all shards for those samples. Perhaps I'm misunderstanding---can you clarify?. @mwalker174 can we once and for all clearly document the issue with the transpose? Perhaps by pointing to specific WGS runs that have issues with call caching? I think being able to pinpoint the exact issue will help us identify the right solution---whether that be choosing an appropriate bundling scheme, taking advantage of #5781 to reduce the number of shards, batching during the postprocessing step, removing unnecessary outputs, etc. Recall that we'd like to be able to use the same WDL locally (when you have easy access to all GermlineCNVCaller results from all genomic shards) and in the cloud, with minimal duplication of output from bundling when running locally, if possible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765:3154,access,access,3154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765,1,['access'],['access']
Security,"periment with the LL score discussed there (see https://www.aaai.org/Papers/ICML/2003/ICML03-060.pdf for the original paper---although note that despite the paper's high citation count, I'm not sure what the canonical name for this metric actually is, but it doesn't appear to be ""LL score""---perhaps someone else knows or has better Google-fu and can figure it out) before moving on to their methods for estimating F1. Doing a literature search for other discussions of optimizing F1 or other metrics in the context of positive-unlabeled learning might be worthwhile, but I think most methods will probably involve some sort of estimation of the base rate in unlabeled data. I think we may have to add some mechanism for holding out a validation set during training if we want to automatically tune thresholds in a rigorous fashion. Shouldn't be too bad---we can just have the training tool randomly mask out a set of the truth and pass the mask to the scoring tool (or maybe just determine the threshold in the training tool, if we are running in positive/negative mode and have access to unlabeled data)---but does add a couple of parameters to the tool interfaces. This also adds additional dependence on the quality of the truth resources. I think an implicit assumption in any use of the truth---even just thresholding/calibrating by sensitivity---is that it is a random sample; however, I'm not sure how true this is in actual use. For example, in malaria, it looks like we may have to resort to using a callset that has been very conservatively filtered as truth, which will bias us towards high scores and the peaks of the positive distribution. Perhaps we can also experiment with just treating training/truth on an equal footing (I think the distinction between the two is somewhat blurry in the original VQSR design, anyway). Perhaps @davidbenjamin has some thoughts? I see some related stuff going on in ThresholdCalculator, but I have to admit that I can't tell whether that's used in a ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241:1602,access,access,1602,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241,1,['access'],['access']
Security,thanks for the review @kcibul. I made some changes accordingly. re: PrepareCallset file of sample names. That would be nice! It would make this workflow simpler and it also simplifies the access requirements for PrepareCallset. re: Dockstore. We actually ruled this out because Terra says that the definition of a method configuration can change automatically if its updated in dockstore. Which can be useful but it adds a security risk since a compromised Dockstore can change the definition of the production AoU extraction WDL which runs with highly elevated permissions. We already have a script that creates method configurations from github so I can probably add something a little hacky to resolve relative imports to the raw github file that it refers to.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7242#issuecomment-846494686:188,access,access,188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7242#issuecomment-846494686,2,"['access', 'secur']","['access', 'security']"
Security,"this for a first workflow to target?. 1) Run ExtractVariantAnnotations on a training set of chromosomes. You can keep training/truth labels as in Best Practices, for now.; 2) Run TrainVariantAnnotationsModel on that. We'll use the truth scores generated here for any sensitivity conversions---i.e., we'll be calibrating scores only to the truth sites that are contained in the training set of chromosomes.; 3) Use the trained model to run a single shard of ScoreVariantAnnotations on a validation set of chromosomes.; 4) Run some variation of the above script on the resulting outputs to determine SNP and INDEL score thresholds for optimizing the corresponding LL scores. We can also add some code to the script to use the truth scores from step 2 to convert these score thresholds into truth-sensitivity thresholds.; 5) Provide these truth-sensitivity thresholds to ScoreVariantAnnotations and use them to hard filter. Evaluate on a test set of chromosomes. If all looks good, we can later move steps 3-4 into the train tool and automate the passing of sensitivities in 5 via outputs in the model directory. This will let us keep the basic interface of ScoreVariantAnnotations the same, but we'll have to add a few basic parameters to TrainVariantAnnotationsModel to control the train/validation split. So I think all this branch is missing is step 5---we'll simply need to add command-line parameters for the SNP/INDEL sensitivity thresholds and then do the hard filtering in the VCF writing method highlighted above. Do you think you can handle implementing that in this branch, and then the rest at the WDL level? I can help with the python script for the LL stuff (or anything else), if needed. Not sure if you got a chance to check out what your collaborators are doing in the methods you're looking to compare against, but it would be good to understand if this basic scheme for train/validation/test splitting can be replicated over there. You'll want to compare apples to apples, after all!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084:1366,validat,validation,1366,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084,2,['validat'],['validation']
Security,"ward. Not sure how much we can conclude, but that the validation and test F1s are similar and that the validation LL score isn't *too* far off are encouraging. That said, there is a pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian consistency, white = reference. The second plot shows the training/truth positives used to train the model and to calculate the LL score in the validation shard. The third plot shows the ""orthogonal truth"" positives/negatives used to calculate F1. So we can see that the difficulty in deriving F1 as a function of the score along the horizontal axis to give the third plot lies in collapsing the columns in the top plot into a single condition positive or condition negative status. Again, hard to do so without some arbitrariness; I simply came up with some rules to convert various amounts of red, yellow, green, etc. in each column to a red/white/green status. If you're using a single gold-standard sample, this should definitely be more straightforward. In any case, the optimal validation LL score at ~0.02 does appear to line up quite well visually with where one might manually set a threshold. It corresponds pretty well with the transition from the yellow/red/grey junk to the clean green/white sites in the top plot. Here's the same for the test set:. ![image](https://user-images.gith",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:2142,validat,validation,2142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,1,['validat'],['validation']
Testability," 254, AlnModType.NONE);; final AlignmentInterval region3 = new AlignmentInterval(new SimpleInterval(""20"", 1, 413), 613, 1025, TextCigarCodec.decode(""612H413M""), true, 60, 0, 413, AlnModType.NONE);. final AlignedContig alignedContig = new AlignedContig(""asm00001:tig0001"", contigSequence, Arrays.asList(region0, region1, region2, region3), false);. final List<ChimericAlignment> assembledBreakpointsFromAlignmentIntervals = ChimericAlignment.parseOneContig(alignedContig, SVDiscoveryTestDataProvider.seqDict, true, StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigsAlignmentsSparkArgumentCollection.DEFAULT_MIN_ALIGNMENT_LENGTH, StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigsAlignmentsSparkArgumentCollection.CHIMERIC_ALIGNMENTS_HIGHMQ_THRESHOLD, true);. Assert.assertEquals(assembledBreakpointsFromAlignmentIntervals.size(), 1);; final ChimericAlignment chimericAlignment = assembledBreakpointsFromAlignmentIntervals.get(0);; Assert.assertEquals(chimericAlignment.sourceContigName, ""asm00001:tig0001"");; final NovelAdjacencyReferenceLocations breakpoints = new NovelAdjacencyReferenceLocations(chimericAlignment, contigSequence, SVDiscoveryTestDataProvider.seqDict);; }; ```. In versions of the code prior to #3752 (I think) this set of alignments was being filtered out by the method `isNotSimpleTranslocation` in the `parseOneContig` method of `ChimericAlignment`. Now that check's logic has changed and `isLikelySimpleTranslocation` returns false instead of true and so this alignment is not being filtered out any more. . When it gets to `NovelAdjacencyReferenceLocations.TanDupBreakpointsInference()` both `upstreamBreakpointRefPos` and `downstreamBreakpointRefPos` are being set to zero. It's not immediately clear to me how to fix this. A few thoughts:. - Are we supposed to be processing this `ChimericAlignment` through the main code path right now? ; - Why do we subtract 1 from the start position of the `rightReferenceInterval.getStart()",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3874#issuecomment-347627504:2811,assert,assertEquals,2811,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3874#issuecomment-347627504,1,['assert'],['assertEquals']
Testability," `AssemblyContigAlignmentsConfigPicker`; ; ----------; ## Consolidate logic, bump test coverage and update how variants are represented. ### consolidate logic; When initially prototyped, there's redundancy in logic for simple variants, now it's time to consolidate. - [x] `AssemblyContigWithFineTunedAlignments`; - [x] `hasIncompletePicture()`. - [x] `AssemblyContigAlignmentSignatureClassifier`; - [x] Don't make so many splits; - [x] Reduce `RawTypes` into fewer cases; ; - [x] `ChimericAlignment`; - [x] update documentation; - [x] implement a `getCoordinateSortedRefSpans()`, and use in `BreakpointsInference`; - [x] `isNeitherSimpleTranslocationNorIncompletePicture()`; - [x] `extractSimpleChimera()`. ### bump test coverage; Once code above is consolidated, bump test coverage, particularly for the classes above and the following poorly-covered classes; - [x] `ChimericAlignment`; - [x] `isForwardStrandRepresentation()`; - [x] `splitPairStrongEnoughEvidenceForCA()` ; - [x] `parseOneContig()` (needs testing because we need it for simple-re-interpretation for CPX variants) Note that `nextAlignmentMayBeInsertion()` is currently broken in the sense that when using this to filter out alignments whose ref span is contained by another, check if the two alignments involved are head/tail. - [x] `BreakpointsInference` & `BreakpointComplications`. - [x] `NovelAdjacencyAndAltHaplotype`; - [x] `toSimpleOrBNDTypes()`. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence`; - [x] serialization test. - [x] `AnnotatedVariantProducer`; - [x] `produceAnnotatedBNDmatesVcFromNovelAdjacency()`. - [x] `BreakEndVariantType`. - [ ] `SvDiscoverFromLocalAssemblyContigAlignmentsSpark` integration test; . ### update how variants are represented ; Implement the following representation changes that should make type-based evaluation easier; - [x] change `INSDUP` to`INS` when the duplicated ref region, denoted with annotation `DUP_REPEAT_UNIT_REF_SPAN`, is shorter than 50 bp.; - [x] change scarred del",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021:1217,test,test,1217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021,3,['test'],"['test', 'testing']"
Testability," acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:34 INFO util.Utils: Successfully started service 'sparkDriver' on port 45754.; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering MapOutputTracker; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering BlockManagerMaster; 17/10/13 18:11:34 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 17/10/13 18:11:34 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 17/10/13 18:11:34 INFO storage.DiskBlockManager: Created local directory at /tmp/hdfs/blockmgr-ea0e0669-2981-4277-80a0-a67eddf1001d; 17/10/13 18:11:34 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering OutputCommitCoordinator; 17/10/13 18:11:34 INFO util.log: Logging initialized @3816ms; 17/10/13 18:11:34 INFO server.Server: jetty-9.3.z-SNAPSHOT; 17/10/13 18:11:34 INFO server.Server: Started @3902ms; 17/10/13 18:11:34 INFO server.AbstractConnector: Started ServerConnector@131ba51c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 17/10/13 18:11:34 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@710ae6a7{/jobs,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b211077{/jobs/json,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62b0bf85{/jobs/job,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f07d414{/jobs/job/json,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40faff12{/stages,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.Conte",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:5295,log,log,5295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['log'],['log']
Testability," and capturing/resolving these BND's that are not suitable for _THIS PARTICULAR_ logic. I guess in general my personal preference is to put less algorithm-related information in VCF for analysts (less reading for them), and produce add on files for tool developers. What's your thoughts?. > Does this even have to be a spark tool? It looks like you are just reading the variants into a parallel spark context, filtering, and then collecting them to actually process them. Why not just make this a non-spark tool and process it all in memory on one node?. Answer: Agree. It doesn't have to be, at least in theory, and it probably is going to be faster as we don't need to incur the Spark overhead for such a typically small job. But (I'm saying too many buts....) up to this point all SV tools are under the package `hellbender.tools.spark.sv`, so I'm following suit here. Note the two classes's main interface methods mentions nothing about RDDs (that's on purpose). ; On the other hand, this is an engineering question I believe, and it depends on whether we want to put as much of discovery code as possible into `StructuralVariationDiscoveryPipelineSpark` (the last commit actually hooks the two classes into it, so a single invocation of the tool produces more variants), or we go wdl in pipelining the whole process. -------. All in all, I think the comments and critics are generally about the ""filtering""/""classifying"" part, and the most serious concern about it is false negatives. Am I understanding correctly? If so, given that the filtering step is only picking the BND's that are suitable to the linking logic, I can imagine the false negative problem be solved in the future by other logics (e.g. more relaxed requirement on pair matching, or not even requiring matching INV55/INV33 pairs, etc.) In fact, that's what I'm planning on.; Another part of the problem is how much I can accomplish in this single PR, and how large it should be&mdash;the forever existing problem for new tools.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:6856,log,logic,6856,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929,2,['log'],"['logic', 'logics']"
Testability," are fairly easy to fix or suppress. The exception is the Javadoc API [com.sun.javadoc](https://docs.oracle.com/en/java/javase/11/docs/api/jdk.javadoc/com/sun/javadoc/package-summary.html), which has been replaced by [jdk.javadoc.doclet](https://docs.oracle.com/en/java/javase/11/docs/api/jdk.javadoc/jdk/javadoc/doclet/package-summary.html). The javadoc tools in `org.broadinstitute.hellbender.utils.help` may need to be re-written (and it's not clear if it's possible to support Java 8 and Java 11 simultaneously).; * Travis build. Getting this to build and test on Java 11 in addition to the current builds may be fairly involved as the matrix is already quite complicated. (The current PR just changes Java 8 to Java 11 for testing purposes - we'd need a way of getting both to run.). The vast majority of tests are passing on Java 11, the following are failing:; * Missing `TwoBitRecord` (from ADAM); * `ReferenceMultiSparkSourceUnitTest`; * `ImpreciseVariantDetectorUnitTest`; * `SVVCFWriterUnitTest`; * `DiscoverVariantsFromContigAlignmentsSAMSparkIntegrationTest`; * `StructuralVariationDiscoveryPipelineSparkIntegrationTest`; * `SvDiscoverFromLocalAssemblyContigAlignmentsSparkIntegrationTest`; * `java.lang.NoSuchMethodError: java.nio.ByteBuffer.clear()Ljava/nio/ByteBuffer;`; * `SeekableByteChannelPrefetcherTest`; * `GatherVcfsCloudIntegrationTest`; * `Could not serialize lambda`; * `ExampleAssemblyRegionWalkerSparkIntegrationTest`; * `PileupSparkIntegrationTest`; * Native HMM library code caused the tests to crash on my Mac:; ```; Running Test: Test method testLikelihoodsFromHaplotypes[0](org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM@6282d367, true)(org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest); dyld: lazy symbol binding failed: can't resolve symbol __ZN13shacc_pairhmm9calculateERNS_5BatchE in /private/var/folders/cj/wyp4zgw17vj4m9qdmddvmcc80000gn/T/libgkl_pairhmm13775554937319419112.dylib because dependent dylib #1 could not be loaded",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359:1352,test,tests,1352,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359,1,['test'],['tests']
Testability, args to java side; major update to germline WDLs; all optional python args exposed to WDLs as optional args. commit 50cb6fd08de15469a9080cbb27ff30c8b7ee7e21; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:50:45 2017 -0500. missing serialVersionUID. commit 5f0f31eab63b0e6f6105708ded7f86c96c830781; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:35:33 2017 -0500. annotated intervals kebab case; updated germline WDL workflows. commit 29cc6234dbfb8db12559217a650c6ceb170c5797; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:15:28 2017 -0500. cleanup test files. commit 08a35bb4e65eceb735adcd41a91132e9a34d2b66; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 02:50:19 2017 -0500. update WDL scripts. commit 12bcfa192ee6fa6da21239ebf5b513633efe974f; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 02:47:33 2017 -0500. significant updates to GermlineCNVCaller; integration tests for GermlineCNVCaller w/ sim data in both run modes. commit 151416a4af735ca721bd75e4b54a780c17ac9397; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 01:42:05 2017 -0500. hybrid ADVI abstract argument collection w/ flexible default values; hybrid ADVI argument collection for contig ploidy model; hybrid ADVI argument collection for germline denoising and calling model. commit 56e21bf955d3dc0c52aceb384f28cf6173959de0; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 23:18:39 2017 -0500. rewritten python-side coverage metadata table reader using pandas to fix the issues with comment line; change criterion for cohort/case based on whether a contig-ploidy model is provided or not; simulated test files for ploidy determination tool; proper integration test for ploidy determination tool and all edge cases; updated docs for ploidy determination tool. commit 7fa104b2e9170770cfc5b338835e41215d7fd39c; Author: Mehrtash Babadi <mehrtash@broadinstitut,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:6934,test,tests,6934,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['test'],['tests']
Testability," changes for scaling density filter by coverage; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/StructuralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/re",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:1859,test,test,1859,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,1,['test'],['test']
Testability," did not modify the javadoc for any class in that package. The integration test runs `com.sun.tools.javadoc.Main.execute` and asserts that the output code is zero, which does not yield a useful error message. In order to produce something more meaningful I hacked the test to output the entire `stdout` and `stderr` as follows:. ```; final StringWriter out = new StringWriter();; final PrintWriter err = new PrintWriter(out);. final int result = com.sun.tools.javadoc.Main.execute(""program"", err, err, err, ""doclet"",docArgList.toArray(new String[] {}));; err.flush(); // probably not needed; String message = out.toString(); // message contains the entire stdout and stderr of the call to execute; Assert.assertEquals(result, 0, message);; ```. The output is about 2000 lines, but a lot of it is clearly innocuous. Removing lines such as; * `2022-08-16T00:09:07.2336106Z [parsing completed 1ms]`; * `2022-08-16T00:09:07.4456202Z [loading ZipFileIndexFileObject[/jars/gatk-package-4.2.6.1-56-gad9a538-SNAPSHOT-test.jar(org/broadinstitute/hellbender/tools/walkers/variantutils/ReblockGVCFIntegrationTest.class)]]`; * `2022-08-16T00:09:07.4459732Z [loading RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalIntervalArgumentCollection.java]]`; * `2022-08-16T00:09:07.4462012Z [parsing started RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalReferenceInputArgumentCollection.java]]`; * 2022-08-16T00:09:07.2322755Z [loading ZipFileObject[/gatk/gatk-package-unspecified-SNAPSHOT-local.jar(htsjdk/samtools/SAMSequenceDictionary.class)]]. brings it down to 353 lines, the majority of which look like . ```2022-08-16T00:09:07.4435974Z src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java:479: error: cannot find symbol; 2022-08-16T00:09:07.4436105Z @VisibleForTesting; ```. Here's that 353-line file:. [log-no-parsing-loading.txt](https://github.com/broadinstitute/gatk/files/9355026/log-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488:1169,test,test,1169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488,1,['test'],['test']
Testability," inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) for future use. Comment by @cwhelan ; > I'm a bit confused by this comment: this method is still being called in several places, so how is it obsolete?. Reply by @SHUANG-Broad (also copied to update the ""TODO"" message; > this predicate is currently used in two places (excluding appearance in comments): `BreakpointComplications.IntraChrStrandSwitchBreakpointComplications`, where it is use to test if the input simple chimera indicates an inverse tandem duplication and trigger the logic for inferring duplicated region; and `BreakpointsInference.IntraChrStrandSwitchBreakpointInference`, where it is used for breakpoint inference. The problem is, the contig will not even be sent here, because `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` defines a simple chimera that has strand switch and the two alignments overlaps on reference as ""incomplete"", so in practice the two uses are not going to be triggered. But when we come back later and see what can be extracted from such ""incomplete"" contigs, these code could be useful again. So it is kept. ------------; ### On the problem of writing out SAM records of ""Unknown"" contigs efficiently. First round comment by @cwhelan ; > This seems like a very inefficient way to write these three files. You end up calling collect on the RDD three different times and then traversing the local collection three times. Why not make a map of contig name to bam fil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:2932,test,test,2932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030,2,"['log', 'test']","['logic', 'test']"
Testability," pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian consistency, white = reference. The second plot shows the training/truth positives used to train the model and to calculate the LL score in the validation shard. The third plot shows the ""orthogonal truth"" positives/negatives used to calculate F1. So we can see that the difficulty in deriving F1 as a function of the score along the horizontal axis to give the third plot lies in collapsing the columns in the top plot into a single condition positive or condition negative status. Again, hard to do so without some arbitrariness; I simply came up with some rules to convert various amounts of red, yellow, green, etc. in each column to a red/white/green status. If you're using a single gold-standard sample, this should definitely be more straightforward. In any case, the optimal validation LL score at ~0.02 does appear to line up quite well visually with where one might manually set a threshold. It corresponds pretty well with the transition from the yellow/red/grey junk to the clean green/white sites in the top plot. Here's the same for the test set:. ![image](https://user-images.githubusercontent.com/11076296/158385662-6693a6c9-709c-482f-9a7e-5bb7030b3383.png). Happy to chat more about how you might implement this in your WDL---should be pretty straightforward!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:3050,test,test,3050,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,1,['test'],['test']
Testability," publish another artifact so we can move our projects forward. The crux of what I'm trying to achieve with these changes is for our VariantQC tool; however, I'm bumping into several situations where MultiVariantWalkers need to be able to determine the FeatureInput source of the variants. I recognize that this and my original PR #6973 has taken a non-trivial amount of @cmnbroad time, but we have basically been stalled with an otherwise approved PR since Feb 8. The PRs blocking that PR are this one and the related #7021. They both involve creating a way to connect VariantContext to FeatureInput - a capability that would benefit the GATK engine and I have been told by @cmnbroad you're interested in having. It seems like the primary problem associated with these changes is ensuring tests and VariantContext comparison code still works, since VariantContexts are going to tend to report a source. I dont know your internal conversations, so I'm guessing based on what's written in github. As I've said, I'd like to do whatever I can to get these changes into a form that takes as little of your effort as possible. . While this particular PR seems close, there is clearly some cleanup needed from what's there now, including code review from @lbergelson that no one ever fixed. Would it help if I put together #4571 and #7021 into a new branch where I also work through associated test changes and try to get this into one concise piece of code to review? Basically try to put everything together to be the minimal amount of work needed on your side? I havent heard anything one way or the other from GATK staff as to whether this would actually be helpful or not. Are there higher order design decisions that need to be considered that I'm not seeing? . I completely understand your time constraints - I'm just trying to figure out some solution that let's this go forward. Again, if we cant unblock this soon I'm going to probably fork GATK and need to start working off that project. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4571#issuecomment-822691252:1575,test,test,1575,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4571#issuecomment-822691252,1,['test'],['test']
Testability, the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field QD - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field QD - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field SOR - the field will NOT be part of INFO fields in the generated VCF records; 15:02:24.449 INFO FeatureManager - Using codec BEDCodec to read file file:///scratch/jsalt-test/ABBA-BABA/vcf/cv-genome-uce-coords-newchrom-500flank.bed; 15:02:24.508 INFO IntervalArgumentCollection - Processing 9612471 bp from intervals; 15:02:24.531 INFO GenotypeGVCFs - Done initializing engine; 15:02:24.609 INFO ProgressMeter - Starting traversal; 15:02:24.609 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; WARNING: No valid combination operation found for INFO field AC - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field AF - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field AN - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field DS - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field FS - the fiel,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640:6009,test,test,6009,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640,1,['test'],['test']
Testability," will be optimized separately for SNPs and INDELs. How about this for a first workflow to target?. 1) Run ExtractVariantAnnotations on a training set of chromosomes. You can keep training/truth labels as in Best Practices, for now.; 2) Run TrainVariantAnnotationsModel on that. We'll use the truth scores generated here for any sensitivity conversions---i.e., we'll be calibrating scores only to the truth sites that are contained in the training set of chromosomes.; 3) Use the trained model to run a single shard of ScoreVariantAnnotations on a validation set of chromosomes.; 4) Run some variation of the above script on the resulting outputs to determine SNP and INDEL score thresholds for optimizing the corresponding LL scores. We can also add some code to the script to use the truth scores from step 2 to convert these score thresholds into truth-sensitivity thresholds.; 5) Provide these truth-sensitivity thresholds to ScoreVariantAnnotations and use them to hard filter. Evaluate on a test set of chromosomes. If all looks good, we can later move steps 3-4 into the train tool and automate the passing of sensitivities in 5 via outputs in the model directory. This will let us keep the basic interface of ScoreVariantAnnotations the same, but we'll have to add a few basic parameters to TrainVariantAnnotationsModel to control the train/validation split. So I think all this branch is missing is step 5---we'll simply need to add command-line parameters for the SNP/INDEL sensitivity thresholds and then do the hard filtering in the VCF writing method highlighted above. Do you think you can handle implementing that in this branch, and then the rest at the WDL level? I can help with the python script for the LL stuff (or anything else), if needed. Not sure if you got a chance to check out what your collaborators are doing in the methods you're looking to compare against, but it would be good to understand if this basic scheme for train/validation/test splitting can be replicated ove",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084:1014,test,test,1014,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084,1,['test'],['test']
Testability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2396?src=pr&el=h1) Report; > Merging [#2396](https://codecov.io/gh/broadinstitute/gatk/pull/2396?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/3c10554709a4f254300a3d38f24216c42da5913c?src=pr&el=desc) will **increase** coverage by `0.028%`. ```diff; @@ Coverage Diff @@; ## master #2396 +/- ##; ==============================================; + Coverage 76.14% 76.168% +0.028% ; - Complexity 10824 10829 +5 ; ==============================================; Files 748 748 ; Lines 39372 39372 ; Branches 6856 6856 ; ==============================================; + Hits 29978 29989 +11 ; + Misses 6802 6791 -11 ; Partials 2592 2592; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2396?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...efe544dd515af5f5f25f6c73e8d54726fceca914?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `87.2% <ø> (ø)` | `37 <ø> (ø)` | :x: |; | [...institute/hellbender/engine/FeatureDataSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...efe544dd515af5f5f25f6c73e8d54726fceca914?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2UuamF2YQ==) | `77.477% <ø> (+0.901%)` | `38% <ø> (+1%)` | :white_check_mark: |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...efe544dd515af5f5f25f6c73e8d54726fceca914?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `79.747% <ø> (+6.329%)` | `22% <ø> (+4%)` | :white_check_mark: |. --,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2396#issuecomment-278174747:905,test,test,905,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2396#issuecomment-278174747,1,['test'],['test']
Testability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=h1) Report; > Merging [#2540](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/5ccfd0097c39b44464692fab1566d850f73aa5c7?src=pr&el=desc) will **increase** coverage by `0.523%`.; > The diff coverage is `82.075%`. ```diff; @@ Coverage Diff @@; ## master #2540 +/- ##; ===============================================; + Coverage 75.865% 76.388% +0.523% ; - Complexity 10839 10918 +79 ; ===============================================; Files 754 755 +1 ; Lines 39552 39653 +101 ; Branches 6907 6925 +18 ; ===============================================; + Hits 30006 30290 +284 ; + Misses 6936 6741 -195 ; - Partials 2610 2622 +12; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L01pbmlDbHVzdGVyVXRpbHMuamF2YQ==) | `89.474% <100%> (+1.974%)` | `7 <3> (+2)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `71.318% <100%> (+22.481%)` | `34 <1> (+7)` | :arrow_up: |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `80.612% <80.612%> (ø)` | `19 <19> (?)` | |; | [...institute/hellbender/engine/FeatureDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGV,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2540#issuecomment-290122549:957,test,test,957,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2540#issuecomment-290122549,1,['test'],['test']
Testability,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=h1) Report; > Merging [#2566](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/6859a1202a79c1b123eac73a3f70162c6a90783c?src=pr&el=desc) will **decrease** coverage by `0.015%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2566 +/- ##; ==============================================; - Coverage 76.386% 76.37% -0.015% ; + Complexity 10898 10895 -3 ; ==============================================; Files 754 754 ; Lines 39552 39552 ; Branches 6907 6907 ; ==============================================; - Hits 30212 30206 -6 ; - Misses 6727 6732 +5 ; - Partials 2613 2614 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ellbender/utils/test/CommandLineProgramTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0NvbW1hbmRMaW5lUHJvZ3JhbVRlc3Rlci5qYXZh) | `85.714% <0%> (-4.762%)` | `7% <0%> (-1%)` | |; | [...oadinstitute/hellbender/utils/GenomeLocParser.java](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vbWVMb2NQYXJzZXIuamF2YQ==) | `85.95% <0%> (-4.132%)` | `55% <0%> (-2%)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=footer). Last update [6859a12...1df1909](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=lastupdated). Read the [comment docs](https://doc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2566#issuecomment-291909459:923,test,test,923,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2566#issuecomment-291909459,1,['test'],['test']
Testability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=h1) Report; > Merging [#2595](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/bf993d8c6f6925ce6bdb67f50c0e33c6e5bc3336?src=pr&el=desc) will **increase** coverage by `1.133%`.; > The diff coverage is `61.842%`. ```diff; @@ Coverage Diff @@; ## master #2595 +/- ##; ===============================================; + Coverage 75.992% 77.126% +1.133% ; - Complexity 11033 11147 +114 ; ===============================================; Files 769 771 +2 ; Lines 40058 40115 +57 ; Branches 6979 6982 +3 ; ===============================================; + Hits 30441 30939 +498 ; + Misses 6978 6513 -465 ; - Partials 2639 2663 +24; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L01pbmlDbHVzdGVyVXRpbHMuamF2YQ==) | `89.474% <ø> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...sv/DiscoverVariantsFromAlignedSGAContigsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9EaXNjb3ZlclZhcmlhbnRzRnJvbUFsaWduZWRTR0FDb250aWdzU3BhcmsuamF2YQ==) | `0% <0%> (ø)` | `0 <0> (?)` | |; | [.../sv/DiscoverVariantsFromContigAlignmentsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9EaXNjb3ZlclZhcmlhbnRzRnJvbUNvbnRpZ0FsaWdubWVudHNTcGFyay5qYXZh) | `100% <100%> (ø)` | `7 <1> (?)` | |; | [...stitute/hellbender/tools/spark/sv/SVVCFWriter.java](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-293918558:956,test,test,956,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-293918558,1,['test'],['test']
Testability,"## Update:. ### A [broad institute forum post](https://gatk.broadinstitute.org/hc/en-us/community/posts/360061666671/comments/360010377231) gave the solution:. #### If you paste this text into the `gatkcondaenv.yaml` file:. ```; # Conda environment for GATK Python Tools; #; # Only update this environment if there is a *VERY* good reason to do so!; # If the build is broken but could be fixed by doing something else, then do that thing instead.; # Ensuring the correct environment for canonical (or otherwise reasonable) usage of our standard Docker takes precedence over edge cases.; # If you break the environment, you are responsible for fixing it and also owe the last developer who left this in a reasonable state a beverage of their choice.; # (This may be yourself, and you'll appreciate that beverage while you tinker with dependencies!); #; # When changing dependencies or versions in this file, check to see if the ""supportedPythonPackages"" DataProvider; # used by the testGATKPythonEnvironmentPackagePresent test in PythonEnvironmentIntegrationTest needs to be updated; # to reflect the changes.; #; name: gatk; channels:; # if channels other than conda-forge are added and the channel order is changed (note that conda channel_priority is currently set to flexible),; # verify that key dependencies are installed from the correct channel and compiled against MKL; - conda-forge; - defaults; dependencies:. # core python dependencies; - conda-forge::python=3.6.10 # do not update; - pip=20.0.2 # specifying channel may cause a warning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-for",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:981,test,testGATKPythonEnvironmentPackagePresent,981,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,2,['test'],"['test', 'testGATKPythonEnvironmentPackagePresent']"
Testability,(Note that the failure in the cloud tests is expected due to an ongoing GCS bucket region migration -- it should clear up next week),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8318#issuecomment-1546280764:36,test,tests,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8318#issuecomment-1546280764,1,['test'],['tests']
Testability,"+1MOn Sat, Feb 28, 2015 at 1:14 PM, ldgauthier notifications@github.com wrote: It would be wonderful to be able to use SelectVariants with a query like -select ""AF > 0.1"" on a VCF containing multiallelics and have it filter multiallelics by the allele with the highest AF. (Possibly conversely for ""AF < X""queries. Right now it crashes unless you use a crazy JEXL or pull out the multiallelics. Maybe we could make a maxAF/minAF in htsjdk/JEXLmap.java which equals AF for biallelics?. Internally, it might be nice to have a Map with the AF (or AC) for each allele for the SelectVariants issue and to simplify some of the crazy logic already in VariantAnnotator to deal with different allele ordering. As part of this task, we should also make 100% sure that allele ordering is preserved so that AF/AC array ordering is preserved during VC reading/writing/manipulation. —Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/241#issuecomment-76543438:627,log,logic,627,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/241#issuecomment-76543438,1,['log'],['logic']
Testability,"-insertions_default_quality 45 --deletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --interval_set_rule UNION --interval_padding 0 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_less_than 6 --useOriginalQualities false --defaultBaseQualities -1 --runner LOCAL --client_secret client-secrets.json --help false --version false --VERBOSITY INFO --QUIET false; [June 1, 2015 5:51:02 PM EDT] Executing as pgrosu@eofe5 on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:version-unknown-SNAPSHOT JdkDeflater; 17:51:02.969 [main] INFO org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow - Initializing engine; 17:51:02.970 [main] INFO org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow - Done initializing engine; 17:51:03.123 [main] INFO org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow - Shutting down engine; [June 1, 2015 5:51:03 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=1077936128; Exception in thread ""main"" java.lang.NoSuchMethodError: com.google.common.collect.Sets.newConcurrentHashSet()Ljava/util/Set;; at com.google.cloud.dataflow.sdk.options.PipelineOptionsFactory.<clinit>(PipelineOptionsFactory.java:426); at org.broadinstitute.hellbender.engine.dataflow.DataflowCommandLineProgram.doWork(DataflowCommandLineProgram.java:77); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:97); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:150); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:71); at org.broadinstitute.hellbender.Main.main(Main.java:86); ```. I don't have a billing-enabled GCS account, so I did this test to see if I could make it run with my `client-secrets.json` file. Any guidance would be appreciated. Thank you,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:5787,test,test,5787,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['test'],['test']
Testability,"...cab0d179986f7f7587e0e005a7b8e54d20168a65?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :white_check_mark: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/521128573b0d1a01ee60725c2b84e4a4f6a12fa3...cab0d179986f7f7587e0e005a7b8e54d20168a65?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `75.694% <0%> (+2.083%)` | `36% <0%> (ø)` | :x: |; | [...oadinstitute/hellbender/utils/GenomeLocParser.java](https://codecov.io/gh/broadinstitute/gatk/compare/521128573b0d1a01ee60725c2b84e4a4f6a12fa3...cab0d179986f7f7587e0e005a7b8e54d20168a65?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vbWVMb2NQYXJzZXIuamF2YQ==) | `90.083% <0%> (+4.132%)` | `57% <0%> (+2%)` | :white_check_mark: |; | [...ellbender/utils/test/CommandLineProgramTester.java](https://codecov.io/gh/broadinstitute/gatk/compare/521128573b0d1a01ee60725c2b84e4a4f6a12fa3...cab0d179986f7f7587e0e005a7b8e54d20168a65?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0NvbW1hbmRMaW5lUHJvZ3JhbVRlc3Rlci5qYXZh) | `90.476% <0%> (+4.762%)` | `8% <0%> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2423?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2423?src=pr&el=footer). Last update [5211285...cab0d17](https://codecov.io/gh/broadinstitute/gatk/compare/521128573b0d1a01ee60725c2b84e4a4f6a12fa3...cab0d179986f7f7587e0e005a7b8e54d20168a65?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-282342687:2435,test,test,2435,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-282342687,1,['test'],['test']
Testability,".samuel.k@gmail.com>; Date: Fri Dec 8 00:55:14 2017 -0500. updated somatic PoNs for PreprocessIntervals drop Ns. commit cff64984d9fb42364001bda4c73d54cf68d85a5c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 00:37:24 2017 -0500. sudo travis yml. commit 89025941febd2089d426cfa1e0f0aa6a6712e2a9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 00:23:22 2017 -0500. travis/Docker config update (g++-6, Miniconda3); python test group assignment. commit 31f96398106c2b8577b8c25d110abea3ebe7f836; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:44:53 2017 -0500. WDL test bugfix. commit 9b2fb820536ec355bea0256471bd093d547f5c99; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:20:36 2017 -0500. update WDL test JSON files. commit e3d97644d1a2c40a5c364a96f8b67246154179c9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:18:14 2017 -0500. assertions in inference task base; removed a ASCII > 128 character in log messages. commit 526cf92e623a3bbd5f9d375132b6ca046fc47620; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:03:04 2017 -0500. redirect tqdm progress bar to python logger. commit 2e45bd30968b921fae225de3901fb97ece690b0c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:45:49 2017 -0500. more arg related fixes. commit bb89a3bb338d88199881e8aca65f656f2acd7c0a; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:41:20 2017 -0500. arg related bugfixes in WDL, python, and java CLIs. commit 23569787ee2c8cc6c9227a44170cbbd02fe4427f; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 17:21:05 2017 -0500. fixed issue with python boolean argparse (they use weird semantics). commit ae841c9ed4cd9b2ca1ac0e9082d175ff8ea98298; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 16:44:02 2017 -0500. shorter gCNV WDL tests. commit 5466b806e36df16cad2d045be074e7f",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:4739,assert,assertions,4739,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,2,"['assert', 'log']","['assertions', 'log']"
Testability,"/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/resources/sv_classifier_test_data.json; src/test/resources/sv_features_test_data.json",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:2674,test,test,2674,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,5,['test'],"['test', 'tests']"
Testability,"/test/resources/large/org/broadinstitute/hellbender/tools/walkers/vqsr/scalable/train/expected/extract.nonAS.snpIndel.posUn.train.snpIndel.posOnly.IF.snp.trainingScores.hdf5 /repo/extract.nonAS.snpIndel.posUn.train.snpIndel.posOnly.IF.snp.trainingScores.hdf5. file1 file2; ---------------------------------------; x x / ; x x /data ; x x /data/scores . group : </> and </>; 0 differences found; group : </data> and </data>; 0 differences found; dataset: </data/scores> and </data/scores>; size: [445] [445]; position scores scores difference ; ------------------------------------------------------------; [ 60 ] -0.419202 -0.419202 5.55112e-17 ; 1 differences found; ```. Looks pretty negligible to me! :stuck_out_tongue_closed_eyes: Probably a result of the native code being called by the python/ML packages used in these tools; even minor changes in the compilers across Ubuntu versions might introduce differences like these. A quick fix might be to replace all system calls to `h5diff` in these tests with `h5diff --use-system-epsilon`; seems to do the trick here. But if that doesn't fix all test cases, then perhaps you can relax things with `h5diff -p EPSILON`, where `EPSILON` is a relative threshold. Probably OK to pick something like `1E-6`. OK if I leave it to you to try this or otherwise check the rest of the cases?. Sorry for the inconvenience! I think the exact-match test worked as intended here, but I probably could've put in better messaging originally. Unfortunately, it's a bit awkward to grab the output of system commands. And thanks for dealing with conda again (a necessary evil, unless we want to reimplement the entire field of machine learning in Java)! I'll experiment to see if I can't get the more recent version used in #8561 (23.10) working with the current environment---probably just some minor tweak to the pip version is needed to get around the error you're seeing. You could try unpinning it to see what gets pulled in. It would be great if we could get off",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848796931:1378,test,tests,1378,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848796931,1,['test'],['tests']
Testability,"1. Yes it it reproducible. It happens every time. . 2. No, I have not examined the output files, but I have run the same app on my local machine with ; ```; spark-submit --master local[2] gatk-package-4.alpha.2-252-gf627ed4-SNAPSHOT-spark.jar PrintReadsSpark -I chr1.bam -O output.bam; ```; And the output.bam has nearly the same size in bytes as the output from the yarn cluster, so it doesn't appear to be truncated. The output .bai files have identical sizes. . Here is the full stderr log.; ```; Log Type: stderr; Log Upload Time: Fri May 05 17:04:00 +0000 2017; Log Length: 18471; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/mnt/yarn/usercache/hadoop/filecache/37/__spark_libs__6987413740287883326.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 17/05/05 17:03:30 INFO SignalUtils: Registered signal handler for TERM; 17/05/05 17:03:30 INFO SignalUtils: Registered signal handler for HUP; 17/05/05 17:03:30 INFO SignalUtils: Registered signal handler for INT; 17/05/05 17:03:30 INFO ApplicationMaster: Preparing Local resources; 17/05/05 17:03:32 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1493961816416_0010_000002; 17/05/05 17:03:32 INFO SecurityManager: Changing view acls to: yarn,hadoop; 17/05/05 17:03:32 INFO SecurityManager: Changing modify acls to: yarn,hadoop; 17/05/05 17:03:32 INFO SecurityManager: Changing view acls groups to: ; 17/05/05 17:03:32 INFO SecurityManager: Changing modify acls groups to: ; 17/05/05 17:03:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, hadoop); groups with view permissions: Set(); users with modify permissions: Set(yar",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046:489,log,log,489,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046,1,['log'],['log']
Testability,"17 -0500. assertions in inference task base; removed a ASCII > 128 character in log messages. commit 526cf92e623a3bbd5f9d375132b6ca046fc47620; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:03:04 2017 -0500. redirect tqdm progress bar to python logger. commit 2e45bd30968b921fae225de3901fb97ece690b0c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:45:49 2017 -0500. more arg related fixes. commit bb89a3bb338d88199881e8aca65f656f2acd7c0a; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:41:20 2017 -0500. arg related bugfixes in WDL, python, and java CLIs. commit 23569787ee2c8cc6c9227a44170cbbd02fe4427f; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 17:21:05 2017 -0500. fixed issue with python boolean argparse (they use weird semantics). commit ae841c9ed4cd9b2ca1ac0e9082d175ff8ea98298; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 16:44:02 2017 -0500. shorter gCNV WDL tests. commit 5466b806e36df16cad2d045be074e7f9afec0957; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 16:38:15 2017 -0500. fixed arg issues in somatic WDL; exposed all missing args to java side; major update to germline WDLs; all optional python args exposed to WDLs as optional args. commit 50cb6fd08de15469a9080cbb27ff30c8b7ee7e21; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:50:45 2017 -0500. missing serialVersionUID. commit 5f0f31eab63b0e6f6105708ded7f86c96c830781; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:35:33 2017 -0500. annotated intervals kebab case; updated germline WDL workflows. commit 29cc6234dbfb8db12559217a650c6ceb170c5797; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:15:28 2017 -0500. cleanup test files. commit 08a35bb4e65eceb735adcd41a91132e9a34d2b66; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 02:50:19 2017 -0500. update WDL",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:5735,test,tests,5735,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['test'],['tests']
Testability,"1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-18.75%)` | `6% <0%> (ø)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `10.169% <0%> (-13.559%)` | `1% <0%> (-1%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `77.6% <0%> (-9.6%)` | `28% <0%> (-8%)` | |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/2467?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2467?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2467?src=pr&el=footer). Last update [e1e71d7...71c03a3](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=footer&el=lastupda",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2467#issuecomment-287565894:4278,test,test,4278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2467#issuecomment-287565894,1,['test'],['test']
Testability,"2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8/./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true --input ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --reference ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --run_without_dbsnp_potentially_ruining_quality false --solid_recal_mode SET_Q_ZERO --solid_nocall_strategy THROW_EXCEPTION --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_default_quality -1 --insertions_default_quality 45 --deletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --interval_set_rule UNION --interval_padding 0 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_less_than 6 --useOriginalQualities false --defaultBaseQualities -1 --runner LOCAL --client_secret client-secrets.json --help false --version false --VERBOSITY INFO --QUIET false; [June 1, 2015 5:51:02 PM EDT] Executing as pgrosu@eofe5 on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:version-unknown-SNAPSHOT JdkDeflater; 17:51:02.969 [main] INFO org.broadinstitut",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:3497,test,test,3497,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['test'],['test']
Testability,3RvckxvZ2xlc3NQYWlySE1NLmphdmE=) | `83.562% <62.5%> (-3.535%)` | `10 <0> (+1)` | |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-80.612%)` | `0% <0%> (-19%)` | |; | [...institute/hellbender/utils/gcs/GATKGCSOptions.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvR0FUS0dDU09wdGlvbnMuamF2YQ==) | `0% <0%> (-66.667%)` | `0% <0%> (ø)` | |; | [...lbender/engine/datasources/ReferenceAPISource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <0%> (-62.264%)` | `8% <0%> (-26%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-25%)` | `6% <0%> (-1%)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcv,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2574#issuecomment-292193941:2421,test,test,2421,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2574#issuecomment-292193941,1,['test'],['test']
Testability,"4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like it’s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so it’s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the task data - or use a Spark accumulator), and then seeing if there's some way to make it more uniform.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:3162,log,logging,3162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884,2,['log'],['logging']
Testability,4e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-18.75%)` | `6% <0%> (ø)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `10.169% <0%> (-13.559%)` | `1% <0%> (-1%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `77.6% <0%> (-9.6%)` | `28% <0%> (-8%)` | |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `70.946% <0%> (-6.757%)` | `18% <0%> (-4%)` | |; | ... and [4 more](https://codecov.io/gh/broadinstitute/gatk/pull/2455?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2455?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/do,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285859315:3918,test,test,3918,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285859315,1,['test'],['test']
Testability,"4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2UuamF2YQ==) | `77.477% <ø> (+0.901%)` | `38% <ø> (+2%)` | :white_check_mark: |; | [...tools/walkers/genotyper/AlleleSubsettingUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9BbGxlbGVTdWJzZXR0aW5nVXRpbHMuamF2YQ==) | `87.037% <ø> (+0.926%)` | `40% <ø> (+1%)` | :white_check_mark: |; | [...ark/pipelines/metrics/MeanQualityByCycleSpark.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvbWV0cmljcy9NZWFuUXVhbGl0eUJ5Q3ljbGVTcGFyay5qYXZh) | `91.667% <ø> (+1.042%)` | `10% <ø> (ø)` | :x: |; | [...ute/hellbender/utils/test/IntegrationTestSpec.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0ludGVncmF0aW9uVGVzdFNwZWMuamF2YQ==) | `74.194% <ø> (+1.075%)` | `26% <ø> (+1%)` | :white_check_mark: |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/2404/changes?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2404?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2404?src=pr&el=footer). Last update [30365e7...09a6f24](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2404#issuecomment-279082842:4362,test,test,4362,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404#issuecomment-279082842,1,['test'],['test']
Testability,"64e; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 08:04:19 2017 -0500. mkl. commit 43e2a65201286161fcd5bfe7dbb21ae888e19dac; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 06:56:20 2017 -0500. added cpu argument for germline tasks. commit 4433a62c2173c7f29d0f264c084bbaf2f6738782; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 02:45:38 2017 -0500. revert travis yml forks; verbose logging germline wdl. commit ae05801e33c37b3bf2685fba202032a270804873; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 00:55:14 2017 -0500. updated somatic PoNs for PreprocessIntervals drop Ns. commit cff64984d9fb42364001bda4c73d54cf68d85a5c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 00:37:24 2017 -0500. sudo travis yml. commit 89025941febd2089d426cfa1e0f0aa6a6712e2a9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 00:23:22 2017 -0500. travis/Docker config update (g++-6, Miniconda3); python test group assignment. commit 31f96398106c2b8577b8c25d110abea3ebe7f836; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:44:53 2017 -0500. WDL test bugfix. commit 9b2fb820536ec355bea0256471bd093d547f5c99; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:20:36 2017 -0500. update WDL test JSON files. commit e3d97644d1a2c40a5c364a96f8b67246154179c9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:18:14 2017 -0500. assertions in inference task base; removed a ASCII > 128 character in log messages. commit 526cf92e623a3bbd5f9d375132b6ca046fc47620; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:03:04 2017 -0500. redirect tqdm progress bar to python logger. commit 2e45bd30968b921fae225de3901fb97ece690b0c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:45:49 2017 -0500. more arg related fixes. commit bb89a3bb338d88199881e8aca65f656f2acd7c0a; Author: ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:4248,test,test,4248,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['test'],['test']
Testability,"72; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 11:13:58 2017 -0500. editable, full path. commit d998f2d5c2b33dd41e291be9bfeaea72fe479b8a; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:56:24 2017 -0500. revert Dockerfile, change yml. commit 930d7486b7d2cf918fcb16dd03394bb9c9f0611b; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:34:46 2017 -0500. more Dockerfile. commit 94112131526b514ef254bcc2c50a239dbae35aa1; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:25:13 2017 -0500. more Dockerfile. commit 7d2646240a65f5c0f09f5f25f3e19e9d9bf004d9; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:06:11 2017 -0500. more Dockerfile. commit f1235c25aeba85570b5ce389a34975f1b7b5ec3c; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 09:39:46 2017 -0500. Dockerfile edit. commit 3df84dd4693f28e4e8b34fd33f877e99749dffce; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 16:08:06 2017 -0500. Update test PoNs. commit 2c3b20e62a1cba7af24c0b0846eb1629422f51e6; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 15:49:38 2017 -0500. Update test files. commit c65c6e9144ef396792364ab2e06b7b436bb97684; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 15:30:59 2017 -0500. Adding no-GC/do-GC WDL tests. commit 56451843066a456d9cf8e6eac55ae4df2c518ec3; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 12:51:17 2017 -0500. Updates to handle SAM header changes from sl_wgs_acnv_headers and updates to mb_gcnv_python_kernel. commit d02d04df684a2820308a1d1c2bfda4b7d1c5f05e; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Mon Nov 13 12:52:33 2017 -0500. Added CLIs and WDL for python gCNV pipeline. commit 66ed74b68375d43514ef84658e7a6c771ed9053c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Nov 15 01:50:03 2017 -0500. Polished code, ready for review; ; gCNV computational kernel (initial release); ; renaming gammas_s to psi_s to uniformity (sampl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:9520,test,test,9520,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['test'],['test']
Testability,749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZmlsdGVycy9NYXBwaW5nUXVhbGl0eVJlYWRGaWx0ZXIuamF2YQ==) | `100% <100%> (ø)` | `5 <5> (+2)` | :white_check_mark: |; | [...institute/hellbender/utils/gcs/GATKGCSOptions.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvR0FUS0dDU09wdGlvbnMuamF2YQ==) | `0% <0%> (-66.667%)` | `0% <0%> (ø)` | |; | [...lbender/engine/datasources/ReferenceAPISource.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <0%> (-62.264%)` | `8% <0%> (-26%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `43.75% <0%> (-31.944%)` | `27% <0%> (-9%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGlu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2433#issuecomment-283613034:2083,test,test,2083,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2433#issuecomment-283613034,1,['test'],['test']
Testability,"79b8a; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:56:24 2017 -0500. revert Dockerfile, change yml. commit 930d7486b7d2cf918fcb16dd03394bb9c9f0611b; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:34:46 2017 -0500. more Dockerfile. commit 94112131526b514ef254bcc2c50a239dbae35aa1; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:25:13 2017 -0500. more Dockerfile. commit 7d2646240a65f5c0f09f5f25f3e19e9d9bf004d9; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:06:11 2017 -0500. more Dockerfile. commit f1235c25aeba85570b5ce389a34975f1b7b5ec3c; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 09:39:46 2017 -0500. Dockerfile edit. commit 3df84dd4693f28e4e8b34fd33f877e99749dffce; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 16:08:06 2017 -0500. Update test PoNs. commit 2c3b20e62a1cba7af24c0b0846eb1629422f51e6; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 15:49:38 2017 -0500. Update test files. commit c65c6e9144ef396792364ab2e06b7b436bb97684; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 15:30:59 2017 -0500. Adding no-GC/do-GC WDL tests. commit 56451843066a456d9cf8e6eac55ae4df2c518ec3; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 12:51:17 2017 -0500. Updates to handle SAM header changes from sl_wgs_acnv_headers and updates to mb_gcnv_python_kernel. commit d02d04df684a2820308a1d1c2bfda4b7d1c5f05e; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Mon Nov 13 12:52:33 2017 -0500. Added CLIs and WDL for python gCNV pipeline. commit 66ed74b68375d43514ef84658e7a6c771ed9053c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Nov 15 01:50:03 2017 -0500. Polished code, ready for review; ; gCNV computational kernel (initial release); ; renaming gammas_s to psi_s to uniformity (sample-specific unexplained variance); ; renamed determine_ploidy_and_depth.py to cohort_determine_ploidy_and_depth.py; finite-temperature forward-backward",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:9669,test,test,9669,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['test'],['test']
Testability,917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0NvbW1hbmRMaW5lUHJvZ3JhbS5qYXZh) | `93.258% <100%> (ø)` | `29 <1> (ø)` | :x: |; | [...institute/hellbender/utils/gcs/GATKGCSOptions.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvR0FUS0dDU09wdGlvbnMuamF2YQ==) | `0% <0%> (-66.667%)` | `0% <0%> (ø)` | |; | [...lbender/engine/datasources/ReferenceAPISource.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <0%> (-62.264%)` | `8% <0%> (-26%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `43.75% <0%> (-29.861%)` | `27% <0%> (-9%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGlu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285859315:2048,test,test,2048,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285859315,1,['test'],['test']
Testability,92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZVdhbGtlci5qYXZh) | `89.655% <100%> (+6.897%)` | `9 <2> (+1)` | :arrow_up: |; | [...institute/hellbender/utils/gcs/GATKGCSOptions.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvR0FUS0dDU09wdGlvbnMuamF2YQ==) | `0% <0%> (-66.667%)` | `0% <0%> (ø)` | |; | [...lbender/engine/datasources/ReferenceAPISource.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <0%> (-62.264%)` | `8% <0%> (-26%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `43.75% <0%> (-29.861%)` | `27% <0%> (-9%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGlu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2467#issuecomment-287565894:2408,test,test,2408,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2467#issuecomment-287565894,1,['test'],['test']
Testability,9f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvQXNzZW1ibHlSZWdpb25XYWxrZXJTcGFyay5qYXZh) | `0% <ø> (-90.566%)` | `0% <ø> (ø)` | |; | [...institute/hellbender/utils/gcs/GATKGCSOptions.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvR0FUS0dDU09wdGlvbnMuamF2YQ==) | `0% <ø> (-66.667%)` | `0% <ø> (ø)` | |; | [...lbender/engine/datasources/ReferenceAPISource.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <ø> (-62.264%)` | `8% <ø> (+8%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <ø> (-60.87%)` | `2% <ø> (+2%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `44.444% <ø> (-29.861%)` | `28% <ø> (+28%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2385#issuecomment-279409892:3578,test,test,3578,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2385#issuecomment-279409892,1,['test'],['test']
Testability,": Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 10:20:30 2017 -0500. the proper python environment yml for mkl and open -- leads to orders of magnitude speedup!. commit fea6bf874e0b62262a3b1d239ce4d76792d5c416; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 09:31:43 2017 -0500. revert. commit 456c53f88d01b603f4175d8896a0dac036af03f8; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 08:17:22 2017 -0500. enabled openmp g++ linking in theano. commit e2afef14ddb957f2dbdea76fd783d3bfb8d7a64e; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 08:04:19 2017 -0500. mkl. commit 43e2a65201286161fcd5bfe7dbb21ae888e19dac; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 06:56:20 2017 -0500. added cpu argument for germline tasks. commit 4433a62c2173c7f29d0f264c084bbaf2f6738782; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 02:45:38 2017 -0500. revert travis yml forks; verbose logging germline wdl. commit ae05801e33c37b3bf2685fba202032a270804873; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 00:55:14 2017 -0500. updated somatic PoNs for PreprocessIntervals drop Ns. commit cff64984d9fb42364001bda4c73d54cf68d85a5c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 00:37:24 2017 -0500. sudo travis yml. commit 89025941febd2089d426cfa1e0f0aa6a6712e2a9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 00:23:22 2017 -0500. travis/Docker config update (g++-6, Miniconda3); python test group assignment. commit 31f96398106c2b8577b8c25d110abea3ebe7f836; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:44:53 2017 -0500. WDL test bugfix. commit 9b2fb820536ec355bea0256471bd093d547f5c99; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:20:36 2017 -0500. update WDL test JSON files. commit e3d97644d1a2c40a5c364a96f8b67246154179c9; Author: Mehrtash Babadi <mehrtash",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:3686,log,logging,3686,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['log'],['logging']
Testability,======; + Hits 30174 30401 +227 ; - Misses 6767 6830 +63 ; - Partials 2619 2629 +10; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2488?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `68.217% <ø> (-5.394%)` | `33 <0> (-3)` | |; | [...nder/tools/walkers/genotyper/GenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `45.267% <0%> (-1.635%)` | `36% <0%> (+4%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `86.4% <0%> (-0.8%)` | `35% <0%> (-1%)` | |; | [...notyper/GenotypeCalculationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwZUNhbGN1bGF0aW9uQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `100% <0%> (ø)` | `2% <0%> (ø)` | :arrow_down: |; | [...ute/hellbender/cmdline/StrictBooleanConverter.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbG,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2488#issuecomment-287907716:1703,test,test,1703,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2488#issuecomment-287907716,1,['test'],['test']
Testability,"> @Ben-Habermeyer We had a few PRs in late 2021 that may have fixed this. If it's still occurring in the latest GATK version I would like to take a look at it. ok @davidbenjamin I got a chance to test with latest release `4.3.0.0` and the issue seems to be mostly resolved when running `--alleles` on our test samples. Additionally, `FilterMutectCalls` works on low DP variants. . For control samples, using the `--alleles` option results in an error due to the value of the stats `callable`. . Combination of this call:; ```; chr18 77560878 . AA TT . . AS_SB_TABLE=0,0|0,0;DP=1;ECNT=2;MBQ=0,90;MFRL=0,100;MMQ=60,60;MPOS=29;POPAF=7.30;TLOD=4.20 GT:AD:AF:DP:F1R2:F2R1:FAD:PGT:PID:PS:SB 0|1:0,1:0.667:1:0,1:0,0:0,1:0|1:77560878_AA_TT:77560878:0,0,0,1; ```; and the stats file containing:; ```; callable 1.0; ```; results in FilterMutectCalls exception; ```; java.lang.IllegalArgumentException: logValues must be non-infinite and non-NAN; at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeFromLogToLinearSpace(NaturalLogUtils.java:27); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.posteriorProbabilityOfError(Mutect2FilteringEngine.java:93); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:140); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSomaticVariant(SomaticClusteringModel.java:146); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.performEMIteration(SomaticClusteringModel.java:345); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:330); at org.broadinstitute.hellbe",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7276#issuecomment-1293969047:196,test,test,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7276#issuecomment-1293969047,3,"['log', 'test']","['logValues', 'test']"
Testability,"> @ilyasoifer do you have the means to quickly run minimap2? I would recommend simply realigning src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam with minimap2 and making a quick ""are tests consistent with pervious versions"" test with a checked in vcf output. I don't know how to wrangle minimap2 to handle mates correctly however so i don't know if this is easy. Added test like this",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8337#issuecomment-1560517319:101,test,test,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8337#issuecomment-1560517319,4,['test'],"['test', 'tests']"
Testability,"> At the very least we should add a unit test that generates the evidenceIndexBySampleIndex cache, then calls marginalize() (both types) and asserts that we have emptied the cache. I would do the same for appendEvidence() and addMissingAlleles(). It's simpler than this because allele operations such as `marginalize()` and `addMissingAlleles` don't modify the evidence list. While they require care with the likelihoods arrays they don't require anything at all from the evidence-to-index caches. As I mentioned above, I left the cache updating in `appendEvidence` as it was because it was so simple. I will try to write the test for removing evidence tomorrow. Tempting to try tonight, but I'm trying to accept the reality that working until 2 am is a bad idea.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6593#issuecomment-633180869:41,test,test,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6593#issuecomment-633180869,3,"['assert', 'test']","['asserts', 'test']"
Testability,"> Hi Kevin, our team would like to get this merged into `ah_var_store` soon per VS-1254. I'm aware of only a handful of outstanding issues:; > ; > * The failing PGEN tests. I'm happy to help here in any way I can though right now I don't have a sense of what could be causing this beyond the platform differences you suggested.; > ; > * The `10` vs `10.0` change we discussed recently to avoid division by zero.; > ; > * We'll want to merge / rebase from `ah_var_store` and then build a new GATK Docker image which would be entered into `GetToolVersions` in `GvsUtils.wdl`. I'm happy to take on building this image once the merge / rebase is ready. Hi Miguel, sorry about the delay. I'm working on the failing tests issue this afternoon. I know what the issue is, so I just have to implement a fix, which I think should be fairly simple. Once I have that ready and have confirmed the tests aren't failing anymore, I'll do the rebase and then let you know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8708#issuecomment-2004517723:166,test,tests,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708#issuecomment-2004517723,3,['test'],['tests']
Testability,"> Perhaps we want that the author certifies it is in a usable state for its intended use?. It's good that the author certifies it, but we are going to be tearing the tools apart and rebuilding them without necessarily understanding the details of the output. We need to have tests that will tell us if we've broken them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/382#issuecomment-93804346:275,test,tests,275,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/382#issuecomment-93804346,1,['test'],['tests']
Testability,"> The four test failures in `ExtractCohortToPgenTest` appear to be real:; > ; > ```; > 2024-03-12T20:53:02.3169070Z Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortToPgenTest > testFinalVQSRLitePgenfromRangesAvro �[31mFAILED�[39m�[0K; > 2024-03-12T20:53:02.3171276Z java.lang.AssertionError: expected [-1] but found [13570]�[0K; > 2024-03-12T20:53:02.3172934Z at org.testng.Assert.fail(Assert.java:97); > 2024-03-12T20:53:02.3173927Z at org.testng.Assert.assertEqualsImpl(Assert.java:136); > 2024-03-12T20:53:02.3174922Z at org.testng.Assert.assertEquals(Assert.java:118); > 2024-03-12T20:53:02.3175853Z at org.testng.Assert.assertEquals(Assert.java:729); > 2024-03-12T20:53:02.3176775Z at org.testng.Assert.assertEquals(Assert.java:739); > 2024-03-12T20:53:02.3178703Z at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortToPgenTest.testFinalVQSRLitePgenfromRangesAvro(ExtractCohortToPgenTest.java:78); > ```. That's weird, because it looks like it's succeeding in the other test tasks where it's running. The place it's failing is an extremely simple equality assertion of two compressed files, though. I wonder if there's something about operating system differences that can change the compression slightly. I'll see if I can find a better way to do that check",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8708#issuecomment-2000109415:11,test,test,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708#issuecomment-2000109415,15,"['assert', 'test']","['assertEquals', 'assertEqualsImpl', 'assertion', 'test', 'testFinalVQSRLitePgenfromRangesAvro', 'testng']"
Testability,"> This Python scripts could use some docs (and perhaps tests?); it's not clear to me when / how one would use them. yes, that's fair. I modified the doc I created (CREATING_WEIGHTED_BED_FILES_ORIGINAL.md). In truth, ""original"" may be a misnomer. It's the information in the github issue ALONG WITH updated instructions for when to use the new python files. Given the feedback, I further updated the document with a comment at the top saying when they should be run (TL;DR, on a new genome or a new reference... so basically extremely rarely). Given this, I don't think unit test or extensive documentation are needed for them. The files are small, the inputs are few, and the code is commented. We are likely to not need to run them for again for years, so I don't consider extra documentation or testing to be worth the effort at the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8507#issuecomment-1717680203:55,test,tests,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8507#issuecomment-1717680203,3,['test'],"['test', 'testing', 'tests']"
Testability,">> interval_list. > this will need to be a parameter (but can be optional and have a default) in order to have our integration tests run GvsJointVariantCalling.wdl on exomes. Genomes too, the integration test specifies a 20/X/Y interval list. >> filter_set_name; >> extract_table_prefix. > these two can just default to the call_set_identifier with weird characters parsed out. Yeah I think that would work for the integration test(s), each variation goes into a different BQ dataset anyway. @RoriCremer can correct me if I'm wrong, but I thought the raison d'être of the beta WDL was specifically to hardcode away as many parameters as possible (even optional ones with defaults) to present a simplified interface for non-expert users. I agree we'll probably have to allow some additional parameters for testability (`gatk_override` at a minimum), but do we really want to add all of these?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8404#issuecomment-1634248008:127,test,tests,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8404#issuecomment-1634248008,4,['test'],"['test', 'testability', 'tests']"
Testability,"@EdwardDixon I did not know that! In that case master does already require AVX. If it only impacts this tool and we provide sufficient warning and instructions, I think the single intel-optimized conda environment will be so much easier to test and maintain. Users who don't have AVX can simply install an older tensorflow in their environment, but GATK doesn't need to worry about it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429142837:240,test,test,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429142837,1,['test'],['test']
Testability,"@SHuang-Broad . Here are the tail end of the PrintReadsSpark jobs (tested on bam, cram and sam filetypes) . Writing to a bam failed while writing bai index... (others worked while writing to a cram and sam); ```; 2019-06-03 22:34:34 INFO IndexFileMerger:100 - Done merging .sbi files; 2019-06-03 22:34:34 INFO IndexFileMerger:69 - Merging .bai files in temp directory hdfs:///project/casa/gcad/adsp.cc/sv/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.bam.parts/ to hdfs:///project/casa/gcad/adsp.cc/sv/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.bam.bai; 2019-06-03 22:34:48 INFO AbstractConnector:318 - Stopped Spark@6be766d1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-06-03 22:34:48 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4040; 2019-06-03 22:34:48 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-06-03 22:34:48 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-06-03 22:34:48 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-06-03 22:34:48 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-06-03 22:34:48 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-06-03 22:34:48 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-06-03 22:34:49 INFO MemoryStore:54 - MemoryStore cleared; 2019-06-03 22:34:49 INFO BlockManager:54 - BlockManager stopped; 2019-06-03 22:34:49 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-06-03 22:34:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-06-03 22:34:49 INFO SparkContext:54 - Successfully stopped SparkContext; 22:34:49.027 INFO PrintReadsSpark - Shutting down engine; [June 3, 2019 10:34:49 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 3.72 minutes.; Runtime.totalMemory()=3829923840; htsjdk.samt",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-498502370:67,test,tested,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-498502370,1,['test'],['tested']
Testability,"@SHuang-Broad I see. So the conversion to SAM and back when we write the file actually changes the results (or at least their annotations). It makes me a little nervous that in one version of the pipeline the records go through `BwaMemAlignmentUtils.applyAlignment` and in the other they don't, since that method has some complex logic. Right now we have two possible paths:. `AlignedAssemblyOrExcuse -> SAMRecord -> writeToFile -> GATKRead -> AlignmentRegion`. or . `AlignedAssemblyOrExcuse -> AlignmentRegion`. What if we always converted to `SAMRecord`? It's a little more expensive but it would cut down on alternate code paths and conversion code, and IMO would make the code a lot simpler to read if I didn't have to think about which code path I was in. I'm also worried that the different conversions could lead to bugs that will be hard to debug since you have to know the code path that generated them. @tedsharpe what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294168022:330,log,logic,330,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294168022,1,['log'],['logic']
Testability,"@SebastianHollizeck I believe the bug is not in `FilterMutectCalls` but upstream in `LearnReadOrientationModel` in the edge case of 3-base contexts that have no data in some of the samples. It's strange because we have an integration test for this already, and I would appreciate getting your input files to `LearnReadOrientationModel` for debugging. I think the following quick fix will work: untar your artifact priors, delete all but sample b, and re-tar, then run `FilterMutectCalls` as before. Is there a reason why all samples except b have very little data, and have no data at all for most 3-base contexts? To be clear, we want to fix the bug even if the data are weird, but I want to double-check that this is expected.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-597735514:234,test,test,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-597735514,1,['test'],['test']
Testability,"@Vzzarr Don't worry, these things are complicated and take time to learn! You can use whatever you want, I just know more gradle than maven so it's easier for me to help with that. . I'm having trouble reproducing the error you're having. Is your project on github? Or at a minimum could you paste your pom file here? . In the meantime, could you try cloning https://github.com/lbergelson/gatk-downstream-test and seeing if `mvn compile` completes successfully?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-339761069:405,test,test,405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-339761069,1,['test'],['test']
Testability,"@akiezun @droazen @lbergelson Found the issue with FTZ being cleared. The FTZ setting only applies to the thread where FTZ is set. When running tests in gradle/testng, each test is run in a new thread. However, the pairHMM native library is only loaded for the first HaplotypeCaller test, since code in `VectorLoglessPairHMM.java` prevents the library from being loaded more than once in the same JVM. This means only the first test uses FTZ. A code change that loads the pairHMM native library for each test resolves the issue, and all `HaplotypeCallerIntegrationTest` tests pass. Another option to explore is setting FTZ in `main`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701:144,test,tests,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701,7,['test'],"['test', 'testng', 'tests']"
Testability,"@akiezun I have switched a lot so far; currently I'm stuck on `GenotypeLikelihoodCalculator`, which relies on `GenotypeLikelihoods` in htsjdk, which is log10. I could write a simple wrapper class to present a natural log interface. Is there a better solution?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/917#issuecomment-149321469:217,log,log,217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/917#issuecomment-149321469,1,['log'],['log']
Testability,"@asmirnov239 I've borrowed the CopyNumberTestUtils class from #7889 into which you moved the method for detecting deltas in the doubles. I'm going to merge this PR once tests pass, so just be aware of this when rebasing your branch if you make any further changes. We might consider adding a simple test of the test method itself. I'll let you do it in your branch, or we can file an issue and tackle it after everything is merged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1165717972:169,test,tests,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1165717972,3,['test'],"['test', 'tests']"
Testability,"@bbimber @mlathara Here is a pretty good article for optimizing the GenomicsDBImport [https://gatk.broadinstitute.org/hc/en-us/articles/360056138571-GDBI-usage-and-performance-guidelines] There is some advice about handling many small contigs that may be useful. . To troubleshoot the GenomicsDBImport high memory issue my script have, I reran the script on chr1 to narrow down the source of the high memory issue. These are running on reblocked gvcfs. . 1. Without --bypass-feature-reader and -consolidate; 2. With --bypass-feature-reader; 3. With --consolidate without --bypass-feature-reader (This ended up on a node with 384gb.) The other ran on 256GB nodes. . Test 2 ran the fastest with the lowest memory requirements (Wall clock 76 hours); Test 1 ran slower and required more memory 40-50% of 256GB (Wall Clock 94 hours); Test 3 ran initially faster with less memory than test 1 but by batch 65 it was using 75% of 384 GB. This job has not finished and appears stuck on importing batch 65. So the consolidate option appears to have a memory leak or using just requiring too much memory. The -consolidate option was the culprit. So rerunning chr1-3 with just the --bypass-feature-reader option (test2) ran fine without lots of memory being used. Below is the time output from chr1. The output shows the Maximum resident set size (kbytes): **2630440**. Using GATK jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar defined in environment variable GATK_LOCAL_JAR; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx200g -Xms16g -jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reade",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687:879,test,test,879,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687,1,['test'],['test']
Testability,"@bbimber Good questions all. Whenever we do these ports we have to strike a balance between minimizing the changes and updating to GATK4 standards. Generally, though, we want the code to conform to GATK4. This PR is large and likely to require some iteration so I'd be ok with starting with just the minimal ""porting"" changes to keep things simple, and then doing a code hygiene pass at the end. The ""porting"" changes should include things like updated javadoc, GATK4-style command line arguments, updating of outdated GATK3 terminology such as ""ROD"", Utils.nonNull assertions, etc. The finals and curly braces can wait for a separate pass (we will want to do those in this PR though). If you're not sure what to include or not just ask. I like the idea of keeping the GATK3 tests working as we go along. We should make a clear distinction between the old and new tests though. Ideally the GATK3 tests would be in a separate commit that we can just delete at the end, but that can get unwieldy if the files in the commit need to change as we go along. Alternatively you could isolate them into a separate directory. They should either be disabled or made dependent on a test method (see the `@Test` annotation properties `enabled` and `dependsOn`) that is easily toggled so they can be run locally, but don't run on the CI server. Otherwise the CI server build will always fail. In general, its really helpful to have the first commit in the PR contain the completely unmodified GATK3 source files. It makes it much easier for the reviewer to see what changed for the port. I noticed that you have 2 new plugins included in this. I'm not sure if that was suggested by someone on the GATK team (I'm wondering if we want to go down that path...) but I can tell you that the existing plugins required an enormous amount of test development and review iteration. If we do decide to make them plugins, I think it would be a good idea to do so in a separate PR. Also, if we choose to make an AbstractPlugin ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352:566,assert,assertions,566,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352,4,"['assert', 'test']","['assertions', 'tests']"
Testability,"@bbimber Unfortunately it's not so simple -- some of the GATK3 test data cannot be shared externally at all due to, eg., IRB restrictions. Someone will have to look at the test data in question to make sure that it can be shared. We'll update you once we've done this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358032795:63,test,test,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358032795,2,['test'],['test']
Testability,"@bbimber We believe that this should be fixed by https://github.com/broadinstitute/gatk/pull/7670, which will go out in the next GATK release. If you're able to test with that patch and give feedback on whether it resolves the error for you, that would be helpful!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7687#issuecomment-1048160591:161,test,test,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7687#issuecomment-1048160591,1,['test'],['test']
Testability,"@bhanugandham @fleharty this issue touches upon our discussion of https://gatkforums.broadinstitute.org/gatk/discussion/24335/loh-detection-using-gatk4s-somatic-cnv-workflow. We might consider just a simple modification of the genotyping step (e.g., keeping all ROHs longer than a hard threshold) to start, which would probably cover the most common use cases with minimal effort. Can use 100% HCC1143 in tumor-only mode as an initial test, but it would be good to collect other examples.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3915#issuecomment-531833700:435,test,test,435,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3915#issuecomment-531833700,1,['test'],['test']
Testability,"@chandrans Hmn, I don't really know what's happening. We wouldn't expect gatk4 haplotype caller to be that much slower. . It looks like they're running beta2 which is kind of old as well. Can you ask them what exact version they're using?. Can you ask if they have the log (stdout + stderr) for the gatk4 non-spark run? I can't tell what pairhmm they're actually running with and the logs would help with that. . Can you also find out what sort of hardware they're running on? Specifically, is it an intel machine with support for AVX?. A good setting for` --nativePairHmmThreads` is probably 4-8, you won't see any improvement after that. I also noticed that they're setting -XX:+UseParallelGC -XX:ParallelGCThreads=32 for the gatk3. They would be better off setting it to 2-4 threads. Performance gets worse beyond that typically from what I've seen. They can set the same thing for gatk4 using`--javaOptions ' -XX:+UseParallelGC -XX:ParallelGCThreads=4'`. Their spark configuration looks wrong in a number of ways which is probably a big part of why they're not seeing any improvement. In general you want executors with ~4-8 cores and at least 4g of memory per core. I don't know how much memory their nodes have, and I don't know if they're running with autoscaling turned on, but I suspect they're only allocating 1 executor on 1 node and then it's thrashing memory because it's trying to run 32 threads at once. Spark tuning for haplotype caller is going to be complicated though and I don't know how to do it will yet, we will be revisiting it in the next quarter probably. They're also running withs spark 2.1.0, we currently require spark 2.0.2 which is an unfortunately specific version, we're planning on upgrading to spark 2.2.+ in the next quarter. . You should make it clear to them that the results will not be the same between 3, 4, and 4-spark yet and that 4 is in rapid state of flux and has known performance issues that we're planning on working soon. Even so though, that slowdow",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332879964:269,log,log,269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332879964,2,['log'],"['log', 'logs']"
Testability,"@cmnbroad - yes, missed one expected file after subsetting the data. tests passed. is seeing the status something i could have seen by myself? in travis i only see the projects I own, with no clear way to look up this unless i'm linked from the PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-432261666:69,test,tests,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-432261666,1,['test'],['tests']
Testability,"@cmnbroad : first - would it be possible to kick off travis tests? i refactored this and dont seem to be able to do that. Second, yes, I was trying to reorder and condense the commits but clearly didnt work. I think the problem was trying to put your GATK3 commit first (which would seem to make sense). in any case, I just recreated this, putting a pristine GATK3 first, following a consolidated set of my commits with 1) the limited core changes, 2) the meat of the VariantEval port, and 3) A separate commit with a port of GATK3 VariantEvalIntegrationTest which is useful for validation but should not be merged. To your points:. 1) I substantially cut down the incoming large files, mostly by limiting the intervals of new large VCFs. 2) On the plugin: this was discussed above, and I initially also pointed out this should ultimately go into Barclay. You are actually the one who proposed staging it in GATK. I am not entirely sure I understand the reticence on plugins; however, my goal is to get VariantEval ported by touching as little of it as possible. This is already sucking up a ton of time. I flipped VariantEvalUtils to gather a list of classes from the appropriate package instead of a full-on plugin. That should satisfy that concern?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431913735:60,test,tests,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431913735,1,['test'],['tests']
Testability,"@cmnbroad @droazen Good morning - I'm writing to follow up again. I appreciate that this not GATK's main priority, but I'm at the point where I need to either unblock this or fork GATK and publish another artifact so we can move our projects forward. The crux of what I'm trying to achieve with these changes is for our VariantQC tool; however, I'm bumping into several situations where MultiVariantWalkers need to be able to determine the FeatureInput source of the variants. I recognize that this and my original PR #6973 has taken a non-trivial amount of @cmnbroad time, but we have basically been stalled with an otherwise approved PR since Feb 8. The PRs blocking that PR are this one and the related #7021. They both involve creating a way to connect VariantContext to FeatureInput - a capability that would benefit the GATK engine and I have been told by @cmnbroad you're interested in having. It seems like the primary problem associated with these changes is ensuring tests and VariantContext comparison code still works, since VariantContexts are going to tend to report a source. I dont know your internal conversations, so I'm guessing based on what's written in github. As I've said, I'd like to do whatever I can to get these changes into a form that takes as little of your effort as possible. . While this particular PR seems close, there is clearly some cleanup needed from what's there now, including code review from @lbergelson that no one ever fixed. Would it help if I put together #4571 and #7021 into a new branch where I also work through associated test changes and try to get this into one concise piece of code to review? Basically try to put everything together to be the minimal amount of work needed on your side? I havent heard anything one way or the other from GATK staff as to whether this would actually be helpful or not. Are there higher order design decisions that need to be considered that I'm not seeing? . I completely understand your time constraints - I'm ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4571#issuecomment-822691252:977,test,tests,977,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4571#issuecomment-822691252,1,['test'],['tests']
Testability,"@cmnbroad After thinking about this I went ahead and created VariantEvalEngine. Doing this in one PR will simplify some of the sticking points around what is a final change vs. what it expected to be fixed later. With this change, the goal is to strip most logic from VariantEval into the engine. This engine can be constructed with a VariantEvalArgumentCollection, and any kind of GATKTool as the owner. I tried to minimize the amount of context the VariantEvalEngine needed to hang on to. This means all the child classes have visibility on the VariantEvalEngine, but are no longer directly exposed to either the walker class or the argument collection. . All the logic around gathering the arguments to form DrivingVariants is moved to a static method in VariantEvalEngine. . I also rebased and fixed conflicts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-750428516:257,log,logic,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-750428516,2,['log'],['logic']
Testability,"@cmnbroad I refactored the training java wrapper into separate wrappers to write tensors (CNNVariantWriteTensors.java) and to train (CNNVariantTrain.java) I think this simplified the meaning/necessity of many of the arguments, which was unclear when all those tools were rolled together. . I'm working on a release-style integration test that chains all the tools together, like @droazen discussed a few meetings ago, but for this PR I think I will have to do something simpler. Because of some issues with the GSA5 environment and GPU, I still have to write in a Python2/3 agnostic way, which precludes the use of type hints. I would like to update, but I'm blocked by BITs in the short term.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4245#issuecomment-367449432:333,test,test,333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4245#issuecomment-367449432,1,['test'],['test']
Testability,"@cmnbroad I understand that I could have retained a bunch of single-use text files, but it seemed like the more permutations one adds, the less it makes sense to have a separate, very redundant, static text file to check each scenario. There's a ton of VariantContext-related tests that parse the output VCF to test some feature as opposed to checking in a bunch of VCF text files.... While I'll grant the 4th test case I added (where we pass chr 2) isnt especially compelling over just testing chr 1, one could argue more breadth is a good thing here. if you want clarity, pulling that VariantEval report parsing code into a method called extractUniqueContigsFromEvalReport(), or simply adding a comment line, supports this goal. Anyway, I'm checking in slightly clarified version of this now, simply to get tests running. If you respond to the above, maybe we go with that. In the interest of time, I'll stage and check in the version which restores the text files and goes that route.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7238#issuecomment-831459741:276,test,tests,276,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7238#issuecomment-831459741,5,['test'],"['test', 'testing', 'tests']"
Testability,"@cmnbroad I've implemented a compromise approach in `ReservoirDownsampler.consumeFinalizedItems()` that I think satisfies both of our concerns:. * If `consumeFinalizedItems()` is called after end of input has been signaled, it always clears state (including the end of input flag itself), regardless of whether there are any finalized items; * If `consumeFinalizedItems()` is called before end of input has been signaled, it returns an empty List and does not clear state, since in that case the downsampling process is still ongoing and we want to preserve pending items. I've also added tests to verify this new behavior. Let me know what you think.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5594#issuecomment-458679171:589,test,tests,589,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5594#issuecomment-458679171,1,['test'],['tests']
Testability,"@cmnbroad Sorry for the very long delay before reviewing this. I think this is a good solution. I think the test is a bit weird at the moment, but it's a weird thing to try and test. . 1: Could you make `setLoggingLevel` a static method in `Utils`, and give it a clear comment saying that it changes the global logging level (it does change the global logging level yes? Do I misunderstand what's happening? ) . 2: Could you then move the test to `UtilsUnitTest` run through all 4 levels of logging in the test, and then reset it to the initial state (if that's possible..) . Thanks for figuring this out and sorry for delay.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930:108,test,test,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930,7,"['log', 'test']","['logging', 'test']"
Testability,@cmnbroad Would you mind profiling `setHeaderStrict()` against `setHeader()` when you get a chance to determine whether the former is more costly than the latter? We've seen a spike in our test suite runtimes today and I want to rule this out as a potential cause. A simple loop that calls each method on each read in a bam + the unix time command should suffice.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1541#issuecomment-191873209:189,test,test,189,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1541#issuecomment-191873209,1,['test'],['test']
Testability,"@cmnbroad could the failing WDL test simply be due to some Spark configuration issue, rather than memory? Locally, for both 1) the WDL test within the Docker and 2) CreateReadCountPanelOfNormalsIntegrationTest using 17.0.3 without the Docker, I seem to hit the exception discussed here: https://stackoverflow.com/questions/72724816/running-unit-tests-with-spark-3-3-0-on-java-17-fails-with-illegalaccesserror-cl. Not sure why CreateReadCountPanelOfNormalsIntegrationTest seems to pass in the CI environments, but perhaps it'll be more obvious to you?. Just for context, note that this tool relies on the Spark MLlib implementation of PCA.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8035#issuecomment-1409180990:32,test,test,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8035#issuecomment-1409180990,3,['test'],"['test', 'tests-with-spark-']"
Testability,"@cmnbroad hopefully a simple update, it compiles fine but we'll see if tests pass....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3962#issuecomment-351552151:71,test,tests,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3962#issuecomment-351552151,1,['test'],['tests']
Testability,"@cmnbroad thanks for the additional info. Some more detail from my side in case others stumble upon the same problem... * My input file comes from gnomad (`gs://gnomad-public/release/2.0.2/vcf/genomes/gnomad.genomes.r2.0.2.sites.chr18.vcf.bgz`). I editied it only to turn chromosome ""18"" into ""chr18"". * bcftools handles the duplicate INFO correctly and it fixes it! In case someone find it useful this is the command I used to retain only the AF tag and discard missing values:. ```; bcftools annotate -O z -i 'INFO/AF > 0' -x ^INFO/AF gnomad.r2.0.2.biallelic.hg38.chr18.vcf.gz > gnomad.r2.0.2.simple.hg38.chr18.vcf.gz; ```. * Unrelated to this particular issue, `gatk GetPileupSummaries` (next command in my workflow) doesn't like tags with missing values, I get a NumberFormatException error (I think, I don't have the logs). Hence the option `INFO/AF > 0` in bcftools.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-365640704:822,log,logs,822,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-365640704,1,['log'],['logs']
Testability,"@cmnbroad, that's not wholly unreasonable, but i'd like to push back on a number of these points. . 1) First - would GATK consider simply letting us take over VariantEval and maintain as a GATK4-based tool in another repo? My understanding from GATK4 issues is that plan was to never migrate VariantEval (i think in favor of other picard/gatk QC tools). There is a bit of a conflict between keeping a lean core engine and having all these tools built off it. I would think there's an argument for keeping your core engine and the many tools built off it separated (GATK3 seemed to include some dead tools, for example). I appreciate we're the ones pushing this migration, but I hope on the other side you can appreciate the bar is pretty significant on our time. . 2) What new plugins are you talking about? VariantStratification and VariantEvaluator are part of GATK3's VariantEval? Yeah, I wrote a base PluginDescriptor class patterned on how ReadFilters work. It probably should exist in a more core position in code. While there's some good ideas in the argument-parsing/plugin code of GATK/Barlcay, frankly seems like much of it isnt fully developed yet, which is why I kept this separated at the moment. . 3) Be aware, the GATK3 tests depend on ~30GB of files. I dont know the limits of git lfs, but I did not currently have plans to check those in. I assumed I would convert these to use GATK4 chr20/21 data for a final commit, but felt there was a lot of value in using unaltered GATK3 data to confirm parity (and it was during the migration).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407123968:1235,test,tests,1235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407123968,1,['test'],['tests']
Testability,"@cwhelan . Thanks for the review!; I've incorporated most of your review suggestions, with the fowling exception because I need to think about what need to be done to make less review rounds. > This logic does more than detect variants, though.. it also annotates existing variants with the imprecise evidence. I'm also a little hesitant to move this all into its own separate class -- we really should be moving towards a model where we look at all three sources of evidence (breakpoint assemblies, imprecise evidence clusters, and coverage) simultaneously for eg @mwalker174 's work, and splitting handling of imprecise evidence into its own class seems like a step in the wrong direction. I agree. That's what I'm thinking about for complex inversions as well. So what about the following in this particular PR:. 1. move `StructuralVariationDiscoveryPipelineSpark.makeEvidenceLinkTree()` into `ImpreciseVariantDetector`;; 2. drop `ImpreciseVariantDetector.detectImpreciseVariantsAndAddReadAnnotations()` considering it really only delegates to `processEvidenceTargetLinks()`; 3. rename `ImpreciseVariantDetector` as `EvidenceTargetLinkHandler`; 4. reduce the work of `DiscoverVariantsFromContigAlignmentsSAMSpark.discoverVariantsAndWriteVCF()` into detecting only simple variants based on assemblies and name it `discoverSimpleVariants()`; 5. let `StructuralVariationDiscoveryPipelineSpark` call into `EvidenceTargetLinkHandler.processEvidenceTargetLinks()` to get back VariantContexts, then write VCF . `processEvidenceTargetLinks()` really does two things at the moment: annotation on breakpoints and call imprecise deletions; preferably, we should go the all-evidence-at-the-same-time approach and decouple the two but I am trying to not mess with it right now. If you agree, I'll implement it in a separate commit.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3934#issuecomment-357345426:199,log,logic,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3934#issuecomment-357345426,1,['log'],['logic']
Testability,"@cwhelan Is there any chance you could run an SV pipeline with this change and see if it works? We added the classpath setting a long time ago for mysterious reasons, and have been afraid to remove it because we don't have good automated tests that run on the actual dataproc environment. I ran our very simple tests with this change but I want to check that it doesn't have negative consequences for your tools. I would really like to merge this if we can because it's recommended that you don't use this option unless you absolutely have to.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3946#issuecomment-357066967:238,test,tests,238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3946#issuecomment-357066967,2,['test'],['tests']
Testability,@cwhelan thanks for the clear up on the tests! Just updated with some fix and the output is now the same as master up to some formatting changes.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2258#issuecomment-261665373:40,test,tests,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2258#issuecomment-261665373,1,['test'],['tests']
Testability,"@davidadamsphd Sure, here is a quick guide to the code:. -`BaseRecalibratorSpark` is the standalone BQSR tool, and calls into the `BaseRecalibratorSparkFn` (which is also called from `ReadsPipelineSpark`). -`ApplyBQSRSpark` is the standalone ApplyBQSR tool, and calls into the `ApplyBQSRSparkFn` (also called from `ReadsPipelineSpark`). -Integration tests for the above are in `BaseRecalibratorSparkIntegrationTest` and `ApplyBQSRSparkIntegrationTest`. -Almost all other changes in the branch are related to the BQSR engine refactoring, which I summarize below:; - We pulled out the guts of the walker `BaseRecalibrator` tool, combined it with all of the code from the former `RecalibrationEngine` class (now deleted) to make a new `BaseRecalibrationEngine` class under `utils/recalibration`.; - We stripped out all copies of the code in `BaseRecalibrationEngine` from the walker, dataflow, and spark versions of BQSR, and modified them to call into `BaseRecalibrationEngine`.; - We moved all auxiliary classes needed by the `BaseRecalibrationEngine` (eg., the covariates, etc.) into `utils/recalibration`.; - We refactored the argument collections. Now there is a single shared `RecalibrationArgumentCollection` that contains **only** the parameters for the `BaseRecalibrationEngine` itself, and this argument collection is exposed by all 3 versions of the tool. Input/output arguments have been removed from this argument collection and put into the individual implementations of BQSR, since they vary between the walker, dataflow, and spark versions of the tool. This eliminates awkward problems such as having both a `knownSites` argument AND a `BQSRKnownVariants` exposed at the same time, with only 1 of them usable for a given version of a tool. The dataflow-only `BaseRecalibrationArgumentCollection` has been deleted completely as no longer needed.; - We tweaked the names of some tool arguments to enforce consistency between the 3 versions of the tool as well as the rest of hellbender (eg.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073:350,test,tests,350,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073,1,['test'],['tests']
Testability,"@davidbenjamin As discussed in person, it's my hope that the `AssemblyRegionWalker` changes in https://github.com/broadinstitute/gatk/commit/1ef09b3ca265209e0777c77a8519da74480908ce (which have now been merged into master!) will address `Mutect2` memory usage, and make these somewhat confusing new downsampling arguments unnecessary. That patch reduces the number of reads stored in memory at once by the engine by roughly an order of magnitude without doing any extra downsampling at all. I suggest that we do an evaluation to test whether this really resolves the issues you encountered. `Mutect2` is already hooked up to the new, lower-memory traversal code in the latest gatk/master, so all you have to do is re-run your benchmarking test. I'd suggest that you:. 1. Run with default settings in the latest master, and see if that alone does the trick!. 2. If not, try turning up the existing downsampling a bit. Eg., run with `--maxReadsPerAlignmentStart 10` instead of the default of 50. 3. If that still doesn't resolve the problem, we can revisit this PR and consider a simplified version of the downsampling args here for merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233:529,test,test,529,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233,3,"['benchmark', 'test']","['benchmarking', 'test']"
Testability,@davidbenjamin Can you craft a simple unit test for these annotations to make sure this stays fixed?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2239#issuecomment-257574869:43,test,test,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2239#issuecomment-257574869,1,['test'],['test']
Testability,"@davidbenjamin Can you implement a simple integration test for this arg, to ensure it doesn't break again?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4128#issuecomment-357045030:54,test,test,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4128#issuecomment-357045030,1,['test'],['test']
Testability,@davidbenjamin Can you look at the test? I didn't want to check in a file with the old erroneous behavior so its hard to demonstrate what this fixed but i tried to make it clear in the comments.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7740#issuecomment-1082335904:35,test,test,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7740#issuecomment-1082335904,1,['test'],['test']
Testability,"@davidbenjamin I figured out that particular case we talked about earlier. The case (`depth = 0` but `PileupElement` is not empty) happens when all the reads have deletion at the locus. Instead of logging a message, now I simply skip such a locus.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3721#issuecomment-348321755:197,log,logging,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3721#issuecomment-348321755,1,['log'],['logging']
Testability,@davidbenjamin I have refactored this branch to account for changes to the codebase adjacent to this code. In the interest of not possibly harming any of the old results I have made this a toggle and I have also made the setting apply symmetrically to tails and heads and added a few simple tests in the existing framework.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6113#issuecomment-640870830:291,test,tests,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6113#issuecomment-640870830,1,['test'],['tests']
Testability,"@davidbenjamin I made the changes you requested, plus some additional cleanup. Since this function takes a `normalizedTable` it only ever actually sees tables whose sums are less than 400. The smallest p-value we'd expect given that we can't have entries that are larger than 400 is around 1e-120. Therefore we don't actually have to take the log of the probability and normalize it, we can just take the probability straight from `HypergeometricDistribution`. We also don't need `relErr`. . Also given this, I didn't make the changes @lh3 described, although this would clearly be a good way to reduce the computation needed for calculating the p-value with larger tables. . If you think it would be useful to keep these numerical stability features, I can add them back in, but removing them feels more readable to me given that we are only calculating small tables.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266828114:343,log,log,343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266828114,1,['log'],['log']
Testability,"@davidbenjamin sorry to bug, but we're really hoping to get this resolved. see my last post - while i think it's clear GATK wasnt pruning alleles in any kind of force-output/output-all case, my latest changes add this and also add test cases around it. i'm now using one of the pre-existing gVCFs as test data, which improves the test coverage as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-578837930:231,test,test,231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-578837930,3,['test'],['test']
Testability,"@davidbenjamin, et al. I have two recommendations:. 1) Though I prefer to work symbolically and through proofs, it might be nice to first expand on the validation by proof in the JavaDoc - including for the specific function's header - and anywhere else where necessary across the GATK code, just for sanity's sake, and for tying things together neatly and properly. This process of always going through the mathematical steps alerts me when I code that I have not missed anything. . 2) When dealing with multiple levels of transformations, it probably would be good to formulate a collection of complete set of simple tests. Since like you mentioned {phased} is a subset (⊂) of {unphased}, then the paths of phased genotypes one works with would also be ideal to test on. Does this function have any validation tests confirming the correct likelihoods, which would be performed for both phased and unphased genotypes? These can be generated tests, if original files do not exists. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221:619,test,tests,619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221,4,['test'],"['test', 'tests']"
Testability,"@davidbernick Since you set up the jenkins tests for us, would you be able to help us out with this ticket? We've gotten weary of catching Spark regressions post-merge with jenkins, and want to set up fast dataproc-based tests in travis that run on every pull request, and just run simple Spark tools like PrintReadsSpark to try to catch at least the most basic kinds of breakage before a branch is merged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-286240726:43,test,tests,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-286240726,2,['test'],['tests']
Testability,"@droazen - Still some questions about integration tests (on the comment with your suggestion, but here too):. * I am not sure if the test that you are proposing will work with all the implementations of `createTempFile`: depends on how it is handle, as a `File` or as a `Path`; * I think that this depends a lot on the parts of the codebase that we are looking at, so maybe before accepting this a pass should be done for the usages and how the `java.io.tmpdir` is used. Waiting for your feedback before doing something that does not make sense...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4469#issuecomment-385942269:50,test,tests,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4469#issuecomment-385942269,2,['test'],"['test', 'tests']"
Testability,"@droazen I hacked one of the TrainVariantAnnotationsModelIntegrationTest cases to run in your Docker (only necessary because it seems like `gradlew test --tests *TrainVariantAnnotationsModelIntegrationTest` doesn't recognize tests that use a `DataProvider`, but perhaps I did something wrong). Here are the differences:. ```; (gatk) root@a87e0994889e:/repo# h5diff -v /repo/src/test/resources/large/org/broadinstitute/hellbender/tools/walkers/vqsr/scalable/train/expected/extract.nonAS.snpIndel.posUn.train.snpIndel.posOnly.IF.snp.trainingScores.hdf5 /repo/extract.nonAS.snpIndel.posUn.train.snpIndel.posOnly.IF.snp.trainingScores.hdf5. file1 file2; ---------------------------------------; x x / ; x x /data ; x x /data/scores . group : </> and </>; 0 differences found; group : </data> and </data>; 0 differences found; dataset: </data/scores> and </data/scores>; size: [445] [445]; position scores scores difference ; ------------------------------------------------------------; [ 60 ] -0.419202 -0.419202 5.55112e-17 ; 1 differences found; ```. Looks pretty negligible to me! :stuck_out_tongue_closed_eyes: Probably a result of the native code being called by the python/ML packages used in these tools; even minor changes in the compilers across Ubuntu versions might introduce differences like these. A quick fix might be to replace all system calls to `h5diff` in these tests with `h5diff --use-system-epsilon`; seems to do the trick here. But if that doesn't fix all test cases, then perhaps you can relax things with `h5diff -p EPSILON`, where `EPSILON` is a relative threshold. Probably OK to pick something like `1E-6`. OK if I leave it to you to try this or otherwise check the rest of the cases?. Sorry for the inconvenience! I think the exact-match test worked as intended here, but I probably could've put in better messaging originally. Unfortunately, it's a bit awkward to grab the output of system commands. And thanks for dealing with conda again (a necessary evil, unless we want ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848796931:148,test,test,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848796931,4,['test'],"['test', 'tests']"
Testability,"@droazen I think I see how #4801 could introduce a rounding error that creates an extremely small positive log10 probability, which triggers the error. The old code was ; ```; log10PNoVariant += log10GenotypePosteriors[HOM_REF_GENOTYPE_INDEX]; ```. and the new code to handle spanning deletion is; ```; log10PNoVariant += MathUtils.log10SumLog10(nonVariantLog10Posteriors); ```; where `nonVariantLog10Posteriors` includes but the hom ref posterior *and* the posteriors of ref / span del het genotypes. So instead of A, where A is the log 10 hom ref posterior, we have log10(10^A + 10^B), where B is the ref/span del het log10 posterior. This latter quantity should never be positive, but the `log10SumLog10` method it relies on doesn't know that and has finite precision. Given that the problematic number is truly miniscule, `2.559797571100845E-21`, my money is on that explanation. I think a reasonable solution is just to replace it by zero, because we know that's where it comes from. That is, the code should become; ```; log10PNoVariant += Math.min(MathUtils.log10SumLog10(nonVariantLog10Posteriors), 0);; ```. If there is a way for me to debug without having to learn to use GenomicsDB I would like to confirm this myself. Otherwise, @sooheelee can I give you a jar to try out on the tutorial data where you spotted the problem earlier?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-402175972:534,log,log,534,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-402175972,1,['log'],['log']
Testability,"@droazen [OSU Open Source Lab](http://osuosl.org/services/powerdev) provides the POWER8 cluster for open-source projects. Is it usable for your testing on PPC? Many open-source projects are using it. . With the source tree in a single repo that I propose, changes that are specific to AVX will be made only for the files under ""avx/"" directory, which are not used for building the PPC binary. For example, build.gradle will specify ""avx/"" when building the binary on x86_64 (""power8/"" on ppc64le). If the files under ""common/"" are changed (e.g., the package name is renamed from hellbender to gatk4), the changes should work on PPC if the tests don't fail on x86_64.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-215324733:144,test,testing,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-215324733,2,['test'],"['testing', 'tests']"
Testability,"@droazen thanks for the quick response! Just to be clear, my concerns were about testing that I didn't somehow screw up the original behavior through the exposure, not just testing that *some* behavior was exposed. But message received---will keep things on the simple side!. Also, please see the plots in #5564 to get an idea of the effect on outputs, if you haven't already. Would appreciate any thoughts you might have on that thread!. Will try to get this done in the next day or two, thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-896328697:81,test,testing,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-896328697,2,['test'],['testing']
Testability,"@droazen thanks for the review. I have now addressed all of your feedback, with the main changes being; - Revert the change to ShardBoundary that uses the padded interval as its interval, and use an anonymous class instead.; - Add a new join strategy (OVERLAPS_PARTITIONER) so that running BQSR can be done using the old way still.; - Add a check for overly long read sizes. If exceeded the job will fail with an exception.; - Be more conservative about the partition end point by using the maximum read length, rather than just the length of the read that happens the start the next partition.; - Rather than making a field in FeatureManager transient, do a better job of reinstating the field that is not serializable (a logger).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2190#issuecomment-257368260:723,log,logger,723,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2190#issuecomment-257368260,1,['log'],['logger']
Testability,"@gbrandt6 So now the protocol is that you wait for tests to pass (although it's unlikely this could break them...) and then you can merge with ""squash and merge"". You can edit the commit message in the browser to make sure it is clear. `Fix typo in --tmp-dir argument in GenomicsDB docs` is a pretty good description for this one though :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6785#issuecomment-684939263:51,test,tests,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6785#issuecomment-684939263,1,['test'],['tests']
Testability,"@ilyasoifer do you have the means to quickly run minimap2? I would recommend simply realigning src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam with minimap2 and making a quick ""are tests consistent with pervious versions"" test with a checked in vcf output. I don't know how to wrangle minimap2 to handle mates correctly however so i don't know if this is easy",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8337#issuecomment-1559878651:99,test,test,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8337#issuecomment-1559878651,3,['test'],"['test', 'tests']"
Testability,"@jamesemery I will gladly review. If I understand the code change it seems like there was already basically the right logic to avoid this _but_ the code was neglecting to put the force calling alleles in a representation consistent with the output VCF. And the fix is simply to compute `forcedAlleles = AssemblyBasedCallerUtils.getAllelesConsistentWithGivenAlleles(givenAlleles, vc)` a bit upstream of where we were doing so previously. If I've got that right, this PR gets my :thumbsup:.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7740#issuecomment-1081342841:118,log,logic,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7740#issuecomment-1081342841,1,['log'],['logic']
Testability,"@jamesemery What I'm actually trying to do is essentially `GENOTYPE_GIVEN_ALLELES` except that I don't know the ALTs. I realize that may sound silly. What I've ended up doing for now is generate a gVCF, genotype it, and then write a custom tool that consumes VCF and gVCF and inserts hom-ref genotypes based on a) where there is no call in the VCF and b) there is confidence in the hom-ref genotype in the gVCF. My point in logging this issue is mostly that `EMIT_ALL_SITES` is pretty misleading as it stands. I think it would be good, at a minimum, to update the documentation for that option to make it very clear that it does not in fact ""emit all sites"" but instead ""emits more sites, but still a subset of all sites in the region being called"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6059#issuecomment-530535087:424,log,logging,424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6059#issuecomment-530535087,1,['log'],['logging']
Testability,"@jamesemery sorry to bug on this topic, but I'm hoping to make a push early this year to fully migrate my lab off GATK3 . I looked more closely at the specific annotations we need to migrate. I decided that I will implement our walker, 'DiscvrVariantAnnotator', which is basically a light wrapper around VariantAnnotation. This will make it easier to spike in custom annotations. In that walker, I will override makeVariantAnnotations(). I will make a new marker interface for EngineAwareAnnotation, and test that on all the Annotation classes, and use this to inject FeatureManager. So no core GATK changes needed. I did find one thing I'd like to propose. You probably know PedigreeAnnotation is special-cased in GATK. Annotations that use it have automatic argument validation and have the SampleDB injected. Currently, PedigreeAnnotation is a subclass of InfoFieldAnnotation, so isnt available to GenotypeAnnotations. There doesnt appear to be a solid reason why. I tried to fix that and my best idea is the proposal here: #7041 . The core idea is to convert InfoFieldAnnotation and GenotypeAnnotation to interfaces. This is generally a trivial switch in existing code. With that, it becomes possible for classes that currently extend PedigreeAnnotation (which I switched to no longer extend InfoFieldAnnotation) to simply PedigreeAnnotation and implement InfoFieldAnnotation. This makes it possible for future classes to extend PedigreeAnnotation and implement GenotypeAnnotation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-760424063:504,test,test,504,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-760424063,1,['test'],['test']
Testability,"@jnktsj see this PR to get an idea of how minimal the changes would be to switch the FC Featured versions of the ModelSegments WDLs to use NIO. For various reasons, I cannot easily make the switch to the WDLs in the repo here (as you can see, this causes tests to fail). So if you would like to go ahead and start testing, I would suggest that you simply clone the Featured WDLs and make the changes yourself, if that's something you're comfortable with.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-406044277:255,test,tests,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-406044277,2,['test'],"['testing', 'tests']"
Testability,"@jonn-smith Could you comment on this one? The tool output clearly states that we don't support this version of Gencode, and that errors may occur:. ```; GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; ```. Do we claim to support 38 anywhere? (eg., in documentation, etc.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7385#issuecomment-891227342:223,test,tested,223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7385#issuecomment-891227342,1,['test'],['tested']
Testability,@jonn-smith Thank you. I will be looking at the ucsc. I also found the following tool that implements the ucsc liftover file creation....the logic seems simple.; https://github.com/konstantint/pyliftover,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6837#issuecomment-800419348:141,log,logic,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6837#issuecomment-800419348,1,['log'],['logic']
Testability,"@jonn-smith, I did see XsvLocatableTableCodec and the .config file path, but this does not appear to work. To be clear this is something like:; ```; gatk IndexFeatureFile -I ./hg19/testTextSource.config; ```; In IndexFeatureFile (https://github.com/broadinstitute/gatk/blob/abe8148bda234edf6bd00fa51df44d456e8e2641/src/main/java/org/broadinstitute/hellbender/tools/IndexFeatureFile.java#L118), it does identify the correct codec; however, it then calls:. IndexFactory.createDynamicIndex(featurePath.toPath(), ...). where featurePath is the config file. This calls IndexFactory to open a lineReader on the config file (not the backing data source): https://github.com/samtools/htsjdk/blob/6d3fc7bc1f613ecfce1c22d368f3ae17cb86823d/src/main/java/htsjdk/tribble/index/IndexFactory.java#L598. . This then fails during XsvLocatableTableCodec.readActualHeader(), since this is trying to read the config file, not the TXT file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1591678472:181,test,testTextSource,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1591678472,1,['test'],['testTextSource']
Testability,"@kdatta Looks like the integration tests passed on travis after clearing the cache! Once you address comments, squash, and rebase onto the latest gatk master the unit tests should pass as well, since you just need the TestNG fix that got merged into master. This means we can merge this today in all likelihood!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829:35,test,tests,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829,2,['test'],['tests']
Testability,"@kdatta Thanks for the update -- we do need to get these tests passing with our `assertVariantContextsAreEqual()` comparison routine, rather than your diffing tool, but we can relax this routine to be agnostic to allele ordering. About the `MIN_DP` issue: can you explain what was causing it? Why was it working with a `VCFCodec` and not a `BCF2Codec`? I still don't understand. Does it work now with our comparison routine and using a `BCF2Codec` internally, or does it still require a `VCFCodec` to pass? . And what was the ""ExcessHet problem"" (don't see a description of it above)? Was it just the ""no combination operation"" warning? What did the output look like before and after the fix for this annotation?. @cmnbroad has volunteered to have a look at the tests in this branch this afternoon, so we should be able to give you some more feedback soon!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277:57,test,tests,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277,3,"['assert', 'test']","['assertVariantContextsAreEqual', 'tests']"
Testability,"@kguraj Thanks for the response(s). The test in the PR was super useful as a temporary test, but as you mentioned it runs pretty slowly, and as it stands the test passes on current master anyway. It seems to require on the order of 9000-1000 intervals instead rather than 1000 to actually hit stack overflow. Since that would be a very slow running test, I'm inclined to back it out. Also, the user who originally reported the issue was using 11k intervals, and it seems that the stack overflow fix is unlikely to help in that case. Is there any guidance for users on what is a reasonable number of intervals per process ? It sounds like the intention was that it be used with pretty small intervals. Should we issue a warning message in GenomicsDBImport at some threshold number of intervals ?. Are you planning to produce a jar with the error messages suppressed for this PR?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902:40,test,test,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902,4,['test'],['test']
Testability,"@kgururaj As I start to think about upgrading exome joint calling to use GenomicsDBImport the 100 interval threshold seems like it might be problematic. I've been working with WGS data, so I don't have much intuition for benchmarking with missing data. Is there any performance downside to running over larger intervals that include missing data? For example, if we want to scatter the exome 50 ways, each subset of the exome interval list will have ~4000 intervals, but the GVCFs won't have data outside those intervals. Does it make sense to pass to GenomicsDBImport a single interval encompassing all of those?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-409956462:221,benchmark,benchmarking,221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-409956462,1,['benchmark'],['benchmarking']
Testability,"@kgururaj and @kdatta Here's some GVCF data for a trio of samples, each called to haploid (ploidy 1) and tetraploid (4) genotypes. I included the reference (just chromosome 20) and the intervals list. This is data from one of our workshop tutorials so many of the intervals in the list I used don't have any data (so lots of no-calls) but there should be enough usable calls for testing purposes. Let me know if this isn't sufficient to get you started. . Thanks for looking into this, it's very important for a non-trivial subset of our users. . [genomicsdb.zip](https://github.com/broadinstitute/gatk/files/1171416/genomicsdb.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3342#issuecomment-317577301:379,test,testing,379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3342#issuecomment-317577301,1,['test'],['testing']
Testability,"@lbergelson . I ran the FlagStatSpark tool on a bam with a splitting-index on the hdfs. It ran blazing fast with no delay at all with a total time of 1m41s. So it looks like it must be related to the splits on the cram. I see a similar delay(30-40 min) when testing the StructuralVariationDiscoveryPipelineSpark jobs on 50 crams. Below are some excerpts from the log of the fast FlagStatSpark on the bam. . No delay and tasks start right up....; ```; 18/03/07 20:31:40 INFO cluster.YarnClientSchedulerBackend: Application application_1507317228518_0369 has started running.; 18/03/07 20:31:40 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41567.; 18/03/07 20:31:40 INFO netty.NettyBlockTransferService: Server created on 10.48.225.55:41567; 18/03/07 20:31:40 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 18/03/07 20:31:40 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.48.225.55, 41567, None); 18/03/07 20:31:40 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.48.225.55:41567 with 8.4 GB RAM, BlockManagerId(driver, 10.48.225.55, 41567, None); 18/03/07 20:31:40 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.48.225.55, 41567, None); 18/03/07 20:31:40 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.48.225.55, 41567, None); 18/03/07 20:31:40 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4d9cf71d{/metrics/json,null,AVAILABLE}; 18/03/07 20:31:48 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (192.168.18.186:36002) with ID 8; 18/03/07 20:31:48 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (192.168.18.196:45956) with ID 2; 18/03/07 20:31:48 INFO storage.BlockManagerMasterEndpoint: Registering block manag",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371353888:258,test,testing,258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371353888,2,"['log', 'test']","['log', 'testing']"
Testability,@lbergelson Can you provide @kgururaj with your simple test case that replicates this issue?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3814#issuecomment-344285682:55,test,test,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3814#issuecomment-344285682,1,['test'],['test']
Testability,"@lbergelson Do you have an opinion on the best way to pip install the gcnvkernel python package and dependencies for Travis testing? I've verified that the pip install works within a basic conda environment with python=3.6. We'll need to load this environment both for unit/integration tests as well as WDL tests. As long as this is the only python environment we need, I think we can simply use the base environment in the Docker. If more environments are required (e.g., for @lucidtronix), then maybe we'll need to be more clever for unit/integration tests, but we can still load them manually in the scripts that kick off the WDL tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-348073948:124,test,testing,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-348073948,5,['test'],"['testing', 'tests']"
Testability,"@lbergelson I disagree -- it's very clear to me that those tests will trigger Google authentication, just by tracing through the code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2706#issuecomment-300806909:59,test,tests,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706#issuecomment-300806909,1,['test'],['tests']
Testability,"@lbergelson I understand the change of userClassPathFirst can cause problems.; About the new parameter, I shouldn't have ""pull request"" it. It's absolutly unnecessary as `--conf 'spark.submit.deployMode=cluster'` works. Plus, as you said, it's hardcoded...; About #3933, I didn't retry the `-- --deploy-mode` solution after my userClassPathFirst modification. I added the parameter in my fork to test. I still think it's important to have it in gatk's parameters because it's simple for users, but it's not an emergency. For me, the userClassPathFirst change is important. Or a parameter to specify it. Without it, I can't get my jobs to work in cluster mode.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3946#issuecomment-351440051:396,test,test,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3946#issuecomment-351440051,1,['test'],['test']
Testability,"@lbergelson has been able to replicate the bug using a simple test case, so this is confirmed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3814#issuecomment-343230937:62,test,test,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3814#issuecomment-343230937,1,['test'],['test']
Testability,"@lbergelson or @droazen: this is related to your comments on #8752 about preserving the default behavior of just writing the first source. This PR overloads simpleMerge(), meaning code needs to opt-in to the feature of preserving all source names. . I cannot kick off tests - would one of you be able to to that?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8871#issuecomment-2164367475:268,test,tests,268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8871#issuecomment-2164367475,1,['test'],['tests']
Testability,@lbergelson thank you for the comment and sorry for my bit late response. I excluded the dependency to the jsr203-s3a and tested that both local- and spark-gatk can access s3a files by dynamically loading it. I also added a new directory `scripts/s3a` for documentation and simple tests for s3a demonstration.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-665484597:122,test,tested,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-665484597,2,['test'],"['tested', 'tests']"
Testability,"@lbergelson thanks - could you take another look?. I agree about the many modes -- I'm not sure there's a way to clear that up. We're game to help with documentation/blog stuff that can help clarify what sort of usecases would benefit from different modes/features if that would help. Do you have any pointers for sample data that I could use for testing the ""many contigs to several"" case?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6681#issuecomment-667421004:347,test,testing,347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6681#issuecomment-667421004,1,['test'],['testing']
Testability,@lbergelson the tests are not running on [gatk-jenkins.broadinstitute.org](url) so it's not usable yet. What remains to be done?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1400#issuecomment-199324556:16,test,tests,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1400#issuecomment-199324556,1,['test'],['tests']
Testability,"@lbergelson you beat me because I was stuck trying to actually run a Picard tool in the integration test. (For future reference, that needs a workaround because the test running adds the ERROR level logging to all command lines and Barclay can't parse that for Picard tools for some reason.). The big reason I was using this instead of IntervalListTools is because the Picard version creates a terrible output file structure that I was having trouble capturing with a simple glob in WDL. I agree that the functionality here is largely redundant, but it was helping me get my workflow working faster at the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435894196:100,test,test,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435894196,3,"['log', 'test']","['logging', 'test']"
Testability,"@lbergelson, @akiezun AFAIK picard tools need you to specify specifically FLAG=true/ FLAG=false if it's a boolean flag. it is true that, if you want, any argument can have a default value (true or false) but to change it you will still need to assign true or false (i.e even if there is a default you cannot simply have FLAG on the commandline). Yes, the logic of the pipeline specifies all the commandline arguments, regardless of defaults so that if the defaults change (which the GATK used to do all the time!) the pipeline will not change. Thus the use case has to include being able to set all arguments to their value boolean or otherwise.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/133#issuecomment-94434510:355,log,logic,355,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/133#issuecomment-94434510,1,['log'],['logic']
Testability,"@lbergelson. Our simple s3 nio library is not currently open source, but we would be willing to make it so after testing it properly. We found that to get the s3 nio library work with GATK, a few minor changes needed to be made in the GATK source. This is especially true for the spark tools because, on AWS EMR Spark clusters, s3 uris can be treated exactly as if they are HDFS uris. Therefore, it was not quite as simple for us to just add the s3 nio library to the classpath and have everything work as expected. For that reason we put the project on hold until GATK is closer to release. Thanks,; David. ________________________________; From: Louis Bergelson <notifications@github.com>; Sent: Monday, July 31, 2017 11:57 AM; To: broadinstitute/gatk; Cc: David Brown; Mention; Subject: Re: [broadinstitute/gatk] update com.google.guava version (#3102). @david-wb<https://github.com/david-wb> Is your s3 plugin available as an open source plugin that others could use? We had another question about s3 support in gatk and I thought you might have some insight about it. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319146368>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ABxO-d5XTtUyeAI0GzCFLP5eVGYiyQJEks5sThWegaJpZM4N31U->.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834:113,test,testing,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834,1,['test'],['testing']
Testability,"@ldgauthier & @droazen I've done as you've suggested. There is now a check in `GenomicsDBImport`, by wrapping the FeatureReader. It's a little ugly but it gets the job done. I've also added a simple test for GenotypeGVCFs to genotype a GVCF that has an MNP in it. I _think_ this is probably now ready for review. Let me know if you think further tests are needed!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5182#issuecomment-422989371:199,test,test,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5182#issuecomment-422989371,2,['test'],"['test', 'tests']"
Testability,"@ldgauthier Emitting the spanning-deletion-only sites completely would definitely make things simpler, since I could then use EMIT_ALL_CONFIDENT_SITES and GenotypingEngine would just naturally do the right thing. Also, the result would comport with my own naive expectations. Using the current PR, outputs could contain LowQual sites that GATK3 wouldn't have included. So if you're good with that, I'll update the PR and tests to reflect that before any more reviewing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5219#issuecomment-424454872:421,test,tests,421,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5219#issuecomment-424454872,1,['test'],['tests']
Testability,@ldgauthier Thanks for the feedback -- I'll see if I can add some additional assertions about the actual alleles retained at each site that had more than the maximum.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4497#issuecomment-370811446:77,assert,assertions,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4497#issuecomment-370811446,1,['assert'],['assertions']
Testability,"@ldgauthier The test is much clearer now, thanks for pointing me to the example. This will end up being tested in WARP with the next GATK release and I'm not sure how easy it is to test two commits of GATK in WARP against each other. If it's possible to do that without updating the official truth data, then I could run that before we merge this. Otherwise we'll end up catching any issues when we update WARP after the next GATK release (which I'm motivated to do when the time comes).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8862#issuecomment-2159220666:16,test,test,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8862#issuecomment-2159220666,3,['test'],"['test', 'tested']"
Testability,"@ldgauthier This looks good, its a pretty simple change and there are unit tests and integration tests that enforce the new behavior. I would squash the two commits and give them an informative commit message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3409#issuecomment-320707124:75,test,tests,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3409#issuecomment-320707124,2,['test'],['tests']
Testability,"@lh3 We agree about the low-cov joint calling and have already tested it on 191 low-coverage (roughly 3-4x) samples from 1kg. The calls were identical except for one site with qual just above 30 that used to be just below, but this difference is basically arbitrary. If we simply add 15 to the qual threshold (as we should, because the new qual is systematically more permissive due to learning a minor allele fraction that may be greater than the average genome-wide heterozygosity), results are completely identical.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258436797:63,test,tested,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258436797,1,['test'],['tested']
Testability,"@lucidtronix @mbabadi @samuelklee I think the best solution would be to establish a single, common Python environment, with a single set of dependencies, that all GATK Python tools depend on. We would establish a single docker image that has all of these dependencies pip installed, and could also include a conda env for the GATK environment for users who don't want to use the docker image. If we could do that, it would eliminate the need load per-tool conda environments. From what I've seen so far based on existing branches, the two environments we need (gCNV and CNN-VQSR) don't look that far apart in terms of dependencies. gCNV is using Theano, and CNN Tensorflow, but the rest looks [pretty close](https://docs.google.com/a/broadinstitute.org/spreadsheets/d/1RV7--uBQ0ctlXzMH09cmr0VimpZYIU68DdxJzE60y-c/edit?usp=sharing). So a strawman proposal for the main components for a common environment would be:. Python 3.6; Numpy >= 1.13.1; Scipy 1.0.0; Theano .0.9.0; Tensorflow 1.4.0; Pymc3 3.1; Keras 2.1.1. Can you all chime on on whether you think we can converge in a single environment ? If so, it would greatly simplify things, and we can start with getting a docker image built for running travis tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3692#issuecomment-348188451:1209,test,tests,1209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3692#issuecomment-348188451,1,['test'],['tests']
Testability,"@magicDGS It looks like you have triggered a few new compiler errors in the last branch, namely in the following places:. ```; /gatk/src/test/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/SVDiscoveryTestDataProvider.java:33: error: cannot find symbol; BaseTest.b38_reference_20_21, ReferenceWindowFunctions.IDENTITY_FUNCTION);; ^; symbol: variable BaseTest; location: class SVDiscoveryTestDataProvider; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/formats/SampleLocatableCollectionUnitTest.java:30: error: cannot find symbol; private static final String TEST_SUB_DIR = toolsTestDir + ""copynumber/formats"";; ^; symbol: variable toolsTestDir; location: class SampleLocatableCollectionUnitTest; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedregion/SimpleAnnotatedGenomicRegionUnitTest.java:18: error: cannot find symbol; private static final String TEST_FILE = publicTestDir + ""org/broadinstitute/hellbender/tools/copynumber/utils/combine-segment-breakpoints-with-legacy-header-learning-combined-copy-number.tsv"";; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259:137,test,test,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259,3,['test'],['test']
Testability,"@magicDGS Review complete for now. Looks good but I have some nitpicks. I think they're almost all due to it being ancient gatk3 code that no one has updated in a long time. I'd recommend dropping the deprecated formats and only supporting mpilup single sample format which should allow for massive simplification of both the Codec and the Feature. . We need some unit tests for the codec itself since it has a bunch of different potential error cases, and we should have some integration tests for the tool that show it correctly failing on cases with errors.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179:369,test,tests,369,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179,2,['test'],['tests']
Testability,"@magicDGS, after much discussion with @droazen, we will make this test set even smaller. Also, the current test set will be temporary, to unimpede you, until updates to the main test set can be made (discussion to start next week). @cmnbroad says we need to keep an eye out for coverage in the tests, given the need to make this dataset extremely small. So I removed chr17 from the mini-reference, such that only four small snippets of reference from various chromosomes remained. However, it appears there are only two usable indel realignments going on with GATK3. Note thate these are targeted exome samples with comparatively low coverage. [for_magicDGS.zip](https://github.com/broadinstitute/gatk/files/1820785/for_magicDGS.zip). There are two samples, so as to enable testing nWayOut, and an artificial reference `hg38_Shl01`. The two sites to hone in on are chr11:177568 and chr11:207134. Note also that the BAMs are in an invalid state according to ValidateSamFile. However, GATK3 RealignerTargetCreator and IndelRealigner did not seem to mind. I think these tools should allow processing of BAMs in any validation state. Apologies for the meager state of the data. On the bright side, the data set including the ref is only 23MB and will meet with @droazen's approval in terms of size. . Good luck @magicDGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600:66,test,test,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600,5,['test'],"['test', 'testing', 'tests']"
Testability,"@magicDGS:. 1. Yes, you're right that you're already correctly testing that you get the right number of shards back. No need for an extra assertion. 2. I think that realistically-sized reads (of differing lengths!) does add to the unit test, since it's important to test with reads that overlap extensively with other reads. You'll also want to vary the read lengths to test with reads of different lengths at the same start position, shorter reads that start after longer reads, and any other arrangements that are significant to your `ShardingIterator`. . It shouldn't be too difficult to write a simple method that uses `ArtificialReadUtils` to create small pileups of reads within a given interval (might be worth extracting into `ArtificialReadUtils` itself so that future tests can use it).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389998139:63,test,testing,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389998139,6,"['assert', 'test']","['assertion', 'test', 'testing', 'tests']"
Testability,"@matthdsm this was intentionally left out of the recent 4.6 release, but should go into the next minor release. Would of course appreciate any testing/feedback from the community before then!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2200883096:143,test,testing,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2200883096,1,['test'],['testing']
Testability,"@mbabadi I've updated my PR to use miniconda3. @mbabadi @lucidtronix @samuelklee I think we should aim for tools that at least run out-of-box, without depending on any out-of-band configuration other than the conda env. On top of that we can provide guidance/configs for users on how to enable further optimizations, like g++. Does that sound like an achievable goal ?. As for the docker, we're going to have strike the right balance between image bloat and performance(including test performance). I think we're around 4+ gig now, and counting. Before the Python integration we were at 1.9G, and trying to find ways to reduce it. So lets see where we wind up but keep that in mind. Finally, we need to find a way to install the (GATK) python package(s) without depending on access to the GATK repo. Right now I think the gCNV branch has a ""pip install from source"" added to the conda env .yml. That will work on the docker at the moment (and thus on travis), but that won't work for non-docker users how don't have source/repo access. Also, one of the proposals to reduce the size of the docker is to remove the repo clone that is currently there. My proposal is that we change the gradle build to create an archive/zip of the python source (this would include the VQSR-CNN package code as well as gCNV kernel). We can then copy that on to the docker image, and pip-install it from the copy. That would retain the ability to always run travis tests based on the code in the repo, and also keep the nightly docker image in sync. We'll also have deliver the archive as an artifact somehow (perhaps including PyPi) for non-docker users.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277:480,test,test,480,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277,2,['test'],"['test', 'tests']"
Testability,"@mbabadi you're welcome! . About those two issues--I learned that with VPN on, Spark tools error locally (thanks to Steve). I turned off my VPN connection and am able to run PileupSpark locally. (There is an issue ticket on this at https://github.com/broadinstitute/gatk/issues/1534.). One other thing to note for PipeupSpark documentation--the tool will error if the output filename already exists. That is, unlike other GATK tools, it will not overwrite existing file names. Either this unusual behavior should be fixed or mentioned in the tooldoc. I'm testing this with dataproc now. When running locally, neither CollectBaseDistributionByCycleSpark nor CollectInsertSizeMetricsSpark output the PDF file. So this seems a bug and I'll put in an issue ticket if there isn't one already.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4068#issuecomment-356028074:555,test,testing,555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4068#issuecomment-356028074,1,['test'],['testing']
Testability,@meganshand 1 very minor comment about the tests. 👍 After that. This is awesome to find such a simple solution.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2629#issuecomment-297761108:43,test,tests,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2629#issuecomment-297761108,1,['test'],['tests']
Testability,"@meganshand Here's a quick example:. ![image](https://user-images.githubusercontent.com/11076296/158385742-20a3303b-d8ce-4335-b42f-622da9bfa8d3.png); ![image](https://user-images.githubusercontent.com/11076296/158385777-6174f8b8-7abb-4b31-92d1-11cc8064854b.png). Note that the malaria data used was pretty small: chr1-2 training (~20k positive training/truth variants, ~50k negative training variants; note also that the threshold for determining negative training was not tuned---a threshold corresponding to a 98% truth sensitivity was arbitrarily chosen), chr3 validation (~50k variants), and chr4-6 test (~150k variants). The LL score is calculated from a validation set held out from the training/truth positives used to train the model, while the F1 score is calculated using ""orthogonal truth"" positives/negatives determined using 3 families of ~30 trios each. However, there's some arbitrariness in how we define the boundary for the latter positives/negatives, and hence some arbitrariness in the F1 score itself. But I'd expect using gold-standard GIAB truth would be more straightforward. Not sure how much we can conclude, but that the validation and test F1s are similar and that the validation LL score isn't *too* far off are encouraging. That said, there is a pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian con",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:603,test,test,603,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,1,['test'],['test']
Testability,"@mlathara The nodes have considerably more (256 or so). is there any rule or thumb or guidance on expected memory needs based on number of gVCFs and/or type of input (WES vs WGS)?. I do think you might be onto something though. Out default cluster submission code takes our slurm job memory request, subtracts only a few GB and passes the remainder to -Xmx/Xms. I will update to leave more buffer as you suggest. Our cluster happens to be undergoing maintenance this week, so this particular job was killed. I'll update the GATK version, add --genomicsdb-shared-posixfs-optimizations, and adjust the memory. One other thing: i noticed GenomicsDBImport is not nearly as verbose in logging as typical GATK tools. Is that expected, or a symptom of whatever problem we're having?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-656259475:680,log,logging,680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-656259475,1,['log'],['logging']
Testability,"@mlathara To those points:. - Yes, the test case I tried to make using GATK's test data doesnt show this. When I was working quickly I thought I repro'd it, but as you point out the REF bases for chr20:10-20 are actually Ns, so in this instance GenotypeGVCFs is doing the right thing. - Since my original posts, we figured out new information (posted above). When we simply do a SelectVariants on the genomicsDB workspace over these intervals, it produces a gVCF with Ns listed as the reference. In the actual reference, those sites are not Ns. That's making us look in a different direction than I originally thought. Our current plan is to remake one of our workspaces (exome data from ~800 samples) and see if this repros.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7018#issuecomment-755549351:39,test,test,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7018#issuecomment-755549351,2,['test'],['test']
Testability,"@mwalker174 ; Hi Mark, I've finished writing the tests and would you please check again?; Here's the log running the whole pipeline (the number of simple variants extracted is approximately 1.5X the number of complex variants):. ```; .... below is output for complex variants only; 23:09:25.288 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 1334 variants.; 23:09:25.288 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 1334; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - INV: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - INS: 0; ..... below is output from this tool; 23:09:48.167 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 688 variants.; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - INV: 1; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 125; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - INS: 562; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 0; 23:09:48.215 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 1555 variants.; 23:09:48.216 INFO StructuralVariationDiscoveryPipelineSpark - INV: 21; 23:09:48.216 INFO StructuralVariati",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4602#issuecomment-389343644:49,test,tests,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602#issuecomment-389343644,2,"['log', 'test']","['log', 'tests']"
Testability,"@nalinigans Yes, it's been surprising me quite a bit too. When you say 'can you run SelectVariants', do you mean simply trying to select from the source GenomicsDB workspace as a test to see if java has enough resources? I can try this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1201585570:179,test,test,179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1201585570,1,['test'],['test']
Testability,@nh13 I wrote a test for your branch (its very simple it just reruns the gvcf mode tests with --disable-optimizations enabled) that should work for your branch. Its in the branch je_addTestForDisableOptimizations. Since you submitted this PR from your own clone of the GATK I cannot push this onto the branch as it stands. Would you be able to copy it into this branch?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7125#issuecomment-793077846:16,test,test,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7125#issuecomment-793077846,2,['test'],"['test', 'tests']"
Testability,@pawel125 This looks like a filesystem error - `I/O error in the advisory file locking logic (disk I/O error)`. Are you using an NFS file system to store the datasources or some other kind of network-mounted drive?. To be clear - the first issue you had was **not** a typo. The v1.7 data sources are not backwards compatible and the code changes haven't been merged yet.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661885327:87,log,logic,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661885327,1,['log'],['logic']
Testability,"@samuelklee I support exposing these parameters via the command line, but I'd be opposed to any consolidation of parameters that changes the HaplotypeCaller output prior to the initial DRAGEN-GATK release in November, as the evaluations in that project are difficult enough as it is. If you want to do an evaluation to find the best set of SW parameters now, that's fine of course -- but we wouldn't be able to actually merge any breaking HaplotypeCaller changes until after the November DRAGEN-GATK release, and we'd also have to check whether the proposed changes affect the functional equivalence of GATK and DRAGEN (we're developing tests now that can check this). If you want to expose the SW parameters on the CLI now, I think 12 arguments is fine. Just give each argument a clear prefix indicating what it applies to (eg., `--read-to-haplotype-mismatch-penalty`). If a user has gotten to the point where they feel the need to mess with the SW parameters, their command line is probably already long and complex as it is, so adding a few additional arguments won't ruin their day.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705081291:637,test,tests,637,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705081291,1,['test'],['tests']
Testability,"@samuelklee I think it's right for what we're doing. We mount the test data as `/testdata` and then create a symlink from src/test/resources to /testdata to provide it to the test files. It seems to work.; ; I'm not clear what they get more of the other way around. More tests? Are they using our create docker script? Or our travis file? Or something else? I think we might just be able to just directly mount test data to src/test/resources and avoid the symlink, but I probably had a reason when I set it up that way... I think this is a non-issue unless they can provide more information.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3730#issuecomment-339439156:66,test,test,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3730#issuecomment-339439156,8,['test'],"['test', 'testdata', 'tests']"
Testability,"@samuelklee I'd say lets leave 2.1 base image up there for now, and yes on the cache clearing. Once tests pass with the cache cleared it should be good to merge. Feel free to squash and rebase if you like.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408453109:100,test,tests,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408453109,1,['test'],['tests']
Testability,"@sooheelee - I think that to have a proper test suite similar to GATK's 3, we need also some test that exercise some code paths that requires some specific scientifically meaningful data (for example, known indels that are also included in some sample). I am not that familiar with the test data on this or GATK3 repository, so I am not sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:43,test,test,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115,3,['test'],['test']
Testability,"@t-ogasawara @frank-y-liu @gspowley @paolonarvaez @droazen @lbergelson please comment on the following proposal. The proposal is that we would spin off native PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:666,test,testing,666,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864,1,['test'],['testing']
Testability,"@tedsharpe I've addressed some of your comments here -- all the simpler stuff plus:. - I now only make distal targets for split reads with one supplementary alignment. We can make a ticket to handle more complex cases at some point.; - I renamed the concept of strand in the `EvidenceTargetLink` and related classes -- I'm now calling it `evidenceUpstreamOfBreakpoint`.; - I canonicalize `EvidenceTargetLinks` and only create them when the source is upstream of the target. This allowed me to get rid of the de-duplication code, so thanks for the suggestion. It seemed tricky to me to try to cluster these links during the initial pass over the reads while at the same time keeping track of coherent evidence. In my testing it doesn't seem like it is slow to run over the `EvidenceRDD` again to do this, but we could think about trying to change this sometime if we're looking for optimizations. . Want to take another look?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328300806:716,test,testing,716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328300806,1,['test'],['testing']
Testability,@tomwhite . I found this 1000 genomes cram that also generates the error. The test cram above would take a lot of paper work to make available. ```; wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/data/GBR/HG04302/alignment/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram; wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/data/GBR/HG04302/alignment/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram.crai. ```. Here is the error with this cram.... ```; gatk CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -- --spark-runner SPARK --spark-master yarn; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar; Running:; /share/pkg/spark/2.3.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 2019-01-07 11:33:18 WARN SparkConf:66 - The configuration key 'spark.yarn.executor,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:78,test,test,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['test'],['test']
Testability,"@tomwhite . I tried rerunning the job using a bgzip reference to work around the problem. However, the same error is being generated (htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice). . ```; gatk CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/wgs.hg38/pipelines/hc/cram.test/GRCh38_full_analysis_set_plus_decoy_hla.fa.gz -- --spark-runner SPARK --spark-master yarn. Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar; Running:; /share/pkg/spark/2.3.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/wgs.hg38/pipelines/hc/cram.test/GRCh38_full_analysis_set_plus_decoy_hla.fa.gz --spark-master yarn; 2019-01-09 13:35:04 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:05 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... usi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:269,test,test,269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['test'],['test']
Testability,"@tomwhite @davidadamsphd could you take a look and see if it's in the right direction?. Some notes:; - This is only impl'd for pure Spark at the moment. Gonna work on adding support for dataflow as well.; - It was easier to remove the `final` modifier on the `GATKRead` impl rather than reimplement all the methods and simply pass them through. Let me know if that's ok.; - Should we target Parquet IO only in the context of writing to Hadoop? Or should I make sure it works anytime a local/single bam file is being written?; - Definitely need to add more tests. One thing that's annoying is that the cleanup for `readsSinkParquetTest` doesn't seem to happen.; - Registered `AlignmentRecord` with the `GATKRegistrator`, but there aren't any tests that exercise it. There's a touch of scala-to-java ugliness there, so let me know if we should just reimplement the `AvroSerializer` class.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/888#issuecomment-139423282:556,test,tests,556,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/888#issuecomment-139423282,2,['test'],['tests']
Testability,"@tovanadler Review complete. Looks good, just a few comments. I have a few comments about the organization of duplicate marking. I think you've inherited some very old style code that could maybe use some refactoring. I think we do need to also include the histogram and the metrics headers. Those could be done in a separate ticket though. I'm a bit worried that the test is indeterministic. Unless I overlooked something which is likely, it seems like it might depend on the ordering of a PCollection which is undefined. This isn't problematic for the actual metric file, but might be for the tests. What do you think about reorganizing to output an annotation on only 1 of the ""best"" reads with the count of all optical duplicates in it's group. That would simplify the code, and since we only care about the global count it wouldn't change the information content.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958:368,test,test,368,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958,2,['test'],"['test', 'tests']"
Testability,@vdauwera The latest game plan for this ticket (which is being addressed in #2021) is to have SplitNCigarReads create a supplementary alignment from each split and soft clip the bases that align elsewhere. This allows the bam to be more usable by other tools (like BQSR) since the logic to handle supplementary reads already exists. This means that we need an additional argument to HaplotypeCaller so that allows using supplementary alignments (See #2043). . My question is how can we document that SplitNCigarReads is not recommended for use with the changes to make the reads supplementary until #2043 is resolved? Is it possible to make the tool hidden in GATK4 or give it some tag that makes it clear it isn't ready for use yet?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1394#issuecomment-235062440:281,log,logic,281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1394#issuecomment-235062440,1,['log'],['logic']
Testability,"@yurivict The large files under `src/main/resources/large/` are required to build GATK, since they are packaged inside the GATK jar and used by tools at runtime. These are things like ML models and native C/C++ libraries used for acceleration of certain tools. The large files under `src/test/resources/large/`, on the other hand, are only required by the test suite when running tests. Hope this clears things up!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8912#issuecomment-2223906223:288,test,test,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8912#issuecomment-2223906223,3,['test'],"['test', 'tests']"
Testability,A couple unrelated tests failing; hopefully a rebase will clear those up.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-240491388:19,test,tests,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-240491388,1,['test'],['tests']
Testability,"A few points:; 1. When we wrote the Java interface, the 'agreement' was that the sample names would be unique and consistent within the VCF headers. Hence, the assert statement in the Java code.; 1. Having said that, the sample name in the VCF header is ignored completely if checks are disabled (which are disabled by default). This includes the assert statement and a couple of other checks in the importer code. The sample name is taken from the name to reader map provided in the constructor call. This map is created from the tab delimited file.; 1. Would it be possible to provide a simple test case to replicate the bug? I couldn't replicate it. Here is what I did.; 1. Three VCF files - t0.vcf.gz, t1.vcf.gz, t0_dup.vcf.gz. t0 and t0_dup are identical except for the GT field in one location. So, these 2 files have the same header (same sample name in the header).; 1. Tab file (unique sample names). HG00141 test_inputs/vcf_test_inputs/t0.vcf.gz; HG0155 test_inputs/vcf_test_inputs/t0_dup.vcf.gz; HG00192 test_inputs/vcf_test_inputs/t1.vcf.gz. 1. Import. ./gatk-launch GenomicsDBImport --genomicsDBWorkspace /tmp/ws -L 1:1-1000000 --sampleNameMap test_inputs/gatk4_dup_test_list --batchSize 2; 1. Query prints the output correctly. ./bin/gt_mpi_gather -j test_inputs/query/gatk4-generated.json --produce-Broad-GVCF. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT HG00141 HG00192 HG0155; 1 12144 . G <NON_REF> . . . GT 0/0 . 0/0; 1 12191 . T <NON_REF> . . . GT 0/0 0/0 0/0; 1 17385 . G A,T,<NON_REF> . . . GT 0/1 2/2 1/1. ./gatk-launch SelectVariants -V gendb:///tmp/ws --output t.vcf.gz -R Homo_sapiens_assembly19.fasta; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT HG00141 HG0155 HG0192; 1 12141 . C <NON_REF> . . END=12144 GT:DP:GQ:MIN_DP:PL ./.:2:0:0:0,0,0 ./.:2:0:0:0,0,0 .; 1 12145 . C <NON_REF> . . END=12277 GT:DP:GQ:MIN_DP:PL ./.:2:0:0:0,0,0 ./.:2:0:0:0,0,0 ./.:3:0:0:0,0,0; 1 12278 . C <NON_REF> . . END=12295 GT:DP:GQ:MIN_DP:PL ./.:2:0:0:0,0,0 ./.:2:0:0:0,0,0 .; 1 17385 rs987;d345",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3814#issuecomment-343344853:160,assert,assert,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3814#issuecomment-343344853,3,"['assert', 'test']","['assert', 'test']"
Testability,"ADAM has no header. I realize I'm only coming into the game rather late, but would it be possible to ditch `SAMRecord`? Since you're already using Google `Read`-backed data as well, instead of writing against an interface (`GATKRead`), you could simply come up with your own, more-awesome concrete data structure. (Perhaps even an Avro `SpecificRecord` a la `AlignmentRecord`). In cases where ADAM wants a header back, at the moment it actually runs an aggregation across all the reads to rebuild it. (I'm trying to add a patch that allows you to specify a header, though, because it's breaking a hellbender test for reading/writing parquet.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-140990644:608,test,test,608,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-140990644,1,['test'],['test']
Testability,Added a very simple test for sorting iterator. Back to you @droazen.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2249#issuecomment-267139593:20,test,test,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2249#issuecomment-267139593,1,['test'],['test']
Testability,Added some changes per Laura's review. Mainly:; - added a `--genomicsdb-update-workspace-path` that specifies the path to an existing workspace for incremental import and interval_list generation cases; - removed `--incremental` since the above made it superfluous; - added an `--output-interval-list-to-file` argument that will just generate a picard style interval_list at the location specified by the argument for an existing workspace. No import done when this is used; - changed the existing tests to use multiple intervals instead of a single interval. @ldgauthier I'm not entirely sure about the picard interval_list generation. Any chance you could help with providing some expected input/output for that so that I can add a test for it? I ran through a simple test with it but not really sure what the output should look like.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-518447724:498,test,tests,498,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-518447724,3,['test'],"['test', 'tests']"
Testability,"Addressed feedback, updated so it can write reports and added test that checks the exact same reports as the BaseRecalibrator integration test (they pass).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/511#issuecomment-101783201:62,test,test,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-101783201,2,['test'],['test']
Testability,"After discussing with @ldgauthier, I'm going to approve this PR as-is, and Laura will address the remaining TODOs in a separate PR. For the record, the three remaining issues that need addressing are:. * Get rid of the `instanceof VariantWalker` check in `FeatureManager` by making `GATKTool.getGenomicsDBOptions()` return null (or `new GenomicsDBOptions(referenceArguments.getReferencePath())`) instead of throwing an exception, and then having `GATKTool.initializeFeatures()` (and its overrides) pass the GenomicsDB options in to the `FeatureManager` constructor, which can then propagate them down here. * Add a simple direct integration test for the new `--floor-blocks` HaplotypeCaller arg. * Address my maintenance concerns about `AnnotationUtils.isAlleleSpecific()` by adding an empty marker interface for AS annotations (open to other ideas here if you don't like that one)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-517350314:641,test,test,641,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-517350314,1,['test'],['test']
Testability,Ahh... I see https://github.com/disq-bio/disq/pull/124 was such a simple fix.... How did we find this bug in the first place? Was the split guessing failing in some case? I think on this end its probably acceptable to just accept this branch and point to the PR in disq. Can we not write a unit test for the SBI index there where we provide an invalid SBI index in a place where the split guessing would have worked and assert that it is indeed trying to use the index?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6323#issuecomment-567092790:295,test,test,295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6323#issuecomment-567092790,2,"['assert', 'test']","['assert', 'test']"
Testability,"Also, MQ filtering results in stochastic coverage dropout. It is likely that low MQ regions significantly overlap across samples, in which case, downstream CNV can learn such biases and correct the coverage. Will test this in validations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375722179:213,test,test,213,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375722179,1,['test'],['test']
Testability,"Also, just to provide some context to all tagged: certain users of the old CNV pipeline expressed somewhat vague concerns with the non-fragment-based coverage collection strategies—which also differed across WES and WGS, to boot—-but didn’t offer any compelling demonstrations that fragment-based strategies were better. For the new version of the pipelines, the main priority was to pick a single strategy to unify WES/WGS coverage collection. We decided to give a simple fragment-based strategy a shot—-with the intention of using automated evaluations to test it in a rigorous manner. Although those aren’t in place yet, I’m comfortable with making the call against it at this point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375126971:558,test,test,558,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375126971,1,['test'],['test']
Testability,"Am 90% certain at this point that the jar they were using for testing was built incorrectly. The `CloudStorageReadChannel.class` file in the latest `google-cloud-nio-0.19.0-alpha-shaded.jar` should be 6169 bytes, but their jar has 5401 bytes for that class. And that's the primary class JP patched in the latest release. So I am pretty hopeful that this was just a simple build error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306947226:62,test,testing,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306947226,1,['test'],['testing']
Testability,"Anecdotally, some people have seen logging issues around the Conda; progress bars filling logs. I know Conda had a change request to add a; silent or quiet mode. Suggestions on the back of a postcard... On Mon 15 Oct 2018, 21:27 Chris Norman, <notifications@github.com> wrote:. > Changing the exception throwing code won't help with either of those -; > that code would only execute when the tool is actually run. Currently, the; > PR is failing to even build on Travis during the part of the build where it; > creates the Docker image on which the tests will run. Travis is killing it; > because its producing so much progress output during the conda environment; > creation - right when its resolving tensorflow packages.; >; > My suggestion above was to see if we can (at least temporarily) get past; > that so we can see how big the Docker is, and whether the CNNScoreVariants; > tests pass with the new environment. Then we can figure out if we have any; > additional issues to resolve.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5291#issuecomment-430000969>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AG6lrwV2gaTYs6ur_AXznl6iV0AMxQ8Cks5ulO_DgaJpZM4XNHdi>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-430007451:35,log,logging,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-430007451,4,"['log', 'test']","['logging', 'logs', 'tests']"
Testability,"Another thing that just come to my mind is to rely on [SLF4J](https://www.slf4j.org/) for logging - downstream projects can configure which logger they want to use, and they can have their own ways of setting logging verbosity. If the logging system from HTSJDK wants to be maintain, it can also add a simple implementation of SLF4J with the verbosity levels that are in the current implementation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-371401288:90,log,logging,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-371401288,4,['log'],"['logger', 'logging']"
Testability,"Apologies @bbimber -- your efforts to port this tool to GATK4 are much appreciated. Our team has been extremely busy with the lead up to the 4.0 release, which is why we haven't been as responsive lately. I'll have someone take a look at the test data in question to see if it can be publicly shared.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358029799:242,test,test,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358029799,1,['test'],['test']
Testability,"Apologies on the poor report. There are no other users in these compute nodes (I am the tester) and for all intents and purposes the ulimit is pretty high (hard limit of 8192 max files). I am using GATK version 4.1.4.1, although it might be the one that has been optimised for IBM power9 systems by @ruzhuchen. Currently I am waiting for the sys admin to increase the max files further, but I believe that this is far from ideal. Here is the (simplified) command:; ```bash; gatk --java-options ""-Xmx40g -Djava.library.path=/bio/apps/gatk_4.1.4/gatk-4.1.4.1/libs -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" Mutect2 -R Homo_sapiens_assembly38.fa -I illuminaN_hg38.br.recal.bam --max-mnp-distance 0 -O illuminaN.vcf.gz; ```; May be I am running it wrong?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-598233814:88,test,tester,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-598233814,1,['test'],['tester']
Testability,"Applied feedback, rebased, squashed. Merge pending passing tests. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107694686:59,test,tests,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107694686,1,['test'],['tests']
Testability,"Applied feedback, reproduced bug and updated our description (#650), submitted bug report (https://github.com/google/google-http-java-client/issues/297), squashed. Merging once tests pass.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/835#issuecomment-132698966:177,test,tests,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/835#issuecomment-132698966,1,['test'],['tests']
Testability,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:1175,test,test,1175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661,3,['test'],['test']
Testability,"As we discussed on Slack, this will fix the NaNs, but I'm not convinced that we should allow the single-contig use case without at least a warning. The ploidy step will essentially perform no inference, since I think the per-contig bias and ploidy factors will cancel out with the way the likelihood is written---it will simply return the prior, and all samples will be guaranteed to have ploidy = 2. @asmirnov239 is going to do some more testing to make sure we understand this right and perhaps add a warning/documentation. The current likelihood is a bit confusing (I tried to address some of these issues in the unmerged ploidy-model update), but in any case, the problem is degenerate and it's hard to define appropriate behavior without additional priors and model structure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6613#issuecomment-631693589:439,test,testing,439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6613#issuecomment-631693589,1,['test'],['testing']
Testability,"Back to @meganshand. I put in a simple mitochondrial integration test. Given that our MC3 validation already covers this particular bug I actually don't think it needs a new test for mitochondria. Also, for later, are any of your spike-in bams public (or rather, public + public)? I noticed that the NA12878 truth doesn't have very low AFs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5057#issuecomment-408649991:65,test,test,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5057#issuecomment-408649991,2,['test'],['test']
Testability,Because there is currently no way in travis to prevent the build stages from being triggered in every pull request it was decided to simply upload the nightly build without tests instead. An example of how to use build stages can be seen in this branch for future reference: https://github.com/broadinstitute/gatk/tree/je_travisBuildStages,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3396#issuecomment-319721849:173,test,tests,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3396#issuecomment-319721849,1,['test'],['tests']
Testability,"BucketUtils was a solution before we had Filesystem providers. It's stuck around as a parallel set of code because we couldn't trust the providers at first. In the long run it should be removed and replaced entirely by `Files` operations. We need to test that all the functionality exists / works as expected though, and it hasn't been a high priority to do so. Particularly, I'm not sure we have a lot of faith in the HDFS NIO plugin, so we may need to keep around special cases for that. It could definitely at least be simplified a lot though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3569#issuecomment-328993020:250,test,test,250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3569#issuecomment-328993020,1,['test'],['test']
Testability,"By modifying the test I was able to isolate the error in the `org.broadinstitute.hellbender.tools.walkers.variantutils` package, which is strange because the PR did not modify the javadoc for any class in that package. The integration test runs `com.sun.tools.javadoc.Main.execute` and asserts that the output code is zero, which does not yield a useful error message. In order to produce something more meaningful I hacked the test to output the entire `stdout` and `stderr` as follows:. ```; final StringWriter out = new StringWriter();; final PrintWriter err = new PrintWriter(out);. final int result = com.sun.tools.javadoc.Main.execute(""program"", err, err, err, ""doclet"",docArgList.toArray(new String[] {}));; err.flush(); // probably not needed; String message = out.toString(); // message contains the entire stdout and stderr of the call to execute; Assert.assertEquals(result, 0, message);; ```. The output is about 2000 lines, but a lot of it is clearly innocuous. Removing lines such as; * `2022-08-16T00:09:07.2336106Z [parsing completed 1ms]`; * `2022-08-16T00:09:07.4456202Z [loading ZipFileIndexFileObject[/jars/gatk-package-4.2.6.1-56-gad9a538-SNAPSHOT-test.jar(org/broadinstitute/hellbender/tools/walkers/variantutils/ReblockGVCFIntegrationTest.class)]]`; * `2022-08-16T00:09:07.4459732Z [loading RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalIntervalArgumentCollection.java]]`; * `2022-08-16T00:09:07.4462012Z [parsing started RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalReferenceInputArgumentCollection.java]]`; * 2022-08-16T00:09:07.2322755Z [loading ZipFileObject[/gatk/gatk-package-unspecified-SNAPSHOT-local.jar(htsjdk/samtools/SAMSequenceDictionary.class)]]. brings it down to 353 lines, the majority of which look like . ```2022-08-16T00:09:07.4435974Z src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java:479: error: cannot find symbol; 2022-08-1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488:17,test,test,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488,5,"['assert', 'test']","['assertEquals', 'asserts', 'test']"
Testability,"Can you give a bit more information here? If I'm understanding correctly, it's not clear that the same issue is at play here. The original issue was that duplicate/incomplete fragments were causing queries to the workspace to fail. . In this latest instance, it seems you are appending additional samples to the existing workspace. Is that right? If so,; - are you seeing the same/similar error? That is, it's a core dump? Can you share the error messages, any logs, core dump files etc?; - did you clean up the workspace before importing? That is, remove the incomplete fragment @nalinigans identified and the duplicated ones?. My first instinct is that even if the incomplete/duplicated fragments weren't cleaned up, the incremental import shouldn't have an issue -- at least not till it gets to the consolidate phase, which only happens after all batches are imported. Sounds like you were seeing an issue at batch 3 of 4, so might have something to do with the samples in that batch...or some other import issue. You mentioned that previous imports to this particular contig failed -- were those just transient failures that worked when rerun, or was there some configuration that you changed to get that to work?. For completeness, the way I identified duplicate fragments was to do an md5sum check on some of the internal files. If any pair of fragments have the same md5sum they are likely duplicates. So, from the workspace directory, something like:. ```; find . -name ""ALT.tdb"" -exec md5sum {} \;|sort; ```; That will highlight the fragments that are potentially duplicate. To confirm that the fragments are indeed duplicates, you'll then want to take that list of potentially duplicate fragments and check that all corresponding files within each pair of potentially duplicate fragments actually have the same md5sum. I have a crude bash script that I can share if you want.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722541707:461,log,logs,461,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722541707,1,['log'],['logs']
Testability,"Can you please test this change with the `HaplotypeCaller` in protected and make sure nothing changes? In particular, can you run `HaplotypeCallerIntegrationTest` and `HaplotypeCallerEngineUnitTest` and make sure they pass with this change? This PR makes me a little nervous given the centrality of the classes touched, even though the optimization itself is simple enough...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216595202:15,test,test,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216595202,1,['test'],['test']
Testability,"Certainly the author can't just ""certify"" that the tool works without writing tests to prove it on an ongoing basis as the tool is modified -- I don't think that was what @nh13 meant (but please correct me if I misunderstood). I read that as ""if tests pass, the author certifies that the tool is in a usable state for its intended use(s)""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/382#issuecomment-93806764:78,test,tests,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/382#issuecomment-93806764,2,['test'],['tests']
Testability,"Cleaning the bam (and the sam) test files is almost done - it's taken some time because some have been like mini debugging exercises in themselves, followed by the capturing and substitution of the new expected output files for the tests. There are a few issues that still remain, however, and unfortunately I am out of time - I'm headed to an overseas meeting on Monday and will be out for two weeks. I had hoped to finish before the trip, but my BMC Bioinformatics paper came through so I had to spend time on proofs etc.. I will resume ASAP after I get back.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/569#issuecomment-122571160:31,test,test,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-122571160,2,['test'],"['test', 'tests']"
Testability,"Completing https://github.com/broadinstitute/hellbender/issues/673 will take care of the case of ""missing reference for CRAM"", but we also need to make sure we're handling the case of ""wrong reference"" elegantly (where elegantly means ""throw a `UserException` with a descriptive error message). We want a test case with a reference that is the wrong reference for a CRAM, but has a compatible sequence dictionary (so that it won't be caught by the sequence dictionary validation). Both the wrong and missing reference cases should have a simple integration test that runs, eg., `PrintReads` with `expectedExceptions = UserException.class`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374:305,test,test,305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374,2,['test'],['test']
Testability,"Considering that this PR has lasted through my absence and the holiday season, I want to take the chance of summarizing the concerns you have issued here:. --------. Resolved as requested (or at least made efforts to):. * documenting the logic and methods; documented. * emit VCF instead of custom file format; emit both VCF custom file format now. * bug in determining if alignment signature satisfies `allMiddleAlignmentsDisjointFromAlphaOmega`; bug fixed in commit b4f7568b03b91eb77d256bcfe8117001bce040ec. --------. Unresolved yet:; the fact that gap split happens after the alignment configuration scoring step is considered backwards. I agree in principle but due to AS and MQ were used in the scoring step, and split-copy leads to technically wrong AS & MQ, I originally decided to score first, then split. Splitting the gapped alignments was introduced originally to have a centralized logic in inferring type and location of the events. . The tension is that AS is used in the scoring but becomes practically useless after that. >> Correct, but I am having thoughts about this now (not to pick only one—that; would be wrong—but to ditch them altogether probably under some condition; and redo the alignment step), exactly because of this behavior I observe.; Think about the case where one originating gapped (say insertion); alignment, after splitting, has one of the two children contained in; another alignment (not its sibling, that's impossible) in terms of their; read span. Now the originating gapped alignment probably should be filtered; out, or not, because if we keep it, an insertion would be called but; apparently there are alternative explanations due to the other alignment.; I'm not sure how to deal with this case, and if this scenario is common; enough. It probably is the case that such alignments happen mostly in STR; regions, so getting the exact alignments correct there is no easy task.; ; > Is that enough of a concern to worry about. In such a case I feel like we; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-354976980:238,log,logic,238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-354976980,2,['log'],['logic']
Testability,"Did a quick test with sklearn's BayesianGaussianMixture, fitting 8 components to 2.5M 10D points generated from 4 isotropic blobs. On a Google Colab instance (which I believe are n1-highmem-2s), 150 iterations (which I think is the current maximum) completed in 14 minutes, with `%memit` reporting a memory peak of ~1.5GB. Note that convergence within the default tolerance isn't actually reached in 150 iterations for this toy data (as usual, it takes a while for the weights of unused components to shrink to zero). In any case, we'd have to compare against the number of iterations currently required to converge with the real data (and perhaps also check that the convergence criteria match up) to get a better idea of real runtime. Various tweaks to priors or other runtime options (such as k-means vs. random initialization) could also affect convergence speed. Minibatching isn't built in, but I think it should be pretty trivial to hack together something with the `warm_start` option; we could probably just do a warm start with a subset of the data. See also https://github.com/scikit-learn/scikit-learn/pull/9334.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6425#issuecomment-594205039:12,test,test,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6425#issuecomment-594205039,1,['test'],['test']
Testability,"E);; final AlignmentInterval region2 = new AlignmentInterval(new SimpleInterval(""20"", 283, 651), 383, 751, TextCigarCodec.decode(""382H369M274H""), true, 60, 23, 254, AlnModType.NONE);; final AlignmentInterval region3 = new AlignmentInterval(new SimpleInterval(""20"", 1, 413), 613, 1025, TextCigarCodec.decode(""612H413M""), true, 60, 0, 413, AlnModType.NONE);. final AlignedContig alignedContig = new AlignedContig(""asm00001:tig0001"", contigSequence, Arrays.asList(region0, region1, region2, region3), false);. final List<ChimericAlignment> assembledBreakpointsFromAlignmentIntervals = ChimericAlignment.parseOneContig(alignedContig, SVDiscoveryTestDataProvider.seqDict, true, StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigsAlignmentsSparkArgumentCollection.DEFAULT_MIN_ALIGNMENT_LENGTH, StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigsAlignmentsSparkArgumentCollection.CHIMERIC_ALIGNMENTS_HIGHMQ_THRESHOLD, true);. Assert.assertEquals(assembledBreakpointsFromAlignmentIntervals.size(), 1);; final ChimericAlignment chimericAlignment = assembledBreakpointsFromAlignmentIntervals.get(0);; Assert.assertEquals(chimericAlignment.sourceContigName, ""asm00001:tig0001"");; final NovelAdjacencyReferenceLocations breakpoints = new NovelAdjacencyReferenceLocations(chimericAlignment, contigSequence, SVDiscoveryTestDataProvider.seqDict);; }; ```. In versions of the code prior to #3752 (I think) this set of alignments was being filtered out by the method `isNotSimpleTranslocation` in the `parseOneContig` method of `ChimericAlignment`. Now that check's logic has changed and `isLikelySimpleTranslocation` returns false instead of true and so this alignment is not being filtered out any more. . When it gets to `NovelAdjacencyReferenceLocations.TanDupBreakpointsInference()` both `upstreamBreakpointRefPos` and `downstreamBreakpointRefPos` are being set to zero. It's not immediately clear to me how to fix this. A few thoughts:. - Are we supposed to be proc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3874#issuecomment-347627504:2639,assert,assertEquals,2639,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3874#issuecomment-347627504,1,['assert'],['assertEquals']
Testability,"Executive summary: . My main concern is that the amount of unsupported code is continuing to grow. Adding this PR would bring the total to about ~5k lines of WDL, Java, and test code. In comparison, the amount of corresponding supported CNV code clocks in around ~33k---this includes all of gCNV, as well! Development time has also been non-negligible and dates back to pre-4.0 release. Another concern is that the number of users of this unsupported code is also growing. In fact, it seems like we are actively pointing users to it. This seems unsustainable going forward. Finally, I don't think we have satisfactorily demonstrated which of the functions accomplished by this code (format conversion, post-hoc blacklisting, germline/""CNLOH"" tagging and imputation) are necessary or cannot be performed by existing code or more streamlined and principled methods. (Some of these functions, such as IGV conversion, are already performed by existing code.) Of those functions, I think format conversion is the only one we should retain from this code in an unsupported fashion. So if this PR introduces a useful GISTIC conversion, no harm in merging that. This all sounds like a decision for the new tech lead! @mwalker174 any thoughts? . More detailed responses follow:. > Users are already using this branch and giving me positive feedback (definitely more positive than adjusting num_changepoints_penalty_factor). I suggest merging mostly for practical reasons. It buys us more time to put in a principled solution. And this workflow is clearly marked as an unsupported prototype anyway (as are the GATK CLIs). I want to emphasize that this whole workflow is not a long-term solution. In other words, I would like to get this in and then focus on a supported solution. While it's great that users are giving positive feedback, I refer you to CellBender team's manifesto at https://github.com/broadinstitute/CellBender/commit/28f02f8dbd716aff922bb8da1e56da29347b245b. Can these users help us definitiv",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199:173,test,test,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199,1,['test'],['test']
Testability,"FilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/reso",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:2089,test,test,2089,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,1,['test'],['test']
Testability,"Fixing the tests failures. Some simple ones (test groups now ""_todo"", rename test inputs on git too). One of them is weird, and only reproduces with the command line... after many minutes of running...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/812#issuecomment-132282122:11,test,tests,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/812#issuecomment-132282122,3,['test'],"['test', 'tests']"
Testability,"For a quick analysis, I made a serialized versions of DBSNP (13572728 variants from `dbsnp_135.b37.excluding_sites_after_129.vcf`), size of VCF on disk 2175071049 bytes (2.0G). (all false postive probs are predicted, it'd be easy to measure it too). Map keys are contig names. ```; Map of String->BloomFilter with 0.001 false positive prob = 27320529 bytes (26M); Map of String->BloomFilter with 0.0004 false positive prob = 30943001 bytes (30M); Map of String->BloomFilter with 0.0001 false positive prob = 36423625 bytes (35M); Map of String->BloomFilter with 0.00004 false positive prob = 40046089 bytes (38M); Map of String->BloomFilter with 0.00001 false positive prob = 45526681 bytes (43M); Map of String->BloomFilter with 0.000001 false positive prob = 54629745 bytes (52M); Map of String->int[] of positions = 60790452 bytes (58M); List<GATKVariant> made just like the one in spark BQSR = 366463957 bytes (349M); ```. Variants from dbSNP cover 0.004 of the genome (15195436 bases of 3101804739) so if we want reasonable precision (number of false positives over all reported hits), say 0.9 precision (of 10 hits only 1 can be false) we need (1-0.9) x 0.004 false positive prob = 0.0004. For 0.99 precision (of 100 hits only 1 can be false) we need (1-0.99) x 0.004 false postive prob = 0.00004. These are approximations of course. Given these numbers, I conclude that, for now, exploring BloomFilters does not seem to make sense (too little saving and too many complications with using a probabilistic data structure - eg we'd need to use it too for the walker BQSR). It does make sense however to explore alternatives to the list of GATKVariants because it's very big when serialized (maybe Kryo does a better job but it's still a big object). A simple alternative like sorted int[] may be sufficient and has attractive properties (trivial to implement and understand, O(log) lookups, 0% false positives, small size when serialized).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1407#issuecomment-203727133:1881,log,log,1881,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1407#issuecomment-203727133,1,['log'],['log']
Testability,"For reporting the number of reads that fails each of the filters, the composed filter could be changed by a `CountingReadFilter`; using the `getSummaryLine()` method will provide the number of reads failing each of the components. Developers could have in their tools a field with the `WellFormedReadFilter` and call a new method for reporting the summary, to log a warning/debug line. For exploding depending on the tool, maybe an advance/hidden argument can be added to the filter (something like `--failOnMalformed`) to throw an exception if true; developers might add a default filter with this value equals to true if they want to enforce by default this behaviour. I think that this a simpler idea for allow the developer to choose, and give some flexibility to the user to change the behaviour as its own risk (they can disable all filters anyway, which is also risky).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699:360,log,log,360,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699,1,['log'],['log']
Testability,"Good catches, thank you! I've switched to using paging as you recommend since the code's a big shorter and simpler that way. I'll squash&merge once tests pass, unless you object.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/869#issuecomment-135875333:148,test,tests,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/869#issuecomment-135875333,1,['test'],['tests']
Testability,"Hello @cmnbroad. My current solution satisfy all the constraints and it's not too complicated, although is not as simple as a common generic class that just need to be extended. Have a look and if you like it I can implement some tests for `CountingVariantFilter`; if not, I could come back to a separate `CountingVariantFilter` with its own and/or/negate inner classes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272482490:230,test,tests,230,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272482490,1,['test'],['tests']
Testability,"Here's a debugging report so far:. A small unit test that reproduces the error if added to `NovelAdjacencyReferenceLocationsUnitTest`:. ```; @Test(groups = ""sv""); public void testZeroZeroTanDupInterval() throws Exception {; final byte[] contigSequence = ""AAGCAGCCATAAAAAATGATGAGTCCATGTCGTTTGTAGGGACATGGATGAAATTCGAAATCATCATTCTCAGTAATCTATCGCAAGAACAAAAAACCAAACACTGCATATTCTCACTCATAGGTGGGAACTGAACAATGAGATCACATGGACACAGGAAGGGGAATATCACACTCTGGGGACTGTTGTGGGGTGGTGGGAGGGGGGAGGGATAGCATCGGGAGATATACCTAATGCTAGATGACGAGTTAGTAGGTGCAGCGCACCAGCATGGCACATGTATACATATATAACTAACCTGCACAATGTGCACATGTACGCTAAAACTTAAAAGTATAATAAAAAAAAAAAAAAAGAAAAAAAAAAGAATGCAACAACAAAAAAAAAGAGTGTCTCAAAACTGCTCTATCAAAAGGCAGGTTCAACTCCGTGAGTTGATTGAACACATAACAAAGAAGTTTCTGAGAATGCTTCTGTCTATTTTTTCTGTGAAGATATTCCCGTTTCAACCATAGGTCTCAAAGTGCTCCAAATATCCACTTGCAGATTCTACAAAACGAGTCTTTCAAAACTGCTCTATCAATACGAAGGTTCAACTCTGTGAGTTGAATGCACACATCACAAAGAAGTTTCTGAGAATGCTTCTGTCTAGTTTTTATGTGAAGATATTCCCGTTTCCAATGAAAGCCTCAAAGCCATCCAAATGTCCACTTGCAGATTCTACAAAAAGAGTGTTTGAAAACTGCTCTATCAAAAGAAGATTCAACTCTGTGAGTTGAAAGCACACATCAGAAAGAATTTCCTGATAATGCTTCTGTCTAGCTTTTATGTGGAGATATTCCCGTTTTCAACGAAGGCCTCAAAGCAGTCCAAATATCCATTTGCAGGTTCTACAAAAAGAGTGTCTCAAAACTGCTCTATCAAAAGGCAGGTTAAACTCCGTGAGTTGACTGCACACATAACAAAGAAGTTTCTGAGAATGCTTCTGTCTATTTTTTCTGTGAAGATATTCCCATTTCAACTGT"".getBytes();. final AlignmentInterval region0 = new AlignmentInterval(new SimpleInterval(""21"", 96869186, 96869532), 1, 347, TextCigarCodec.decode(""347M678S""), false, 4, 9, 305, AlnModType.NONE);; final AlignmentInterval region1 = new AlignmentInterval(new SimpleInterval(""21"", 48872354, 48872986), 383, 1014, TextCigarCodec.decode(""382H375M1D257M11H""), false, 4, 73, 255, AlnModType.NONE);; final AlignmentInterval region2 = new AlignmentInterval(new SimpleInterval(""20"", 283, 651), 383, 751, TextCigarCodec.decode(""382H369M274H""), true, 60, 23, 254, AlnModType.NONE);; final AlignmentInterval region3 = new AlignmentInterval(new SimpleInterval(""20"", 1, 413), 613, 1025, TextCigarCodec.decode(""612H413M""), true, 60, 0, 4",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3874#issuecomment-347627504:48,test,test,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3874#issuecomment-347627504,2,['test'],"['test', 'testZeroZeroTanDupInterval']"
Testability,"Here's a prototype showing one way of doing this: https://github.com/broadinstitute/gatk/commit/3cf7f9078b6aedcb87a4ca77c2aa8e84e7e5fe99. The idea is that `GATKTool` has a `-spark` flag, so you can run a tool in Spark mode. The parallel `GATKSparkTool` hierarchy would disappear, and tool writers would optionally be able to support Spark as an alternative to the regular walker, by providing the relevant code in the same tool class. As an example, the `FlagStat` implementation looks like this:. ``` java; @Override; public void traverse() {; if (sparkArgs.useSpark) {; sum = getReadsRdd().aggregate(new FlagStatus(), FlagStatus::add, FlagStatus::merge);; } else {; sum = getReadsStream().collect(FlagStatus::new, FlagStatus::add, FlagStatus::merge);; }; }; ```. Note that the walker version uses the Java 8 Streams API, while the Spark version uses the RDD API. While these are different APIs, many of the concepts are similar, so it should be possible to share the underlying logic by encapsulating it in classes, in much the same way as is done by the `FlagStatus` class for `FlagStat`. It would also be possible to keep the existing `apply` method for walker versions of tools that simply iterate over each record. Removing the parallel `GATKSparkTool` hierarchy has a number of benefits, such as reducing duplication, making it easier for tool writers to add Spark support, and making it easier to share tests. Thoughts? /cc @DR, @lbergelson, @cmnbroad",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254180398:980,log,logic,980,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254180398,2,"['log', 'test']","['logic', 'tests']"
Testability,"Hey @lbergelson, @droazen or @jamesemery: sorry to keep bugging you all here, but I'm not sure what else to do here. I'm hoping we can finalize and close out this feature, or identify what is needed to do so. I recapped the state of this PR in my post above. . I could make a clean feature branch with everything in one place if you want; however, if someone with write permissions could simply approve (https://github.com/broadinstitute/gatk/pull/8871) , merge into this branch, and kick off tests that is all I need right now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8752#issuecomment-2243039952:493,test,tests,493,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8752#issuecomment-2243039952,1,['test'],['tests']
Testability,"Hey @samuelklee. That is true, the default location for a gradle test output is tests/#taskname#. I'm not sure what the problem is however. If you look at the gradle test logs there is a line pointing to where the reports live: "" Test report will be written to https://storage.googleapis.com/hellbender-test-logs/build_reports/master_20802.3/tests/test/index.html."" Those links seem to work on the latest master. We fix this by simply renaming the files before we upload them however. Is this explanation adequate or would you like the build system changed?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5029#issuecomment-406332294:65,test,test,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5029#issuecomment-406332294,7,"['log', 'test']","['logs', 'test', 'test-logs', 'tests']"
Testability,"Hi @Neato-Nick @davidbenjamin . Apologies for posting this message here. I have posted this message few days before at the regular GATK forum and also using the direct inbox option but have got no response so maybe something wrong with my account. The issue is - I have done variant calling on 384 potato samples following, mostly, GATK best ##practices and have applied hard filters to select SNPs for further usage. However, I am noticing that '--max-nocall-fraction', '--max-nocall-number' and '--max-fraction-filtered-genotypes' arguments for 'SelectVariants' are not working properly. I have tried with various cutoff settings and every time I am observing SNPs with a much larger number of genotypes (~246 out of 384 with 0.10 setting) with 'no call' than the set thresholds. I have searched the forum first but couldn't find any relevant threads. I am using the latest GATK version (4.0.7.0). I am attaching three example sets of (1) log files (2) subset vcf files and (3) vcf index file for the three main vcfs. I would appreciate if you could provide any feedback on this issue and/or if this behaviour has been observed by some other users also. The link to the original post is here:; https://gatkforums.broadinstitute.org/gatk/discussion/12688/possible-bug-in-selectvariants-tool#latest; [SelectVariantBugReport.zip](https://github.com/broadinstitute/gatk/files/2291206/SelectVariantBugReport.zip). Regards,; Sanjeev",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-413285177:941,log,log,941,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-413285177,1,['log'],['log']
Testability,Hi @RWilton i'm sorry to hear that. I suspect its unrelated given how simple this PR is but its quite possible that filter has been broken since evidently nobody was able to use and adjust it for a long time. This is the logic here that the filter uses:; `return read.isPaired() && !read.isUnmapped() && !read.mateIsUnmapped() &&; (Math.abs(read.getStart() - read.getMateStart()) >= mateTooDistantLength || !read.getContig().equals(read.getMateContig()));`. That logic is correct for what the filter is doing. It should be noted that the filter does the opposite of what you expect it to (since its intended for our SV pipeline) in that it filters out all reads that are NOT having distant mates. This means if you try to run HaplotypeCaller with this setting you will be throwing away every read pair EXCEPT the distant ones which results in mostly no reads. We should perhaps rename this filter to be a little less confusing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1103010098:221,log,logic,221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1103010098,2,['log'],['logic']
Testability,"Hi @cwhelan , I've expanded this PR to do more than what it originally was trying to fix, and separated the patches by commits as usual:. * the originally proposed fix, which brings back the annotation that are available to simple variants but go missing due to a careless bug, is now done in commit 50f1b640a31ddb528dc763b83b26a9d98dce8556; this commit also accordingly refactors the giant class `CpxVariantDetector` into three new classes; * in the 2nd commit 734516383fb665a79796de76535560fc03cb754b, I did more refactoring on how we group the descriptions for the annotation keys, and updated the test VCF files accordingly.; * because of the refactoring, the review comments were gone, so I added them back in the 3rd commit b7619c45a949dfba21d65a5ed876bc72e832aa77, which contains the comments and my replies. They come in as TODO's but are going to be removed ultimately; * in the following commits, I added tests for the CPX code path, selecting three representative cases (there's no limit how complex the scenario can go). One particular commit 224c97c7b736e94ed6b4d8b067ec830a9f8f2403 is large but most of it is for adding a flat file that contains the chromosome names in hg38 and their lengths for building a bare bone sequence dictionary used in building test data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4330#issuecomment-372761525:601,test,test,601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4330#issuecomment-372761525,3,['test'],"['test', 'tests']"
Testability,"Hi @jean-philippe-martin ,. A `Feature` in our codebase has a specific meaning that is different from ""interval"": it is a record that 1. has a location on the genome plus (typically) some metadata information about that location and 2. is in one of the formats supported by our file-parsing framework tribble and is the product of a tribble codec. A VCF record is an example of a `Feature`. . The common interface between `Feature` and `SimpleInterval` is called `Locatable`. I recommend (for now) that you simply alter your uprooted version of BQSR to take a `List<? extends Locatable>` instead of a `List<? extends Feature>` in `apply()`. This should require no code changes beyond changing method parameter types, and it will allow you to feed BQSR `SimpleIntervals` for the known sites for now, and `Features` like VCF records later on when we're ready for that. Please do return `ArtificialTestFeature` to the `FeatureDataSourceUnitTest` from which it came -- this is a very incomplete class meant only for testing purposes and not for external use.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247:1012,test,testing,1012,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247,1,['test'],['testing']
Testability,"Hi @magicDGS. I went down this same path (composition of CountingFilter) when I originally implemented CountingReadFilter, but I abandoned it for the model we currently have. I think the current model is much simpler in a number of ways. This is an interesting problem, but I would say lets just implement a straight CountingVariantFilter/tests and not try to do the common implementation. . BTW, when looking at this PR I noticed two bugs in the existing code that we should make sure not to propagate to the Variant one (feel free to fix/test these as part of this PR):. - CountingReadFilter.resetFilterCount only resets the root filter count; it needs an override in BinOp to propagate the reset call to the lhs/rhs operands.; - there is a bug in the getSummary tests/code; you can see the fix [here](https://github.com/broadinstitute/gatk/commit/9ef1458271834aed9b64a5d66f94df33f025eafb). Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272954313:339,test,tests,339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272954313,3,['test'],"['test', 'tests']"
Testability,"Hi @qindan2008 - is this the full log file that is produced, or is there more to it? If there is more to the log file can you post it? Would you mind posting one or two of your variants as well? They can be simplified - I only the need position and alleles. Also, did you happen to make any modifications to the data sources? If you enabled gnomAD, Funcotator will try to read the gnomAD data sources via the Google Cloud API, which may be slow or fail depending on your internet connection and settings. You could experience similar issues if you added another web-facing data source.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7135#issuecomment-799646869:34,log,log,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7135#issuecomment-799646869,2,['log'],['log']
Testability,"Hi @tedsharpe ! . I also commented about it on the helpdesk but should probably reply directly here. The .bam file was aligned to a reference , the same reference I used to run the tool. I was wondering If the bam still contained unmapped reads and so used ; `samtools view -b -F 4` on the file to retain only mapped reads and re-run the GATK tool. However this did not improve the situation. Best,; Domniki. error log:. 22/03/11 06:13:57 INFO SparkUI: Stopped Spark web UI at http://10.222.0.104:4040; 22/03/11 06:13:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 22/03/11 06:13:58 INFO MemoryStore: MemoryStore cleared; 22/03/11 06:13:58 INFO BlockManager: BlockManager stopped; 22/03/11 06:13:58 INFO BlockManagerMaster: BlockManagerMaster stopped; 22/03/11 06:13:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 22/03/11 06:13:58 INFO SparkContext: Successfully stopped SparkContext; 06:13:58.369 INFO FindBreakpointEvidenceSpark - Shutting down engine; [March 11, 2022 6:13:58 AM GMT] org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark done. Elapsed time: 3.28 minutes.; Runtime.totalMemory()=29312942080; java.lang.ArithmeticException: / by zero; at org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.removeUbiquitousKmers(FindBreakpointEvidenceSpark.java:640); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.addAssemblyQNames(FindBreakpointEvidenceSpark.java:507); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.gatherEvidenceAndWriteContigSamFile(FindBreakpointEvidenceSpark.; at org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.runTool(FindBreakpointEvidenceSpark.java:136); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:546); at org.broadinstitute.hellbender.engine.spark.SparkCommandLinePro",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7710#issuecomment-1064823936:415,log,log,415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7710#issuecomment-1064823936,1,['log'],['log']
Testability,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:120,test,testing,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857,5,['test'],"['test', 'testing']"
Testability,"Hi Jose,; Your system ulimit setting is too low. Please do this with root at; /etc/security/limits.conf; * soft memlock unlimited; * hard memlock unlimited; * hard nofile 20480; * soft nofile 20480; * hard nproc 40960; * soft nproc 40960; * soft stack unlimited; * hard stack unlimited. Ruzhu; -------------------------------------------; Ruzhu Chen, PhD (845) 433-8426(T/L 293-8426); Email: ruzhuchen@us.ibm.com, Mobile: (845) 337-7238; Sr. Technical Solution Architect, HPC / Genomics & Life Sciences; IBM Systems, 2455 South Road, Poughkeepsie, NY 12601. From:	Jose Sergio Hleap <notifications@github.com>; To:	broadinstitute/gatk <gatk@noreply.github.com>; Cc:	ruzhuchen <ruzhuchen@us.ibm.com>, Mention; <mention@noreply.github.com>; Date:	03/12/2020 11:37 AM; Subject:	[EXTERNAL] Re: [broadinstitute/gatk] Got ""Too many open files""; when use BaseRecalibratorSpark (#5316). Apologies on the poor report. There are no other users in these compute; nodes (I am the tester) and for all intents and purposes the ulimit is; pretty high (hard limit of 8192 max files). I am using GATK version; 4.1.4.1, although it might be the one that has been optimised for IBM; power9 systems by @ruzhuchen. Currently I am waiting for the sys admin to; increase the max files further, but I believe that this is far from ideal.; Here is the (simplified) command:. gatk --java-options ""-Xmx40g; -Djava.library.path=/bio/apps/gatk_4.1.4/gatk-4.1.4.1/libs; -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" Mutect2 -R; Homo_sapiens_assembly38.fa -I illuminaN_hg38.br.recal.bam; --max-mnp-distance 0 -O illuminaN.vcf.gz. May be I am running it wrong?. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-598269062:967,test,tester,967,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-598269062,1,['test'],['tester']
Testability,"Hi Marissa - I think we're all in agreement that we'd like to find a way to make Intel-TF the default, but whether or not we can have CNNScoreVariants require AVX to run is less clear. Naturally, we'd prefer to not have to provide a custom TF distribution for a fallback, but there are 3 cases where we may not have a choice: user with old hardware, Travis/CI testing, and GCE. We may need to provide a fallback environment for those (I'll try to get resolution on that). If it turns out we do, I'm actually not suggesting the fallback be automatic (3 in your list), just that we have a graceful failure mode and an instructive error message. . In the meantime, there is still the issue that this PR fails to even build on Travis. It looks like it produces so much output building the Docker image that it exceeds the allowable Travis build log size. That will need to be resolved, and we'll also need to understand the impact of this change on the size of our Docker image, which is already large, and continues to be a challenge for us.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429451059:360,test,testing,360,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429451059,2,"['log', 'test']","['log', 'testing']"
Testability,"Hi all, thanks again for working to integrate this code!. Saw some confusion in the comments above and just wanted to clarify: if you take a look at the VQSR-lite PR https://github.com/broadinstitute/gatk/pull/7954/commits that the current branch is rebased upon, you'll see that it contains a version of the Joint Genotyping WDL (which was put together by Megan for Ultima) along with Java code for the tools (which was written by me). Both the WDL and the code have been updated in subsequent PRs. The WDL was rewritten by me in #8074; the main difference is that we no longer run SNPs and indels filtering in ""series"", but instead run them in a single step. However, this requires that you use the same annotations for both SNPs and indels; GVS might not be ready for that just yet, since the default WARP implementation uses different annotations. (But see also the comment here: https://github.com/broadinstitute/gatk/pull/8074#issue-1423991277. The gist is we can easily reimplement Megan's/WARP's ""serial"" SNP-then-indel workflow using the simpler single-step workflow.) (EDIT: I was originally confused here, Megan’s WDL simply runs SNPs and indels separately—thanks to George for correcting me here!). Note also that test infrastructure was moved from Travis to Github Actions between these PRs, so the Travis references above have already been cleaned up. There have also been a few additional minor PRs merged in the interim, with a couple more incoming. These PRs do not fundamentally change the interfaces of the tools/WDL, however, so I think you can update to them when you're ready. Punchline: this branch should suffice for a first cut of a VQSR/VQSR-lite bakeoff, and although it is already slightly out of date, it shouldn't be too much work to get things updated after the first cut is done.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8157#issuecomment-1412640649:1226,test,test,1226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8157#issuecomment-1412640649,1,['test'],['test']
Testability,"I added integration tests for simple output and including features or verbose. While doing it, I realized that GATK 3.5 included some filters that wasn't included here, and that indels weren't tracked, so I changed also the code to fit the previous implementation. Back to you @akiezun.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1836#issuecomment-221651158:20,test,tests,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1836#issuecomment-221651158,1,['test'],['tests']
Testability,"I added the split size option back, and wrote a test for `dirSize`. All feedback should have been addressed now. Back to @droazen.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1432#issuecomment-173633156:48,test,test,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1432#issuecomment-173633156,1,['test'],['test']
Testability,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:78,test,tests,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208,1,['test'],['tests']
Testability,"I am working on a test for #1572. I am not sure what a test for #3069 would look like, or if it is really necessary. We simply changed the way GKL outputs warnings and information. Any ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3177#issuecomment-312295566:18,test,test,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3177#issuecomment-312295566,2,['test'],['test']
Testability,"I can't reproduce this yet. I tried downloading the jar, unzipping it, and running the example command you gave, but I can't reproduce what you're seeing. I modified it for my local files:; ```; java -jar gatk-package-4.2.5.0-local.jar \; GenotypeGVCFs \; -R /Users/louisb/Workspace/gatk/src/test/resources/large/Homo_sapiens_assembly19.fasta.gz \; --variant gendb:///Users/louisb/Workspace/gatk/output \; -O out.vcf \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 \; --max-alternate-alleles 6 \; --force-output-intervals 20 \; -L 20 \; --only-output-calls-starting-in-intervals \; --genomicsdb-shared-posixfs-optimizations; ```; It runs to completion on my machine. ; My md5sum matches yours so that's not the problem. It's not clear to me what's going on here. Are the previous releases working on your cluster still?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042010522:292,test,test,292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042010522,1,['test'],['test']
Testability,"I created a panel of normals from 90 WGS TCGA samples with 250bp (~11.5M) bins, which took **~57 minutes** total and produced an **11GB PoN** (this file includes all of the input read counts---which take up 20GB as a combined TSV file and a whopping 63GB as individual TSV files---as well as the eigenvectors, filtering results, etc.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:387,test,test,387,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503,1,['test'],['test']
Testability,"I did a simple experiment and changed the version of Java used in the non-Docker (""17"", although again I'm not sure what this actually resolves to) to that used in the Docker (17.0.1+12). This causes both non-Docker and Docker tests to now fail, rather than just the Docker tests; see https://github.com/broadinstitute/gatk/pull/8174#issuecomment-1402974502. Moreover, the test failures produce exactly the same discrepant numerical results. I think we can probably conclude that the expected test results were generated with ""17"" and that changing to 17.0.1+12 generates different results. This is not too unreasonable; see the Slack thread linked in https://github.com/broadinstitute/gatk/pull/8111#issuecomment-1331407680, for example, which shows that we might be getting into pretty hairy territory and that even changes to things like how HotSpot Intrinsics are implemented in each JVM can cause the numerical differences we see here. So perhaps we can either 1) change the Docker version to the version corresponding to ""17"" or 2) change the non-Docker version to 17.0.1+12 and update the expected results?. Not sure about the failing WDL test yet, but hopefully this is enough to get us started!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8035#issuecomment-1403016955:227,test,tests,227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8035#issuecomment-1403016955,5,['test'],"['test', 'tests']"
Testability,"I don't know how to accomplish that with gradle. Is just keeping the test jvm going all we need? Or does the ui shutdown after each test? We could add an infinitely running ""test"" in a special test group to If we want to be keep the test jvm open. Alternatively if we really need to be able to run tests and then view the UI afterwards we could put together something using https://github.com/hammerlab/spree. It's a minor pain to set up, I had weird ruby packaging problems getting meteor installed, but it solves the problem of ""how do we collect spark logs in a usable way"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676:69,test,test,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676,7,"['log', 'test']","['logs', 'test', 'tests']"
Testability,"I finished the implementation for the draft `SlidingWindowWalker` (I should implement an example and an integration test, but I would like to wait till some issues are solved). made a ""TODO"" about the way in which the intervals are constructed, because I will need a that `ReadShard` have a way to construct a shard without `ReadSource` (either null or empty source), just in case that the implemented `SlidingWindowWalker` does not require reads. @droazen, could you review and give me some feedback about this, because this class is important for other parts of GATK?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-215710163:116,test,test,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-215710163,1,['test'],['test']
Testability,I fixed the artifact uploading as well now. Everything should be good provided tests are passing. It turned out to be something really simple and dumb.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6007#issuecomment-506503209:79,test,tests,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6007#issuecomment-506503209,1,['test'],['tests']
Testability,"I fully understand, and realize this isnt a priority for the group. Nonetheless, just getting the test data seems like it should be a simple thing if at all possible. i dont know the full reasoning behind why the GATK3 test data are not public, but I have no need to share it beyond myself if that makes this easier.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358031120:98,test,test,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358031120,2,['test'],['test']
Testability,"I had a look at the source code of [HypergeometricDistribution](HypergeometricDistribution). If I am right, we are doing the following. We are invoking `logProbability()` for all possible `x[0][0]`. For a table with large numbers, we have to compute logBinomial for many iterations (see line 202–222 in the HypergeometricDistribution source code). Typically logBinomial calls three logGamma and each logGamma calls `log()` twice. This involves lots of computation and is not the fastest way to implement Fisher's exact test. A faster way to implement the test takes the advantage of two observations. 1) When carrying the test, we are calling hypergeo(i,m+n,m,k), hypergeo(i+1,m+n,m,k), ... in order, and we can derive hypergeo(i+1,m+n,m,k) from hypergeo(i,m+n,m,k) by simply multiplying a number. This will be much faster than doing the full hypergeo->logBionomial->logGamma->log computation for each `i`. 2) For a large table, often when `i` is sufficiently smaller or larger than `x[0][0]`, the hypergeo probability is small enough to be ignored from the sum. It is not necessary to calculate hypergeo for the full range of `lo<=i<hi`. This trick can also dramatically reduce the number of iterations for large tables. htslib has a [exact test implementation](https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/kfunc.c#L201) that considers the two observations. I understand that the time spent on the `FisherExactTest` class probably won't show up at all in a profiler. I am not requesting to improve the implementation now. Just let you know the tricks. In addition, when we use this class for other purposes, a fast exact test may become a good thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212:153,log,logProbability,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212,14,"['log', 'test']","['log', 'logBinomial', 'logBionomial', 'logGamma', 'logProbability', 'test']"
Testability,"I have a few lines of code that dynamically sets the log4j level for command line tools to match the existing VERBOSITY arg, It seems to work in simple testing so I don't think we need to downgrade to do it. Let me know if you want the code, or if you haven't started you can reassign this to me.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/243#issuecomment-115810391:152,test,testing,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/243#issuecomment-115810391,1,['test'],['testing']
Testability,"I have a good feeling about numerical instability from this point forward because:. * My terminology was lazy. It's not really ""numerical instability,"" which is a deep and frightening topic, but rather just plain old finite precision, which is not nearly so hydra-headed a problem.; * I learned the general rule for avoiding finite precision problems with a qual score, which is: always calculate probabilities of alleles being absent. Previously I was calculating the probability that samples had an allele and subtracting (in log space) that from 1. The problem with that is that for very good GQs this probability is so closed to 1 that quals can become infinite. In this PR we add up the probabilities of genotypes that don't have the allele, which is small but non-zero and everything works fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434769078:528,log,log,528,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434769078,1,['log'],['log']
Testability,"I have to say, too bad we don't have a mechanism in place that allows for the full reference, e.g. NIO only the contigs or portions thereof that are needed for a particular analysis @droazen @cmnbroad. That would make making test data so much easier. I would imagine this is simple to implement, given the reference is indexed. Such a feature would be useful for cloud analyses. I have to jump through ridiculous hoops to make small test data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373131560:225,test,test,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373131560,2,['test'],['test']
Testability,"I implemented a very simple test for tracking Ns. Is it enough, @akiezun?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1833#issuecomment-220032813:28,test,test,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1833#issuecomment-220032813,1,['test'],['test']
Testability,I made you a page to start collecting such reminders at https://github.com/broadinstitute/gatk/wiki/Checks-and-tests-guidelines.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-468819341:111,test,tests-guidelines,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-468819341,1,['test'],['tests-guidelines']
Testability,"I propose to still hide from the command line and docs the example walkers. They are meant only for developers, to show how to use some kind of walkers and have a running tool for integration tests. Having then in the command line will generate software users to run them instead of use them for developmental purposes... In addition, I think that this is a good moment to also generate a sub-module structure (as I suggested in #3838) to separate artifact for different pipelines/framework bits (e.g., engine, Spark-engine, experimental, example-code, CNV pipeline, general-tools, etc.). For the aim of this issue, this will be useful for setting documentation guidelines in each of the sub-modules: e.g., example-code should be documented for developers, but not for the final user; experimental module should have the `@Experimental` barclay annotation in every `@DocumentedFeature`; etc.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346291829:192,test,tests,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346291829,1,['test'],['tests']
Testability,"I ran `FindBreakpointEvidenceSpark` and did some high-level checks to see if there are any opportunities for performance improvements. (cc @tedsharpe @cwhelan). This is the command line I ran. (Earlier I had run more executors with smaller memory settings, but the job didn't complete then.); ; ```bash; ./gatk-launch FindBreakpointEvidenceSpark \; -I hdfs:///user/$USER/broad-svdev-test-data/data/NA12878_PCR-_30X.bam \; -O hdfs:///user/$USER/broad-svdev-test-data/assembly \; --exclusionIntervals hdfs:///user/$USER/broad-svdev-test-data/reference/GRCh37.kill.intervals \; --kmersToIgnore hdfs:///user/$USER/broad-svdev-test-data/reference/Homo_sapiens_assembly38.dups \; -- \; --sparkRunner SPARK --sparkMaster yarn-client --sparkSubmitCommand spark2-submit\; --driver-memory 16G \; --num-executors 5 \; --executor-cores 7 \; --executor-memory 25G; ```. What does FindBreakpointEvidenceSpark do, from the perspective of Spark?. * [runTool] filter out secondary and supplementary alignments; * [getMappedQNamesSet] filter out duplicate reads, reads that failed vendor checks, unmapped reads; * Job 0 [ReadMetadata] mapPartitions to find partition stats; * Job 1 [getIntervals] filter and multiple map partitions to find breakpoint intervals ; * Job 2 [removeHighCoverageIntervals] mapPartitionsToPair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you want",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:383,test,test-data,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884,4,['test'],['test-data']
Testability,"I see `Timeout (30 minutes) reached. Terminating ""./gradlew jacocoTestReport""`. It's not clear to me how my changes could have introduced a deadlock or similar problem. . I ran the full test suite (`./gradlew test`) locally to take a look and it passes. Took 20min. Running `SeekableByteChannelPrefetcherTest` by itself also passes, unsurprisingly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-277399501:186,test,test,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-277399501,2,['test'],['test']
Testability,"I see. . Yes. That's what I'm planning on (except that `AssemblyContigAlignmentsConfigPicker` is upstream of this unit), and here's the thought for why:; * I'd try to place the alignment picking step in a single place as much as possible, this makes improvements to the alignment picking/filtering step easier; * the size-based filter can be tuned, even by an CLI argument, this would affect the number of segments in the CPX logic, and the alt_arrangment annotations, and the simple variants re-interpreted by `CpxVariantReInterpreterSpark`, but it won't affect the alt haplotype sequence, which IMO is what really is important. ; * I'm developing a downstream variant filter, which hopefully can cut down the false-positives. And for the question of ""why 2 instead of 1"", I think what you are suggesting is to change; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 2;; if (one.getSizeOnRead() >= MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; to; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 1;; if (one.getSizeOnRead() > MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; Am i right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353:426,log,logic,426,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353,1,['log'],['logic']
Testability,I think it's simply that HDF5Library uses `org.apache.log4j.LogManager` rather than `org.apache.logging.log4j.LogManager`.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3763#issuecomment-340867902:96,log,logging,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3763#issuecomment-340867902,1,['log'],['logging']
Testability,"I think that a proper example would be the one in the tutorial from @sooheelee (https://software.broadinstitute.org/gatk/blog?id=7847, see also https://github.com/broadinstitute/gatk/issues/3104#issuecomment-314886000). I divided the PRs for the `IndelRealignment` into 4 different sections for better review (2 components of indel-realignment, `RealignerTargetCreator`and `IndelRealignment`). This strategy is because I dissected the pipeline into the easy `RealignerTargetCreator` to found regions worth to look at (this could be marked as experimental/beta before the indel-realignment is in) and the more complicated and component-based `IndelRealigner` (the same as with other tools, this can be marked as experimental/beta until a really good coverage is achieved - in the meantime, I have some test with the current data in the repository and the GATK3 counterpart). There are two parts that are usable outside `IndelRealigner` that are worthy to separate into two commits, and might be useful for other tools/downstream projects: `ConstrainedMateFixingManager` and `NWaySAMFileWriter`. That's the reason of making the port in split PRs. One option can be to have the PRs open, and reviewed independently without acceptance until every component is ready. Otherwise, I think that an experimental tag would be good until we find a good set of tests for edge cases. Does this approach make sense for you, @cmnbroad?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366187605:801,test,test,801,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366187605,2,['test'],"['test', 'tests']"
Testability,I think this will be difficult and we don't have a python unit testing framework. Let's try for some simple tests in #4375.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4464#issuecomment-459532819:63,test,testing,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4464#issuecomment-459532819,2,['test'],"['testing', 'tests']"
Testability,"I think without a matched normal, there is not much you can do for high purity samples in LOH regions. Flipping the binomial test to filter against the null hypothesis of hom (rather than a null of f = 0.5, as in GetHetCoverage) seems to work well otherwise. Expanding the allele-fraction model to include hom sites is an option, but then you would be guided by the prior. Closing for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2856#issuecomment-335586260:125,test,test,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2856#issuecomment-335586260,1,['test'],['test']
Testability,"I tried a quick upgrade of our guava dependency from 18 -> 22, but it ends in test failures. It looks like at least one of our hadoop dependencies requires guava <= 18. I'm not totally clear if it's an issue for hadoop-core or only in hadoop-minicluster which is a library we use for running tests. If you're not using hdfs I think you won't have any problems including 22, but I'm afraid we can't upgrade our default version without some work. . Hopefully hadoop 3.x will solve the problem in general by shading their internal version of guava. ; https://issues.apache.org/jira/browse/HADOOP-14284, https://issues.apache.org/jira/browse/HADOOP-10101. It sounds like you have a reasonable workaround, let us know if you have further issues with it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516:78,test,test,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516,2,['test'],"['test', 'tests']"
Testability,"I tried to do it, and I'm afraid that it won't be trivial as I expected: because it is a facade, there is not accesibility to `Logger.setLogLevel()` and this is required to set the verbosity level in the command line. After explore a bit the code, it seems that [`LoggingUtils`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java) is the only place where a concrete implementation should be used. My suggestion is to move this class to a package that could be excluded by the backend user (because it contains methods to change the logging of log4j, I suggest `org.apache.logging.log4j`), which implements a simple interface/abstract class `org.broadinstitute.hellbender.utils.LoggingUtils` to set the log level (LoggingUtils.setLoggingLevel(final Log.LogLevel verbosity)`. The default implementation (that could be used by final users callid`super.setLoggingLevel(final Log.LogLevel verbosity)`) could setup the htsjdk and the java.util.logger.Logger. This implementation requires to change the `CommandLineProgram` to have a setter for the `LoggingUtils` to use, that could be set in `Main` (as in my PR for improve the extensibility of this class). The only pronblem is that it requires to be initialize with a simple implementation class of `LoggingUtils`, which should use the default. I think that this design does not break the behaviour of GATK, but introduce more complexity in the code. If you think that this is worthy, I could implement it today. @lbergelson, I'm not able to run Spark tools in a cluster yet, neither in gcloud dataproc, sorry. I'll wait for your answers on this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062:600,log,logging,600,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062,4,['log'],"['log', 'logger', 'logging']"
Testability,"I utilized Mutect2 during clinical tumor testing, and the example I provided earlier clearly represents a false positive site. Regrettably, Mutect2 failed to accurately identify it, thereby leading to the inclusion of such false positive sites in clinical medical reports. This outcome is entirely unacceptable. If it is inappropriate to categorize these false positives as STRs, are there alternative methods available for determining these erroneous sites?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8340#issuecomment-1613985690:41,test,testing,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8340#issuecomment-1613985690,1,['test'],['testing']
Testability,"I was mistaken about this not being faster - I was using a counting function that Spark can optimise by pulling onto the map side so that the records don't go through the shuffle. I changed this to simply dump the processed reads so they have to go through the shuffle, and I got the following timings when processing a 121GB BAM file.; - With shuffle: 27 min; - No shuffle (two scans over input): 24.7 min (8% saving); - No shuffle (one scan over input): 17 min (37% saving). The version that does two scans is faster, but not hugely so. Removing a scan is possible, but requires the use of a sequence dictionary to find the end points of contigs. I've done this in the latest version of my branch (https://github.com/broadinstitute/gatk/compare/tw_overlap_partitioner), but there are more edge cases to test. Before I do this, however, it would be worth trying this approach with the Haplotype Caller to see if it works, and if it is appreciably faster. If the number of reads is filtered significantly so only a fraction go through the shuffle, then the performance gains will be smaller, and may not in fact be worth the increase in code complexity. @droazen, what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-249590040:805,test,test,805,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-249590040,1,['test'],['test']
Testability,"I'd rather keep the message more generic, and think of the check as simply defining what a valid `CopyRatio` object can be: an interval associated with a finite double value. One might imagine that someone would try to create such an object that does not originate from a BAM (perhaps for test data, or for imputing missing values in pre-existing data, etc.). This check says that they must create it with some finite value. A more appropriate place for the sort of message you suggest is in the relevant denoising method. In the edge case you encountered, you used a BAM that was almost completely uncovered in all bins at the specified resolution, resulting in a sample median of zero. Since one of the steps in standardization is dividing by the sample median, this results in a divide by zero. I've added the corresponding check.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4292#issuecomment-365726746:289,test,test,289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4292#issuecomment-365726746,1,['test'],['test']
Testability,"I'm new at this, so I may be missing something, but it looks to me like there is an issue with the conversion code in GenomicsConverter.makeSAMRecord() in com.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:276,test,tests,276,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033,4,['test'],"['test', 'tests']"
Testability,"I'm not completely opposed to that way of dealing with this, but I'm not yet convinced either. . I'm not sure I see how having an extra argument is somehow shorter than having one special value that is included in the description of the original argument. As in:. --trimWhatever | -trimWvr -- bla bla bla; default w; min x max y; to disable trimming, use z. . As for the documentation auto-generator showing the two args together, that is dependent on setting up the arguments so that the code specifies they are related, and adding some logic to the auto-generation to pull related arguments together. (As a contributing developer to a documentation auto-generator --the GATKDocs-- I can tell you that is not necessarily trivial and adds even more moving parts.) This also generates additional complexity for third-party developers of wrappers (such as Galaxy). Finally, it can be a source of confusion for users who are trying to look up an argument called ""-dont-Trim-whatever"" since presumably it's only going to be listed under T (-Trim-whatever) and not under D in the alphabetical list. Or should it be listed twice? . A reference manual can be very ""nice"" and helpful, and it must be organized in the most intuitive way possible, especially since there is no way we can provide examples that cover every single use case under the sun (trust me, there's not).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/143#issuecomment-71122930:538,log,logic,538,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/143#issuecomment-71122930,1,['log'],['logic']
Testability,"I'm sorry, I don't know another tool that does the same thing. If you could provide some samples that cause the problem that would actually be really helpful. It *should* be a simple fix so I could try to get a path out soon, but it's always faster with a test case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8427#issuecomment-1646250031:256,test,test,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8427#issuecomment-1646250031,1,['test'],['test']
Testability,"I've added a new end-to-end test for SelectVariants that writes to GCS. Sadly, the IntegrationTestSpec class uses Files throughout, so it wasn't possible to do this simply without first completely refactoring IntegrationTestSpec (which should probably be its own pull request). . Doing this refactoring would have the advantage that changing existing end-to-end tests from local to GCS would be trivial. For now instead I went with an ad-hoc approach. It works, and the test passes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-455686612:28,test,test,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-455686612,3,['test'],"['test', 'tests']"
Testability,"I've added some non-trivial (code-wise, logical wise very simple) code to single out slow assemblies more obviously. That is, generating another txt file collecting the runtime for those slow ones.; Example here ; /user/shuang/experiments/NA12878_PCR-_30X; /user/shuang/experiments/NA12878_PCR-_30X/assembly_betterLogging_longOnes",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1997#issuecomment-245295941:40,log,logical,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1997#issuecomment-245295941,1,['log'],['logical']
Testability,"I've added the additional test you requested, and confirmed that it passes. The `SkipExceptions` are there to skip JBWA tests on platforms for which we don't have a build of the library -- I've extracted a `skipJBWATestOnUnsupportedPlatforms()` method to make this clearer.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1847#issuecomment-220757508:26,test,test,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847#issuecomment-220757508,2,['test'],"['test', 'tests']"
Testability,"I've addressed @davidadamsphd's feedback. The tests were passing on Friday, but now the build is failing due to https://github.com/broadinstitute/gatk/pull/1185, so that should be merged before this one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1174#issuecomment-158912039:46,test,tests,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1174#issuecomment-158912039,1,['test'],['tests']
Testability,I've addressed all the feedback and all tests are passing so I'm going to squash and merge this now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3452#issuecomment-325607019:40,test,tests,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3452#issuecomment-325607019,1,['test'],['tests']
Testability,"I've implemented the Gaussian-kernel binary-segmentation algorithm from this paper: https://hal.inria.fr/hal-01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM m",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:425,log,log,425,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666,1,['log'],['log']
Testability,"I've incorporated your feedback to #5601 @ldgauthier and the commit is undergoing tests. Please feel free to merge the PR if you accept the changes and have no further comments. . As for [Article#11074](https://software.broadinstitute.org/gatk/documentation/article?id=11074), given we have addressed the original issues (e.g. Latex), I am going to consider the additional recommendations as something for the not-so-near future. Would that be okay with you @ldgauthier?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5409#issuecomment-458728389:82,test,tests,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5409#issuecomment-458728389,1,['test'],['tests']
Testability,"I've made some improvements to this PR, including:; - Made it easier to use the `joinOverlapping` method by making the function you supply only have to worry about one interval (shard) at a time. This simplifies the callers code, so PileupSpark (for example) is now shorter.; - Added some documentation. I've also used the same technique to improve `AddContextDataToReadSpark` so that references are filled in on a per shard basis, rather than per read. In tests on a 6.6GB file I managed to get BaseRecalibratorSpark's runtime down from 10.61 minutes to 3.73 minutes, which is over 60% faster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2190#issuecomment-250750843:457,test,tests,457,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2190#issuecomment-250750843,1,['test'],['tests']
Testability,I've ran this one in my test and it's clear the current code doesn't have the problem of prefetching a prefetcher.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2643#issuecomment-298468693:24,test,test,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2643#issuecomment-298468693,1,['test'],['test']
Testability,"IMO. In fact ideally I'd like to write a design doc or something similar, and only code after the design is agreed on. --------. > I would of course prefer not to have to have a hard filter on length. This would mean we would never call a large inversion even if it exists. . Answer: Totally agree. Now looking back, it get clearer to me that this proposal contains two parts: the filtering part, and the breakpoint linking part, separated into two major classes `InversionBreakendPreFilter` and `LinkedInversionBreakpointsInference`. That being said, it doesn't make much sense to separate them into two PRs because _currently_ the filtering part is designed around the linking part, i.e. it is trying to check which BND's are suitable to the logic implemented in the linking part, and if the logic isn't applicable to an BND, the BND simply slips through without generating any new interpretations. So `InversionBreakendPreFilter` is a filter and a classifier at the same time, it function is really diverting different BND's to be handled by different logics, and it definitely should be improved.; If you buy this argument, I am also fully aware of the code design issue that it is preferable to NOT divert&mdash;gather&mdash;send through different handlers like it currently is for calling variants from the assembly contigs, instead it should be a single stream pass through all the BND's. I'll try to follow the preferred design. > What about some other filters more specifically aimed at the artifacts that cause these false large calls? I think it's a good idea to check annotations -- ie. do the mates lie at two regions that are segmental duplications of each other, or one side of the mate looks like a transposable element insertion? I guess it's ok to put in a tool with this limit temporarily, though. Answer: Agree. And I think these kind of checking are not only good, but a mandate for a good caller, i.e. to take advantage of prior knowledge. I am also thinking about improving it ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:1434,log,logics,1434,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929,1,['log'],['logics']
Testability,"If you're interested in BWASpark tool I might wait a bit. There are a lot of issues with it as it currently stands, it's one of the least tested tools we have. We have someone working on a different more efficient implementation of the bwa bindings that may eventually be integrated into mainline gatk, so we've sort of stopped most development on BWASparkEngine until we're clear on the direction that the new work is going to take.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119998:138,test,tested,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119998,1,['test'],['tested']
Testability,"In good news, the spark mailing list announced that spark master builds and runs all tests on 11 now. So it looks like support for java 11 coming in spark 3.0. When that is is going to be release isn't clear though. We should start moving to support java 11 in advance of that so we're ready when it releases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6053#issuecomment-525337068:85,test,tests,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6053#issuecomment-525337068,1,['test'],['tests']
Testability,"In the course of investigating https://github.com/broadinstitute/gsa-unstable/issues/1409 a few things came up that I want to document for when it's time to test the new model:; - Throw away the spanning deletions alleles! They shouldn't affect the QUAL anymore since we won't be using the independent alleles approximation, but I don't want them mucking with the site type and choice of prior (SNP vs INDEL); - For the purposes of QD for VQSR, we should be using AS_QD very shortly, in which case the choice of prior for mixed sites will be clear because we're evaluating per-allele; - For the purposes of QUAL for emission, at mixed sites I'm in favor of continuing to apply the SNP prior in accordance with the doctrine of maximal sensitivity until VQSR",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1697#issuecomment-230464015:157,test,test,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1697#issuecomment-230464015,1,['test'],['test']
Testability,"In the interest of getting this merged, I've addressed the remaining blocking issue via documentation and naming: tool is now named `ConvertHeaderlessHadoopBamShardToBam`, and the docs make it clear what kinds of Hadoop bam shards it should be used with, and which it shouldn't. Will merge once tests pass.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1278#issuecomment-221101168:295,test,tests,295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1278#issuecomment-221101168,1,['test'],['tests']
Testability,"Initial commit by Mark DP in 2009: . > simpleComplement function() in BaseUtils. Generic framework for clipp…; > …ing reads along with tests. Support for Q score based clipping, sequence-specific clipping (not1), and clipping of ranges of bases (cycles 1-5, 10-15 for example). Can write out clipped bases as Ns, quality scores as 0s, or in the future will support softclipping the bases themselves. https://github.com/broadinstitute/gsa-unstable/commit/d6385e0d884cbd80c34e16e848297c3694f85a5a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/263#issuecomment-95101660:135,test,tests,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/263#issuecomment-95101660,1,['test'],['tests']
Testability,It does in this very simple test I ran on dataproc.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-266636027:28,test,test,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-266636027,1,['test'],['test']
Testability,It looks like it made the tests substantially slower.... I'm not totally clear on why. Maybe because it has to re-optimize code every time it restarts the jvm.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6093#issuecomment-521372663:26,test,tests,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6093#issuecomment-521372663,1,['test'],['tests']
Testability,"It's not clear to me what code path you're going through when using a `gs://` URI for the input bam in your second test. `CountReadsSpark` calls `GATKSparkTool.getReads()` which calls `JavaSparkContext.newAPIHadoopFile()`, but the question is how Hadoop-BAM handles your `gs://` URI. In other parts of the GATK (eg., `ReferenceTwoBitSource`) we call into `BucketUtils.openFile()`, which handles GCS URIs directly by calling into `GcsUtil.open()`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213153356:115,test,test,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213153356,1,['test'],['test']
Testability,"It's not immediately obvious why the Spark tests are failing. It could be something to do with the Spark context which is shared between tests, and which may be picking up some state that is not cleared from one test to the next. The tests are passing on Travis, which also runs all of them. Do they fail if you run them individually?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5511#issuecomment-448275058:43,test,tests,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511#issuecomment-448275058,4,['test'],"['test', 'tests']"
Testability,"Java implementation of segmentation is now in the sl_wgs_segmentation dev branch, with a few simple unit tests. I'll expand on these and add tests for denoising in the future, but for now we have a working revised pipeline up through segmentation. The CLI is simply named ModelSegments (since my thinking is that it could eventually replace ACNV). I ran it on some old denoised exomes. Runtime is <10s, comparable to CBS. Here's a particularly noisy exome:. CBS found 1398 segments:; ![cbs](https://user-images.githubusercontent.com/11076296/30165095-cdf6251a-93ac-11e7-91fb-dcc8f48fe07f.png). Kernel segmentation with a penalty given by a = 1, b = 0 found 1018 segments:; ![kern](https://user-images.githubusercontent.com/11076296/30165106-dbbe0b40-93ac-11e7-99ec-5d58d8417d8b.png). Kernel segmentation with a penalty given by a = b = 1 (which is probably a reasonable default penalty, at least based on asymptotic theoretical arguments) reduced this to 270 segments :; ![kern-smooth](https://user-images.githubusercontent.com/11076296/30165113-e2b545a8-93ac-11e7-97a9-a692e43ebbdf.png). The number of segments can similarly be controlled in WGS. WGS runtime is ~7min for 250bp bins, ~30s of which is TSV reading, and there is one more spot in my implementation that could stand a bit of optimization, which might bring the runtime down. In contrast, I kicked off CBS 45 minutes ago, and it's still running... @LeeTL1220 this is probably ready to hand off to you for some WDL writing and preliminary evaluation. ; Although I can't guarantee that there aren't bugs, I ran about ~80 exomes with no problem. We can talk later today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936:105,test,tests,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936,2,['test'],['tests']
Testability,"Just some notes before I forget:. Using these test samples, I made some tweaks to the ploidy model that made it more robust to incorrect ploidy calls and added a simple modeling of mosaicism:. ```` ; # per-contig bias; bias_j = Gamma('bias_j',; alpha=100.0,; beta=100.0,; shape=(ploidy_workspace.num_contigs,)); norm_bias_j = bias_j / tt.mean(bias_j). # per-sample depth; depth_s = Uniform('depth_s',; lower=0.0,; upper=10000.0,; shape=(ploidy_workspace.num_samples,)); ; # per-sample probability of mosaicism; pi_mosaicism_s = Beta(name='pi_mosaicism_s',; alpha=1.0,; beta=50.0,; shape=(ploidy_workspace.num_samples,)). # per-sample-and-contig mosaicism factor; f_mosaicism_sj = Beta(name='f_mosaicism_sj',; alpha=10.0,; beta=1.0,; shape=(ploidy_workspace.num_samples, ploidy_workspace.num_contigs,)); norm_f_mosaicism_sj = f_mosaicism_sj / tt.max(f_mosaicism_sj, axis=1).dimshuffle(0, 'x'). # per-contig mapping error; eps_j = HalfNormal('eps_j', sd=0.01, shape=(ploidy_workspace.num_contigs,)). # negative-binomial means; mu_sjk = depth_s.dimshuffle(0, 'x', 'x') * t_j.dimshuffle('x', 0, 'x') * norm_bias_j.dimshuffle('x', 0, 'x') * \; (ploidy_workspace.int_ploidy_values_k.dimshuffle('x', 'x', 0) + eps_j.dimshuffle('x', 0, 'x')); mu_mosaic_sjk = norm_f_mosaicism_sj.dimshuffle(0, 1, 'x') * mu_sjk. # ""unexplained variance""; psi = Uniform(name='psi', upper=10.0). # convert ""unexplained variance"" to negative binomial over-dispersion; alpha = tt.inv((tt.exp(psi) - 1.0)). def _get_logp_sjk(_n_sj):; _logp_sjk = logsumexp([tt.log(1 - pi_mosaicism_s.dimshuffle(0, 'x', 'x')) + commons.negative_binomial_logp(mu_sjk, alpha.dimshuffle('x', 'x', 'x'), _n_sj.dimshuffle(0, 1, 'x')),; tt.log(pi_mosaicism_s.dimshuffle(0, 'x', 'x')) + commons.negative_binomial_logp(mu_mosaic_sjk, alpha.dimshuffle('x', 'x', 'x'), _n_sj.dimshuffle(0, 1, 'x'))],; axis=0)[0]; return _logp_sjk. DensityDist(name='n_sj_obs',; logp=lambda _n_sj: tt.sum(q_ploidy_sjk * _get_logp_sjk(_n_sj), axis=2),; observed=n_sj); ````. Brie",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-371334890:46,test,test,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-371334890,1,['test'],['test']
Testability,"Let's hear what others say, but I think I would strongly prefer to simply take over VariantEval in another repo if this was something you'd consider. I'd likely do much of what you propose anyway (certainly WRT testing); however, perhaps not the microscope we went through with the core GATK changes earlier. On plugins: I like what seems to be shaping up w/ Barclay. I carried over the Stratifier and Evaluator as plugins because it seems like it would make sense to allow tools to provide extensions (VariantEval, our tool, does). If I took this PR a step further, I would have migrated many arguments currently top-level on VariantEval into the plugins themselves (a good feature in Barclay). As an aside: I dont think VariantAnnotator is migrated yet, but we have many GATK3 plugins related to annotation, and hope that tool retains Annotator plugins when it get migrated. My impressions of barclay are probably a little out of date. I agree the main argument parsing framework is pretty robust. Specifically on plugins, it seems a little less so, or at least there are not many tools I visibly see exercising that part of the code. For example, there really should be a default implmentation or base class between Barclay's plugins and ReadFilter plugins. I'm guessing if more tools in GATK4 were using plugins this would have happened. I created something like this for VariantEval, and without a ton of work that could probably get generalized; however, doing so would throw a lot higher bar on me and as noted above I'm trying to take on less, not more at the moment. If we do take over VariantEval, I'm certainly happy to try to contribute code and experiences to improve the core, through more targeted PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407202501:211,test,testing,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407202501,1,['test'],['testing']
Testability,"Looks great!. One quick note: I don't get the idea behind `Poisson` -- shouldn't we simply use negative binomials w/ modeled `mu_sj` and `alpha_sj`, evaluated at observed counts (`tt.arange(min_count, max_count + 1)`), and weighted with the number bins for each count (`_hist_sjm`)? i.e. if one observes an empirical distribution `P_obs(x)` rather than `x` draws, then the appropriate max likelihood objective function is `\sum_x P_obs(x) log P_model(x | \theta)`. Perhaps this is exactly what you've done and I don't get it. Another quick note: what I had in mind was _either_ modeling `mu_sj` at quantized ploidy states, _or_ let the ploidy state be unrestricted w/ a penalty via. a Bernoulli process (possibly w/ different per-contig penalties to account for e.g. higher rate of X/Y loss). We have enough samples in the cohort to select the quantized model (and those samples pin down the per-contig biases `b_j`). The samples that do not conform to quantized ploidy states can then choose whatever (variable) ploidy state they wish by paying a (hefty) price. We would also need to mask contigs that have variable ploidy calls from gCNV.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376286536:439,log,log,439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376286536,1,['log'],['log']
Testability,"Lots of refactoring was done for the Segmenter classes in #6499. At least for segmentation, all use cases (CR-only, AF-only, CR+AF, single-sample, multi-sample) now go through `MultisampleMultidimensionalKernelSegmenter`. `AlleleFractionKernelSegmenter` and `CopyRatioKernelSegmenter` classes still exist, but both simply call the `MultisampleMultidimensionalKernelSegmenter` class; this was done so preexisting tests for those two classes could be reused. I'm fine with calling this done. We can always open a new issue in the unlikely event we refactor the modelling code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5625#issuecomment-900609908:412,test,tests,412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5625#issuecomment-900609908,1,['test'],['tests']
Testability,"Merging this now to have usable VCF NIO support in master -- continuous tests to prove that the wrapper is applied will be added in a separate PR, but my ad-hoc tests on the latest version of this branch suggest it's working fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2393#issuecomment-277697090:72,test,tests,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2393#issuecomment-277697090,2,['test'],['tests']
Testability,"My GenotypeGVCFs run for a single chromosome returned the following completion statement:; 18:54:40.516 INFO ProgressMeter - Traversal complete. Processed 606308 total variants in 75.2 minutes. However, there are only 46814 variant rows (excluding 52 header rows) in the corresponding vcf file. Does the above figure of 606308 correspond to a multiple of 'variants x number of samples'?. Also, there are only 16863 lines in my log file, does this mean that the 'Current Locus' column in the log file doesn't correspond to a single genomic location (bp) in the fasta file?. I am curious to know what is the relation between all these figures to fully understand what is happening while processing the gCVF files. Also, on the inbreeding coefficient warning issue, I understand from your @Neato-Nick feedback that the variants with these warnings may still be fine and can be retained. However, this still leaves me worrying that out of 384 samples the locus doesn't even have 10 samples for generating the required metrics. Such variants won't be of any use for downstream analyses anyway where any variants with more than 80% missing samples will be removed. Therefore, I wish to seek some more information about this 10 sample thing - does it have some other context or does it literally mean that there are only less than 10 samples carrying that variant?. Regards,; Sanjeev",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-409255344:427,log,log,427,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-409255344,2,['log'],['log']
Testability,N0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQXBwbHlCUVNSLmphdmE=) | `91.667% <100%> (ø)` | `6 <2> (+1)` | :arrow_up: |; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvR0FUS1Rvb2wuamF2YQ==) | `91.772% <80%> (-0.795%)` | `77 <3> (+3)` | |; | [...institute/hellbender/utils/gcs/GATKGCSOptions.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvR0FUS0dDU09wdGlvbnMuamF2YQ==) | `0% <0%> (-66.667%)` | `0% <0%> (ø)` | |; | [...lbender/engine/datasources/ReferenceAPISource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <0%> (-62.264%)` | `8% <0%> (-26%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-18.75%)` | `6% <0%> (ø)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-290039637:2651,test,test,2651,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-290039637,1,['test'],['test']
Testability,"Need some guidance here. The CompareSAMs tool was not propagating the validation stringency. I have a fix for that, but that alone doesn't fix the compareBAMFiles test in BaseRecalibrationIntegrationTest.java, since that uses SamAssertionUtils.assertSamsEqual, which also doesn't propagate (or accept) a validation stringency. Changing SamAssertionUtils to use either SILENT or LENIENT does fix the integration test, and all the other tests pass, but it seems like a relaxing of the stringency, and I'm not sure it should be necessary to the BQSR test. If relaxing the stringency for BQSR test _IS_ the right path, one possibility is to add a new method to SamAssertionUtils that accepts a validation stringency argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266:163,test,test,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266,6,"['assert', 'test']","['assertSamsEqual', 'test', 'tests']"
Testability,"No problem – I can see how this would be challenging to review – maybe it’s not even practical – if you decide it’s best just not to use it that’s OK - working on these issues has been a valuable learning experience for me. . I sent an archive with the new validations and the diffs between the old and new bams to akiezun. Although in many cases the files shared types of errors, I had to look at each file individually to take into account the particular errors in each file and how to fix them without (to the best of my knowledge) interfering with the purpose of the test. I did write a python script to use where necessary for converting multiple unpaired reads in a file to single reads, and I used bash scripts to call the picard tools to convert multiple files at a time from bam to sam for editing and then back again after they were modified. In some cases I had to modify the values in test output files to match the values produced by the test using the modified bams/sams, or just capture the new output files and use them to replace the old with the new. In two cases where file format errors appeared to be necessary but the filename did not indicate this, I renamed the files to make this clear. From: Louis Bergelson ; Sent: Thursday, August 20, 2015 2:13 PM; To: broadinstitute/hellbender ; Cc: nenewell ; Subject: Re: [hellbender] Issue 569 - bam and sam file cleanup. (#809). @nenewell Sorry this has been sitting. We've been trying to figure out how to review this one. Could you describe how you made the changes? Did you script it or go through by hand and modify them all?. —; Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051:571,test,test,571,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051,3,['test'],['test']
Testability,"Not sure this is ready for a complete review yet, but wanted to get feedback early (not a lot of testing here).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6274#issuecomment-556364217:97,test,testing,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6274#issuecomment-556364217,1,['test'],['testing']
Testability,"Now that I've spent a day surveying the landscape of p-values, I'm thinking what must've happened is that an undocumented mid-p-value correction was added to the one-tailed p-value we claim to calculate. This is probably a fine thing to calculate, but such a correction is not discussed in the annotation docs nor in the Wigginton paper cited there. Nor is this what is calculated by any of the other implementations out there, as far as I can tell. So we could keep the original calculation, while further clarifying what exactly we are calculating in the docs (and add references, if appropriate). In this case, we can just keep the cleanup and improved tests as a bonus. (As a further bonus, we won't lose our old friend 3.0103!). Or we could move to this calculation. In this case, I believe we would match the bcftools ExcHet p-value (although I still need to check this claim for correctness). I'm not sure if the original intent was to be more/less conservative in retaining sites during hard filtering prior to VQSR. But since the filtering threshold used in Best Practices seems very conservative, I would guess that we wanted to err on the side of not throwing out possible variants, even if they are pretty out of HWE. Which makes the choice of a mid p-value puzzling, since it's strictly smaller and will thus lead to more rejections (I think, if I've got everything the right way around!). Happy to go either direction. In the end, I have just reaffirmed my dislike for p-values, which I had hitherto thought to be saturated.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-893597356:656,test,tests,656,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-893597356,1,['test'],['tests']
Testability,"OK - PedigreeValidationType is now set in the constructor and is final. This does not separate the two intertwined codepaths around PedigreeFile vs. FounderIds, but that was a pre-existing problem. It doesnt doesnt change the pre-existing weirdness around the timing of setting pedigreeFile and/or founderIds within GATKAnnotationPluginDescriptor, where PedigreeAnnotation gets special treatment. I dont think this makes that situation any worse. if you still have concerns on this proposal, I actually think I could make our code work if you simply exposed a protected getPedigreeFile() method on PedigreeAnnotation. I can make the SampleDB instance in my code without needed to share code here. It seemed useful to expose some of that code to avoid duplication, but if it's going to over-complicate we can remove it. Also: that one test failure seems potentially unrelated (https://travis-ci.com/github/broadinstitute/gatk/jobs/510624560)? A compile issue with javadoc?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853986169:834,test,test,834,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853986169,1,['test'],['test']
Testability,"OK, I experimented a bit with removing the R install from the base image and adding the R dependencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:269,test,tests,269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954,4,['test'],"['test', 'tests']"
Testability,"OK, that's reasonable. I'll dig into the other test changes. I can answer a few:. - Regarding passing the VariantWalker: I agree that's not an improvement by itself, but I would argue it's not that much different than it was. My plan is to pass a VariantEvalContext object, which would obscure any need to have knowledge of the walker. In an attempt to keep this PR simpler, I didnt complete that work. I do expect to make a second PR in relatively short order, once we get this resolved. - With respect to testEvalTrackWithoutGenotypesWithSampleFields and the different reference: I think the issue is that the old version (master GATK branch) didnt validate as strictly. When switching to MultiVariantWalkerGroupedOnStart, the reference is required, and the tool will error if the contigs dont match. VariantEval on the master branch didnt really need the reference for anything, and was apparently more permissive if it didnt line up. It probably preferentially grabbed the dictionary from the VCF header. I will look into those other questions",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-744698072:47,test,test,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-744698072,2,['test'],"['test', 'testEvalTrackWithoutGenotypesWithSampleFields']"
Testability,"OK, the first test run I tried was with 1kb bins and *no additional normals*. Coverage takes about an hour to collect per BAM and ploidy inference takes about 10 minutes. A few things:. 1) Looks like we are concordant with the truth CN on X for all but 3/40 of the samples. The GQs for these discordant calls are low (~3, 23, and 25 compared with ~400 for most of the others). 2) However, we are striking out on over half of the samples on Y. We mostly call 1 copy when the truth calls 0. Mehrtash thinks this is because a) I didn't mask out any PARs or otherwise troublesome regions on Y and b) I didn't include any other normals. I'll try rerunning with a mask first, then with other normals, and then with both. Hopefully this should clear up with just the mask. 3) There are a few samples where we strike out because the truth calls 2 copies on Y and we call 1. Mehrtash pointed out that this is most likely because the prior table we put together assumes Y can have at most 1 copy. So hopefully these are trivially recovered once we relax this. 4) The GQs are weirdly high on 1, X, and Y compared to the rest of the autosomes. @ldgauthier any idea why this might be? If there's no reason, then something funny is going on within the tool. I haven't gotten a chance to plot any of the counts data yet, either, which may make things more obvious. I'll do this today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-364234449:14,test,test,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-364234449,1,['test'],['test']
Testability,"On a whim I took the latest code from master and commented out the two lines in HaplotypeCallerEngine:257-258 that disable phsyical phasing if `emitReferenceConfidence()` is false, and tried running HC to generate a genotyped VCF with phase. At least on a simple test of a ~200bp locus with a pair of phased variants it appears to do the right thing and not cause any errors. I know testing calling in one small locus isn't exactly comprehensive, and I'm trying now to call a larger set of regions and compare the calls generated to expected phase. Does anyone recall why this restriction was in place? I'm hoping that perhaps it was needed at the time, but isn't now and was just left in place because nobody needed it removed? I see the lines in question were last touched by @droazen in April 2016, but even that commit seems to be a large scale moving around of code rather than a commit that addressed this specific issue. I'm going to open a PR to remove those lines - mostly so I can have the tests run up in CI, and see if anything breaks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470618640:263,test,test,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470618640,3,['test'],"['test', 'testing', 'tests']"
Testability,"On the first question, we definitely appreciate how much work this will take. Often, porting the code is the easy part; developing new tests and test data can be a huge effort. I can try to find out if it would be possible for you to take the tool over - I know this kind of thing has come up before for other tools, but I'd have to ask around to find that out. @vdauwera do you have input on this ?. As for the plugins, currently in your branch `VariantStratification` and `VariantEvaluator` are modeled as Barclay command line plugin descriptors, and I was questioning whether thats necessary. Being a plugin is not necessarily required - `ReadFilter` and `Annotation` are both plugins, but they didn't have to be, and it takes quite a bit of work (again, mostly test development) to get a plugin right. Also, I'd consider the Barclay plugin framework to be pretty developed at this point, so I'd be curious to learn more about what issues you see. And yes, definitely don't check any of the large GATK3 test files into the repo, even temporarily. Take a look at [General guidelines for GATK4 developers](https://github.com/broadinstitute/gatk#dev_guidelines) if you haven't already. As you pointed out, new GATK4 tests that use smaller files would have to be developed. We'd want those to be included, and passing tests on the CI server, before we started reviewing the branch, so we know we're reviewing code that works and is covered by tests as much as possible. The second commit in my list above would have only your GATK3 java test files, etc (but not the big files, which you appear to have locally). The third commit would have your ported tool code, as well as the new test code, with the new tests enabled, as well as the smaller input files and expected results files. At the end we'd remove commit #2.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407185633:135,test,tests,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407185633,10,['test'],"['test', 'tests']"
Testability,"Overall this looks good to me. I've added a few comments inline. Note that I haven't reviewed for style particularly, or consistency with the existing codebase. > 5) There are unit tests for all code except for the skeleton itself. This could be as simple as a Spark variant of `ReadsPreprocessingPipelineIntegrationTest`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/850#issuecomment-134164842:181,test,tests,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/850#issuecomment-134164842,1,['test'],['tests']
Testability,"Overall, this looks fine, just a few minor comments. Two broader questions/issues:; - Does the test data exercise all of the code paths and edge cases?; - In general, putting the majority of testing in integration tests instead of unit tests is bad pattern. It have several bad consequences (1) it becomes less clear which cases are being tested (2) it's slower than just running unit tests and (3) it makes it unhelpful to (perhaps someday) move to a testing framework that only runs tests relevant to the code directly affected (because all integration tests must be run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490:95,test,test,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490,9,['test'],"['test', 'tested', 'testing', 'tests']"
Testability,Please make this option hidden if it's only being kept for testing purposes (and document clearly that that is the case). Users should not have access to options that are not expected to have value.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2231#issuecomment-316842338:59,test,testing,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2231#issuecomment-316842338,1,['test'],['testing']
Testability,"Rationale for engine changes:; This tool opens a large number of feature files (TSVs, not VariantContexts) and iterates over them simultaneously. No querying, just a single pass through each.; Issue 1: When a feature file lives in the cloud, it takes unacceptably long (several seconds, typically) to initialize it. A few seconds doesn't seem like a long time, but when there are large numbers of feature files to open, it adds up. This is caused by a large number of codecs (mostly the vcf-processing codecs) opening and reading the first few bytes of the file in the canDecode method. To avoid this I've reversed the order in which we test each codec, checking first if it produces the correct subtype of Feature, and only then calling canDecode. If you don't know what specific subtype you need, you can just ask for any Feature by passing Feature.class. It's much faster that way.; Issue 2: Each open feature source soaks up a huge amount of memory. That's because text-based feature reading is optimized for VCFs, which can have enormously long lines. So huge buffers are allocated. The problem is compounded for cloud-based feature files for which we allocate a large cloud prefetch buffer. (Though that feature can be turned off, which helps a little.) But the biggest memory hog is the TabixReader, which always reads in the index, regardless of whether it's used or not. Tabix indices are very large. To avoid this, I've created a smaller, simpler FeatureReader subclass called a TextFeatureReader that loads the index only when necessary. The revisions allow the new tool to run using an order of magnitude less memory. Faster, too.; Issue 3: The code in FeatureDataSource that creates a FeatureReader is brittle, and tests for various subclasses. To allow use of the new TextFeatureReader, I added a FeatureReaderFactory interface that allows one to ask the codec for an appropriate FeatureReader.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1284340770:637,test,test,637,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1284340770,2,['test'],"['test', 'tests']"
Testability,"Ready for second pass review, @lbergelson. Now the implementation is much more simple than the previous one, and I added unit tests for the codec.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224670233:126,test,tests,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224670233,1,['test'],['tests']
Testability,"Rebased and squashed on top of sl_wgs_acnv_headers_docs. Here is the log of squashed commits, for reference:. ````; commit 3eda4b18888f38249be39f99901d8453a4de50d6; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Dec 28 14:56:27 2017 -0500. updated command lines for WDL tests for C29. commit 7ce1369943cce4ae9cfb5e96455d18d3960e9b77; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Dec 28 13:30:21 2017 -0500. use C29 and decrease gcnv_max_training_epochs. commit 68772cba486b44ebc8cf8bfc2b600c1e8a406c61; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Sun Dec 17 19:20:05 2017 +0330. documentation update of GermlineCNVCaller and DetermineGermlineContigPloidy. commit c032281f8c43a80e4ec8cb96eb66397ad2acf9b7; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 15 18:14:35 2017 -0500. Fixed imr kebab case in WDL, moved argument classes, removed GenomeLocParser, fixed up gCNV WDL readme. commit be84a804f6ab6fbb815995db9c116d1db950ab8b; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 13:55:08 2017 -0500. removed extra comma in gCNV Case WDL test JSON. commit cb379b866d425f12f5525ecb28ad0b636a528d44; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 12:58:59 2017 -0500. added missing cpu parameters to gCNV Case WDL tasks. commit eed85f6c70f4a7f15e0765b5f15a1bf8541c151e; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 11:26:31 2017 -0500. disabled some gCNV WDL tests. commit 6d8ca07fef41518b5b157fb9a214d4536c617156; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 10:54:54 2017 -0500. fixed DenoiseReadCountsIntegrationTest files. commit adfbef12f2ab90f93b49a4f786979549648e1f22; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Mon Dec 11 02:22:56 2017 -0500. removed CNV evaluation code from this branch. commit 18c8d31f39a1964474c5d7b12ee8cbfafc4ac9e2; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Sun Dec 10 00:19:58 2017 -0500. GS VCF parser outputs dict for samples ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:69,log,log,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,2,"['log', 'test']","['log', 'tests']"
Testability,"Regarding GenomicsDbImport with intervals, here is a quick test. Again, the thing I'm trying to evaluate is whether it matters how I chunk the genome for GenomicsDBImport->GenotypeGVCFs. Downstream of this, I would pass the workspace to GenotypeGVCFs, with --only-output-calls-starting-in-intervals. The concern is whether we have variants spanning the intervals of two jobs, and whether separating the jobs would impact calls. In this example, GenotypeGVCFs would run over 1:1050-1150. For example, if we had a multi-NT variant that spanned 1148-1052, we'd want that called correctly no matter what intervals were used for the jobs. I tried using running GenomicsDBImport with -L over a small region, or I ran SelectVariants on the gVCF first (which behaves a little differently), and then used that subset gVCF as input to GenomicsDBImport, where GenomicsDBImport is given the entire contig as the interval. The resulting workspaces will be slightly different, with the latter containing information over a wider region (GenomicsDBIport truncates start/end of the input records to just the target interval). . So if either of these workspaces is passed to GenotypeGVCFs, using --only-output-calls-starting-in-intervals and -L 1:1050-1150:. I think any upstream padding doesnt matter. If you have a multi-nucleotide polymorphism that starts upstream of 1050 but spans 1050, this job wouldnt be responsible for calling that. The prior job, which has an interval set upstream of this one should call it. I think GenomicsDbImport's behavior is fine here. If you have a multi-NT variant that starts within 1050-1150, but extends outside (i.e. deletion or insertion starting at 1148), this could be a problem. The GenomicsDB workspace created with the interval 1:1050-1150 lacks the information to score that, right? The workspace created using the more permissive SelectVariants->GenomicsDBImport contains that downstream information and presumably would make the same call as if GenotypeGVCFs was given ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221558244:59,test,test,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221558244,1,['test'],['test']
Testability,"Regarding the non-Docker integration tests failing earlier today, I think this was because the R packages were added to the Travis cache in #3101. @cmnbroad cleared the cache to see if we could reproduce a compiler error introduced in #3934 on Travis (for the record, we could reproduce it on my local Ubuntu machine and gsa5, but not on Travis). This removed the cached getopt dependency, which then caused tests to fail. See #4246.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4209#issuecomment-359999441:37,test,tests,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4209#issuecomment-359999441,2,['test'],['tests']
Testability,"Removed the `splitContextByReadGroup()`, simplified methods and testing exception thrown by `splitBySample()`. Back to you for review again, @akiezun.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1774#issuecomment-219648141:64,test,testing,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1774#issuecomment-219648141,1,['test'],['testing']
Testability,"ScoreVariantAnnotations:. Scores variant calls in a VCF file based on site-level annotations using a previously trained model. TODOs:. - [x] Integration tests. Exact-match tests for (non-exhaustive) configurations given by the Cartesian product of the following options:; * Java Bayesian Gaussian Mixture Model (BGMM) backend vs. python sklearn IsolationForest backend; (BGMM tests to be added once PR for the backend goes in.); * non-allele-specific vs. allele-specific; * SNP-only vs. SNP+INDEL (for both of these options, we use trained models that contain both SNP and INDEL scorers as input) ; - [x] Tool-level docs. Minor TODOs:. - [x] Parameter-level docs.; - [x] Parameter/mode validation.; - [x] Double check or add behavior for handling previously filtered input, clearing present filters, etc. Future work:. - [ ] The `score_samples` method of the sklearn IsolationForest is single-threaded. See (possibly stalled) PR at https://github.com/scikit-learn/scikit-learn/pull/14001 and some workarounds using e.g. `multiprocessing` ibid.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948563:153,test,tests,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948563,3,['test'],['tests']
Testability,"See some of my findings about numerical differences across 8/11/17 and possible causes in this old Slack thread: https://broadinstitute.slack.com/archives/C1HH1V5EC/p1657634295565369 We’re starting to get into some relatively hairy issues there, IMO!. But just in case it wasn’t clear: 1) None of these numerical differences should be scientifically concerning in the end, and 2) I think we still have numerical reproducibility within each fixed Java version (although if we happen to see any evidence to the contrary, please point to them here). So I don’t think we have too much to worry about once the test infrastructure settles.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8111#issuecomment-1331407680:605,test,test,605,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8111#issuecomment-1331407680,1,['test'],['test']
Testability,"Since we never actually look to see if something IS an optical duplicate and only care about the total number, we could just output a single annotation on one read in the best read pair with the number of optical duplicates found for that set of reads. It would make the code simpler but maybe not make as much sense logically?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126755071:317,log,logically,317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126755071,1,['log'],['logically']
Testability,"Some comments/questions for the review:; - I'll add a separate ticket to rewrite the integration tests, all of which pass and most of which are disabled since they require access to large files on the broad file system. In the meantime I need to add a couple of small tests to get the coverage back up, and would like to get the CR process started.; - I ported a bunch of support files but need feedback on whether they're in the right location.; - Somewhere I saw something that said GATK no longer supports .ped files ? If not, what should the replacement be in the tests require pedigree input?; - Is it a requirement to support Ploidy > 2 ? The current GATK tool, and thus the HB tool, do not; - I did not port the WalkerTestSpec.disableShadowVCF? Is that needed in Hellbender ?; - Are there other headers I should be applying to the output variant file ?. Command Line Arguments:; - I didn't port the GATK command line argument ""-no_cmd_line_in_header"". Should I ? And if not, should the command line args automatically be propagated to the output vcf file ? I didn't see GATK do this anywhere.; - There was one test that used --variant:dbsnp on the command line but I couldn't find the code that processed that in GATK, not sure what the means on the command line.; - I replaced ""-U LENIENT_VCF_PROCESSING"" with ""--lenient"" (testFileWithoutInfoLineInHeaderWithOverride needs this to pass).; - I replaced ""-L"" with --interval since HB seems to use -L for ""lane"" ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027:97,test,tests,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027,5,['test'],"['test', 'testFileWithoutInfoLineInHeaderWithOverride', 'tests']"
Testability,"Some offline discussions have led us to the conclusion that this is best handled by tools upstream. Adapters should not be simply soft-clipped, so it shouldn't be the responsibility of M2 or HC to include logic to remove adapters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6346#issuecomment-575334816:205,log,logic,205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6346#issuecomment-575334816,1,['log'],['logic']
Testability,"Sorry @droazen, the previous commit had an error in the tests. I'm rebasing/squashing to make a clear PR and when all check pass (except CLOUD), you can review if you have time. Thank you very much.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-246346004:56,test,tests,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-246346004,1,['test'],['tests']
Testability,"Sorry, but this bug still isn't fixed as of v4.2.6.1. Reproduce as follows:. ```; --read-filter MateDistantReadFilter; --mate-too-distant-length 1500; ```. Instead of a run-time exception (as in v4.2.5.0), HaplotypeCaller simply produces no variant calls at all. Expected behavior would be to exclude paired-end mappings whose TLEN exceeds the parameterized value. Perhaps there is an implementation bug, unrelated to the original problem, that contains faulty logic for doing this. Thanks...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1102943692:461,log,logic,461,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1102943692,1,['log'],['logic']
Testability,"Tests with `TEST_DOCKER = true` failed, I'm not entirely clear why. Here's a bit of the log:. > Building 85% > :test > Resolving dependencies ':jacocoAgent'aven.org/maven2/org/jacoco/org.jacoco.agent/0.7.7.201606060606/org.jacoco.agent-0.7.7.201606060606.jar; > Building 85% > :test > 207 KB/233 KB downloaded> Building 85% > :test > 0 tests completed> Resolving dependencies ':testRuntime':test FAILED",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367:88,log,log,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367,7,"['log', 'test']","['log', 'test', 'testRuntime', 'tests']"
Testability,"Thank you @vruano for your diligent review. I've implemented logger classes to encapsulate the metrics classes. Unfortunately the metrics classes must remain public in order to write output using `MetricsUtils.saveMetrics()`, but at least the tools aren't using them directly. There are two logging class groups - one for Filter and one Score. For Filter, there is an interface `PSFilterLogger` that is implemented by a file-logging class `PSFilterFileLogger` and a dummy class `PSFilterEmptyLogger` that does nothing. There are analogous classes for Score, but there is no Empty logger because it's not actually necessary. This adds a lot of new classes (maybe you can think of a better way) but usage has been greatly simplified. As we discussed in person, I don't think there is a faster way to count the reads in Spark. If you wanted to count the reads as they pass through, you would have to use some kind of atomic type that would be slow. Also it may be impossible to account for cases when tasks fail and restart. @lbergelson @droazen In this PR, I wanted to use htsjdk's MetricsFile and MetricBase classes for writing metrics to a file. I notice that these classes are mostly used for picard-related things. Is this the preferred way to do things? They do force you to expose public variables and also use an upper-case naming convention. On the other hand, they are somewhat convenient.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-334308160:61,log,logger,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-334308160,4,['log'],"['logger', 'logging']"
Testability,Thank you at @ldgauthier for volunteering to review and for all of the feedback. The Travis checks are still in progress but the only code change is with the logger type.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5601#issuecomment-456978262:158,log,logger,158,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5601#issuecomment-456978262,1,['log'],['logger']
Testability,"Thanks @davidbenjamin for the feedback and sorry for the slow response. We have been working on improving PairHMM by adding AVX-512 (#3615) and FPGA (#2725) implementations. . We are also adding AVX2 (#3701) and AVX-512 (future PR) Smith-Waterman, which will improve the performance of Mutect2. We have the data above and will provide benchmarking results of your Mutect2 command with these improvements.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2562#issuecomment-338732871:335,benchmark,benchmarking,335,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2562#issuecomment-338732871,1,['benchmark'],['benchmarking']
Testability,"Thanks @droazen, I suspected that was the case from looking at the history. Though it's not clear to me from @eitanbanks' commit why he would disable it for non-ERC modes. FWIW my PR looks to have only failed where the HC tests are comparing against existing files, and the existing files don't have phasing (whereas newly generated test files do). I'm going through and double-checking that that is the case, and will hopefully amend that PR shortly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470680198:222,test,tests,222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470680198,2,['test'],"['test', 'tests']"
Testability,"Thanks @ilyasoifer ! Was the test bam you added aligned with minimap2? If not, we should make sure to add at least one simple HaplotypeCaller regression test that takes an actual minimap2-aligned input.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8337#issuecomment-1558029097:29,test,test,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8337#issuecomment-1558029097,2,['test'],['test']
Testability,Thanks @jean-philippe-martin! I've addressed your other feedback points and submitted a new pull request against the main repo (so that tests are run): https://github.com/broadinstitute/hellbender/pull/827. I'm closing this one now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/804#issuecomment-131862850:136,test,tests,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/804#issuecomment-131862850,1,['test'],['tests']
Testability,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:93,test,tests,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791,6,['test'],['tests']
Testability,"Thanks @lbergelson! I agree that it might be good to break into more layers—could be worth talking to SV team and seeing what lessons they learned in putting together their hierarchy of images. Also, note that I pushed the install of miniconda into the base, but I did not push down the setup of the GATK conda environment itself (which takes the bulk of the time during the main-image build, as it requires lots of downloading). I think I commented elsewhere that a good strategy might be to set up the conda environment with the non-GATK python dependencies in the base, and then update the environment via a pip install of the GATK python packages in the main image. This would let us make python code changes without having to rebuild the base, but might require a bit of scripting to create a final yml for non-Docker users. I also agree that it would be nice to cut down the Travis time, might be worth taking a look at other strategies to do that—could save everyone a lot of time!. Will try to add the test you suggested sometime tomorrow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-621487662:1010,test,test,1010,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-621487662,1,['test'],['test']
Testability,"Thanks for all the feedback Adam. I got a bunch of the metrics code written; today and hopefully once I have that I can actually test this code and port; the other tests. I will merge that PR into this one and fix these changes; and get back to you. On Thu, Jul 16, 2015 at 9:04 PM, Adam Kiezun notifications@github.com; wrote:. > Assigned #631 https://github.com/broadinstitute/hellbender/pull/631 to; > @tovanadler https://github.com/tovanadler.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/pull/631#event-358132720.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/631#issuecomment-122149505:129,test,test,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/631#issuecomment-122149505,2,['test'],"['test', 'tests']"
Testability,"Thanks for the quick review, @ldgauthier!. I don't think my fix will address any non-determinism in the integration tests. I'm inclined to just do better with the new tools---there does seem to be enough duct tape in the integration tests regarding re/setting the RNG so that the exact-match tests consistently pass. As for learning how to run the WARP tests, I think that would indeed be pretty useful---for anyone that might have to update code for VQSR or the new tools in the future! Can we teach everyone to fish? Isn't this what CARROT is for?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1061830649:116,test,tests,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1061830649,4,['test'],['tests']
Testability,"Thanks for the response, @droazen! Technically, yes, that would be satisfactory & accurate... and if that's easiest, I'm fine with that. . From a user perspective though, it might be beneficial to report the first occurrence of this error, as that's most likely where I would go back to do future testing & troubleshooting. That being said, all of the overlapping intervals are already outputted to stderr, so all the information is retained regardless, and I could just look through the logs to find that first problematic interval. As an aside, I find it a bit weird that the overlapping interval message shows up as a _warning_ even when using the `-no-overlaps` option (I would assume it would be an error, not a warning). In my experience, most errors cause the program to quit immediately. So, perhaps instead, if this warning were an _error_ when using the `-no-overlaps` option, the program would stop after the first occurrence of this error... and then the error message would be accurate. Maybe that was the original intent of this code. But, again, if that requires much more testing & changes, when a quick rewording would also suffice, there's no need. If it's simply a rewording, I'm happy to make a pull request. Let me know what you think.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8103#issuecomment-1329747570:297,test,testing,297,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103#issuecomment-1329747570,3,"['log', 'test']","['logs', 'testing']"
Testability,Thanks for the review @droazen. I've addressed all your feedback in the latest commit. I'll merge once the tests pass.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1432#issuecomment-175651593:107,test,tests,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1432#issuecomment-175651593,1,['test'],['tests']
Testability,"Thanks for the review and running those tests, @ldgauthier! Will restore the aforementioned GnarlyGenotyperIntegrationTests and update a few other exact matches in the rebase this afternoon. You also asked above if there was a theoretical reason to change the threshold. Since it seems the original was relatively arbitrary (at least from what I've been told, happy to be corrected), I think we can leave it. The new annotation is strictly larger, so we will then be slightly more conservative about keeping sites if we leave the threshold fixed. You can think of this as a slight change in the decision boundary in genotype-count space---perhaps I can add some plots to this thread this afternoon to demonstrate. In practice, what we care about is whether: 1) many sites flicker across the change in boundary after hard filtering, and/or 2) these sites result in discrepancies post-VQSR. I think the tests you ran suggest that we don't need to worry much about the second issue, and I can take a closer look later to check about the first (which will depend simply on the number of samples and the allele frequency spectrum). We can also take a basic look at how things might change with e.g. more samples using the aforementioned plots.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-914471272:40,test,tests,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-914471272,2,['test'],['tests']
Testability,"Thanks for your feedback, @cmnbroad. In my case, I think that `IntegrationTestSpec` is a good way of avoid complicated code to test tool results, but it is true that it have some problems (one that I had was the usage for testing programs where the outputs are determined by a prefix in the command line, but with different suffixes). I think, from the API user point of view, that a class like `IntegrationTestSpec` to facilitate program output testing (including user exceptions) will be nice for developing purposes. Nevertheless, this is just a convenience that I asked for here, but I can try to solve the issues with the `BaseTest` instead. By the way, I would love to have this interface in GATK at least for now, because several of my tools rely on the `IntegrationTestSpecs` for development...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889:127,test,test,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889,3,['test'],"['test', 'testing']"
Testability,"Thanks, @droazen. While I understand the effects of the funding landscape on academic resources, it seems to me this is a full capitulation of the GATK developer team given a serious bug, especially in light of the fact that the team seems to have enough resources to continue working on Mutect3. Mutect2 has been one of the best performing variant callers of the last years and is a major reason for the Broad's good reputation in the oncology bioinformatics field. GATK and Mutect2 are used by hundreds of institutions in clinical practice, affecting thousands of real patients' lives. Almost all of these institutions are likely to use clinical WES assays due to cost reasons and will thus have been directly affected by this issue _for the last three years_. Also, almost all of these institutions will never learn of this bug since they likely trusted in the developers to have proper functional regression tests in place. If this is indeed the best the Broad can do as an institution, then I will take your offer of providing a build of Mutect2 4.1.8.1 with the log4j vulnerability patched out - thank you. The one thing that I am asking for in addition (for the sake of the overall oncology bioinformatics community), however, is that you conduct a best effort to notify organizations (universities, hospitals, and biotechs/pharmaceuticals that you know are using Mutect2) and best-practise workflow owners (Nextflow, Snakemake, WDL, CWL etc. that include Mutect2) of the forced downgrade. Also, I think it makes sense to include a very prominent warning into the Mutect2 READMEs and GATK best practice documentations and guides. I know that this is work, too, but with success comes responsibility, and I can just hope that providing proper warnings uses less developer bandwidth than applying binary search to find out which of these [10 commits between 4.1.8.1 and 4.1.9.0 that are touching variant filtering (see below)](https://github.com/broadinstitute/gatk/compare/4.1.8.1...4.1.9.0) bro",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1535909226:912,test,tests,912,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1535909226,1,['test'],['tests']
Testability,"Thanks, @lbergelson. I was also thinkinhg that this code is mostly deprecated, but I wanted to ported as is for the first pass review. I just need to support the new mpileup version (unique sample, because if not it is more difficult), because the consensus one is deprecated. I will update the codec and add some tests for it. In addition, ~~I was thinking to create a list of `PileupElement` inside the feature to make easier to compare the internal pileup, but with ""reads"" of one base-pair.~~ Update to this: `PileupElement`is difficult to generate without including `GATKRead` simple implementation, and I think that it is not worthy. On the other hand, I will improve the walker itself. I will tell you when I finished with the changes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224419331:314,test,tests,314,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224419331,1,['test'],['tests']
Testability,"Thanx for feedback. I obviously don’t know much if anything about the underlying logic but; have had enough experience to look in unusual places. Have a good weekend. RDB. On Fri, Nov 1, 2019 at 4:24 PM JP Martin <notifications@github.com> wrote:. > @rdbremel <https://github.com/rdbremel> for ""mystery 1"" see issue #5447; > <https://github.com/broadinstitute/gatk/issues/5447>. This should be an; > innocuous warning that it can't initialize the Google Cloud Storage code; > and shouldn't cause a failure unless you try to access paths that start; > with ""gs://"". Going through the Cloud initialization steps described in the; > README should remove the warning (though again, this isn't required if you; > don't need to read files from the cloud).; >; > Mystery 2: For what it's worth, ""GC overhead limit exceeded"" indicates; > that the VM was spending too much time in GC. Running low on memory is a; > possible cause but generating too many small objects or being stuck in an; > infinite loop of allocation/deallocation are others. In the past these have; > been caused by inputs that were malformed in some way. This isn't the place; > for this discussion though, please file a separate issue since it's a; > separate bug.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/6182?email_source=notifications&email_token=ANCR2VHWQ6XDSUQ6KEGISFDQRSM7TA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEC4GNZY#issuecomment-548955879>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ANCR2VEC5ARUEQRTEDGJ3TDQRSM7TANCNFSM4I2MRFQA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548989454:81,log,logic,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548989454,1,['log'],['logic']
Testability,"That would not be a valid test, since it wouldn't be testing the way the code actually handles invalid intervals. All we want to know is that we throw when we encounter an invalid interval. Did I mention that this very simple change is urgently needed by many branches?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/526#issuecomment-104401315:26,test,test,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/526#issuecomment-104401315,2,['test'],"['test', 'testing']"
Testability,"That's very strange @cmnbroad -- in the test I did in front of you yesterday, I added only the exclusion above and it worked fine for me. Are you building with `gradle` or `gradlew`?. Recommend we add whatever exclusions are necessary in a separate, simple PR, independent from the GenomicsDB PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2578#issuecomment-292619716:40,test,test,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2578#issuecomment-292619716,1,['test'],['test']
Testability,"The Talkowski lab version of this is in R and requires some packages that don't seem to be available anymore as well as the python tool svtk, also developed in their lab. It also localizes all the files with a separate Java program they developed. Their implementation is here (most critically gCNV_Pipeline.Rmd and gCNV_helper.jar): https://github.com/theisaacwong/talkowski/tree/master/gCNV It appears to be under active development. My simplified implementation is at https://app.terra.bio/#workspaces/broad-firecloud-dsde-methods/gCNV-CMG-test/notebooks/launch/perform_clustering.ipynb but it's still under development with some help from Brian in TAG.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5632#issuecomment-926857837:543,test,test,543,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5632#issuecomment-926857837,1,['test'],['test']
Testability,"The advantage of using SLF4J is that it is a general facade, so it makes simpler to change for one logging system to other if the bound is implemented. For the most common logging systems (log4j, jul, JLC, etc.), there are this implementation and even no-op logging. One of the nice things from slf4j is that it allows to use the logging format set by the software to every library dependency, controlling the verbosity of other libraries too. . After having a look to the gradle dependencies, it seems that ADAM and Spark use slf4j. This will allow better integration with the two libraries: now the `slf4-jdk` is completely removed, and I don't know if this will blow up at some point if some of the ADAM/Spark classes try to load them. In addition, it will make GATK4 more general. Regarding features, I'm not using more that what log4j is providing, but I'm quite familiar with logback and I have a bias to use it if possible, but the GATK framework as it is implemented now ""force"" to use log4j. But anyway, I'm happy also with using log4j and I was only suggesting this for make GATK4 more general (and to come back in my work to logback, but that is just personal taste). @lbergelson, feel free to close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054:99,log,logging,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054,6,['log'],"['logback', 'logging']"
Testability,"The command I used is `gatk --java-options ""-Xms8g -Xmx8g"" Mutect2 -R ${ref} -I ${bam} -L ${interval} --max-mnp-distance 0 -O ${vcf} --native-pair-hmm-threads 1 >>${logdir}/${index}.oe 2>&1`. The interval file contains one interval and there are 3200 in total across the human reference genome are being processed by a driver script using Open MPI. ```; $ tail -n 1 000?-scattered.interval_list; ==> 0000-scattered.interval_list <==; chr1 10001 990401 + .; ==> 0001-scattered.interval_list <==; chr1 990402 1970802 + .; ==> 0002-scattered.interval_list <==; chr1 1970803 2951203 + .; ==> 0003-scattered.interval_list <==; chr1 2951204 3931604 + .; ==> 0004-scattered.interval_list <==; chr1 3931605 4912005 + .; ==> 0005-scattered.interval_list <==; chr1 4912006 5892406 + .; ==> 0006-scattered.interval_list <==; chr1 5892407 6872807 + .; ==> 0007-scattered.interval_list <==; chr1 6872808 7853208 + .; ==> 0008-scattered.interval_list <==; chr1 7853209 8833609 + .; ==> 0009-scattered.interval_list <==; chr1 8833610 9814010 + .; ```. The H.P.C. administrator and I don't know what you mean by _thread names_. > I'm not sure what you mean by ""name"" of the threads. From a system perspective, threads don't have names distinct from the process as a whole. I guess it's possible that whatever code you're running attaches an internal name to the thread, but that's purely in the domain of your program – it's not something I can see at a system level.; > ; > To be clear, I'm talking about actual runnable tasks at a system level, i.e. multiple ""lightweight processes"" sharing the same address space – i.e. POSIX threads. How your application (or any runtime framework that you build upon, e.g. Java) is launching and distributing work across these, I don't know – again, that's the domain of your application.; > ; > All I can say is that multiple of these threads were active and causing a significant number of contexts switches between then. Since you're binding your processes to a single core, i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7156#issuecomment-803684782:165,log,logdir,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7156#issuecomment-803684782,1,['log'],['logdir']
Testability,"The goal of this PR is to investigate if what needs doing to compile GATK and run its tests on Java 11. Some notes:. * The Scala 2.12 version of Spark 2.4 is needed to run on Java 11. I disabled `ADAMKryoRegistrator` in this branch since the version we are using is compiled against Scala 2.11. We might consider removing it entirely and no longer support ADAM formats directly in GATK - the workaround would be to use ADAM to convert to/from BAM/VCF. ADAM is also needed for reading 2bit files.; * Java 11 deprecates some APIs. Most of these are fairly easy to fix or suppress. The exception is the Javadoc API [com.sun.javadoc](https://docs.oracle.com/en/java/javase/11/docs/api/jdk.javadoc/com/sun/javadoc/package-summary.html), which has been replaced by [jdk.javadoc.doclet](https://docs.oracle.com/en/java/javase/11/docs/api/jdk.javadoc/jdk/javadoc/doclet/package-summary.html). The javadoc tools in `org.broadinstitute.hellbender.utils.help` may need to be re-written (and it's not clear if it's possible to support Java 8 and Java 11 simultaneously).; * Travis build. Getting this to build and test on Java 11 in addition to the current builds may be fairly involved as the matrix is already quite complicated. (The current PR just changes Java 8 to Java 11 for testing purposes - we'd need a way of getting both to run.). The vast majority of tests are passing on Java 11, the following are failing:; * Missing `TwoBitRecord` (from ADAM); * `ReferenceMultiSparkSourceUnitTest`; * `ImpreciseVariantDetectorUnitTest`; * `SVVCFWriterUnitTest`; * `DiscoverVariantsFromContigAlignmentsSAMSparkIntegrationTest`; * `StructuralVariationDiscoveryPipelineSparkIntegrationTest`; * `SvDiscoverFromLocalAssemblyContigAlignmentsSparkIntegrationTest`; * `java.lang.NoSuchMethodError: java.nio.ByteBuffer.clear()Ljava/nio/ByteBuffer;`; * `SeekableByteChannelPrefetcherTest`; * `GatherVcfsCloudIntegrationTest`; * `Could not serialize lambda`; * `ExampleAssemblyRegionWalkerSparkIntegrationTest`; * `PileupSpa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359:86,test,tests,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359,1,['test'],['tests']
Testability,"The marginalization over all paths, not the the likeliest one, would make a small difference, but I'm not sure in which direction. In fact, I would even guess it would tend to favor the reference over a deletion because there are more contributions from things like TTTTT -> TT(DELETE A T)TT(INSERT A T)T that don't actually change the bases but do contribute to the sum. That is, if the reference has a longer poly-T, there are more places to put the cancelling deletion(s) and insertion(s). However, all this stuff incurs gap opening penalties and I would be surprised if it could make a difference as big as 0.07. I think I know what can, however. If you take a look at the top of page 3 of our pair-HMM docs https://github.com/broadinstitute/gatk/blob/master/docs/pair_hmm.pdf, you will see, as @yfarjoun guessed, that there is a preference for shorter haplotypes in the form of a 1/haplotype length factor. And in fact, this amount seems to be in the right ballpark. For example, @ldgauthier's case is a 28-base deletion. If you calculate the log_10 ratio of the 1/length factors assuming a 100-base deletion haplotype, you get log_10(128/100) = 0.107. This, by the way, would explain why we don't see this sort of thing with insertions. We could simply say that any uninformative read (a log likelihood difference of 0.2) should go to the reference if that is the second best. That's basically the prior I was talking about. The M2-only solution would be to use the somatic likelihoods model on the haplotypes and remove unsupported haplotypes from the likelihoods matrix. Similarly, I could use it to align reads to the haplotype with the greatest posterior probability.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393547216:1294,log,log,1294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393547216,1,['log'],['log']
Testability,"The test failures in the branch build are clearly related to the recent travis key migration. The PR build (which is the one we care about) passes, so this should be safe to merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7393#issuecomment-953279491:4,test,test,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7393#issuecomment-953279491,1,['test'],['test']
Testability,"The tests would be very short-running, so each run should be cheap. Most of the runtime will be just spinning up the cluster. If someone malicious opened up a crazy number of PRs, he would at least quickly hit our travis quota limit which would slow him down considerably. We can look into whether other mechanisms exist on travis to deal with this sort of hypothetical situation. We do ideally want these tests to work with forked PRs, but if that's simply not going to be possible on travis then we'll obviously have to choose between having these tests in the same place as all of our other PR-related tests on travis and just skipping them for forked PRs, vs. having them in jenkins and requiring those making and reviewing PRs to deal with/check both CI environments. Let's meet early next week to chat about this issue and come to a decision.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287544886:4,test,tests,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287544886,4,['test'],['tests']
Testability,"There have been a few instances like this though. This one was obviously accidental, but things like the correct spelling of @magicDGS actual name seem like reasonable things to be able to include in the source. Also, testing non-ascii characters seems like something that is going to be increasingly common as we support new versions of the spec so it seems like we should learn to avoid this problem...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5936#issuecomment-492761095:218,test,testing,218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5936#issuecomment-492761095,1,['test'],['testing']
Testability,"There is going to be a behavior different between the old iteration pattern. The existing code will iterate variants to establish whitelist of start sites, and then re-query variants overlapping those starts. This is generally identical in practice to MultiVariantWalkerGroupedOnStart; however, when -L is supplied, it can give differences. I'm not sure which I think is correct. . Example, from VariantEvalIntegrationTest.testFunctionClassWithSnpeff():. // The DbSNP VCF has two variants:; // 1 1423281 rs374004343 GGAAGC G . .; // 1 1423281 rs79849353 G A . .; // If we use 1423281 as the interval, we find both. // The SnpEff VCF has these two:; // 1 1423282 . GAAGC G; // 1 1423286 . C G. The SnpEff VCF is provided as -L when running the tool. Both SnpEff and DbSnp will be used as drivingVariants. Therefore when we hit interval 1423282, the DbSnp variant 1423281:GGAAGC>G variant will be included, but not 1423281:G>A. When using MultiVariantWalkerGroupedOnStart, this means only that variant is passed downstream. Previously, VariantEval would simply establish the whitelist of start sites (meaning the lowest from the group, or 1423281) and iterate them. This means that even though 1423282 is the interval from -L, it picks up the overlapping 1423281:GGAAGC>G, which effectively makes 1423281 a whitelisted start site. The existing behavior would iterate each start site and re-query overlapping variants in VariantEvalUtils.bindVariantContexts(). It would call:. List<VariantContext> prior = featureContext.getValues(track, referenceContext.getInterval().getStart());. In this instance, it would query on start=1423281, meaning it picked up both DbSnp sites, even though 1423281:G>A is not within the intervals supplied by -L. This is sort of a fringe case. I dont see how to fix this without reintroducing the expensive re-query step. I'm currently thinking that the existing behavior is probably what's wrong, but I'm going to consider this a little more. My last commit highlights this c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-730697357:423,test,testFunctionClassWithSnpeff,423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-730697357,1,['test'],['testFunctionClassWithSnpeff']
Testability,"There were a few issues with this case. First, the data source was not constructed 100% correctly. The config file is correct. . The index file is for the tar.gz version of the source data and not for the uncompressed version that they're using. The index should correspond to the source data in the file referenced by the config file itself (not a zipped or otherwise transformed version). Secondly, the source `tsv` data file has the header line for the table commented out. The Xsv codec is aware of leading hash marks as comments and will ignore any such lines. Because of this, the leading hash in the table header is ignored and the file cannot be properly parsed. The fix is simple - just remove the leading hash from the table header (the preceding line with the two hash marks is correctly interpreted as a file header because of the leading hashes acting as comments). Lastly, even if the user fixed the file they would still need to index it with`IndexFeatureFile`. At some point the code underlying this in `HTSJDK` was broken such that no Xsv files can currently be indexed. I have submitted a pull request in `HTSJDK` (https://github.com/samtools/htsjdk/pull/1429) for this and have another ready to go in GATK (#6224) that includes a test for this case so this reversion cannot happen again.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-545186183:1249,test,test,1249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-545186183,1,['test'],['test']
Testability,"Think it might be worth saving a VariantFiltration pass for the bit of code it'd take, but up to you!. ScoreVariantAnnotations will output both the raw ""VQSLOD"" score and the converted sensitivity, so we're free to specify thresholds on either. However, given that different types of models may have scores in different ranges (e.g., BGMM vs. IsolationForest, positive/negative vs. positive-only, etc.), I think it's better to restrict all command-line options to be expressed in terms of a sensitivity. Same thing goes if you decide to filter externally with VariantFiltration for now. Even though you have both quantities available to you, just use the sensitivity. This brings us to questions related to whether we want to keep the old VQSR requirements of having both training and truth sets specified. For example, we could instead drop the distinction between training and truth for the new tools, and always calibrate sensitivity to the training set (you can essentially force this behavior with the current code by specifying training=true,truth=true for all of your resources). And yes, all of the tools should have a variety of command lines in the tests to demonstrate behavior. If you want to explore positive/negative mode, take a look at the *Unlabeled tests. Also feel free to ping me if anything isn't clear!. I'll push another round of minor updates here, too.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1069376523:1159,test,tests,1159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1069376523,2,['test'],['tests']
Testability,"This issue only happens in genomes with a large number of chromosomes, such as hg38. b37 and hg19 are fine. workaround: `--conf spark.driver.extraJavaOptions=-Xss2m --conf spark.executor.extraJavaOptions=-Xss2m`. debug log:; ```; ...; 00:05 DEBUG: [kryo] Write object reference 100367: HLA-DRB1*15:03:01:01; 00:05 DEBUG: [kryo] Write object reference 100369: HLA-DRB1*15:03:01:02; 00:05 DEBUG: [kryo] Write object reference 100371: HLA-DRB1*16:02:01; 21/09/12 22:10:49 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040; 21/09/12 22:10:49 INFO StandaloneSchedulerBackend: Shutting down all executors; 21/09/12 22:10:49 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 21/09/12 22:10:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 21/09/12 22:10:49 INFO MemoryStore: MemoryStore cleared; 21/09/12 22:10:49 INFO BlockManager: BlockManager stopped; 21/09/12 22:10:49 INFO BlockManagerMaster: BlockManagerMaster stopped; 21/09/12 22:10:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 21/09/12 22:10:49 INFO SparkContext: Successfully stopped SparkContext; 22:10:49.533 INFO HaplotypeCallerSpark - Shutting down engine; [September 12, 2021 10:10:49 PM CST] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=1788346368; Exception in thread ""main"" java.lang.StackOverflowError; at com.esotericsoftware.kryo.util.MapReferenceResolver.useReferences(MapReferenceResolver.java:70); at com.esotericsoftware.kryo.Kryo.writeReferenceOrNull(Kryo.java:665); at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:570); at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:79); at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508); at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:575); at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectFiel",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869#issuecomment-917650984:219,log,log,219,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869#issuecomment-917650984,1,['log'],['log']
Testability,"This makes probably at least 214-59=155 test fail. The first one is [assertMatchingAnnotationsFromGenomicsDB_newMQformat](https://github.com/broadinstitute/gatk/blob/423d106612074aa3480b67b321dc66426b1c600a/src/test/java/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFsIntegrationTest.java#L405):; ```; assertMatchingAnnotationsFromGenomicsDB_newMQformat[0](src/test/resources/org/broadinstitute/hellbender/tools/GenomicsDBImport/expected.testGVCFMode.gatk4.g.vcf, src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/newMQcalc.singleSample.genotyped.vcf, 20:1-11000000, src/test/resources/large/human_g1k_v37.20.21.fasta); java.lang.AssertionError: Genotype string expected [C*|T] but found [T|C*]; 	at org.testng.Assert.fail(Assert.java:97); ```. However looking at the input file and the expected output file it is clear that the expected output is wrong here (`:PL:PS `**`0|1`**`:23,38:61:99:1` in the last line) and this PR does the right thing instead:; ```; # input:; $ bcftools view org/broadinstitute/hellbender/tools/GenomicsDBImport/expected.testGVCFMode.gatk4.g.vcf | grep -e '1|0' -e ""10007150""; 20 10007150 . G C,<NON_REF> 669.77 . BaseQRankSum=-4.476;DP=63;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.5,0;MQRankSum=0;RAW_MQandDP=226800,63;ReadPosRankSum=-0.077 GT:AD:DP:GQ:PGT:PID:PL:PS:SB 0|1:38,25,0:63:99:0|1:10007150_G_C:698,0,1479,813,1554,2366:10007150:16,22,14,11; 20 10007175 . C T,<NON_REF> 1350.77 . BaseQRankSum=2.249;DP=61;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.5,0;MQRankSum=1.319;RAW_MQandDP=216841,61;ReadPosRankSum=-1.213 GT:AD:DP:GQ:PGT:PID:PL:PS:SB 1|0:23,38,0:61:99:1|0:10007150_G_C:1379,0,780,1448,894,2343:10007150:9,14,18,20; #""expected"" output (wrong):; $ bcftools view org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/newMQcalc.singleSample.genotyped.vcf | grep -e '1|0' -e ""10007150""; 20 10007150 . G C 690.64 . AC=1;AF=0.5;AN=2;BaseQRankSum=-4.476;DP=63;ExcessHet=0;FS=5.048;MLEAC=1;MLEAF=0.5;MQ=60;MQRankSum=0;QD=10.96;ReadPosRank",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8570#issuecomment-1784915555:40,test,test,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8570#issuecomment-1784915555,7,['test'],"['test', 'testGVCFMode', 'testng']"
Testability,"Update: we have a PoC impl. working with sharded writing and simple indexing implementation in https://github.com/googlegenomics/dataflow-java/tree/sharded-bam-writer , need to do some more benchmarking and merge to main branch.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/621#issuecomment-132294611:190,benchmark,benchmarking,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/621#issuecomment-132294611,1,['benchmark'],['benchmarking']
Testability,"Updated plan. ----------; ## Small improvements in new interpretation tool; ; - [x] Output bam instead of sam for assembly alignments; - [x] Instead of creating directory, new interpretation tool writes files (behavior consistent with current interpretation tool); - [x] Prefix with sample name for output files' names; - [x] Add `INSLEN` annotation when there's `INSSEQ`; - [x] Clarify the boundary between `AlignedContig` and `AssemblyContigWithFineTunedAlignments`; - [x] Increase test coverage for `AssemblyContigAlignmentsConfigPicker`; ; ----------; ## Consolidate logic, bump test coverage and update how variants are represented. ### consolidate logic; When initially prototyped, there's redundancy in logic for simple variants, now it's time to consolidate. - [x] `AssemblyContigWithFineTunedAlignments`; - [x] `hasIncompletePicture()`. - [x] `AssemblyContigAlignmentSignatureClassifier`; - [x] Don't make so many splits; - [x] Reduce `RawTypes` into fewer cases; ; - [x] `ChimericAlignment`; - [x] update documentation; - [x] implement a `getCoordinateSortedRefSpans()`, and use in `BreakpointsInference`; - [x] `isNeitherSimpleTranslocationNorIncompletePicture()`; - [x] `extractSimpleChimera()`. ### bump test coverage; Once code above is consolidated, bump test coverage, particularly for the classes above and the following poorly-covered classes; - [x] `ChimericAlignment`; - [x] `isForwardStrandRepresentation()`; - [x] `splitPairStrongEnoughEvidenceForCA()` ; - [x] `parseOneContig()` (needs testing because we need it for simple-re-interpretation for CPX variants) Note that `nextAlignmentMayBeInsertion()` is currently broken in the sense that when using this to filter out alignments whose ref span is contained by another, check if the two alignments involved are head/tail. - [x] `BreakpointsInference` & `BreakpointComplications`. - [x] `NovelAdjacencyAndAltHaplotype`; - [x] `toSimpleOrBNDTypes()`. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence`; - [x] serialization ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021:484,test,test,484,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021,5,"['log', 'test']","['logic', 'test']"
Testability,"Using the latest version of ADAM (which has a Scala 2.12 version) fixes the 2bit failures. I also added a fix for the `java.nio.ByteBuffer.clear()` problem. All unit tests are passing, and the only integration test failures are the `Could not serialize lambda` problems. It should be possible to fix these by making the relevant classes implement `Serializable` (like in https://github.com/samtools/htsjdk/pull/1408).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527483090:166,test,tests,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527483090,2,['test'],"['test', 'tests']"
Testability,"We don't yet have good regression tests for Spark that run on a cluster and are separate from the jenkins performance tests. https://github.com/broadinstitute/gatk/issues/2298 will satisfy part of the requirements for this ticket once it's done (by catching the most basic regressions before merge), but there's also a need for larger-scale correctness tests whose status is clearly visible on github.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713:34,test,tests,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713,3,['test'],['tests']
Testability,"We're going to release the new model in 3.7 and have users test-drive that for quite a bit before we move to release 4.0, so we should be able to get some feedback on performance in the wild before we need to make any final decisions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589:59,test,test-drive,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589,1,['test'],['test-drive']
Testability,"We've run some basic WGS tests in gatk-sv, and the results look good. Using our 161-sample 30x NYGC 1KGP test panel, here is a summary of per-sample raw calls from gCNV+cnMOPs. That is, records look like:. ```; chr1	103639835	103663835	1kgp_161_bwa_scramble_DEL_9527	HG01552	DEL	cnmops,gcnv; chr1	103643835	103645835	1kgp_161_bwa_scramble_DEL_9531	HG01494	DEL	gcnv; chr1	103643835	103652200	1kgp_161_bwa_scramble_DEL_9534	HG02236	DEL	cnmops,gcnv; chr1	103645600	103724500	1kgp_161_bwa_scramble_DEL_9538	HG03370	DEL	cnmops,gcnv; chr1	103647835	103649835	1kgp_161_bwa_scramble_DEL_9541	HG01495	DEL	gcnv; chr1	103647835	103681000	1kgp_161_bwa_scramble_DEL_9542	HG01494	DEL	cnmops,gcnv; …; ```. Master:; DEL: 641700; DUP: 699063. Branch:; DEL: 640669; Master intersection: 635178 (99.14% sensitivity); DUP: 691469; Master intersection: 677687 (98.00%). Note that this is a simple bedtools intersection requiring 90% reciprocal overlap, but not matching samples. Subsetting just to NA19420:. Master:; DEL: 4974; DUP: 4254. Branch:; DEL: 4958; Master intersection: 4797 (96.75%); DUP: 4210; Master intersection: 3802 (90.30%). Further subsetting to NA19240 variants over 5kbp, which is the default gatk-sv threshold for depth-only calls:. Master:; DEL: 1133; DUP: 916. Branch:; DEL: 1132; Master intersection: 1060 (93.64%); DUP: 893; Master intersection: 757 (84.77%). While it does appear there's an appreciable difference here, if we subset to NA19240 variants that survive gatk-sv filtering to the ""CleanVcf"" stage (in the current master), the differences are much less:. Master:; DEL: 225; DUP: 61. Branch:; DEL: 226; Master intersection: 223 (98.67%); DUP: 58; Master intersection: 58 (95.08%). I think the differences in raw calls is probably ""in the noise"" for WGS, given the high concordance in this test sample after gatk-sv filtering and genotyping are applied. Edit: Thanks to @kirtanav98 for running these tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2186896268:25,test,tests,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2186896268,4,['test'],"['test', 'tests']"
Testability,"Well, when the `clearItems` call is removed from the `consumeFinalizeItems` else branch, some `HaplotypeCallerSparkIntegrationTest`s [fail](https://api.travis-ci.com/v3/job/173147001/log.txt) because `PushToPullIterator` doesn't call clearItems to reset the state when `consumeFinalizeItems` returns no items, and the next submit is rejected because eoi hasn't been reset. So, since `consumeFinalizeItems` can't reset/mutate the state when there are no items, then we can't assert the precondition that `endOfInput==false` in `submit`. So I'm removing the validation of that from both `ReservoirDownsampler` and `PositionalDownsampler`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5594#issuecomment-457870723:183,log,log,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5594#issuecomment-457870723,2,"['assert', 'log']","['assert', 'log']"
Testability,"When reads are mapped onto the genome short reads don't always find the best spot for indels. Sometimes reads are clipped at a position where a particular indel could have been mapped properly. Those regions you showed here are all homopolymer rich regions where assembly and variant calls are usually harder than other places. 3bp insertion within a GC rich region could easily be mapped wrong due to G and C nucleotide positioning and the way G/C nucleotide is handled by the sequencing instrument. Due to chemistry and optics reasons certain basecalls in GC rich regions may get convoluted with wrong nucleotide assertions such as 1 less G and one more C. . Reassembly, Realignment and PairHMM removes such artifacts by looking at basecalling metrics, mapping qualities, regional metrics etc. I cannot see it directly from the image however it is possible that some of those reads could have been soft/hard clipped due to such errors but yet they are still valid and usable by the assembly engine. . You may wish to read about the DepthPerAlleleBySample class and its documentation as it is the object class that calculates AD for variant contexts. ; [DepthPerAlleleBySample](https://gatk.broadinstitute.org/hc/en-us/articles/360037592411-DepthPerAlleleBySample). I hope this helps.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8959#issuecomment-2304798951:615,assert,assertions,615,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8959#issuecomment-2304798951,1,['assert'],['assertions']
Testability,"XRhU291cmNlLmphdmE=) | `90.476% <0%> (+1.587%)` | `61% <0%> (+2%)` | :arrow_up: |; | [...institute/hellbender/exceptions/UserException.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9leGNlcHRpb25zL1VzZXJFeGNlcHRpb24uamF2YQ==) | `66.667% <0%> (+3.252%)` | `4% <0%> (ø)` | :arrow_down: |; | [...g/broadinstitute/hellbender/engine/AuthHolder.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvQXV0aEhvbGRlci5qYXZh) | `15.254% <0%> (+5.085%)` | `2% <0%> (ø)` | :arrow_down: |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `77.703% <0%> (+6.757%)` | `22% <0%> (+4%)` | :arrow_up: |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `86.4% <0%> (+8.8%)` | `35% <0%> (+7%)` | :arrow_up: |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `23.729% <0%> (+13.559%)` | `2% <0%> (+1%)` | :arrow_up: |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2573#issuecomment-291977600:3026,test,test,3026,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2573#issuecomment-291977600,1,['test'],['test']
Testability,"Yeah, I don't like these new interface methods -- they make `GATKRead` significantly worse. We should cache `isUnmapped`, etc. in the adapter to accomplish the same thing, as @lbergelson suggests. Not that hard, and we can just unconditionally invalidate the cached values (using `Boolean` fields set to null) whenever the read is mutated in any way in order to simplify the logic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282:375,log,logic,375,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282,1,['log'],['logic']
Testability,You can consider this fixed if it passes a simple unit test where you run `ReadPosRankSumTest.getReadPosition` on a few artificial read of the form <ref match><short deletion><short ref match of 1-5 or so bases><short deletion>.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5492#issuecomment-445115098:55,test,test,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5492#issuecomment-445115098,1,['test'],['test']
Testability,"Your best bet is to just start analyzing your data with this VCF. Doesn't; sound like your output log file showed any truly problematic errors. Things; like VCF Tools or vcfR (if you're familiar with R or want to start learning; it) give you some basic stats about your vcf file very quickly. This will; alleviate many of your concerns. On Tue, Jul 31, 2018, 11:10 AM sanjeevksh <notifications@github.com> wrote:. > My GenotypeGVCFs run for a single chromosome returned the following; > completion statement:; > 18:54:40.516 INFO ProgressMeter - Traversal complete. Processed 606308; > total variants in 75.2 minutes.; >; > However, there are only 46814 variant rows (excluding 52 header rows) in; > the corresponding vcf file. Does the above figure of 606308 correspond to a; > multiple of 'variants x number of samples'?; >; > Also, there are only 16863 lines in my log file, does this mean that the; > 'Current Locus' column in the log file doesn't correspond to a single; > genomic location (bp) in the fasta file?; >; > I am curious to know what is the relation between all these figures to; > fully understand what is happening while processing the gCVF files.; >; > Also, on the inbreeding coefficient warning issue, I understand from your; > @Neato-Nick <https://github.com/Neato-Nick> feedback that the variants; > with these warnings may still be fine and can be retained. However, this; > still leaves me worrying that out of 384 samples the locus doesn't even; > have 10 samples for generating the required metrics. Such variants won't be; > of any use for downstream analyses anyway where any variants with more than; > 80% missing samples will be removed. Therefore, I wish to seek some more; > information about this 10 sample thing - does it have some other context or; > does it literally mean that there are only less than 10 samples carrying; > that variant?; >; > Regards,; > Sanjeev; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, vi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-409271340:98,log,log,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-409271340,3,['log'],['log']
Testability,"abadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 01:42:05 2017 -0500. hybrid ADVI abstract argument collection w/ flexible default values; hybrid ADVI argument collection for contig ploidy model; hybrid ADVI argument collection for germline denoising and calling model. commit 56e21bf955d3dc0c52aceb384f28cf6173959de0; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 23:18:39 2017 -0500. rewritten python-side coverage metadata table reader using pandas to fix the issues with comment line; change criterion for cohort/case based on whether a contig-ploidy model is provided or not; simulated test files for ploidy determination tool; proper integration test for ploidy determination tool and all edge cases; updated docs for ploidy determination tool. commit 7fa104b2e9170770cfc5b338835e41215d7fd39c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 18:43:17 2017 -0500. kabab case for gCNV-related tools; removed short args (this also partially affected PlotDenoisedCopyRatios and PlotModeledSegments and their integration tests). commit f02cb024331a986213cfd9fae2da706bbc5ddbd9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 18:02:40 2017 -0500. synced with mb_gcnv_python_kernel. commit 2963bbf8c90418d9b88545c93771ae51cf542db9; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 11:38:05 2017 -0500. Fixing typo in travis.yml. commit 6cf589999c716ec66404eb0a2ae4310dd130a772; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 11:13:58 2017 -0500. editable, full path. commit d998f2d5c2b33dd41e291be9bfeaea72fe479b8a; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:56:24 2017 -0500. revert Dockerfile, change yml. commit 930d7486b7d2cf918fcb16dd03394bb9c9f0611b; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:34:46 2017 -0500. more Dockerfile. commit 94112131526b514ef254bcc2c50a239dbae35aa1; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:25:13 2017",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:8134,test,tests,8134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['test'],['tests']
Testability,"adoc for any class in that package. The integration test runs `com.sun.tools.javadoc.Main.execute` and asserts that the output code is zero, which does not yield a useful error message. In order to produce something more meaningful I hacked the test to output the entire `stdout` and `stderr` as follows:. ```; final StringWriter out = new StringWriter();; final PrintWriter err = new PrintWriter(out);. final int result = com.sun.tools.javadoc.Main.execute(""program"", err, err, err, ""doclet"",docArgList.toArray(new String[] {}));; err.flush(); // probably not needed; String message = out.toString(); // message contains the entire stdout and stderr of the call to execute; Assert.assertEquals(result, 0, message);; ```. The output is about 2000 lines, but a lot of it is clearly innocuous. Removing lines such as; * `2022-08-16T00:09:07.2336106Z [parsing completed 1ms]`; * `2022-08-16T00:09:07.4456202Z [loading ZipFileIndexFileObject[/jars/gatk-package-4.2.6.1-56-gad9a538-SNAPSHOT-test.jar(org/broadinstitute/hellbender/tools/walkers/variantutils/ReblockGVCFIntegrationTest.class)]]`; * `2022-08-16T00:09:07.4459732Z [loading RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalIntervalArgumentCollection.java]]`; * `2022-08-16T00:09:07.4462012Z [parsing started RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalReferenceInputArgumentCollection.java]]`; * 2022-08-16T00:09:07.2322755Z [loading ZipFileObject[/gatk/gatk-package-unspecified-SNAPSHOT-local.jar(htsjdk/samtools/SAMSequenceDictionary.class)]]. brings it down to 353 lines, the majority of which look like . ```2022-08-16T00:09:07.4435974Z src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java:479: error: cannot find symbol; 2022-08-16T00:09:07.4436105Z @VisibleForTesting; ```. Here's that 353-line file:. [log-no-parsing-loading.txt](https://github.com/broadinstitute/gatk/files/9355026/log-no-parsing-loading.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488:2075,log,log-no-parsing-loading,2075,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488,2,['log'],['log-no-parsing-loading']
Testability,"and likely to require some iteration so I'd be ok with starting with just the minimal ""porting"" changes to keep things simple, and then doing a code hygiene pass at the end. The ""porting"" changes should include things like updated javadoc, GATK4-style command line arguments, updating of outdated GATK3 terminology such as ""ROD"", Utils.nonNull assertions, etc. The finals and curly braces can wait for a separate pass (we will want to do those in this PR though). If you're not sure what to include or not just ask. I like the idea of keeping the GATK3 tests working as we go along. We should make a clear distinction between the old and new tests though. Ideally the GATK3 tests would be in a separate commit that we can just delete at the end, but that can get unwieldy if the files in the commit need to change as we go along. Alternatively you could isolate them into a separate directory. They should either be disabled or made dependent on a test method (see the `@Test` annotation properties `enabled` and `dependsOn`) that is easily toggled so they can be run locally, but don't run on the CI server. Otherwise the CI server build will always fail. In general, its really helpful to have the first commit in the PR contain the completely unmodified GATK3 source files. It makes it much easier for the reviewer to see what changed for the port. I noticed that you have 2 new plugins included in this. I'm not sure if that was suggested by someone on the GATK team (I'm wondering if we want to go down that path...) but I can tell you that the existing plugins required an enormous amount of test development and review iteration. If we do decide to make them plugins, I think it would be a good idea to do so in a separate PR. Also, if we choose to make an AbstractPlugin base class, we may want that to live in the Barclay repo. As @magicdgs points out, master already has your previous commits, so you should start by rebasing on that. Ideally, the branch would have the following commits bef",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352:1170,test,test,1170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352,1,['test'],['test']
Testability,"avadoc.doclet](https://docs.oracle.com/en/java/javase/11/docs/api/jdk.javadoc/jdk/javadoc/doclet/package-summary.html). The javadoc tools in `org.broadinstitute.hellbender.utils.help` may need to be re-written (and it's not clear if it's possible to support Java 8 and Java 11 simultaneously).; * Travis build. Getting this to build and test on Java 11 in addition to the current builds may be fairly involved as the matrix is already quite complicated. (The current PR just changes Java 8 to Java 11 for testing purposes - we'd need a way of getting both to run.). The vast majority of tests are passing on Java 11, the following are failing:; * Missing `TwoBitRecord` (from ADAM); * `ReferenceMultiSparkSourceUnitTest`; * `ImpreciseVariantDetectorUnitTest`; * `SVVCFWriterUnitTest`; * `DiscoverVariantsFromContigAlignmentsSAMSparkIntegrationTest`; * `StructuralVariationDiscoveryPipelineSparkIntegrationTest`; * `SvDiscoverFromLocalAssemblyContigAlignmentsSparkIntegrationTest`; * `java.lang.NoSuchMethodError: java.nio.ByteBuffer.clear()Ljava/nio/ByteBuffer;`; * `SeekableByteChannelPrefetcherTest`; * `GatherVcfsCloudIntegrationTest`; * `Could not serialize lambda`; * `ExampleAssemblyRegionWalkerSparkIntegrationTest`; * `PileupSparkIntegrationTest`; * Native HMM library code caused the tests to crash on my Mac:; ```; Running Test: Test method testLikelihoodsFromHaplotypes[0](org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM@6282d367, true)(org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest); dyld: lazy symbol binding failed: can't resolve symbol __ZN13shacc_pairhmm9calculateERNS_5BatchE in /private/var/folders/cj/wyp4zgw17vj4m9qdmddvmcc80000gn/T/libgkl_pairhmm13775554937319419112.dylib because dependent dylib #1 could not be loaded; dyld: can't resolve symbol __ZN13shacc_pairhmm9calculateERNS_5BatchE in /private/var/folders/cj/wyp4zgw17vj4m9qdmddvmcc80000gn/T/libgkl_pairhmm13775554937319419112.dylib because dependent dylib #1 could not be loaded; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359:2058,test,tests,2058,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359,2,['test'],"['testLikelihoodsFromHaplotypes', 'tests']"
Testability,"b.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:1242,test,test,1242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857,1,['test'],['test']
Testability,"b/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:1260,test,test,1260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857,1,['test'],['test']
Testability,bWVudENvbGxlY3Rpb24uamF2YQ==) | `100% <100%> (ø)` | `1 <1> (?)` | |; | [...ine/GATKPlugin/GATKReadFilterPluginDescriptor.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yLmphdmE=) | `85.6% <94.737%> (+0.116%)` | `50 <9> (+1)` | :arrow_up: |; | [...institute/hellbender/utils/gcs/GATKGCSOptions.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvR0FUS0dDU09wdGlvbnMuamF2YQ==) | `0% <0%> (-66.667%)` | `0% <0%> (ø)` | |; | [...lbender/engine/datasources/ReferenceAPISource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <0%> (-62.264%)` | `8% <0%> (-26%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...ark/sv/CallVariantsFromAlignedContigsSAMSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9DYWxsVmFyaWFudHNGcm9tQWxpZ25lZENvbnRpZ3NTQU1TcGFyay5qYXZh) | `0% <0%> (-26.087%)` | `0% <0%> (-5%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `48.837% <0%> (-24.774%)` | `27% <0%> (-9%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-287020306:2484,test,test,2484,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-287020306,1,['test'],['test']
Testability,"bb9c9f0611b; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:34:46 2017 -0500. more Dockerfile. commit 94112131526b514ef254bcc2c50a239dbae35aa1; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:25:13 2017 -0500. more Dockerfile. commit 7d2646240a65f5c0f09f5f25f3e19e9d9bf004d9; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:06:11 2017 -0500. more Dockerfile. commit f1235c25aeba85570b5ce389a34975f1b7b5ec3c; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 09:39:46 2017 -0500. Dockerfile edit. commit 3df84dd4693f28e4e8b34fd33f877e99749dffce; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 16:08:06 2017 -0500. Update test PoNs. commit 2c3b20e62a1cba7af24c0b0846eb1629422f51e6; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 15:49:38 2017 -0500. Update test files. commit c65c6e9144ef396792364ab2e06b7b436bb97684; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 15:30:59 2017 -0500. Adding no-GC/do-GC WDL tests. commit 56451843066a456d9cf8e6eac55ae4df2c518ec3; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 12:51:17 2017 -0500. Updates to handle SAM header changes from sl_wgs_acnv_headers and updates to mb_gcnv_python_kernel. commit d02d04df684a2820308a1d1c2bfda4b7d1c5f05e; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Mon Nov 13 12:52:33 2017 -0500. Added CLIs and WDL for python gCNV pipeline. commit 66ed74b68375d43514ef84658e7a6c771ed9053c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Nov 15 01:50:03 2017 -0500. Polished code, ready for review; ; gCNV computational kernel (initial release); ; renaming gammas_s to psi_s to uniformity (sample-specific unexplained variance); ; renamed determine_ploidy_and_depth.py to cohort_determine_ploidy_and_depth.py; finite-temperature forward-backward algorithm; in the ploidy model, replaced alpha_j (NB over-dispersion) with psi_j (unexplained variance) for uniformity. Also, added the possibility of sam",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:9835,test,tests,9835,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['test'],['tests']
Testability,"ch can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorData",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:2176,test,test,2176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['test'],['test']
Testability,"cness of the site is relevant to the false positive deletion.; >; > One could ask what in the GATK engine is responsible.; >; > - The assembly engine, perhaps? No, it is the assembly engine's job to; > propose possible haplotypes, not to call them. In any case, there *is*; > one spanning read with the deletion above the reads shown, so it is a valid; > path in the graph.; > - Pair-HMM? This one confused me for a while, but no. The engine is; > *not* saying that these reads' best alignment to the reference has a; > deletion, which would be false because there is a gap opening penalty.; > Rather, it says that they align equally well (with no deletions) to the ref; > haplotype and to the deletion haplotype. The deletion shown in IGV is the; > deletion of the alt haplotype relative to the reference, not of the reads; > relative to their best haplotype.; > - The bamout writer? Nope, that code is really straightforward and; > does the right thing.; >; > So what's the issue? Well, the bamout writer gets its read alignments from; > the readLikelihoods after the reads have been realigned to their best; > haplotype. In these cases, it turns out that the alignment of the reads to; > their best haplotype, the deletion has a log likelihood better than the; > alignment to the ref haplotype by about 0.00001. The simplest solution; > would be to give an extremely modest prior in favor of the reference and; > break these near-ties in favor of the reference. @droazen; > <https://github.com/droazen> @ldgauthier <https://github.com/ldgauthier>; > @yfarjoun <https://github.com/yfarjoun> if you think this is a good idea; > I can fix it for both HC and M2. Otherwise I'll do an M2-only fix.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4829>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0lv9nhpu6C8LQCF9jGJoX4UAmfJEks5t32IhgaJpZM4UUW5o>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393417801:2093,log,log,2093,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393417801,1,['log'],['log']
Testability,"consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code instead.; - [x] In the meantime, we can try to speed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:6000,test,tests,6000,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,2,['test'],"['test', 'tests']"
Testability,"d against Scala 2.11. We might consider removing it entirely and no longer support ADAM formats directly in GATK - the workaround would be to use ADAM to convert to/from BAM/VCF. ADAM is also needed for reading 2bit files.; * Java 11 deprecates some APIs. Most of these are fairly easy to fix or suppress. The exception is the Javadoc API [com.sun.javadoc](https://docs.oracle.com/en/java/javase/11/docs/api/jdk.javadoc/com/sun/javadoc/package-summary.html), which has been replaced by [jdk.javadoc.doclet](https://docs.oracle.com/en/java/javase/11/docs/api/jdk.javadoc/jdk/javadoc/doclet/package-summary.html). The javadoc tools in `org.broadinstitute.hellbender.utils.help` may need to be re-written (and it's not clear if it's possible to support Java 8 and Java 11 simultaneously).; * Travis build. Getting this to build and test on Java 11 in addition to the current builds may be fairly involved as the matrix is already quite complicated. (The current PR just changes Java 8 to Java 11 for testing purposes - we'd need a way of getting both to run.). The vast majority of tests are passing on Java 11, the following are failing:; * Missing `TwoBitRecord` (from ADAM); * `ReferenceMultiSparkSourceUnitTest`; * `ImpreciseVariantDetectorUnitTest`; * `SVVCFWriterUnitTest`; * `DiscoverVariantsFromContigAlignmentsSAMSparkIntegrationTest`; * `StructuralVariationDiscoveryPipelineSparkIntegrationTest`; * `SvDiscoverFromLocalAssemblyContigAlignmentsSparkIntegrationTest`; * `java.lang.NoSuchMethodError: java.nio.ByteBuffer.clear()Ljava/nio/ByteBuffer;`; * `SeekableByteChannelPrefetcherTest`; * `GatherVcfsCloudIntegrationTest`; * `Could not serialize lambda`; * `ExampleAssemblyRegionWalkerSparkIntegrationTest`; * `PileupSparkIntegrationTest`; * Native HMM library code caused the tests to crash on my Mac:; ```; Running Test: Test method testLikelihoodsFromHaplotypes[0](org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM@6282d367, true)(org.broadinstitute.hellbender.utils.pairhmm",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359:1270,test,testing,1270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359,1,['test'],['testing']
Testability,"d be a single stream pass through all the BND's. I'll try to follow the preferred design. > What about some other filters more specifically aimed at the artifacts that cause these false large calls? I think it's a good idea to check annotations -- ie. do the mates lie at two regions that are segmental duplications of each other, or one side of the mate looks like a transposable element insertion? I guess it's ok to put in a tool with this limit temporarily, though. Answer: Agree. And I think these kind of checking are not only good, but a mandate for a good caller, i.e. to take advantage of prior knowledge. I am also thinking about improving it by checking if there are short read evidence supporting the BND's (sometimes there aren't any, which is strange for the alignments of the assembly to point to such novel adjacencies in the first place).; However, I might have to push back a little on this particular request at this point because such features can be later added on like you said, and probably now is not the best time to do it, because the linking logic is unlikely to be affected by presence of these reference annotations.; One note though, and I think you don't mean it literally either, the large ones are not necessarily artifacts. Like what we discussed during the group meetings, they are ""filtered"" partly because they are more likely to overlap with multiple other BND mates, and are more likely to be artifacts or mobile element insertions. In other words, they are not sent for linking because they are less likely to be suited for the linking logic in `LinkedInversionBreakpointsInference`. But if they do, the linking is totally agnostic about size. > Building in an ability to check the copy number of the region in which an inversion breakpoint lies (by checking against a CNV call for example) would probably be really helpful. Answer: Agree. And it shouldn't be too hard to do that. But again, may be in a different PR?. > When you say 'overlaps with CPX' it mig",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:2795,log,logic,2795,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929,1,['log'],['logic']
Testability,"d from ApplyVQSR. See https://github.com/broadinstitute/gatk/pull/7954#discussion_r933570228.; - [ ] Add behavior for dealing with mixed SNP/INDEL sites in separate passes (and note that the current WDL currently does this, to allow for the use of different annotations across SNPs and INDELs). This might include rescuing previously filtered sites, etc. (e.g., by using the option to ignore the first-pass filter in the second pass). Alternatively, one could use a different FILTER name in each pass, which downstream hard-filtering steps could utilize intelligently. Or one might just split multiallelics upstream. In any case, I would hope that we could move towards running both SNPs and INDELs in a single pass with the same annotations as the default mode of operation.; - [ ] Clean up borrowed code in the `VariantType` class for classifying sites as SNP or INDEL. We mostly retained the VQSR code and logic to make head-to-head comparisons easier. Note also that we converted some switch statements to conditionals, etc. (which I think was done properly, but maybe I missed an edge case). See https://github.com/broadinstitute/gatk/pull/7954#discussion_r934776584.; - [ ] Think more about how to treat empty HDF5 arrays. It's possible we should handle this at the WDL level with optional inputs/outputs. Likely only relevant for atypical edge cases. See https://github.com/broadinstitute/gatk/pull/7954#discussion_r934845337. Next steps:. - [ ] I'll update the BGMM branch and open a PR.; - [ ] I'll start looking at implementing a simple CARROT test. We can just replicate the Cromwell/WDL test for now.; - [ ] Update that initial implementation with non-trivial data and evaluation scripts. EDIT: I see that #7982 was just filed.; - [ ] Implement a CARROT test with malaria data. We already have some evaluation scripts.; - [x] Expand the WDL to enable additional workflow modes (positive-negative, etc.) and the tests to cover them. Right now only vanilla positive-only is enabled/covered.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1209555008:2085,test,test,2085,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1209555008,4,['test'],"['test', 'tests']"
Testability,"d that only include the ""official"" PARs, or also the additional ones you found?. In any case, are we comfortable calling in those regions (here I'm talking about gCNV, not ploidy)? As I show above, I don't think we need mappability to nail the baseline ploidy. Can we then rely on the per-bin bias to account for these regions in gCNV (pinning them back to the correct CN) without mappability filtering? And with mappability filtering, how substantial is the hit to coverage in these regions? Should we blacklist them for the time being?. To summarize, I think the order of events I'd like to see is this:. 1. Cut an **initial Beta** release that incorporates CollectReadCounts, streamline evaluations for the AACR poster, do a bit of tuning, establish a baseline. Hopefully the current ploidy calls suffice, if not, maybe issue a quick PR that implements the naive bin filtering (or whatever is necessary to get good ploidy calls). At the same time, get preliminary feedback from some users running on *small test cohorts* after we have some parameter recommendations.; 2. Do a round of method/model improvement. Start with quick and dirty fixes (e.g., blacklisting PARs) and work our way to more non-trivial changes. This will include many of the suggestions you have brought up, but we should also review user feedback and prioritize accordingly---they may find something that is not even on our radar. Demonstrate improvement (hopefully substantial!) over baseline, cut **second Beta** release.; 3. Run on larger cohorts, iron out remaining minor issues, and then productionize. By this time, @asmirnov239 will have hopefully made some progress on the PoN clustering front as well. **When we are ready, then we will take gCNV out of Beta.** With our current staffing situation, I do not expect this to happen before May 15, but I do enjoy pleasant surprises. :); 4. Run on gnomAD, world domination, etc. Again, getting a **initial Beta** release and some reasonable parameters to users is a high p",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375923639:2728,test,test,2728,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375923639,1,['test'],['test']
Testability,dCountsIntegrationTest files. commit adfbef12f2ab90f93b49a4f786979549648e1f22; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Mon Dec 11 02:22:56 2017 -0500. removed CNV evaluation code from this branch. commit 18c8d31f39a1964474c5d7b12ee8cbfafc4ac9e2; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Sun Dec 10 00:19:58 2017 -0500. GS VCF parser outputs dict for samples instead of list. commit b138be39cd8428342668ee6678079021006f983b; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Sun Dec 10 00:15:19 2017 -0500. renaming. commit eab5c90b74b4eb6bd11acb0fd1e0fa58a3b5b0c7; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 16:23:40 2017 -0500. exposed a global preemptible_attempts to gCNV workflows; set OMP_NUM_THREADS and MKL_NUM_THREADS to the number of requested CPUs. commit ad6fe348d6a7896c169b2b0499e2a4bca34021ad; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 10:21:25 2017 -0500. reverted log level in germline CNV tests. commit d9eb4e504baab834a9efc07cc3479176db2946ce; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 10:20:30 2017 -0500. the proper python environment yml for mkl and open -- leads to orders of magnitude speedup!. commit fea6bf874e0b62262a3b1d239ce4d76792d5c416; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 09:31:43 2017 -0500. revert. commit 456c53f88d01b603f4175d8896a0dac036af03f8; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 08:17:22 2017 -0500. enabled openmp g++ linking in theano. commit e2afef14ddb957f2dbdea76fd783d3bfb8d7a64e; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 08:04:19 2017 -0500. mkl. commit 43e2a65201286161fcd5bfe7dbb21ae888e19dac; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 06:56:20 2017 -0500. added cpu argument for germline tasks. commit 4433a62c2173c7f29d0f264c084bbaf2f6738782; Author: Mehrtash Babadi <mehrtash@broadin,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:2591,log,log,2591,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,2,"['log', 'test']","['log', 'tests']"
Testability,"dinstitute.org>; Date: Fri Dec 8 06:56:20 2017 -0500. added cpu argument for germline tasks. commit 4433a62c2173c7f29d0f264c084bbaf2f6738782; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 02:45:38 2017 -0500. revert travis yml forks; verbose logging germline wdl. commit ae05801e33c37b3bf2685fba202032a270804873; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 00:55:14 2017 -0500. updated somatic PoNs for PreprocessIntervals drop Ns. commit cff64984d9fb42364001bda4c73d54cf68d85a5c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 00:37:24 2017 -0500. sudo travis yml. commit 89025941febd2089d426cfa1e0f0aa6a6712e2a9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 00:23:22 2017 -0500. travis/Docker config update (g++-6, Miniconda3); python test group assignment. commit 31f96398106c2b8577b8c25d110abea3ebe7f836; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:44:53 2017 -0500. WDL test bugfix. commit 9b2fb820536ec355bea0256471bd093d547f5c99; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:20:36 2017 -0500. update WDL test JSON files. commit e3d97644d1a2c40a5c364a96f8b67246154179c9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:18:14 2017 -0500. assertions in inference task base; removed a ASCII > 128 character in log messages. commit 526cf92e623a3bbd5f9d375132b6ca046fc47620; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:03:04 2017 -0500. redirect tqdm progress bar to python logger. commit 2e45bd30968b921fae225de3901fb97ece690b0c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:45:49 2017 -0500. more arg related fixes. commit bb89a3bb338d88199881e8aca65f656f2acd7c0a; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:41:20 2017 -0500. arg related bugfixes in WDL, python, and java CLIs. commit 23569787ee2c8cc6c9227a44170cbbd02fe4427f; Auth",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:4416,test,test,4416,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['test'],['test']
Testability,done. moved getSpanningInterval to IntervalUtils and simplified + added tests for it. Moved IntervalUtilsUnitTest to the right package. No empty string in ctor. back to @lbergelson,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1497#issuecomment-186019881:72,test,tests,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1497#issuecomment-186019881,1,['test'],['tests']
Testability,"ds();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8/./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true --input ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --reference ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --run_without_dbsnp_potentially_ruining_quality false --solid_recal_mode SET_Q_ZERO --solid_nocall_strategy THROW_EXCEPTION -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:2768,test,test,2768,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['test'],['test']
Testability,"e R install from the base image and adding the R dependencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnondeterminism-perl; libfont-afm-perl libfon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:1045,test,tests,1045,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954,1,['test'],['tests']
Testability,"e deleted, and its use `support = range(lo, hi)` should become `support = new IndexRange(lo, hi)`. Then `IntStream.of(support).mapToDouble(___).toArray()` becomes `range.mapToDouble(___)` and `apply(promote(support), ___)` becomes `support.mapToDouble(___)`. * `final double relErr = 1 + pow(10, -7)` should become a `static` constant. * The steps; ```java; final double maxD1 = arrayMax(d1);; final double[] d2 = apply(apply(d1, d -> d - maxD1), d -> exp(d));; final double sumD2 = sum(d2);; return apply(d2, d -> d/sumD2);; ```; inside `dnHyper` are just a home-brewed way to normalize the log-space array `d1`. Let's go throught the steps: 1) find the max. 2) subtract the max -- subtracting a constant in log-space is equivalent to dividing by a constant in real space, and since we're normalizing in the end this constant is arbitrary. It's done for numerical stability. 3) exponentiate to get an unnormalized real-space array. 4) find the sum. 5) divide by the sum to get the normalized result. The log-10 version of this is `MathUtils::normalizeFromLog10ToLinearSpace(d1)`. You could either calculate `d1` in log-10 space or convert it, replacing the above line with `return MathUtils.normalizeFromLog10ToLinearSpace(MathUtils.applyToArrayInPlace(d1, MathUtils::logToLog10))`. The latter option is simpler. * Import static should be avoided except to escape horrible clutter, which is not the case here. * The second argument of `Utils.validateArg(condition, calculated string expression)` should become `Utils.validateArg(condition, () -> calculated string expression)`. In the first version, the expression is calculated *even if* the condition is satisfied, whereas in the second it is only calculated as needed. It's not critical here but it's a good habit to get into. PS I am a zealot of the Boy Scout Rule (always leave the camp site cleaner than you found it). It is a great way to get familiar with a large code base and a great way to make the code more readable for the next person.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155:1625,log,log-,1625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155,2,['log'],['log-']
Testability,"e local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2016-02-14 19:04:19 WARN MetricsSystem:71 - Using default name DAGScheduler for source because spark.app.id is not set.; MinimalVariant -- interval(1:737406-737411), snp(false), indel(true); 19:04:24.266 INFO PrintVCFSpark - Shutting down engine; [Februar",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:1533,test,test,1533,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857,1,['test'],['test']
Testability,"ePicture()`; - [x] `extractSimpleChimera()`. ### bump test coverage; Once code above is consolidated, bump test coverage, particularly for the classes above and the following poorly-covered classes; - [x] `ChimericAlignment`; - [x] `isForwardStrandRepresentation()`; - [x] `splitPairStrongEnoughEvidenceForCA()` ; - [x] `parseOneContig()` (needs testing because we need it for simple-re-interpretation for CPX variants) Note that `nextAlignmentMayBeInsertion()` is currently broken in the sense that when using this to filter out alignments whose ref span is contained by another, check if the two alignments involved are head/tail. - [x] `BreakpointsInference` & `BreakpointComplications`. - [x] `NovelAdjacencyAndAltHaplotype`; - [x] `toSimpleOrBNDTypes()`. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence`; - [x] serialization test. - [x] `AnnotatedVariantProducer`; - [x] `produceAnnotatedBNDmatesVcFromNovelAdjacency()`. - [x] `BreakEndVariantType`. - [ ] `SvDiscoverFromLocalAssemblyContigAlignmentsSpark` integration test; . ### update how variants are represented ; Implement the following representation changes that should make type-based evaluation easier; - [x] change `INSDUP` to`INS` when the duplicated ref region, denoted with annotation `DUP_REPEAT_UNIT_REF_SPAN`, is shorter than 50 bp.; - [x] change scarred deletion calls, which currently output as `DEL` with `INSSEQ` annotation, to one of these; - [x] `INS`/`DEL`, when deleted/inserted bases are < 50 bp and annotate accordingly; when type is determined as`INS`, the `POS` will be 1 base before the micro-deleted range and `END` will be end of the micro-deleted range, where the `REF` allele will be the corresponding reference bases.; - [x] two records `INS` and `DEL` when both are >= 50, share the same `POS`, and link by `EVENT`; - [ ] we are making a choice that treats duplication expansion as insertion. If decide to treat `DUP` as a separate 1st class type, we need to ; - [ ] shift the left breakpoint to the ri",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021:2195,test,test,2195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021,1,['test'],['test']
Testability,"ecrease gcnv_max_training_epochs. commit 68772cba486b44ebc8cf8bfc2b600c1e8a406c61; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Sun Dec 17 19:20:05 2017 +0330. documentation update of GermlineCNVCaller and DetermineGermlineContigPloidy. commit c032281f8c43a80e4ec8cb96eb66397ad2acf9b7; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 15 18:14:35 2017 -0500. Fixed imr kebab case in WDL, moved argument classes, removed GenomeLocParser, fixed up gCNV WDL readme. commit be84a804f6ab6fbb815995db9c116d1db950ab8b; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 13:55:08 2017 -0500. removed extra comma in gCNV Case WDL test JSON. commit cb379b866d425f12f5525ecb28ad0b636a528d44; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 12:58:59 2017 -0500. added missing cpu parameters to gCNV Case WDL tasks. commit eed85f6c70f4a7f15e0765b5f15a1bf8541c151e; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 11:26:31 2017 -0500. disabled some gCNV WDL tests. commit 6d8ca07fef41518b5b157fb9a214d4536c617156; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 10:54:54 2017 -0500. fixed DenoiseReadCountsIntegrationTest files. commit adfbef12f2ab90f93b49a4f786979549648e1f22; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Mon Dec 11 02:22:56 2017 -0500. removed CNV evaluation code from this branch. commit 18c8d31f39a1964474c5d7b12ee8cbfafc4ac9e2; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Sun Dec 10 00:19:58 2017 -0500. GS VCF parser outputs dict for samples instead of list. commit b138be39cd8428342668ee6678079021006f983b; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Sun Dec 10 00:15:19 2017 -0500. renaming. commit eab5c90b74b4eb6bd11acb0fd1e0fa58a3b5b0c7; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 16:23:40 2017 -0500. exposed a global preemptible_attempts to gCNV workflows; set OMP_NUM_THREADS and MKL_NUM_THREADS to the number of requested CPU",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:1447,test,tests,1447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['test'],['tests']
Testability,"ed commits, for reference:. ````; commit 3eda4b18888f38249be39f99901d8453a4de50d6; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Dec 28 14:56:27 2017 -0500. updated command lines for WDL tests for C29. commit 7ce1369943cce4ae9cfb5e96455d18d3960e9b77; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Dec 28 13:30:21 2017 -0500. use C29 and decrease gcnv_max_training_epochs. commit 68772cba486b44ebc8cf8bfc2b600c1e8a406c61; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Sun Dec 17 19:20:05 2017 +0330. documentation update of GermlineCNVCaller and DetermineGermlineContigPloidy. commit c032281f8c43a80e4ec8cb96eb66397ad2acf9b7; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 15 18:14:35 2017 -0500. Fixed imr kebab case in WDL, moved argument classes, removed GenomeLocParser, fixed up gCNV WDL readme. commit be84a804f6ab6fbb815995db9c116d1db950ab8b; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 13:55:08 2017 -0500. removed extra comma in gCNV Case WDL test JSON. commit cb379b866d425f12f5525ecb28ad0b636a528d44; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 12:58:59 2017 -0500. added missing cpu parameters to gCNV Case WDL tasks. commit eed85f6c70f4a7f15e0765b5f15a1bf8541c151e; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 11:26:31 2017 -0500. disabled some gCNV WDL tests. commit 6d8ca07fef41518b5b157fb9a214d4536c617156; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 10:54:54 2017 -0500. fixed DenoiseReadCountsIntegrationTest files. commit adfbef12f2ab90f93b49a4f786979549648e1f22; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Mon Dec 11 02:22:56 2017 -0500. removed CNV evaluation code from this branch. commit 18c8d31f39a1964474c5d7b12ee8cbfafc4ac9e2; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Sun Dec 10 00:19:58 2017 -0500. GS VCF parser outputs dict for samples instead of list. commit b138be39cd8428342668ee6678079021006f983b; Author: Mehrtas",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:1096,test,test,1096,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['test'],['test']
Testability,"en-Habermeyer We had a few PRs in late 2021 that may have fixed this. If it's still occurring in the latest GATK version I would like to take a look at it. ok @davidbenjamin I got a chance to test with latest release `4.3.0.0` and the issue seems to be mostly resolved when running `--alleles` on our test samples. Additionally, `FilterMutectCalls` works on low DP variants. . For control samples, using the `--alleles` option results in an error due to the value of the stats `callable`. . Combination of this call:; ```; chr18 77560878 . AA TT . . AS_SB_TABLE=0,0|0,0;DP=1;ECNT=2;MBQ=0,90;MFRL=0,100;MMQ=60,60;MPOS=29;POPAF=7.30;TLOD=4.20 GT:AD:AF:DP:F1R2:F2R1:FAD:PGT:PID:PS:SB 0|1:0,1:0.667:1:0,1:0,0:0,1:0|1:77560878_AA_TT:77560878:0,0,0,1; ```; and the stats file containing:; ```; callable 1.0; ```; results in FilterMutectCalls exception; ```; java.lang.IllegalArgumentException: logValues must be non-infinite and non-NAN; at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeFromLogToLinearSpace(NaturalLogUtils.java:27); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.posteriorProbabilityOfError(Mutect2FilteringEngine.java:93); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:140); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSomaticVariant(SomaticClusteringModel.java:146); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.performEMIteration(SomaticClusteringModel.java:345); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:330); at org.broadinstitute.hellbende",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7276#issuecomment-1293969047:991,log,logSumExp,991,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7276#issuecomment-1293969047,1,['log'],['logSumExp']
Testability,"endencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnondeterminism-perl; libfont-afm-perl libfontenc1 libgcc-5-dev libgdbm3 libgettextpo-dev; libget",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:1101,test,tests,1101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954,1,['test'],['tests']
Testability,"er.seqDict, true, StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigsAlignmentsSparkArgumentCollection.DEFAULT_MIN_ALIGNMENT_LENGTH, StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigsAlignmentsSparkArgumentCollection.CHIMERIC_ALIGNMENTS_HIGHMQ_THRESHOLD, true);. Assert.assertEquals(assembledBreakpointsFromAlignmentIntervals.size(), 1);; final ChimericAlignment chimericAlignment = assembledBreakpointsFromAlignmentIntervals.get(0);; Assert.assertEquals(chimericAlignment.sourceContigName, ""asm00001:tig0001"");; final NovelAdjacencyReferenceLocations breakpoints = new NovelAdjacencyReferenceLocations(chimericAlignment, contigSequence, SVDiscoveryTestDataProvider.seqDict);; }; ```. In versions of the code prior to #3752 (I think) this set of alignments was being filtered out by the method `isNotSimpleTranslocation` in the `parseOneContig` method of `ChimericAlignment`. Now that check's logic has changed and `isLikelySimpleTranslocation` returns false instead of true and so this alignment is not being filtered out any more. . When it gets to `NovelAdjacencyReferenceLocations.TanDupBreakpointsInference()` both `upstreamBreakpointRefPos` and `downstreamBreakpointRefPos` are being set to zero. It's not immediately clear to me how to fix this. A few thoughts:. - Are we supposed to be processing this `ChimericAlignment` through the main code path right now? ; - Why do we subtract 1 from the start position of the `rightReferenceInterval.getStart()` when setting `downstreamBreakpointRefPos`? In this case the start is 1 so we end up with an invalid coordinate of 0.; - The `upstreamBreakpointRefPos` is also being set to 0 by this line below.. why?. ```; upstreamBreakpointRefPos = leftReferenceInterval.getEnd() - homologyLen; - (complication.getDupSeqRepeatNumOnCtg() - complication.getDupSeqRepeatNumOnRef()) * complication.getDupSeqRepeatUnitRefSpan().size();; ```. @SHuang-Broad I'm not sure what the best way to fix this is, can you take",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3874#issuecomment-347627504:3262,log,logic,3262,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3874#issuecomment-347627504,1,['log'],['logic']
Testability,"erformed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2016-02-14 19:04:19 WARN MetricsSystem:71 - Using default name DAGScheduler for source because spark.app.id is not se",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:1396,test,test,1396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857,1,['test'],['test']
Testability,"ergelson please comment on the following proposal. The proposal is that we would spin off native PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class type of JNIReadDataHolderClass; * @param haplotypeDataHolder",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:1079,test,tests,1079,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864,1,['test'],['tests']
Testability,"es.githubusercontent.com/11076296/158385777-6174f8b8-7abb-4b31-92d1-11cc8064854b.png). Note that the malaria data used was pretty small: chr1-2 training (~20k positive training/truth variants, ~50k negative training variants; note also that the threshold for determining negative training was not tuned---a threshold corresponding to a 98% truth sensitivity was arbitrarily chosen), chr3 validation (~50k variants), and chr4-6 test (~150k variants). The LL score is calculated from a validation set held out from the training/truth positives used to train the model, while the F1 score is calculated using ""orthogonal truth"" positives/negatives determined using 3 families of ~30 trios each. However, there's some arbitrariness in how we define the boundary for the latter positives/negatives, and hence some arbitrariness in the F1 score itself. But I'd expect using gold-standard GIAB truth would be more straightforward. Not sure how much we can conclude, but that the validation and test F1s are similar and that the validation LL score isn't *too* far off are encouraging. That said, there is a pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian consistency, white = reference. The second plot shows the training/truth positives used to train the model and to calculate the LL score in the validation shard. The third plot s",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:1163,test,test,1163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,1,['test'],['test']
Testability,et_plus_decoy_hla.fa.gz -- --spark-runner SPARK --spark-master yarn. Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar; Running:; /share/pkg/spark/2.3.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/wgs.hg38/pipelines/hc/cram.test/GRCh38_full_analysis_set_plus_decoy_hla.fa.gz --spark-master yarn; 2019-01-09 13:35:04 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:05 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 13:35:09.640 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:35:09.799 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compressi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:1450,test,test,1450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['test'],['test']
Testability,f0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yLmphdmE=) | `85.484% <86.538%> (-1.183%)` | `49 <26> (+4)` | |; | [...institute/hellbender/utils/gcs/GATKGCSOptions.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvR0FUS0dDU09wdGlvbnMuamF2YQ==) | `0% <0%> (-66.667%)` | `0% <0%> (ø)` | |; | [...lbender/engine/datasources/ReferenceAPISource.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <0%> (-62.264%)` | `8% <0%> (-26%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...g/broadinstitute/hellbender/utils/NativeUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9OYXRpdmVVdGlscy5qYXZh) | `25% <0%> (-43.75%)` | `3% <0%> (-1%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `43.75% <,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-279424649:2463,test,test,2463,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-279424649,1,['test'],['test']
Testability,f869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-18.75%)` | `6% <0%> (ø)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `10.169% <0%> (-13.559%)` | `1% <0%> (-1%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `77.6% <0%> (-9.6%)` | `28% <0%> (-8%)` | |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `70.946% <0%> (-6.757%)` | `18% <0%> (-4%)` | |; | ... and [4 more](https://codecov.io/gh/broadinstitute/gatk/pull/2433?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2433?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/do,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2433#issuecomment-283613034:3953,test,test,3953,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2433#issuecomment-283613034,1,['test'],['test']
Testability,"fter which the usual modelling and smoothing steps are performed. For the 75% tumor + 25% normal mixture, this yields 122 segments (up from 83):; ![N-25-T-75-SJS modeled](https://user-images.githubusercontent.com/11076296/76558618-015bd180-6474-11ea-996a-48d39770149b.png). For the 25% tumor + 75% normal mixture, this yields 105 segments (up from 50):; ![N-75-T-25-SJS modeled](https://user-images.githubusercontent.com/11076296/76560726-34a05f80-6478-11ea-9027-a54726c46b9e.png). One could imagine that smoothing could be disabled (so that all samples retain the common segmentation after modeling) or made more aggressive (so that private events don't get inadvertently introduced into other samples due to noise, perhaps), depending on the use case. It looks like the joint segmentation allows some additional events to be resolved, although I haven't done any rigorous evaluations. We could probably cook up some evaluations using simulated toy data or in silico mixtures, but there's really no reason why this shouldn't work decently well, especially if the kernel-segmentation method works well on a single sample for your data. It would also be interesting to understand at which point changing segmentation parameters on a single sample can no longer yield the same performance as joint segmentation on a fixed number of samples; however, this is probably a function of various S/N ratios, and it might not be easy to characterize this behavior outside of toy data. The segmentation parameter space is big enough to make this unwieldy even for toy data, too. Perhaps we can get some feedback from test users---not only on performance, but also on the structure of the new workflow. It might also be worth gauging whether a new WDL is warranted. Otherwise, we just need to add some unit tests for correctness of the multisample-segmentation backend class, integration tests for plumbing of the new tool, and perhaps address some of the issues mentioned above. Then I'd say this is good to go.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598386823:2738,test,test,2738,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598386823,3,['test'],"['test', 'tests']"
Testability,"fully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34295.; 17/05/05 17:03:36 INFO NettyBlockTransferService: Server created on 172.30.0.122:34295; 17/05/05 17:03:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 17/05/05 17:03:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.30.0.122, 34295, None); 17/05/05 17:03:36 INFO BlockManagerMasterEndpoint: Registering block manager 172.30.0.122:34295 with 414.4 MB RAM, BlockManagerId(driver, 172.30.0.122, 34295, None); 17/05/05 17:03:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.30.0.122, 34295, None); 17/05/05 17:03:36 INFO BlockManager: external shuffle service port = 7337; 17/05/05 17:03:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.30.0.122, 34295, None); 17/05/05 17:03:36 INFO EventLoggingListener: Logging events to hdfs:///var/log/spark/apps/local-1494003816349; 17/05/05 17:03:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 304.1 KB, free 414.2 MB); 17/05/05 17:03:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.0 KB, free 414.1 MB); 17/05/05 17:03:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.30.0.122:34295 (size: 26.0 KB, free: 414.4 MB); 17/05/05 17:03:38 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:109; 17/05/05 17:03:38 INFO FileInputFormat: Total input paths to process : 1; 17/05/05 17:03:38 INFO SparkContext: Starting job: sortByKey at ReadsSparkSink.java:244; 17/05/05 17:03:38 INFO DAGScheduler: Got job 0 (sortByKey at ReadsSparkSink.java:244) with 1 output partitions; 17/05/05 17:03:38 INFO DAGScheduler: Final stage: ResultStage 0 (sortByKey at ReadsSparkSink.java:244); 17/05/05 17:03:38 INFO DAGScheduler: Parents of final stage: List(); 17/05/05 17:03:38 INFO DAGScheduler: Missing pa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046:5968,log,log,5968,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046,1,['log'],['log']
Testability,"gScores.hdf5 /repo/extract.nonAS.snpIndel.posUn.train.snpIndel.posOnly.IF.snp.trainingScores.hdf5. file1 file2; ---------------------------------------; x x / ; x x /data ; x x /data/scores . group : </> and </>; 0 differences found; group : </data> and </data>; 0 differences found; dataset: </data/scores> and </data/scores>; size: [445] [445]; position scores scores difference ; ------------------------------------------------------------; [ 60 ] -0.419202 -0.419202 5.55112e-17 ; 1 differences found; ```. Looks pretty negligible to me! :stuck_out_tongue_closed_eyes: Probably a result of the native code being called by the python/ML packages used in these tools; even minor changes in the compilers across Ubuntu versions might introduce differences like these. A quick fix might be to replace all system calls to `h5diff` in these tests with `h5diff --use-system-epsilon`; seems to do the trick here. But if that doesn't fix all test cases, then perhaps you can relax things with `h5diff -p EPSILON`, where `EPSILON` is a relative threshold. Probably OK to pick something like `1E-6`. OK if I leave it to you to try this or otherwise check the rest of the cases?. Sorry for the inconvenience! I think the exact-match test worked as intended here, but I probably could've put in better messaging originally. Unfortunately, it's a bit awkward to grab the output of system commands. And thanks for dealing with conda again (a necessary evil, unless we want to reimplement the entire field of machine learning in Java)! I'll experiment to see if I can't get the more recent version used in #8561 (23.10) working with the current environment---probably just some minor tweak to the pip version is needed to get around the error you're seeing. You could try unpinning it to see what gets pulled in. It would be great if we could get off the old version of conda, since more recent versions using the libmamba solver are *MUCH* faster and would cut down all of our Docker build times substantially.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848796931:1764,test,test,1764,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848796931,1,['test'],['test']
Testability,"gion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlying craziness); 2. Change the ActiveRegion traversal so that it respects the dcov value (could be hard -- the LIBS downsampling process discards reads on-the-fly from previous loci when moving to a new locus, but an active region involves data for multiple loci. The potential performance win for the HC is huge, though, if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads in memory is actually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in Locu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:4902,test,testing,4902,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['test'],['testing']
Testability,"h the test data on this or GATK3 repository, so I am not sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage if there is no data for known indels. I will propose very soon two PRs with fully functional tools (without the n-way out feature for indel-realignment), and trying to add simple integration tests with the data already available on the repository and running with GATK3.8-1. If t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:1286,test,test,1286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115,1,['test'],['test']
Testability,"hink the coverage distribution is indeed the correct summary statistic to model for this problem. Total coverage just doesn't provide enough information, but subsampling bins or fitting a per-bin bias model is overkill. However, I think a straightforward, self-contained modeling or masking approach (which need not rely on a mappability track) within the ploidy-determination tool is still quite feasible. I think that if we can easily solve the problem without requiring a mappability track then we should try to do it, as that is a relatively expensive resource to create. For example, some very naive hard filtering (red) of the histogram yields a peak that is easily fit by a negative binomial (green)---even a Poisson fit does not appear to bias the depth estimates, and certainly does not result in incorrect ploidy estimates:. ![masked_fit](https://user-images.githubusercontent.com/11076296/37863641-827a6e8a-2f37-11e8-83d5-cb4af32a898b.png). (Incidentally, it is helpful to plot on a log scale when checking the fit of these distributions.). This strategy also gives us a way to ignore low-level mosaicism or large germline events, which filtering on mappability may not address:. ![mosaic](https://user-images.githubusercontent.com/11076296/37863649-d0ac378c-2f37-11e8-8e98-45e1fa9a3d7a.png). So let's try to encapsulate changes to the ploidy tool. I agree that the histogram creation can be easily done on the Java side, to save on intermediate file writing. We can probably just cap the maximum bin to `k` and pass a samples x contig TSV where each entry is a vector with `k + 1` elements. I agree that there is still a lot of important work to be done in exploring our best practices for coverage collection, and I know that you have been interested in improving them for a while. Ultimately, we may want to consider incorporating mappability or other informative metadata, as we've discussed. However, this will require some non-trivial investment in method/tool development time. Sinc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375881040:1055,log,log,1055,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375881040,1,['log'],['log']
Testability,"i <mehrtash@broadinstitute.org>; Date: Fri Dec 8 02:45:38 2017 -0500. revert travis yml forks; verbose logging germline wdl. commit ae05801e33c37b3bf2685fba202032a270804873; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 00:55:14 2017 -0500. updated somatic PoNs for PreprocessIntervals drop Ns. commit cff64984d9fb42364001bda4c73d54cf68d85a5c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 00:37:24 2017 -0500. sudo travis yml. commit 89025941febd2089d426cfa1e0f0aa6a6712e2a9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 00:23:22 2017 -0500. travis/Docker config update (g++-6, Miniconda3); python test group assignment. commit 31f96398106c2b8577b8c25d110abea3ebe7f836; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:44:53 2017 -0500. WDL test bugfix. commit 9b2fb820536ec355bea0256471bd093d547f5c99; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:20:36 2017 -0500. update WDL test JSON files. commit e3d97644d1a2c40a5c364a96f8b67246154179c9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:18:14 2017 -0500. assertions in inference task base; removed a ASCII > 128 character in log messages. commit 526cf92e623a3bbd5f9d375132b6ca046fc47620; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:03:04 2017 -0500. redirect tqdm progress bar to python logger. commit 2e45bd30968b921fae225de3901fb97ece690b0c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:45:49 2017 -0500. more arg related fixes. commit bb89a3bb338d88199881e8aca65f656f2acd7c0a; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:41:20 2017 -0500. arg related bugfixes in WDL, python, and java CLIs. commit 23569787ee2c8cc6c9227a44170cbbd02fe4427f; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 17:21:05 2017 -0500. fixed issue with python boolean argparse (they use weird semantics). commit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:4581,test,test,4581,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['test'],['test']
Testability,"i_mosaicism_s',; alpha=1.0,; beta=50.0,; shape=(ploidy_workspace.num_samples,)). # per-sample-and-contig mosaicism factor; f_mosaicism_sj = Beta(name='f_mosaicism_sj',; alpha=10.0,; beta=1.0,; shape=(ploidy_workspace.num_samples, ploidy_workspace.num_contigs,)); norm_f_mosaicism_sj = f_mosaicism_sj / tt.max(f_mosaicism_sj, axis=1).dimshuffle(0, 'x'). # per-contig mapping error; eps_j = HalfNormal('eps_j', sd=0.01, shape=(ploidy_workspace.num_contigs,)). # negative-binomial means; mu_sjk = depth_s.dimshuffle(0, 'x', 'x') * t_j.dimshuffle('x', 0, 'x') * norm_bias_j.dimshuffle('x', 0, 'x') * \; (ploidy_workspace.int_ploidy_values_k.dimshuffle('x', 'x', 0) + eps_j.dimshuffle('x', 0, 'x')); mu_mosaic_sjk = norm_f_mosaicism_sj.dimshuffle(0, 1, 'x') * mu_sjk. # ""unexplained variance""; psi = Uniform(name='psi', upper=10.0). # convert ""unexplained variance"" to negative binomial over-dispersion; alpha = tt.inv((tt.exp(psi) - 1.0)). def _get_logp_sjk(_n_sj):; _logp_sjk = logsumexp([tt.log(1 - pi_mosaicism_s.dimshuffle(0, 'x', 'x')) + commons.negative_binomial_logp(mu_sjk, alpha.dimshuffle('x', 'x', 'x'), _n_sj.dimshuffle(0, 1, 'x')),; tt.log(pi_mosaicism_s.dimshuffle(0, 'x', 'x')) + commons.negative_binomial_logp(mu_mosaic_sjk, alpha.dimshuffle('x', 'x', 'x'), _n_sj.dimshuffle(0, 1, 'x'))],; axis=0)[0]; return _logp_sjk. DensityDist(name='n_sj_obs',; logp=lambda _n_sj: tt.sum(q_ploidy_sjk * _get_logp_sjk(_n_sj), axis=2),; observed=n_sj); ````. Briefly, the model includes 1) per-contig bias (normalized to unit mean for identifiability), 2) per-sample depth, 3) per-sample probability of mosaicism, 4) per-sample-and-contig mosaicism factor `f` (in [0, 1], normalized by the per-sample max for identifiability), 5) per-contig mapping error. The likelihood is then a negative-binomial mixture of non-mosaic and mosaic contigs, where the latter have their mean count depressed by the corresponding factor `f`. This model still requires some tuning of priors (which are currently hard coded ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-371334890:1529,log,log,1529,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-371334890,1,['log'],['log']
Testability,"iff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `90.476% <0%> (+1.587%)` | `61% <0%> (+2%)` | :arrow_up: |; | [...institute/hellbender/exceptions/UserException.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9leGNlcHRpb25zL1VzZXJFeGNlcHRpb24uamF2YQ==) | `66.667% <0%> (+3.252%)` | `4% <0%> (ø)` | :arrow_down: |; | [...g/broadinstitute/hellbender/engine/AuthHolder.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvQXV0aEhvbGRlci5qYXZh) | `15.254% <0%> (+5.085%)` | `2% <0%> (ø)` | :arrow_down: |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `77.703% <0%> (+6.757%)` | `22% <0%> (+4%)` | :arrow_up: |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `86.4% <0%> (+8.8%)` | `35% <0%> (+7%)` | :arrow_up: |; | ... and [7 more](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=footer). Last update [5ccfd00...b1d407f](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2540#issuecomment-290122549:3334,test,test,3334,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2540#issuecomment-290122549,1,['test'],['test']
Testability,"igs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8/./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true --input ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --reference ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --run_without_dbsnp_potentially_ruining_quality false --solid_recal_mode SET_Q_ZERO --solid_nocall_strategy THROW_EXCEPTION --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_default_quality -1 --insertions_d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:2891,test,test,2891,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['test'],['test']
Testability,"ima. The problem of finding local minima of a noisy function can be solved by using topological persistence (e.g., https://people.mpi-inf.mpg.de/~weinkauf/notes/persistence1d.html and http://www2.iap.fr/users/sousbie/web/html/indexd3dd.html?post/Persistence-and-simplification). A straightforward watershed algorithm can sort all local minima by persistence in linear time after an initial sort of the data.; 5) These sets of local minima from all window sizes together provide the pool of candidate changepoints (some of which may overlap exactly or approximately). We perform backwards selection using the global segmentation cost. That is, we compute the global segmentation cost given all the candidate changepoints, calculate the cost change for removing each of the changepoints individually, remove the changepoint with the minimum cost change, and repeat. This gives the global cost as a function of the number of changepoints _C_.; 6) Add a penalty _a C + b C log(N / C)_ to the global cost and find the minimum to determine the number of changepoints. For the above simulated data, _a = 2_ and _b = 2_ works well, recovering all of the changepoints in the above example with no false positives:; ![6](https://user-images.githubusercontent.com/11076296/29582517-fbd604be-874a-11e7-8ef7-7bd727f65dcb.png). ![7](https://user-images.githubusercontent.com/11076296/29582518-fddf015c-874a-11e7-89e4-87250d2a52ab.png). In contrast, CBS produces two false positives (around the third and seventh of the true changepoints):. ![8](https://user-images.githubusercontent.com/11076296/29582545-18875126-874b-11e7-9166-9061bb120e43.png). We can change the penalty factor to smooth out less significant segments (which may be due to systematic noise, GC waves, etc.). Setting _a = 10, b = 10_ gives:; ![9](https://user-images.githubusercontent.com/11076296/29582598-515dffe0-874b-11e7-93c4-59422cd43b54.png); ![10](https://user-images.githubusercontent.com/11076296/29583130-0fe5316c-874d-11e7-8504-436189",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:2711,log,log,2711,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586,1,['log'],['log']
Testability,"is TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segme",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:2975,test,test,2975,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['test'],['test']
Testability,"itute.org>; Date: Fri Dec 8 00:37:24 2017 -0500. sudo travis yml. commit 89025941febd2089d426cfa1e0f0aa6a6712e2a9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 00:23:22 2017 -0500. travis/Docker config update (g++-6, Miniconda3); python test group assignment. commit 31f96398106c2b8577b8c25d110abea3ebe7f836; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:44:53 2017 -0500. WDL test bugfix. commit 9b2fb820536ec355bea0256471bd093d547f5c99; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:20:36 2017 -0500. update WDL test JSON files. commit e3d97644d1a2c40a5c364a96f8b67246154179c9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:18:14 2017 -0500. assertions in inference task base; removed a ASCII > 128 character in log messages. commit 526cf92e623a3bbd5f9d375132b6ca046fc47620; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:03:04 2017 -0500. redirect tqdm progress bar to python logger. commit 2e45bd30968b921fae225de3901fb97ece690b0c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:45:49 2017 -0500. more arg related fixes. commit bb89a3bb338d88199881e8aca65f656f2acd7c0a; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:41:20 2017 -0500. arg related bugfixes in WDL, python, and java CLIs. commit 23569787ee2c8cc6c9227a44170cbbd02fe4427f; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 17:21:05 2017 -0500. fixed issue with python boolean argparse (they use weird semantics). commit ae841c9ed4cd9b2ca1ac0e9082d175ff8ea98298; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 16:44:02 2017 -0500. shorter gCNV WDL tests. commit 5466b806e36df16cad2d045be074e7f9afec0957; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 16:38:15 2017 -0500. fixed arg issues in somatic WDL; exposed all missing args to java side; major update to germline WDLs; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:5001,log,logger,5001,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['log'],['logger']
Testability,"itute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2016-02-14 19:04:19 WARN MetricsSystem:71 - Using d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:1378,test,test,1378,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857,1,['test'],['test']
Testability,"king allele with the max score (across SNPs/INDELs for mixed sites, to boot) is borrowed from ApplyVQSR. See https://github.com/broadinstitute/gatk/pull/7954#discussion_r933570228.; - [ ] Add behavior for dealing with mixed SNP/INDEL sites in separate passes (and note that the current WDL currently does this, to allow for the use of different annotations across SNPs and INDELs). This might include rescuing previously filtered sites, etc. (e.g., by using the option to ignore the first-pass filter in the second pass). Alternatively, one could use a different FILTER name in each pass, which downstream hard-filtering steps could utilize intelligently. Or one might just split multiallelics upstream. In any case, I would hope that we could move towards running both SNPs and INDELs in a single pass with the same annotations as the default mode of operation.; - [ ] Clean up borrowed code in the `VariantType` class for classifying sites as SNP or INDEL. We mostly retained the VQSR code and logic to make head-to-head comparisons easier. Note also that we converted some switch statements to conditionals, etc. (which I think was done properly, but maybe I missed an edge case). See https://github.com/broadinstitute/gatk/pull/7954#discussion_r934776584.; - [ ] Think more about how to treat empty HDF5 arrays. It's possible we should handle this at the WDL level with optional inputs/outputs. Likely only relevant for atypical edge cases. See https://github.com/broadinstitute/gatk/pull/7954#discussion_r934845337. Next steps:. - [ ] I'll update the BGMM branch and open a PR.; - [ ] I'll start looking at implementing a simple CARROT test. We can just replicate the Cromwell/WDL test for now.; - [ ] Update that initial implementation with non-trivial data and evaluation scripts. EDIT: I see that #7982 was just filed.; - [ ] Implement a CARROT test with malaria data. We already have some evaluation scripts.; - [x] Expand the WDL to enable additional workflow modes (positive-negative, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1209555008:1440,log,logic,1440,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1209555008,1,['log'],['logic']
Testability,"ld put an `if` statement in the `FisherExactTest` code to switch to an asymptotic approximation, but, like I said, overkill. * This class has it's own `apply` method which replicates `MathUtils::applyToArray`. * Similarly, it's `range` method should be deleted, and its use `support = range(lo, hi)` should become `support = new IndexRange(lo, hi)`. Then `IntStream.of(support).mapToDouble(___).toArray()` becomes `range.mapToDouble(___)` and `apply(promote(support), ___)` becomes `support.mapToDouble(___)`. * `final double relErr = 1 + pow(10, -7)` should become a `static` constant. * The steps; ```java; final double maxD1 = arrayMax(d1);; final double[] d2 = apply(apply(d1, d -> d - maxD1), d -> exp(d));; final double sumD2 = sum(d2);; return apply(d2, d -> d/sumD2);; ```; inside `dnHyper` are just a home-brewed way to normalize the log-space array `d1`. Let's go throught the steps: 1) find the max. 2) subtract the max -- subtracting a constant in log-space is equivalent to dividing by a constant in real space, and since we're normalizing in the end this constant is arbitrary. It's done for numerical stability. 3) exponentiate to get an unnormalized real-space array. 4) find the sum. 5) divide by the sum to get the normalized result. The log-10 version of this is `MathUtils::normalizeFromLog10ToLinearSpace(d1)`. You could either calculate `d1` in log-10 space or convert it, replacing the above line with `return MathUtils.normalizeFromLog10ToLinearSpace(MathUtils.applyToArrayInPlace(d1, MathUtils::logToLog10))`. The latter option is simpler. * Import static should be avoided except to escape horrible clutter, which is not the case here. * The second argument of `Utils.validateArg(condition, calculated string expression)` should become `Utils.validateArg(condition, () -> calculated string expression)`. In the first version, the expression is calculated *even if* the condition is satisfied, whereas in the second it is only calculated as needed. It's not critical here but ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155:1329,log,log-space,1329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155,1,['log'],['log-space']
Testability,"ltSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8/./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true --input ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --reference ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --run_without_dbsnp_potentially_ruining_qu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:2704,test,test,2704,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['test'],['test']
Testability,"m.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.nextMatePosition.position == null ? 8 : 0 #mate_unmapped. so it looks like the doc supports the existing code. Should I submit this as an issue? . Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:1425,test,tests,1425,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033,2,['test'],"['test', 'tests']"
Testability,mantics). commit ae841c9ed4cd9b2ca1ac0e9082d175ff8ea98298; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 16:44:02 2017 -0500. shorter gCNV WDL tests. commit 5466b806e36df16cad2d045be074e7f9afec0957; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 16:38:15 2017 -0500. fixed arg issues in somatic WDL; exposed all missing args to java side; major update to germline WDLs; all optional python args exposed to WDLs as optional args. commit 50cb6fd08de15469a9080cbb27ff30c8b7ee7e21; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:50:45 2017 -0500. missing serialVersionUID. commit 5f0f31eab63b0e6f6105708ded7f86c96c830781; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:35:33 2017 -0500. annotated intervals kebab case; updated germline WDL workflows. commit 29cc6234dbfb8db12559217a650c6ceb170c5797; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:15:28 2017 -0500. cleanup test files. commit 08a35bb4e65eceb735adcd41a91132e9a34d2b66; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 02:50:19 2017 -0500. update WDL scripts. commit 12bcfa192ee6fa6da21239ebf5b513633efe974f; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 02:47:33 2017 -0500. significant updates to GermlineCNVCaller; integration tests for GermlineCNVCaller w/ sim data in both run modes. commit 151416a4af735ca721bd75e4b54a780c17ac9397; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 01:42:05 2017 -0500. hybrid ADVI abstract argument collection w/ flexible default values; hybrid ADVI argument collection for contig ploidy model; hybrid ADVI argument collection for germline denoising and calling model. commit 56e21bf955d3dc0c52aceb384f28cf6173959de0; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 23:18:39 2017 -0500. rewritten python-side coverage metadata table reader using pandas to fix the issues with com,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:6566,test,test,6566,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['test'],['test']
Testability,"ndate for a good caller, i.e. to take advantage of prior knowledge. I am also thinking about improving it by checking if there are short read evidence supporting the BND's (sometimes there aren't any, which is strange for the alignments of the assembly to point to such novel adjacencies in the first place).; However, I might have to push back a little on this particular request at this point because such features can be later added on like you said, and probably now is not the best time to do it, because the linking logic is unlikely to be affected by presence of these reference annotations.; One note though, and I think you don't mean it literally either, the large ones are not necessarily artifacts. Like what we discussed during the group meetings, they are ""filtered"" partly because they are more likely to overlap with multiple other BND mates, and are more likely to be artifacts or mobile element insertions. In other words, they are not sent for linking because they are less likely to be suited for the linking logic in `LinkedInversionBreakpointsInference`. But if they do, the linking is totally agnostic about size. > Building in an ability to check the copy number of the region in which an inversion breakpoint lies (by checking against a CNV call for example) would probably be really helpful. Answer: Agree. And it shouldn't be too hard to do that. But again, may be in a different PR?. > When you say 'overlaps with CPX' it might be helpful have a more particular set of criteria.. A larger inversion event might span over a smaller complex event. Answer: Note taken. I'll improve on this in the next polished implementation that is ready for line-by-line review. > I'd check to see if there are additional filters implemented in SV-Pipe that you could apply here. Answer: Yes. But I'm not sure if by SV-Pipe you mean RDTest repo? Sorry I wasn't around when I was added to that repo so I don't know the context. In addition, I believe this could be another future improvemen",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:3302,log,logic,3302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929,1,['log'],['logic']
Testability,"ning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge::keras=2.2.4 # updated from pip-installed 2.2.0, which caused various conflicts/clobbers of conda-installed packages; # conda-installed 2.2.4 appears to be the most recent version with a consistent API and without conflicts/clobbers; # if you wish to update, note that versions of conda-forge::keras after 2.2.5; # undesirably set the environment variable KERAS_BACKEND = theano by default; - defaults::intel-openmp=2019.4; - conda-forge::scikit-learn=0.22.2; - conda-forge::matplotlib=3.2.1; - conda-forge::pandas=1.0.3. # core R dependencies; these should only be used for plotting and do not take precedence over core python dependencies!; - r-base=3.6.2; - r-data.table=1.12.8; - r-dplyr=0.8.5; - r-getopt=1.20.3; - r-ggplot2=3.3.0; - r-gplots=3.0.3; - r-gsalib=2.1; - r-optparse=1.6.4. # other python dependencies; these should be removed after functionality is moved into",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:2523,log,logsumexp,2523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,1,['log'],['logsumexp']
Testability,"nsFromGenomicsDB_newMQformat](https://github.com/broadinstitute/gatk/blob/423d106612074aa3480b67b321dc66426b1c600a/src/test/java/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFsIntegrationTest.java#L405):; ```; assertMatchingAnnotationsFromGenomicsDB_newMQformat[0](src/test/resources/org/broadinstitute/hellbender/tools/GenomicsDBImport/expected.testGVCFMode.gatk4.g.vcf, src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/newMQcalc.singleSample.genotyped.vcf, 20:1-11000000, src/test/resources/large/human_g1k_v37.20.21.fasta); java.lang.AssertionError: Genotype string expected [C*|T] but found [T|C*]; 	at org.testng.Assert.fail(Assert.java:97); ```. However looking at the input file and the expected output file it is clear that the expected output is wrong here (`:PL:PS `**`0|1`**`:23,38:61:99:1` in the last line) and this PR does the right thing instead:; ```; # input:; $ bcftools view org/broadinstitute/hellbender/tools/GenomicsDBImport/expected.testGVCFMode.gatk4.g.vcf | grep -e '1|0' -e ""10007150""; 20 10007150 . G C,<NON_REF> 669.77 . BaseQRankSum=-4.476;DP=63;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.5,0;MQRankSum=0;RAW_MQandDP=226800,63;ReadPosRankSum=-0.077 GT:AD:DP:GQ:PGT:PID:PL:PS:SB 0|1:38,25,0:63:99:0|1:10007150_G_C:698,0,1479,813,1554,2366:10007150:16,22,14,11; 20 10007175 . C T,<NON_REF> 1350.77 . BaseQRankSum=2.249;DP=61;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.5,0;MQRankSum=1.319;RAW_MQandDP=216841,61;ReadPosRankSum=-1.213 GT:AD:DP:GQ:PGT:PID:PL:PS:SB 1|0:23,38,0:61:99:1|0:10007150_G_C:1379,0,780,1448,894,2343:10007150:9,14,18,20; #""expected"" output (wrong):; $ bcftools view org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/newMQcalc.singleSample.genotyped.vcf | grep -e '1|0' -e ""10007150""; 20 10007150 . G C 690.64 . AC=1;AF=0.5;AN=2;BaseQRankSum=-4.476;DP=63;ExcessHet=0;FS=5.048;MLEAC=1;MLEAF=0.5;MQ=60;MQRankSum=0;QD=10.96;ReadPosRankSum=-0.077;SOR=0.746 GT:AD:DP:GQ:PGT:PID:PL:PS 0|1:38,25:63:99:0|1:10007150_G_C:698,0,1479:1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8570#issuecomment-1784915555:1086,test,testGVCFMode,1086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8570#issuecomment-1784915555,1,['test'],['testGVCFMode']
Testability,"nt bin* provides equal weight---rather than the counts themselves. As usual, modeling each bin as Poisson is close enough to modeling all bins as multinomial for our purposes. If we directly use the NB likelihood and simply weight the count likelihood by occurrences, occurrences in the peak will strongly affect the result, adversely so if the count likelihood is actually misspecified there. As an example, consider trying to fit a Poisson to data that is actually zero-inflated Poisson---fitting the histogram will actually result in a more robust estimate for the mean. Another benefit is that truncated data (as we have here) is straightforwardly handled in an unbiased way. In the special case of complete, trivially-binned data, the full, unbinned likelihood is recovered. I think this sort of histogram fitting is pretty standard in the astro/particle community. We can certainly change up the model to include strictly quantized + free-floating states as you describe (rather than the ""fuzzily quantized"" states I use here), but I just wanted to avoid having another level of mixtures/logsumexps for this quick prototype. However, note that modeling mosaicism on the autosomes is desirable, but there we also want the strong diploid prior to nail down the depth and per-contig bias. So we will have to be a little careful about how we introduce free-floating states. Also, since I was not using gcnvkernel, I had to integrate out all discrete parameters. It may be that we can write down a nice model with discrete parameters if we use your inference framework. Finally, I did not further bin the counts here (or rather, the bin size is 1), which already yields the maximum information, but I did use a maximum-count cutoff. If we use the same cutoff for all samples, this allows us to simply pass a non-ragged matrix from Java (with dimensions of samples x contigs x maximum count) as a TSV. However, we may run into trouble if we hit a case sample with very high depth. So some sort of spa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522:1198,log,logsumexps,1198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522,1,['log'],['logsumexps']
Testability,"ong. We should make a clear distinction between the old and new tests though. Ideally the GATK3 tests would be in a separate commit that we can just delete at the end, but that can get unwieldy if the files in the commit need to change as we go along. Alternatively you could isolate them into a separate directory. They should either be disabled or made dependent on a test method (see the `@Test` annotation properties `enabled` and `dependsOn`) that is easily toggled so they can be run locally, but don't run on the CI server. Otherwise the CI server build will always fail. In general, its really helpful to have the first commit in the PR contain the completely unmodified GATK3 source files. It makes it much easier for the reviewer to see what changed for the port. I noticed that you have 2 new plugins included in this. I'm not sure if that was suggested by someone on the GATK team (I'm wondering if we want to go down that path...) but I can tell you that the existing plugins required an enormous amount of test development and review iteration. If we do decide to make them plugins, I think it would be a good idea to do so in a separate PR. Also, if we choose to make an AbstractPlugin base class, we may want that to live in the Barclay repo. As @magicdgs points out, master already has your previous commits, so you should start by rebasing on that. Ideally, the branch would have the following commits before we start the first review cycle:. 1. A single commit containing the unmodified GATK3 source (unmodified with the exception that if a file is renamed for GATK4, its helpful to rename the GATK3 version in this commit so it's easy to compare in the next commit). This commit doesn't have to compile or run - its just to make the review process easier for us, and will be deleted at some point. I can help with how to get this into your branch if you like.; 2. Your modified GATK3 tests in a single commit. This will also be removed before merge.; 3. A single commit with all o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352:1820,test,test,1820,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352,1,['test'],['test']
Testability,"oposed algorithm description:. Answer: thanks for looking! That's what I intended as I'm not sure if the algorithm itself would face strong critics. If that's the case, time spent on coding is not worth it IMO. In fact ideally I'd like to write a design doc or something similar, and only code after the design is agreed on. --------. > I would of course prefer not to have to have a hard filter on length. This would mean we would never call a large inversion even if it exists. . Answer: Totally agree. Now looking back, it get clearer to me that this proposal contains two parts: the filtering part, and the breakpoint linking part, separated into two major classes `InversionBreakendPreFilter` and `LinkedInversionBreakpointsInference`. That being said, it doesn't make much sense to separate them into two PRs because _currently_ the filtering part is designed around the linking part, i.e. it is trying to check which BND's are suitable to the logic implemented in the linking part, and if the logic isn't applicable to an BND, the BND simply slips through without generating any new interpretations. So `InversionBreakendPreFilter` is a filter and a classifier at the same time, it function is really diverting different BND's to be handled by different logics, and it definitely should be improved.; If you buy this argument, I am also fully aware of the code design issue that it is preferable to NOT divert&mdash;gather&mdash;send through different handlers like it currently is for calling variants from the assembly contigs, instead it should be a single stream pass through all the BND's. I'll try to follow the preferred design. > What about some other filters more specifically aimed at the artifacts that cause these false large calls? I think it's a good idea to check annotations -- ie. do the mates lie at two regions that are segmental duplications of each other, or one side of the mate looks like a transposable element insertion? I guess it's ok to put in a tool with this limit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:1123,log,logic,1123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929,2,['log'],['logic']
Testability,options used in tests: ; --compress; -n ; --simplifyBAM; -L 1; -L unmapped; --readGroup; --sample_name,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/154#issuecomment-71864180:16,test,tests,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/154#issuecomment-71864180,1,['test'],['tests']
Testability,"orcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:2899,test,tests,2899,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['test'],['tests']
Testability,"orming something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8/./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true --input ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --reference ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --run_without_dbsnp_potentially_ruining_quality false --solid_recal_mode SET_Q_ZERO --solid_nocall_strategy THROW_EXCEPTION --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_default_quality -1 --insertions_default_quality 45 --deletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --interval_set_rule UNION --interval_padding 0 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_less_than 6 --useOriginalQualities false --defaultBaseQualities -1 --runner LOCAL --client_secret client-secrets.jso",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:3239,test,test,3239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['test'],['test']
Testability,"osOnly.IF.snp.trainingScores.hdf5 /repo/extract.nonAS.snpIndel.posUn.train.snpIndel.posOnly.IF.snp.trainingScores.hdf5. file1 file2; ---------------------------------------; x x / ; x x /data ; x x /data/scores . group : </> and </>; 0 differences found; group : </data> and </data>; 0 differences found; dataset: </data/scores> and </data/scores>; size: [445] [445]; position scores scores difference ; ------------------------------------------------------------; [ 60 ] -0.419202 -0.419202 5.55112e-17 ; 1 differences found; ```. Looks pretty negligible to me! :stuck_out_tongue_closed_eyes: Probably a result of the native code being called by the python/ML packages used in these tools; even minor changes in the compilers across Ubuntu versions might introduce differences like these. A quick fix might be to replace all system calls to `h5diff` in these tests with `h5diff --use-system-epsilon`; seems to do the trick here. But if that doesn't fix all test cases, then perhaps you can relax things with `h5diff -p EPSILON`, where `EPSILON` is a relative threshold. Probably OK to pick something like `1E-6`. OK if I leave it to you to try this or otherwise check the rest of the cases?. Sorry for the inconvenience! I think the exact-match test worked as intended here, but I probably could've put in better messaging originally. Unfortunately, it's a bit awkward to grab the output of system commands. And thanks for dealing with conda again (a necessary evil, unless we want to reimplement the entire field of machine learning in Java)! I'll experiment to see if I can't get the more recent version used in #8561 (23.10) working with the current environment---probably just some minor tweak to the pip version is needed to get around the error you're seeing. You could try unpinning it to see what gets pulled in. It would be great if we could get off the old version of conda, since more recent versions using the libmamba solver are *MUCH* faster and would cut down all of our Docker build ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848796931:1476,test,test,1476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848796931,1,['test'],['test']
Testability,"osaicism_s.dimshuffle(0, 'x', 'x')) + commons.negative_binomial_logp(mu_sjk, alpha.dimshuffle('x', 'x', 'x'), _n_sj.dimshuffle(0, 1, 'x')),; tt.log(pi_mosaicism_s.dimshuffle(0, 'x', 'x')) + commons.negative_binomial_logp(mu_mosaic_sjk, alpha.dimshuffle('x', 'x', 'x'), _n_sj.dimshuffle(0, 1, 'x'))],; axis=0)[0]; return _logp_sjk. DensityDist(name='n_sj_obs',; logp=lambda _n_sj: tt.sum(q_ploidy_sjk * _get_logp_sjk(_n_sj), axis=2),; observed=n_sj); ````. Briefly, the model includes 1) per-contig bias (normalized to unit mean for identifiability), 2) per-sample depth, 3) per-sample probability of mosaicism, 4) per-sample-and-contig mosaicism factor `f` (in [0, 1], normalized by the per-sample max for identifiability), 5) per-contig mapping error. The likelihood is then a negative-binomial mixture of non-mosaic and mosaic contigs, where the latter have their mean count depressed by the corresponding factor `f`. This model still requires some tuning of priors (which are currently hard coded above), but seems to correctly capture most of the mosaicism in the test samples. Also, I found that it was better to run the aneuploid samples as a cohort or to run them in combination with the 20 panel samples as a cohort, rather than to run them in case mode against the panel. We don't necessarily have to emit anything on the mosaicism inferences for the first revision of this model (or we may end up stripping those parts of the model out for now), but I thought it would be good to record this version of the model for posterity. However, note that this model differs from the one currently in master in the treatment of depth. I think the treatment here is quite natural and may be more robust than the current treatment. @mbabadi is going to take over tuning and tweaking the model from this point in the sl_simple_ploidy branch. Note that I haven't cleaned up some of the code and comments yet, but hopefully the changes are relatively clear. I believe I rebased on one of your other branc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-371334890:2609,test,test,2609,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-371334890,1,['test'],['test']
Testability,"osaicism_sj',; alpha=10.0,; beta=1.0,; shape=(ploidy_workspace.num_samples, ploidy_workspace.num_contigs,)); norm_f_mosaicism_sj = f_mosaicism_sj / tt.max(f_mosaicism_sj, axis=1).dimshuffle(0, 'x'). # per-contig mapping error; eps_j = HalfNormal('eps_j', sd=0.01, shape=(ploidy_workspace.num_contigs,)). # negative-binomial means; mu_sjk = depth_s.dimshuffle(0, 'x', 'x') * t_j.dimshuffle('x', 0, 'x') * norm_bias_j.dimshuffle('x', 0, 'x') * \; (ploidy_workspace.int_ploidy_values_k.dimshuffle('x', 'x', 0) + eps_j.dimshuffle('x', 0, 'x')); mu_mosaic_sjk = norm_f_mosaicism_sj.dimshuffle(0, 1, 'x') * mu_sjk. # ""unexplained variance""; psi = Uniform(name='psi', upper=10.0). # convert ""unexplained variance"" to negative binomial over-dispersion; alpha = tt.inv((tt.exp(psi) - 1.0)). def _get_logp_sjk(_n_sj):; _logp_sjk = logsumexp([tt.log(1 - pi_mosaicism_s.dimshuffle(0, 'x', 'x')) + commons.negative_binomial_logp(mu_sjk, alpha.dimshuffle('x', 'x', 'x'), _n_sj.dimshuffle(0, 1, 'x')),; tt.log(pi_mosaicism_s.dimshuffle(0, 'x', 'x')) + commons.negative_binomial_logp(mu_mosaic_sjk, alpha.dimshuffle('x', 'x', 'x'), _n_sj.dimshuffle(0, 1, 'x'))],; axis=0)[0]; return _logp_sjk. DensityDist(name='n_sj_obs',; logp=lambda _n_sj: tt.sum(q_ploidy_sjk * _get_logp_sjk(_n_sj), axis=2),; observed=n_sj); ````. Briefly, the model includes 1) per-contig bias (normalized to unit mean for identifiability), 2) per-sample depth, 3) per-sample probability of mosaicism, 4) per-sample-and-contig mosaicism factor `f` (in [0, 1], normalized by the per-sample max for identifiability), 5) per-contig mapping error. The likelihood is then a negative-binomial mixture of non-mosaic and mosaic contigs, where the latter have their mean count depressed by the corresponding factor `f`. This model still requires some tuning of priors (which are currently hard coded above), but seems to correctly capture most of the mosaicism in the test samples. Also, I found that it was better to run the aneuploid samples as a cohor",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-371334890:1685,log,log,1685,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-371334890,1,['log'],['log']
Testability,"ot sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage if there is no data for known indels. I will propose very soon two PRs with fully functional tools (without the n-way out feature for indel-realignment), and trying to add simple integration tests with the data already available on the repository and running with GATK3.8-1. If that is OK for you, I will proceed with this approach.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:2193,test,tests,2193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115,1,['test'],['tests']
Testability,"ound first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether any given point is a suitable changepoint; using the data locally surrounding the point typically suffices. With these modifications, ApproxKernSeg can handle both scenarios:; ![wave-kern](https://user-images.githubusercontent.com/11076296/29322762-a679dba6-81ac-11e7-9360-083a4e1da398.png); ![wave-kern-small-waves](https://user-images.githubusercontent.com/11076296/29322801-dad82010-81ac-11e7-8238-e057b0072e1b.png). This local window approach is still linear in time, so runtime is still ~1s for the above (about ~10x faster than CBS). One issue still remains, which is that even this improved approach tends to find directly adjacent possible changepoints around a true changepoint before moving on to another true changepoint. We can probably clean this up with some simple postprocessing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:1981,test,test,1981,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045,1,['test'],['test']
Testability,"ov 200 -minPruning 4 -o /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/eflynn90-test.vcf -L /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/intervals.vcf -stand_emit_conf 10 -pedValidationType SILENT; ```. ---. @eitanbanks said:. Updated command-line:. ```; java -Xmx6g -jar dist/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/1kg/reference/hs37d5.fasta -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND1/UDP2731_1.forGATK.bam -dcov 200 -minPruning 4 -L 1:14464; ```. I can confirm that it appears that down-sampling is not working for the Haplotype Caller (when run through the Unified Genotyper the down-sampling works just fine).; I see in SAMDataSource line 668 that assumeDownstreamLIBSDownsampling is being set to true. But then it doesn't look like LIBS is actually down-sampling. Don't have time to debug more so passing on to David. ---. @droazen said (over multiple comments):. I am looking into this. LIBS is actually calling into the downsamplers correctly in the test case that Eric provided. You can see this by examining readStates.size() for each locus -- it never exceeds the -dcov target of 200. The problem must lie elsewhere -- I'll continue to step through this in the debugger. [...]. After some more debugging and consultation with Ryan, I've found that DP values exceeding dcov are to be expected given the way the ActiveRegion traversal currently works. Here's a summary of what's going on:. -dcov 200 does cause LIBS to cap the depth at each locus to 200, but due to code Mark added a while back LIBS will save all of the undownsampled reads in memory during active region traversals (which kind of defeats the purpose of downsampling in the first place!). -TraverseActiveRegions gets the undownsampled reads from LIBS, and adds them to the active regions that get passed to the walker. -The HaplotypeCaller does a post-hoc downsampling pass on the reads in the active region in finalizeActiveRegion() to a hardcoded (!!!) and compl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:2124,test,test,2124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['test'],['test']
Testability,"park 2.4 is needed to run on Java 11. I disabled `ADAMKryoRegistrator` in this branch since the version we are using is compiled against Scala 2.11. We might consider removing it entirely and no longer support ADAM formats directly in GATK - the workaround would be to use ADAM to convert to/from BAM/VCF. ADAM is also needed for reading 2bit files.; * Java 11 deprecates some APIs. Most of these are fairly easy to fix or suppress. The exception is the Javadoc API [com.sun.javadoc](https://docs.oracle.com/en/java/javase/11/docs/api/jdk.javadoc/com/sun/javadoc/package-summary.html), which has been replaced by [jdk.javadoc.doclet](https://docs.oracle.com/en/java/javase/11/docs/api/jdk.javadoc/jdk/javadoc/doclet/package-summary.html). The javadoc tools in `org.broadinstitute.hellbender.utils.help` may need to be re-written (and it's not clear if it's possible to support Java 8 and Java 11 simultaneously).; * Travis build. Getting this to build and test on Java 11 in addition to the current builds may be fairly involved as the matrix is already quite complicated. (The current PR just changes Java 8 to Java 11 for testing purposes - we'd need a way of getting both to run.). The vast majority of tests are passing on Java 11, the following are failing:; * Missing `TwoBitRecord` (from ADAM); * `ReferenceMultiSparkSourceUnitTest`; * `ImpreciseVariantDetectorUnitTest`; * `SVVCFWriterUnitTest`; * `DiscoverVariantsFromContigAlignmentsSAMSparkIntegrationTest`; * `StructuralVariationDiscoveryPipelineSparkIntegrationTest`; * `SvDiscoverFromLocalAssemblyContigAlignmentsSparkIntegrationTest`; * `java.lang.NoSuchMethodError: java.nio.ByteBuffer.clear()Ljava/nio/ByteBuffer;`; * `SeekableByteChannelPrefetcherTest`; * `GatherVcfsCloudIntegrationTest`; * `Could not serialize lambda`; * `ExampleAssemblyRegionWalkerSparkIntegrationTest`; * `PileupSparkIntegrationTest`; * Native HMM library code caused the tests to crash on my Mac:; ```; Running Test: Test method testLikelihoodsFromHaplotypes[0",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359:1102,test,test,1102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359,1,['test'],['test']
Testability,park/2.3.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/wgs.hg38/pipelines/hc/cram.test/GRCh38_full_analysis_set_plus_decoy_hla.fa.gz --spark-master yarn; 2019-01-09 13:35:04 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:05 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 13:35:09.640 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:35:09.799 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:11.507 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.508 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:1587,test,test,1587,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['test'],['test']
Testability,"r cases; ; - [x] `ChimericAlignment`; - [x] update documentation; - [x] implement a `getCoordinateSortedRefSpans()`, and use in `BreakpointsInference`; - [x] `isNeitherSimpleTranslocationNorIncompletePicture()`; - [x] `extractSimpleChimera()`. ### bump test coverage; Once code above is consolidated, bump test coverage, particularly for the classes above and the following poorly-covered classes; - [x] `ChimericAlignment`; - [x] `isForwardStrandRepresentation()`; - [x] `splitPairStrongEnoughEvidenceForCA()` ; - [x] `parseOneContig()` (needs testing because we need it for simple-re-interpretation for CPX variants) Note that `nextAlignmentMayBeInsertion()` is currently broken in the sense that when using this to filter out alignments whose ref span is contained by another, check if the two alignments involved are head/tail. - [x] `BreakpointsInference` & `BreakpointComplications`. - [x] `NovelAdjacencyAndAltHaplotype`; - [x] `toSimpleOrBNDTypes()`. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence`; - [x] serialization test. - [x] `AnnotatedVariantProducer`; - [x] `produceAnnotatedBNDmatesVcFromNovelAdjacency()`. - [x] `BreakEndVariantType`. - [ ] `SvDiscoverFromLocalAssemblyContigAlignmentsSpark` integration test; . ### update how variants are represented ; Implement the following representation changes that should make type-based evaluation easier; - [x] change `INSDUP` to`INS` when the duplicated ref region, denoted with annotation `DUP_REPEAT_UNIT_REF_SPAN`, is shorter than 50 bp.; - [x] change scarred deletion calls, which currently output as `DEL` with `INSSEQ` annotation, to one of these; - [x] `INS`/`DEL`, when deleted/inserted bases are < 50 bp and annotate accordingly; when type is determined as`INS`, the `POS` will be 1 base before the micro-deleted range and `END` will be end of the micro-deleted range, where the `REF` allele will be the corresponding reference bases.; - [x] two records `INS` and `DEL` when both are >= 50, share the same `POS`, and lin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021:2001,test,test,2001,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021,1,['test'],['test']
Testability,"r gradle; build.gradle. 3. Significant changes to existing code to support/invoke new filter; - add arguments for XGBoostEvidenceFilter, changes for scaling density filter by coverage; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/StructuralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:1707,test,tests,1707,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,2,['test'],"['test', 'tests']"
Testability,"r-contig mapping error; eps_j = HalfNormal('eps_j', sd=0.01, shape=(ploidy_workspace.num_contigs,)). # negative-binomial means; mu_sjk = depth_s.dimshuffle(0, 'x', 'x') * t_j.dimshuffle('x', 0, 'x') * norm_bias_j.dimshuffle('x', 0, 'x') * \; (ploidy_workspace.int_ploidy_values_k.dimshuffle('x', 'x', 0) + eps_j.dimshuffle('x', 0, 'x')); mu_mosaic_sjk = norm_f_mosaicism_sj.dimshuffle(0, 1, 'x') * mu_sjk. # ""unexplained variance""; psi = Uniform(name='psi', upper=10.0). # convert ""unexplained variance"" to negative binomial over-dispersion; alpha = tt.inv((tt.exp(psi) - 1.0)). def _get_logp_sjk(_n_sj):; _logp_sjk = logsumexp([tt.log(1 - pi_mosaicism_s.dimshuffle(0, 'x', 'x')) + commons.negative_binomial_logp(mu_sjk, alpha.dimshuffle('x', 'x', 'x'), _n_sj.dimshuffle(0, 1, 'x')),; tt.log(pi_mosaicism_s.dimshuffle(0, 'x', 'x')) + commons.negative_binomial_logp(mu_mosaic_sjk, alpha.dimshuffle('x', 'x', 'x'), _n_sj.dimshuffle(0, 1, 'x'))],; axis=0)[0]; return _logp_sjk. DensityDist(name='n_sj_obs',; logp=lambda _n_sj: tt.sum(q_ploidy_sjk * _get_logp_sjk(_n_sj), axis=2),; observed=n_sj); ````. Briefly, the model includes 1) per-contig bias (normalized to unit mean for identifiability), 2) per-sample depth, 3) per-sample probability of mosaicism, 4) per-sample-and-contig mosaicism factor `f` (in [0, 1], normalized by the per-sample max for identifiability), 5) per-contig mapping error. The likelihood is then a negative-binomial mixture of non-mosaic and mosaic contigs, where the latter have their mean count depressed by the corresponding factor `f`. This model still requires some tuning of priors (which are currently hard coded above), but seems to correctly capture most of the mosaicism in the test samples. Also, I found that it was better to run the aneuploid samples as a cohort or to run them in combination with the 20 panel samples as a cohort, rather than to run them in case mode against the panel. We don't necessarily have to emit anything on the mosaicism inferences for t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-371334890:1902,log,logp,1902,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-371334890,1,['log'],['logp']
Testability,"r:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-07 11:33:27 INFO Utils:54 - Successfully started service 'sparkDriver' on port 46828.; 2019-01-07 11:33:27 INFO SparkEnv:54 - Registering MapOutputTracker; 2019-01-07 11:33:27 INFO SparkEnv:54 - Registering BlockManagerMaster; 2019-01-07 11:33:27 INFO BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-07 11:33:27 INFO BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up; 2019-01-07 11:33:27 INFO DiskBlockManager:54 - Created local directory at /tmp/blockmgr-08460386-3abb-4431-ba8d-5b7d41a2a05c; 2019-01-07 11:33:27 INFO MemoryStore:54 - MemoryStore started with capacity 408.6 MB; 2019-01-07 11:33:27 INFO SparkEnv:54 - Registering OutputCommitCoordinator; 2019-01-07 11:33:27 INFO log:192 - Logging initialized @10679ms; 2019-01-07 11:33:27 INFO Server:346 - jetty-9.3.z-SNAPSHOT; 2019-01-07 11:33:27 INFO Server:414 - Started @10862ms; 2019-01-07 11:33:27 WARN Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; 2019-01-07 11:33:27 INFO AbstractConnector:278 - Started ServerConnector@f1d88ea{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-01-07 11:33:27 INFO Utils:54 - Successfully started service 'SparkUI' on port 4041.; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5a39e554{/jobs,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@67941d{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2ad2b274{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7114e780{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-07 11",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:6334,log,log,6334,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['log'],['log']
Testability,"r:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-09 13:35:12 INFO Utils:54 - Successfully started service 'sparkDriver' on port 42689.; 2019-01-09 13:35:12 INFO SparkEnv:54 - Registering MapOutputTracker; 2019-01-09 13:35:12 INFO SparkEnv:54 - Registering BlockManagerMaster; 2019-01-09 13:35:12 INFO BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-09 13:35:12 INFO BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up; 2019-01-09 13:35:12 INFO DiskBlockManager:54 - Created local directory at /tmp/blockmgr-dd94d6fb-7e3d-4def-a895-6e60f05d7a05; 2019-01-09 13:35:12 INFO MemoryStore:54 - MemoryStore started with capacity 372.6 MB; 2019-01-09 13:35:12 INFO SparkEnv:54 - Registering OutputCommitCoordinator; 2019-01-09 13:35:12 INFO log:192 - Logging initialized @9845ms; 2019-01-09 13:35:12 INFO Server:346 - jetty-9.3.z-SNAPSHOT; 2019-01-09 13:35:12 INFO Server:414 - Started @9981ms; 2019-01-09 13:35:12 WARN Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; 2019-01-09 13:35:12 INFO AbstractConnector:278 - Started ServerConnector@22fda322{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-01-09 13:35:12 INFO Utils:54 - Successfully started service 'SparkUI' on port 4041.; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@44084713{/jobs,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@43c0c13a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@731db93f{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2ad2b274{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-09 ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:6073,log,log,6073,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['log'],['log']
Testability,"ralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:1969,test,test,1969,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,1,['test'],['test']
Testability,"rom reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class type of JNIReadDataHolderClass; * @param haplotypeDataHolderClass class type of JNIHaplotypeDataHolderClass; */; void jniInitializeClassFields(Class<JNIReadDataHolderClass> readDataHolderClass, Class<JNIHaplotypeDataHolderClass> haplotypeDataHolderClass);. /**; * Real compute kernel; */; void jniComputeLikelihoods(int numReads, int numHaplotypes, JNIReadDataHolderClass[] readDataArray,; JNIHaplotypeDataHolderClass[] haplotypeDataArray, double[] likelihoodArray, int maxNumThreadsToUse);. /**; * Print final profiling information from native code. ; */; default void close() { jniClose(); }. void jniClose();; }; ```. and a class that implements those as native methods . ```; public class AVXNativePairHMMKernel implements NativePairHMMKernel{. @Override; native void jniInitializeClassFields(Class<JNIReadDataHolderClass> readDataHolderClass, Class<JNIHaplotypeDataHolderClass> haplotypeDataHolderClass);. @Override; native void jniComputeLikelihoods(int numReads, int numHaplotypes, JNIReadDataHolderClass[] readDataArray,; JNIHaplotypeDataHolderClass[] haplotypeDataArray, double[] likelihoodArray, int maxNumThreadsToUse);; @Override; native void jniClose();; }; ```. The PPC repo will implement `NativePairHMMKernel` as well and have its own tests",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:3253,test,tests,3253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864,1,['test'],['tests']
Testability,"rval set upstream of this one should call it. I think GenomicsDbImport's behavior is fine here. If you have a multi-NT variant that starts within 1050-1150, but extends outside (i.e. deletion or insertion starting at 1148), this could be a problem. The GenomicsDB workspace created with the interval 1:1050-1150 lacks the information to score that, right? The workspace created using the more permissive SelectVariants->GenomicsDBImport contains that downstream information and presumably would make the same call as if GenotypeGVCFs was given the intact chromosome as input, right?. However, it seems that if I simply create the workspace with a reasonably padded interval (adding 1kb should be more than enough for Illumina, right?), and then run GenotypeGVCFs with the original, unpassed interval, then the resulting workspace should contain all available information and GenotypeGVCFs should be able to make the same call as if it was given a whole-chromosome workspace as input. . Does that logic seem right? . ```; # The Input gVCF; 1	1040	.	A	<NON_REF>	.	.	END=1046	GT:DP:GQ:MIN_DP:PL	0/0:15:24:14:0,24,360; 1	1047	.	T	<NON_REF>	.	.	END=1047	GT:DP:GQ:MIN_DP:PL	0/0:14:4:14:0,4,418; 1	1048	.	G	<NON_REF>	.	.	END=1141	GT:DP:GQ:MIN_DP:PL	0/0:19:26:12:0,26,411; 1	1142	.	C	T,<NON_REF>	115.64	.	BaseQRankSum=-2.237;DP=19;MQRankSum=-2.312;RAW_GT_COUNT=0,1,0;RAW_MQandDP=43640,19;ReadPosRankSum=0.851	GT:AD:DP:GQ:PGT:PID:PL:PS:SB	0|1:15,4,0:19:99:0|1:1142_C_T:123,0,551,168,563,731:1142:9,6,2,2; 1	1143	.	G	<NON_REF>	.	.	END=1168	GT:DP:GQ:MIN_DP:PL	0/0:17:37:16:0,37,475; 1	1169	.	G	A,<NON_REF>	123.64	.	BaseQRankSum=-1.808;DP=18;MQRankSum=-1.313;RAW_GT_COUNT=0,1,0;RAW_MQandDP=30190,18;ReadPosRankSum=1.331	GT:AD:DP:GQ:PGT:PID:PL:PS:SB	0|1:14,4,0:18:99:0|1:1142_C_T:131,0,455,168,467,635:1142:7,7,2,2; 1	1170	.	C	<NON_REF>	.	.	END=1191	GT:DP:GQ:MIN_DP:PL	0/0:15:27:14:0,27,405; 1	1192	.	C	G,<NON_REF>	130.64	.	BaseQRankSum=-1.811;DP=14;MQRankSum=-1.193;RAW_GT_COUNT=0,1,0;RAW_MQandDP=21790,14;ReadPo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221558244:2453,log,logic,2453,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221558244,1,['log'],['logic']
Testability,"s fail. In general, its really helpful to have the first commit in the PR contain the completely unmodified GATK3 source files. It makes it much easier for the reviewer to see what changed for the port. I noticed that you have 2 new plugins included in this. I'm not sure if that was suggested by someone on the GATK team (I'm wondering if we want to go down that path...) but I can tell you that the existing plugins required an enormous amount of test development and review iteration. If we do decide to make them plugins, I think it would be a good idea to do so in a separate PR. Also, if we choose to make an AbstractPlugin base class, we may want that to live in the Barclay repo. As @magicdgs points out, master already has your previous commits, so you should start by rebasing on that. Ideally, the branch would have the following commits before we start the first review cycle:. 1. A single commit containing the unmodified GATK3 source (unmodified with the exception that if a file is renamed for GATK4, its helpful to rename the GATK3 version in this commit so it's easy to compare in the next commit). This commit doesn't have to compile or run - its just to make the review process easier for us, and will be deleted at some point. I can help with how to get this into your branch if you like.; 2. Your modified GATK3 tests in a single commit. This will also be removed before merge.; 3. A single commit with all of your ""minimal"" changes for the port, including the real, new tests. This should compile, and tests should pass on CI with pretty high code coverage. This is what we'll iterate on. After that, its really helpful to have only a single new commit for each review iteration (you can create as many commits as you want as you work, but squash the new commits down before submitting). Just don't squash or rebase anything thats already been pushed up to the repo. Also, note that most of the GATK engine team is out for a few weeks, so progress may be slow in the short term.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352:2704,test,tests,2704,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352,3,['test'],['tests']
Testability,"s method than our previous probabilistic approaches. Even SNP segmentation will be much cheaper. > What is the name of this approach? ""KernSeg""?. Not sure...I couldn't find an R package, although an R/C implementation is mentioned in the paper. But the python implementation is straightforward and a pure Java implementation should not be so bad. There are some cythonized numpy methods that my python implementation used, but I think equivalent implementations of these methods should be relatively fast in pure Java as well. > What variant of the algorithm did you implement? the paper lists several. I implemented what they call ApproxKSeg. It's an approximate version that combines binary segmentation with the low-rank approximation to the Gaussian kernel. > I haven't read the paper in detail yet, but is it possible to choose a conservatively large number of possible break points and then filter bad break points, possibly based on the rapid decline of the change point probability? i.e. does the algorithm naturally produce change point probabilities?. Yes, you can oversegment and then choose which breakpoints to retain. However, there are no proper changepoint probabilities, only changepoint costs. Adding a penalty term based on the number of changepoints seems to perform relatively well in simple tests, but one could certainly devise other ways to filter changepoints (some of which could yield probabilities, if you are willing to assume a probabilistic model). I think we should just think of this as a fast, heuristic, non-parametric method for finding breakpoints in multidimensional data. > Is it possible to throw in additional change points incrementally, without doing extra work, until a certain criterion is met? (see above). The version I implemented adds changepoints via binary segmentation. The time complexity required to split a segment is linear in the number of points contained in the segment, although some care must be taken in the implementation to ensure this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321140715:2809,test,tests,2809,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321140715,1,['test'],['tests']
Testability,s_set_plus_decoy_hla.fa -- --spark-runner SPARK --spark-master yarn; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar; Running:; /share/pkg/spark/2.3.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 2019-01-07 11:33:18 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:19 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 11:33:24.377 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:33:24.549 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:33:26.271 INFO CountR,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:1741,test,test,1741,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['test'],['test']
Testability,"saicism; pi_mosaicism_s = Beta(name='pi_mosaicism_s',; alpha=1.0,; beta=50.0,; shape=(ploidy_workspace.num_samples,)). # per-sample-and-contig mosaicism factor; f_mosaicism_sj = Beta(name='f_mosaicism_sj',; alpha=10.0,; beta=1.0,; shape=(ploidy_workspace.num_samples, ploidy_workspace.num_contigs,)); norm_f_mosaicism_sj = f_mosaicism_sj / tt.max(f_mosaicism_sj, axis=1).dimshuffle(0, 'x'). # per-contig mapping error; eps_j = HalfNormal('eps_j', sd=0.01, shape=(ploidy_workspace.num_contigs,)). # negative-binomial means; mu_sjk = depth_s.dimshuffle(0, 'x', 'x') * t_j.dimshuffle('x', 0, 'x') * norm_bias_j.dimshuffle('x', 0, 'x') * \; (ploidy_workspace.int_ploidy_values_k.dimshuffle('x', 'x', 0) + eps_j.dimshuffle('x', 0, 'x')); mu_mosaic_sjk = norm_f_mosaicism_sj.dimshuffle(0, 1, 'x') * mu_sjk. # ""unexplained variance""; psi = Uniform(name='psi', upper=10.0). # convert ""unexplained variance"" to negative binomial over-dispersion; alpha = tt.inv((tt.exp(psi) - 1.0)). def _get_logp_sjk(_n_sj):; _logp_sjk = logsumexp([tt.log(1 - pi_mosaicism_s.dimshuffle(0, 'x', 'x')) + commons.negative_binomial_logp(mu_sjk, alpha.dimshuffle('x', 'x', 'x'), _n_sj.dimshuffle(0, 1, 'x')),; tt.log(pi_mosaicism_s.dimshuffle(0, 'x', 'x')) + commons.negative_binomial_logp(mu_mosaic_sjk, alpha.dimshuffle('x', 'x', 'x'), _n_sj.dimshuffle(0, 1, 'x'))],; axis=0)[0]; return _logp_sjk. DensityDist(name='n_sj_obs',; logp=lambda _n_sj: tt.sum(q_ploidy_sjk * _get_logp_sjk(_n_sj), axis=2),; observed=n_sj); ````. Briefly, the model includes 1) per-contig bias (normalized to unit mean for identifiability), 2) per-sample depth, 3) per-sample probability of mosaicism, 4) per-sample-and-contig mosaicism factor `f` (in [0, 1], normalized by the per-sample max for identifiability), 5) per-contig mapping error. The likelihood is then a negative-binomial mixture of non-mosaic and mosaic contigs, where the latter have their mean count depressed by the corresponding factor `f`. This model still requires some tuning of ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-371334890:1515,log,logsumexp,1515,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-371334890,1,['log'],['logsumexp']
Testability,simple tests added in #884,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/709#issuecomment-160836902:7,test,tests,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/709#issuecomment-160836902,1,['test'],['tests']
Testability,"simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than trying to figure out where the old one is failing would be best.; - [x] For small bins (250bp), the copy-ratio model is currently a bit memory intensive, since it stores an outlier indicator boolean for every data point (it gets by with -Xmx12g for 100 iterations at 250bp). There is no easy away around storing this at the GibbsSampler level (although we could make some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we could move to a BitSet, but I'm not sure what the performance hit will be. EDIT: It was trivial to switch",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:6704,log,logit,6704,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['log'],['logit']
Testability,such tests mostly exist in VectorPairHMMUnitTest that @gspowley wrote. I think we could just move them there and add a simple java implementation of `computeLikelihoods` that is pretty much a copy of code from `LoglessPairHMM`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2030#issuecomment-234598263:5,test,tests,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2030#issuecomment-234598263,1,['test'],['tests']
Testability,"t https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8/./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true --input ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --reference ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --run_without_dbsnp_potentially_ruining_quality false --solid_recal_mode SET_Q_ZERO --solid_nocall_strategy THROW_EXCEPTION --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_default_quality -1 --insertions_default_quality 45 --deletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --interval_set_rule UNION --interval_padding 0 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_less_than 6 --useOriginalQualities false --defaultBaseQualities -1 --runner LOCAL --client_secret client-secrets.json --help false --version false --VERBOSITY INFO --QUIET false; [June 1, 2015 5:51:02 PM EDT] Executing as p",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:3302,test,test,3302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['test'],['test']
Testability,"tash@broadinstitute.org>; Date: Thu Dec 7 02:50:19 2017 -0500. update WDL scripts. commit 12bcfa192ee6fa6da21239ebf5b513633efe974f; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 02:47:33 2017 -0500. significant updates to GermlineCNVCaller; integration tests for GermlineCNVCaller w/ sim data in both run modes. commit 151416a4af735ca721bd75e4b54a780c17ac9397; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 01:42:05 2017 -0500. hybrid ADVI abstract argument collection w/ flexible default values; hybrid ADVI argument collection for contig ploidy model; hybrid ADVI argument collection for germline denoising and calling model. commit 56e21bf955d3dc0c52aceb384f28cf6173959de0; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 23:18:39 2017 -0500. rewritten python-side coverage metadata table reader using pandas to fix the issues with comment line; change criterion for cohort/case based on whether a contig-ploidy model is provided or not; simulated test files for ploidy determination tool; proper integration test for ploidy determination tool and all edge cases; updated docs for ploidy determination tool. commit 7fa104b2e9170770cfc5b338835e41215d7fd39c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 18:43:17 2017 -0500. kabab case for gCNV-related tools; removed short args (this also partially affected PlotDenoisedCopyRatios and PlotModeledSegments and their integration tests). commit f02cb024331a986213cfd9fae2da706bbc5ddbd9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 18:02:40 2017 -0500. synced with mb_gcnv_python_kernel. commit 2963bbf8c90418d9b88545c93771ae51cf542db9; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 11:38:05 2017 -0500. Fixing typo in travis.yml. commit 6cf589999c716ec66404eb0a2ae4310dd130a772; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 11:13:58 2017 -0500. editable, full path. commit d998f2d5c2b33dd41e291b",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:7680,test,test,7680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,2,['test'],['test']
Testability,"that the breakpoint is downstream of the interval end position; */; ```. What else would you like to see documented there? . - The use of the word strand in this case is largely driven by a mapping of these data structures to the BEDPE format, which is the older format for representing breakpoints implied by paired-end mapping data without assembly. If you only consider read pair mappings, strand has the natural interpretation of being the strand to which reads aligned. For example, a deletion's two intervals have strands `+` and `-` because the `+` reads align at left breakpoint and `-` reads align near the right breakpoint. Extending the concept to supplementary mappings of split reads muddies the concept a bit, which made me change the definition of strand to the existing one: whether the evidence suggests a breakpoint upstream of the interval start or downstream of the interval end. . - I created `StrandedInterval` mostly just as a data container since I was often passing around an interval and an associated strand, and using them in conjunction with the `PairedStrandedIntervalTree` data structure. My goal with those was to have them be utility classes that could be used by anyone without regards to the particular mechanics of imprecise evidence clustering I've implemented here. I'd prefer to put the definition of how we're interpreting the interval and strand in our logic classes (`BreakpointEvidence`, `EvidenceTargetLink`, and EvidenceTargetLinkClusterer`). Does that make sense?. - A ""distal target region"" can be represented by a `StrandedInterval`. So can the original, proximal (non-distal) location of the breakpoint evidence. An `EvidenceTargetLink` has the two `StrandedInterval` objects representing the proximal and distal locations, and the count of evidence types in the link cluster. Does that make things more clear?. I've added some ASCII-art visual examples to `DiscordantReadPairEvidence.getDistalTargets` and `SplitRead.getDistalTargets`. Do those help?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3628#issuecomment-333857471:2518,log,logic,2518,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3628#issuecomment-333857471,1,['log'],['logic']
Testability,"the context. In addition, I believe this could be another future improvements?. > I don't think it's that necessary to provide reports on things that didn't become breakpoints. If they still exist as BNDs in the original perhaps there are some codes we could add as INFO field annotations to indicate that they were considered for inversion calling but filtered, and the reason for doing so. Answer: Agree. And I think the final format can definitely change, as long as the resolved variants, be it deletions, inverted dispersed duplications, and/or real inversions are registered in a VCF. The BED file that is currently produced is for development use, i.e. for me to check why a certain BND is not sent for the analysis so that I can develop ideas on how to catch them in the future (you know, BED are IGV-friendly), and it can certainly be dropped in the final output. ; Regarding adding another INFO field saying they are not used for _THIS PARTICULAR_ logic, I prefer not to do it this way, because we can add more logics in the future, and capturing/resolving these BND's that are not suitable for _THIS PARTICULAR_ logic. I guess in general my personal preference is to put less algorithm-related information in VCF for analysts (less reading for them), and produce add on files for tool developers. What's your thoughts?. > Does this even have to be a spark tool? It looks like you are just reading the variants into a parallel spark context, filtering, and then collecting them to actually process them. Why not just make this a non-spark tool and process it all in memory on one node?. Answer: Agree. It doesn't have to be, at least in theory, and it probably is going to be faster as we don't need to incur the Spark overhead for such a typically small job. But (I'm saying too many buts....) up to this point all SV tools are under the package `hellbender.tools.spark.sv`, so I'm following suit here. Note the two classes's main interface methods mentions nothing about RDDs (that's on pu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:5156,log,logic,5156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929,3,['log'],"['logic', 'logics']"
Testability,"the execution environment only supports a ; small finite number of headers, the header tag could be a small integer, ; or in a different execution environment it could be; a unique hash of the header or something like that. Memory footprint in ; the receiver is minimized because many SAMRecords can all share the same ; header object.; This requires more work to support in each execution environment, but it ; seems like it could be efficient and allows application code written to ; operate on SAMRecords to be portable; across different execution environments without having to contend with ; the possible presence of headerless SAMRecords. -Bob. On 9/17/15 4:28 PM, droazen wrote:. > @davidadamsphd https://github.com/davidadamsphd, @lbergelson ; > https://github.com/lbergelson, and myself met for an hour or two ; > just now to discuss this issue, and after reviewing all the options I ; > think we were convinced by the following argument:; > ; > The |SAMRecord| class currently allows its header to be set to null, ; > so if there are cases where the class won't function properly or can ; > enter into an inconsistent state when a header is not present these ; > should be treated as bugs and patched, and we should add unit tests to ; > htsjdk to prove that headerless |SAMRecords| function properly. Then ; > in hellbender we can freely use headerless |SAMRecords| everywhere, ; > only restoring the header to the record when writing out the final bam ; > (since our bam writers do require that a header be present in the ; > records, I believe).; > ; > I've created #903 ; > https://github.com/broadinstitute/hellbender/issues/903 to make the ; > necessary changes in htsjdk, and assigned it to @cmnbroad ; > https://github.com/cmnbroad. He said he could get to it early next ; > week. What do you guys think of this approach to the problem?; > ; > —; > Reply to this email directly or view it on GitHub ; > https://github.com/broadinstitute/hellbender/issues/900#issuecomment-141218134.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:3315,test,tests,3315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518,1,['test'],['tests']
Testability,"there is one thing i dont like - if i revert the change i made in removeNonRefAndUnusedAltAlleles(), the one to simplify, these tests pass locally for me, with the test files as-is. do you see this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-582253636:128,test,tests,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-582253636,2,['test'],"['test', 'tests']"
Testability,"this for a first workflow to target?. 1) Run ExtractVariantAnnotations on a training set of chromosomes. You can keep training/truth labels as in Best Practices, for now.; 2) Run TrainVariantAnnotationsModel on that. We'll use the truth scores generated here for any sensitivity conversions---i.e., we'll be calibrating scores only to the truth sites that are contained in the training set of chromosomes.; 3) Use the trained model to run a single shard of ScoreVariantAnnotations on a validation set of chromosomes.; 4) Run some variation of the above script on the resulting outputs to determine SNP and INDEL score thresholds for optimizing the corresponding LL scores. We can also add some code to the script to use the truth scores from step 2 to convert these score thresholds into truth-sensitivity thresholds.; 5) Provide these truth-sensitivity thresholds to ScoreVariantAnnotations and use them to hard filter. Evaluate on a test set of chromosomes. If all looks good, we can later move steps 3-4 into the train tool and automate the passing of sensitivities in 5 via outputs in the model directory. This will let us keep the basic interface of ScoreVariantAnnotations the same, but we'll have to add a few basic parameters to TrainVariantAnnotationsModel to control the train/validation split. So I think all this branch is missing is step 5---we'll simply need to add command-line parameters for the SNP/INDEL sensitivity thresholds and then do the hard filtering in the VCF writing method highlighted above. Do you think you can handle implementing that in this branch, and then the rest at the WDL level? I can help with the python script for the LL stuff (or anything else), if needed. Not sure if you got a chance to check out what your collaborators are doing in the methods you're looking to compare against, but it would be good to understand if this basic scheme for train/validation/test splitting can be replicated over there. You'll want to compare apples to apples, after all!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084:1983,test,test,1983,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084,1,['test'],['test']
Testability,"tils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2016-02-14 19:04:19 WARN MetricsSystem:71 - Using default name DAGScheduler for source because spark.app.id is not set.; MinimalVariant -- interval(1:737406-737411), snp(false), indel(true); 19:04:24.266 INFO PrintVCFSpark - Shuttin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:1516,test,test,1516,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857,1,['test'],['test']
Testability,"tor, coverage is passed as a float; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/ReadMetadata.java; - add xgboost maven repository for gradle; build.gradle. 3. Significant changes to existing code to support/invoke new filter; - add arguments for XGBoostEvidenceFilter, changes for scaling density filter by coverage; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/StructuralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbend",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:1592,test,testing,1592,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,1,['test'],['testing']
Testability,tp://gatk.vanillaforums.com/discussion/3094/downsampling-with-haplotypecaller. User specifies -dcov 200 but DP per sample in VCF is higher than that. Local cmdline:. ```; java -Xmx16g -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/gsa-hpprojects/1kg/reference/hs37d5.fa -nct 8 -ped /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/families.ped -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND1/UDP2731_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND2/UDP3478_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND3/UDP4031_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND4/UDP4032_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND5/UDP4033_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND6/UDP4573_1.forGATK.bam -dcov 200 -minPruning 4 -o /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/eflynn90-test.vcf -L /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/intervals.vcf -stand_emit_conf 10 -pedValidationType SILENT; ```. ---. @eitanbanks said:. Updated command-line:. ```; java -Xmx6g -jar dist/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/1kg/reference/hs37d5.fasta -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND1/UDP2731_1.forGATK.bam -dcov 200 -minPruning 4 -L 1:14464; ```. I can confirm that it appears that down-sampling is not working for the Haplotype Caller (when run through the Unified Genotyper the down-sampling works just fine).; I see in SAMDataSource line 668 that assumeDownstreamLIBSDownsampling is being set to true. But then it doesn't look like LIBS is actually down-sampling. Don't have time to debug more so passing on to David. ---. @droazen said (over multiple comments):. I am looking into this. LIBS is actually calling into the downsamplers correctly in the test case that Eric provid,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:1196,test,test,1196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['test'],['test']
Testability,"version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8/./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true --input ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --reference ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --run_without_dbsnp_potentially_ruining_quality false --solid_recal_mode SET_Q_ZERO --solid_nocall_strategy THROW_EXCEPTION --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_default_quality -1 --insertions_default_quality 45 --deletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --interval_set_rule UNION --interval_padding 0 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_less_than 6 --useOriginalQualities false --defaultBaseQualities -1 --runner LOCAL --client_secret client-secrets.json --help false --version false --VERBOSITY INFO --QUIET false; [June 1, 2015 5:51:02 PM EDT] Executing as pgrosu@eofe5 on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:version-unknown-SNAPSHOT JdkDeflater; 17:51:02.969 [main] INFO org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow - Initializing engine; 17:51:02.970 [main] INFO org.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:3629,test,test,3629,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['test'],['test']
Testability,"while you're at it, I noticed a few other things:. * Using `addExact` instead of `+` is overkill, since the overflow it protects against can only occur if we have a read depth of 2,147,483,648. In any case, it just throws an error. If we wanted to be super-scrupulous, we would put an `if` statement in the `FisherExactTest` code to switch to an asymptotic approximation, but, like I said, overkill. * This class has it's own `apply` method which replicates `MathUtils::applyToArray`. * Similarly, it's `range` method should be deleted, and its use `support = range(lo, hi)` should become `support = new IndexRange(lo, hi)`. Then `IntStream.of(support).mapToDouble(___).toArray()` becomes `range.mapToDouble(___)` and `apply(promote(support), ___)` becomes `support.mapToDouble(___)`. * `final double relErr = 1 + pow(10, -7)` should become a `static` constant. * The steps; ```java; final double maxD1 = arrayMax(d1);; final double[] d2 = apply(apply(d1, d -> d - maxD1), d -> exp(d));; final double sumD2 = sum(d2);; return apply(d2, d -> d/sumD2);; ```; inside `dnHyper` are just a home-brewed way to normalize the log-space array `d1`. Let's go throught the steps: 1) find the max. 2) subtract the max -- subtracting a constant in log-space is equivalent to dividing by a constant in real space, and since we're normalizing in the end this constant is arbitrary. It's done for numerical stability. 3) exponentiate to get an unnormalized real-space array. 4) find the sum. 5) divide by the sum to get the normalized result. The log-10 version of this is `MathUtils::normalizeFromLog10ToLinearSpace(d1)`. You could either calculate `d1` in log-10 space or convert it, replacing the above line with `return MathUtils.normalizeFromLog10ToLinearSpace(MathUtils.applyToArrayInPlace(d1, MathUtils::logToLog10))`. The latter option is simpler. * Import static should be avoided except to escape horrible clutter, which is not the case here. * The second argument of `Utils.validateArg(condition, calculat",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155:1212,log,log-space,1212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155,1,['log'],['log-space']
Testability,will **increase** coverage by `0.03%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2500 +/- ##; ==============================================; + Coverage 76.256% 76.287% +0.03% ; - Complexity 10864 10881 +17 ; ==============================================; Files 750 750 ; Lines 39543 39619 +76 ; Branches 6914 6935 +21 ; ==============================================; + Hits 30154 30224 +70 ; - Misses 6772 6774 +2 ; - Partials 2617 2621 +4; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2500?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/58cb99ec6c81917a9ac8ecf52e8fde2bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `90% <0%> (-1.429%)` | `23% <0%> (-1%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/58cb99ec6c81917a9ac8ecf52e8fde2bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `87.2% <0%> (+0.8%)` | `36% <0%> (+1%)` | :arrow_up: |; | [...ellbender/tools/walkers/annotator/RankSumTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/58cb99ec6c81917a9ac8ecf52e8fde2bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9SYW5rU3VtVGVzdC5qYXZh) | `86.957% <0%> (+6.401%)` | `14% <0%> (+1%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/58cb99ec6c81917a9ac8ecf52e8fde2bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlsc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2500#issuecomment-288139597:1301,test,test,1301,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2500#issuecomment-288139597,1,['test'],['test']
Testability,"workflow owners (Nextflow, Snakemake, WDL, CWL etc. that include Mutect2) of the forced downgrade. Also, I think it makes sense to include a very prominent warning into the Mutect2 READMEs and GATK best practice documentations and guides. I know that this is work, too, but with success comes responsibility, and I can just hope that providing proper warnings uses less developer bandwidth than applying binary search to find out which of these [10 commits between 4.1.8.1 and 4.1.9.0 that are touching variant filtering (see below)](https://github.com/broadinstitute/gatk/compare/4.1.8.1...4.1.9.0) broke your flagship product enough to abandon it. (For anyone looking at this issue later, these are the commits I think are most likely to be related to this issue, and which I would propose to systematically leave out of the 4.1.9.0 build to test whether variant calling specificity is restored; assuming the 10 commits are independent and leaving each out in turn produces a working build, this would mean producing 10 Mutect2 builds for functional regression testing (the latter of which @ddrichel could do if we would receive the 10 builds from the GATK team)):. 1. https://github.com/broadinstitute/gatk/commit/a304725a60f5000ec6381040137043a557fc3dc1; 2. https://github.com/broadinstitute/gatk/commit/4982c2fa60e89f699a81150116d058aeac2f7573; 3. https://github.com/broadinstitute/gatk/commit/07aed754995717c02408517f8def57a8b8713ed7; 4. https://github.com/broadinstitute/gatk/commit/3502d4484e994c2d6154db78784a5ff7beafc9e9; 5. https://github.com/broadinstitute/gatk/commit/a269d063e245bf44846e0e54dfaab708b9116920; 6. https://github.com/broadinstitute/gatk/commit/d1d979fc535ac7b5075deb888c34e3a6512160b6; 7. https://github.com/broadinstitute/gatk/commit/650c2b390bbe45853ae8b4c18243fbca3c771a7b; 8. https://github.com/broadinstitute/gatk/commit/22460dafe45fda48b23c629200cc94dbdcfca7ca; 9. https://github.com/broadinstitute/gatk/commit/5a14c7cc72b86d81e3076fb8199da30c7303df2f; 10. https://g",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1535909226:2242,test,test,2242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1535909226,2,['test'],"['test', 'testing']"
Testability,| |; | [...dataSources/gencode/GencodeFuncotationBuilder.java](https://codecov.io/gh/broadinstitute/gatk/pull/6033/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL2dlbmNvZGUvR2VuY29kZUZ1bmNvdGF0aW9uQnVpbGRlci5qYXZh) | `96.825% <ø> (ø)` | `30 <0> (?)` | |; | [...ces/gencode/GencodeFuncotationFactoryUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/6033/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL2dlbmNvZGUvR2VuY29kZUZ1bmNvdGF0aW9uRmFjdG9yeVVuaXRUZXN0LmphdmE=) | `94.658% <ø> (ø)` | `70 <0> (?)` | |; | [...ools/funcotator/FuncotatorArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/pull/6033/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3JBcmd1bWVudERlZmluaXRpb25zLmphdmE=) | `86.957% <ø> (ø)` | `1 <0> (?)` | |; | [...ute/hellbender/utils/test/FuncotatorTestUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/6033/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Z1bmNvdGF0b3JUZXN0VXRpbHMuamF2YQ==) | `96.46% <100%> (ø)` | `25 <2> (?)` | |; | [...s/funcotator/BaseFuncotatorArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/6033/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Jhc2VGdW5jb3RhdG9yQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `100% <100%> (ø)` | `1 <0> (?)` | |; | [...ellbender/tools/funcotator/FuncotationFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/6033/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0aW9uRmFjdG9yeS5qYXZh) | `100% <100%> (ø)` | `4 <4> (?)` | |; | [...uncotator/GCContentFuncotationFactoryUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/6033/diff?src=pr&el=tree#diff-c3JjL3Rlc3Q,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6033#issuecomment-511019437:2398,test,test,2398,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6033#issuecomment-511019437,1,['test'],['test']
Usability," +26 ; Misses 6771 6771 ; - Partials 2618 2619 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2518?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...walkers/genotyper/afcalc/AFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/compare/91b41d8011a1465c637b7548899ff1e7f58f4e40...f741a033e28fe707ade9c9f0ea3fe9a20ecd78fd?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQUZDYWxjdWxhdG9yUHJvdmlkZXIuamF2YQ==) | `66.667% <ø> (ø)` | `4 <0> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/engine/FeatureWalker.java](https://codecov.io/gh/broadinstitute/gatk/compare/91b41d8011a1465c637b7548899ff1e7f58f4e40...f741a033e28fe707ade9c9f0ea3fe9a20ecd78fd?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZVdhbGtlci5qYXZh) | `86.957% <0%> (-2.699%)` | `9% <0%> (ø)` | |; | [...ellbender/tools/walkers/annotator/RankSumTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/91b41d8011a1465c637b7548899ff1e7f58f4e40...f741a033e28fe707ade9c9f0ea3fe9a20ecd78fd?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9SYW5rU3VtVGVzdC5qYXZh) | `86.957% <0%> (+9.179%)` | `14% <0%> (+1%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2518?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2518?src=pr&el=footer). Last update [91b41d8...f741a03](https://codecov.io/gh/broadinstitute/gatk/compare/91b41d8011a1465c637b7548899ff1e7f58f4e40...f741a033e28fe707ade9c9f0ea3fe9a20ecd78fd?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2518#issuecomment-288545729:2205,learn,learn,2205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2518#issuecomment-288545729,1,['learn'],['learn']
Usability," <0%> (+16%)` | :white_check_mark: |; | [.../hellbender/tools/spark/sv/BreakpointEvidence.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...475cd13e0c19561a3569b7816f06ba5a52dabe77?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9CcmVha3BvaW50RXZpZGVuY2UuamF2YQ==) | `83.756% <0%> (+0.508%)` | `24% <0%> (+1%)` | :white_check_mark: |; | [...s/recalibration/covariates/ReadGroupCovariate.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...475cd13e0c19561a3569b7816f06ba5a52dabe77?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWNhbGlicmF0aW9uL2NvdmFyaWF0ZXMvUmVhZEdyb3VwQ292YXJpYXRlLmphdmE=) | `97.826% <0%> (+1.159%)` | `16% <0%> (+3%)` | :white_check_mark: |; | [...ute/hellbender/tools/spark/sv/AlignmentRegion.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...475cd13e0c19561a3569b7816f06ba5a52dabe77?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9BbGlnbm1lbnRSZWdpb24uamF2YQ==) | `65.493% <0%> (+4.203%)` | `22% <0%> (+8%)` | :white_check_mark: |; | ... and [5 more](https://codecov.io/gh/broadinstitute/gatk/pull/2417?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2417?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2417?src=pr&el=footer). Last update [fcd103c...475cd13](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...475cd13e0c19561a3569b7816f06ba5a52dabe77?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2417#issuecomment-281527264:4952,learn,learn,4952,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2417#issuecomment-281527264,1,['learn'],['learn']
Usability," Caller (when run through the Unified Genotyper the down-sampling works just fine).; I see in SAMDataSource line 668 that assumeDownstreamLIBSDownsampling is being set to true. But then it doesn't look like LIBS is actually down-sampling. Don't have time to debug more so passing on to David. ---. @droazen said (over multiple comments):. I am looking into this. LIBS is actually calling into the downsamplers correctly in the test case that Eric provided. You can see this by examining readStates.size() for each locus -- it never exceeds the -dcov target of 200. The problem must lie elsewhere -- I'll continue to step through this in the debugger. [...]. After some more debugging and consultation with Ryan, I've found that DP values exceeding dcov are to be expected given the way the ActiveRegion traversal currently works. Here's a summary of what's going on:. -dcov 200 does cause LIBS to cap the depth at each locus to 200, but due to code Mark added a while back LIBS will save all of the undownsampled reads in memory during active region traversals (which kind of defeats the purpose of downsampling in the first place!). -TraverseActiveRegions gets the undownsampled reads from LIBS, and adds them to the active regions that get passed to the walker. -The HaplotypeCaller does a post-hoc downsampling pass on the reads in the active region in finalizeActiveRegion() to a hardcoded (!!!) and completely arbitrary depth of 1000, ignoring dcov. -The HaplotypeCaller does realignment of reads to the haplotypes, potentially causes the depth of coverage to vary at the locus in question. -The GenotypingEngine then computes DP based on the reads that still overlap the locus post-realignment. The end result is that DP for the HaplotypeCaller represents the undownsampled depth of coverage at the locus in question, subject to the hardcoded cap of 1000 and realignment to the haplotypes. In this particular case, the actual depth at locus 1:14464 is 561 (with no downsampling), and the DP val",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:2696,undo,undownsampled,2696,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['undo'],['undownsampled']
Usability," Fri, Jul 21, 2017 at 12:56 PM, Valentin Ruano Rubio <; notifications@github.com> wrote:. > I have to deal with this component recently and I found the design rather; > awkward.... In general between GATK and htsjdk we don't seem to have a; > proper support for managing and querying Supplementary alignment; > information from read alignment records:; >; > Some of the things that I think smell:; >; > 1.; >; > Querying: implemented in htsjdk consists in forging artificial; > SAMRecords that contain only the alignment info in the SA tag element... It; > seems to me that it makes more sense to create class to hold this; > information alone (e.g. ReadAlignmentInfo or ReadAlignment); SATagBuilder; > already has defined a private inner class with that in mind ""SARead"" so why; > not flesh it out and make it public.; > 2.; >; > Writing: currently SATagBuilder gets attached to a read, parsing its; > current SA attribute content into SARead instances. It provides the; > possibility adding additional SAM record one by one or clearing the list.; > ... then it actually updates the SA attribute on the original read when a; > method (setTag) is explicitly called.; >; > I don't see the need to attach the SATag Builder to a read... it could; > perfectly be free standing; the same builder could be re-apply to several; > reads for that matter and I don't see any gain in hiding the read SA tag; > setting process,... even if typically this builder output would go to the; > ""SA"" tag, perhaps at some point we would like to also write SA coordinate; > list somewhere else, some other tag name or perhaps an error message... why; > impose this single purpose limitation?; >; > I suggest to drop the notion of a builder for a more general custom; > ReadAlignmentInfo (or whatever name) list. Such list could be making; > reference to a dictionary to validate its elements, prevent duplicates,; > keep the primary SA in the first position... etc.; >; > —; > You are receiving this because you are subscr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323:1530,clear,clearing,1530,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323,1,['clear'],['clearing']
Usability," Partials 2620 2618 -2; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2453?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ark/sv/CallVariantsFromAlignedContigsSAMSpark.java](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...9b319acdbc3eb6e5d6b26bffcd1ad2b53f40bc3a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9DYWxsVmFyaWFudHNGcm9tQWxpZ25lZENvbnRpZ3NTQU1TcGFyay5qYXZh) | `0% <0%> (-26.087%)` | `0 <0> (-5)` | |; | [...bender/tools/spark/sv/AssemblyAlignmentParser.java](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...9b319acdbc3eb6e5d6b26bffcd1ad2b53f40bc3a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9Bc3NlbWJseUFsaWdubWVudFBhcnNlci5qYXZh) | `66.917% <85.714%> (+2.211%)` | `38 <6> (+6)` | :white_check_mark: |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...9b319acdbc3eb6e5d6b26bffcd1ad2b53f40bc3a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2453?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2453?src=pr&el=footer). Last update [5d2f859...9b319ac](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...9b319acdbc3eb6e5d6b26bffcd1ad2b53f40bc3a?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2453#issuecomment-285796600:2233,learn,learn,2233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2453#issuecomment-285796600,1,['learn'],['learn']
Usability," `AssemblyContigAlignmentsConfigPicker`; ; ----------; ## Consolidate logic, bump test coverage and update how variants are represented. ### consolidate logic; When initially prototyped, there's redundancy in logic for simple variants, now it's time to consolidate. - [x] `AssemblyContigWithFineTunedAlignments`; - [x] `hasIncompletePicture()`. - [x] `AssemblyContigAlignmentSignatureClassifier`; - [x] Don't make so many splits; - [x] Reduce `RawTypes` into fewer cases; ; - [x] `ChimericAlignment`; - [x] update documentation; - [x] implement a `getCoordinateSortedRefSpans()`, and use in `BreakpointsInference`; - [x] `isNeitherSimpleTranslocationNorIncompletePicture()`; - [x] `extractSimpleChimera()`. ### bump test coverage; Once code above is consolidated, bump test coverage, particularly for the classes above and the following poorly-covered classes; - [x] `ChimericAlignment`; - [x] `isForwardStrandRepresentation()`; - [x] `splitPairStrongEnoughEvidenceForCA()` ; - [x] `parseOneContig()` (needs testing because we need it for simple-re-interpretation for CPX variants) Note that `nextAlignmentMayBeInsertion()` is currently broken in the sense that when using this to filter out alignments whose ref span is contained by another, check if the two alignments involved are head/tail. - [x] `BreakpointsInference` & `BreakpointComplications`. - [x] `NovelAdjacencyAndAltHaplotype`; - [x] `toSimpleOrBNDTypes()`. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence`; - [x] serialization test. - [x] `AnnotatedVariantProducer`; - [x] `produceAnnotatedBNDmatesVcFromNovelAdjacency()`. - [x] `BreakEndVariantType`. - [ ] `SvDiscoverFromLocalAssemblyContigAlignmentsSpark` integration test; . ### update how variants are represented ; Implement the following representation changes that should make type-based evaluation easier; - [x] change `INSDUP` to`INS` when the duplicated ref region, denoted with annotation `DUP_REPEAT_UNIT_REF_SPAN`, is shorter than 50 bp.; - [x] change scarred del",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021:1540,simpl,simple-re-interpretation,1540,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021,1,['simpl'],['simple-re-interpretation']
Usability," `parseOneContig()` (needs testing because we need it for simple-re-interpretation for CPX variants) Note that `nextAlignmentMayBeInsertion()` is currently broken in the sense that when using this to filter out alignments whose ref span is contained by another, check if the two alignments involved are head/tail. - [x] `BreakpointsInference` & `BreakpointComplications`. - [x] `NovelAdjacencyAndAltHaplotype`; - [x] `toSimpleOrBNDTypes()`. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence`; - [x] serialization test. - [x] `AnnotatedVariantProducer`; - [x] `produceAnnotatedBNDmatesVcFromNovelAdjacency()`. - [x] `BreakEndVariantType`. - [ ] `SvDiscoverFromLocalAssemblyContigAlignmentsSpark` integration test; . ### update how variants are represented ; Implement the following representation changes that should make type-based evaluation easier; - [x] change `INSDUP` to`INS` when the duplicated ref region, denoted with annotation `DUP_REPEAT_UNIT_REF_SPAN`, is shorter than 50 bp.; - [x] change scarred deletion calls, which currently output as `DEL` with `INSSEQ` annotation, to one of these; - [x] `INS`/`DEL`, when deleted/inserted bases are < 50 bp and annotate accordingly; when type is determined as`INS`, the `POS` will be 1 base before the micro-deleted range and `END` will be end of the micro-deleted range, where the `REF` allele will be the corresponding reference bases.; - [x] two records `INS` and `DEL` when both are >= 50, share the same `POS`, and link by `EVENT`; - [ ] we are making a choice that treats duplication expansion as insertion. If decide to treat `DUP` as a separate 1st class type, we need to ; - [ ] shift the left breakpoint to the right by 1 base compared to the current implementation, and ; - [ ] `downstreamBreakpointRefPos = complication.getDupSeqRepeatUnitRefSpan().getEnd();`. ----------; ## CPX variant re-interpretation. Send cpx variant for re-interpretation of simple basic types, and check for consistency (this might be the difficult part)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021:3402,simpl,simple,3402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021,1,['simpl'],['simple']
Usability," a better format. The requirements seem to be: (a) efficient serialization/deserialization, and (b) can easily convert to a SAMRecord for compatibility with existing code. We can make things extra efficient by only deserializing things if they are needed (if a phase doesn't need the CIGAR-related structures, no need to deserialize that). We can achieve this by having the deserialization be lazy. The LazyBAMRecord is a step in that direction since it looks up the reference name only if we ask for it, but we could go a lot further in this direction. But before we do that, having an efficient coder for SAMRecords (I vote for @tomwhite's approach of using the BAMEncoder) will get us 80% of the way for 20% of the effort. Then we can introduce our OptimizedSAMRecord incrementally. . on **headers**:. I agree with @tomwhite that adding the header back after a shuffle is the right thing to do. We know where that happens and we control that code.; @davidadamsphd, you worry about newcomers. But we've already decided that we were going to provide our own API for them (one that does the Dataflow copying for them so they don't have to worry about it). This same API will provide them with header-filled reads, so they don't have to worry about this detail. This falls into the general category of ""the 3rd party devs won't have to even know about Dataflow/Spark: they just need to know our nice, simple interface and use that"". If they know more and want to do fancier things then more power to them, but those users will surely be able to fill in headers, too. We have library functions to use the reads without the headers, but the problem is that (at least for the sort of code I'm writing), I'm handing off a SAMRecord to a big black box and I can't force it to use the library functions - it's going to work on the SAMRecord directly. So at least in this case it's important to fill in the header (unless I happen to know that the ""black box"" won't call any of the header-requiring methods).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141151451:1495,simpl,simple,1495,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141151451,1,['simpl'],['simple']
Usability," accounting for 1) sample-specific depth (which determines the means of the negative-binomial distributions), 2) multiplicative contig-specific bias (which is mild, at least for WGS), and 3) additive sample-contig-specific mosaicism or bias (note that the above genotype priors imply that mosaicism/bias on top of a baseline of CN = 2 is the only deviation allowed for the autosomes, which is somewhat restrictive but greatly aids convergence). I put together a pure PyMC3 prototype that seems to work relatively well. Here are the per-contig coverage histograms (unfiltered bins in blue, bins retained after filtering in red, and negative-binomial fit in green) and a heatmap of per-contig ploidy probabilities. Both the panel (first 20) and case (remaining) samples are shown:. ![prototype-result](https://user-images.githubusercontent.com/11076296/37938642-e9fbd804-312c-11e8-8a6c-02ea4e4fa704.png). Although the prototype model is clearly a good fit to the filtered data, some care in choosing the optimizer and its learning parameters is required to achieve convergence to the correct solution. This is because the problem is inherently multimodal and thus there are many local minima. I found that using AdaMax with a naive strategy of warm restarts (to help kick us out of local minima) worked decently; we can achieve convergence in <10 minutes for 60 samples x 24 contigs x 250 count bins:. ![elbo](https://user-images.githubusercontent.com/11076296/37938658-fc176f12-312c-11e8-89e2-40c68e0f9953.png). I expect that @mbabadi's annealing implementation in the gcnvkernel package will handle the local minima much better. The course of action needed to implement this model should be as follows:. 1) Alter Java code to emit per-contig histograms. Change python code to consume histograms, perform filtering, and fit using the above model (or some variation).; 2) Choose learning parameters appropriate with annealing and check that results are still good.; 3) Update gCNV model to consume the d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271:2428,clear,clearly,2428,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271,2,"['clear', 'learn']","['clearly', 'learning']"
Usability," by `-0.002%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2448 +/- ##; ===============================================; - Coverage 76.238% 76.236% -0.002% ; + Complexity 10859 10854 -5 ; ===============================================; Files 751 750 -1 ; Lines 39559 39551 -8 ; Branches 6912 6911 -1 ; ===============================================; - Hits 30159 30152 -7 ; Misses 6780 6780 ; + Partials 2620 2619 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2448?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/e7c90f1da2ac17173e56d352fbc4d926f9d2b871...23ba83e98b5b49ad0285a6366e79ad37e70efd0b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `66.667% <0%> (-3.333%)` | `10% <0%> (ø)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/e7c90f1da2ac17173e56d352fbc4d926f9d2b871...23ba83e98b5b49ad0285a6366e79ad37e70efd0b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2448?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2448?src=pr&el=footer). Last update [e7c90f1...23ba83e](https://codecov.io/gh/broadinstitute/gatk/compare/e7c90f1da2ac17173e56d352fbc4d926f9d2b871...23ba83e98b5b49ad0285a6366e79ad37e70efd0b?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2448#issuecomment-285370809:1813,learn,learn,1813,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2448#issuecomment-285370809,1,['learn'],['learn']
Usability," can hopefully rely on per-bin bias modeling to at least partially account for mappability in gCNV calling (and we certainly wouldn't want to filter out a significant fraction of the genome, in any case). Do we agree?. To answer your first question, the criterion for choosing the peak is quite hacky at the moment, but I found that filtering low-count bins to first check for the presence of a high peak and then falling back to the peak at zero works perfectly fine in practice. . We can certainly try to do something smarter, since, as you say, bin filtering may be desirable---even if we implement mappability filtering---to remove large germline events (it's true that the ""example"" I showed above is indeed from the PAR-like region on X, as you point out, but this is roughly how a large arm-level event would appear even after mappability filtering.) Although the model I fit above, which is simply a sparse mixture of NBs with regularly-spaced means (modulo some sample-specific and contig-specific jitter), could conceivably capture such events as well, we want to avoid models where a single NB might try to capture two or more peaks. Also, just to clarify, the weird mosaic examples are the bottom two plots out of the four above---you can see the shifted (non-X, in one of the examples) single peaks. However, it's interesting that the PARs are still showing up in XY---I'm pretty sure I used the blacklist you provided, although I will double check. Did that only include the ""official"" PARs, or also the additional ones you found?. In any case, are we comfortable calling in those regions (here I'm talking about gCNV, not ploidy)? As I show above, I don't think we need mappability to nail the baseline ploidy. Can we then rely on the per-bin bias to account for these regions in gCNV (pinning them back to the correct CN) without mappability filtering? And with mappability filtering, how substantial is the hit to coverage in these regions? Should we blacklist them for the time bein",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375923639:1152,simpl,simply,1152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375923639,1,['simpl'],['simply']
Usability," current = Allele.create(e.getBase());; }; pralm.add(e, current, DEFAULT_FAKE_LIKELIHOOD);; }; return pralm;; }; ```. The solution that I found after looking at the class was this one, that it's very complicated:. ``` java; public static ReadLikelihoods<Allele> flatPerReadAlleleLikelihoodsFromPileup(final ReadPileup pileup, final Allele refAllele, final SAMFileHeader header) {; final Set<Allele> alleleSet = new TreeSet<Allele>();; final Map<String, List<GATKRead>> reads = new HashMap<>();; final byte ref = refAllele.getBases()[0];; alleleSet.add(refAllele);; for (final PileupElement e : pileup) {; if (e.isDeletion()) {; alleleSet.add(Allele.SPAN_DEL);; } else if (e.getBase() == ref) {; alleleSet.add(refAllele);; } else {; alleleSet.add(Allele.create(e.getBase()));; }; final String sample = ReadUtils.getSampleName(e.getRead(), header);; List<GATKRead> list = reads.getOrDefault(sample, null);; if(list == null) {; list = new ArrayList<>();; reads.put(sample, list);; }; list.add(e.getRead());; }; final ReadLikelihoods<Allele> likelihoods = new ReadLikelihoods<>(new IndexedSampleList(reads.keySet()), new IndexedAlleleList<Allele>(alleleSet), reads);; for(final PileupElement e: pileup) {; final String sample = ReadUtils.getSampleName(e.getRead(), header);; final LikelihoodMatrix<Allele> l = likelihoods.sampleMatrix(likelihoods.indexOfSample(sample));; final int alleleIndex;; if (e.isDeletion()) {; alleleIndex = likelihoods.indexOfAllele(Allele.SPAN_DEL);; } else if (e.getBase() != ref) {; alleleIndex = likelihoods.indexOfAllele(Allele.create(e.getBase());; } else {; alleleIndex = likelihoods.indexOfReference();; }. l.set(alleleIndex, l.indexOfRead(e.getRead()), DEFAULT_FAKE_LIKELIHOOD);; }; return likelihoods;; }; ```. This example is very simple, but in my case what I need its to assign an unique likelihood to each read after calling the variant for that read. I want to use the variant annotation engine for annotate this likelihood map because it is using this interface.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2185#issuecomment-249930107:2396,simpl,simple,2396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2185#issuecomment-249930107,1,['simpl'],['simple']
Usability," inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) for future use. Comment by @cwhelan ; > I'm a bit confused by this comment: this method is still being called in several places, so how is it obsolete?. Reply by @SHUANG-Broad (also copied to update the ""TODO"" message; > this predicate is currently used in two places (excluding appearance in comments): `BreakpointComplications.IntraChrStrandSwitchBreakpointComplications`, where it is use to test if the input simple chimera indicates an inverse tandem duplication and trigger the logic for inferring duplicated region; and `BreakpointsInference.IntraChrStrandSwitchBreakpointInference`, where it is used for breakpoint inference. The problem is, the contig will not even be sent here, because `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` defines a simple chimera that has strand switch and the two alignments overlaps on reference as ""incomplete"", so in practice the two uses are not going to be triggered. But when we come back later and see what can be extracted from such ""incomplete"" contigs, these code could be useful again. So it is kept. ------------; ### On the problem of writing out SAM records of ""Unknown"" contigs efficiently. First round comment by @cwhelan ; > This seems like a very inefficient way to write these three files. You end up calling collect on the RDD three different times and then traversing the local collection three times. Why not make a map of contig name to bam fil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:2950,simpl,simple,2950,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030,1,['simpl'],['simple']
Usability," master #2529 +/- ##; ===============================================; + Coverage 76.266% 76.277% +0.011% ; - Complexity 10877 10879 +2 ; ===============================================; Files 752 752 ; Lines 39584 39586 +2 ; Branches 6922 6923 +1 ; ===============================================; + Hits 30189 30195 +6 ; + Misses 6774 6771 -3 ; + Partials 2621 2620 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ellbender/tools/walkers/annotator/QualByDepth.java](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9RdWFsQnlEZXB0aC5qYXZh) | `94.595% <100%> (+0.15%)` | `17 <0> (+1)` | :arrow_up: |; | [...nder/tools/walkers/annotator/DepthPerSampleHC.java](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9EZXB0aFBlclNhbXBsZUhDLmphdmE=) | `73.913% <100%> (+10.277%)` | `8 <0> (+1)` | :arrow_up: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=footer). Last update [47d8c52...d16a01a](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2529#issuecomment-289058454:1983,learn,learn,1983,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2529#issuecomment-289058454,1,['learn'],['learn']
Usability," merging .sbi files; 2019-06-03 22:34:34 INFO IndexFileMerger:69 - Merging .bai files in temp directory hdfs:///project/casa/gcad/adsp.cc/sv/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.bam.parts/ to hdfs:///project/casa/gcad/adsp.cc/sv/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.bam.bai; 2019-06-03 22:34:48 INFO AbstractConnector:318 - Stopped Spark@6be766d1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-06-03 22:34:48 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4040; 2019-06-03 22:34:48 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-06-03 22:34:48 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-06-03 22:34:48 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-06-03 22:34:48 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-06-03 22:34:48 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-06-03 22:34:48 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-06-03 22:34:49 INFO MemoryStore:54 - MemoryStore cleared; 2019-06-03 22:34:49 INFO BlockManager:54 - BlockManager stopped; 2019-06-03 22:34:49 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-06-03 22:34:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-06-03 22:34:49 INFO SparkContext:54 - Successfully stopped SparkContext; 22:34:49.027 INFO PrintReadsSpark - Shutting down engine; [June 3, 2019 10:34:49 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 3.72 minutes.; Runtime.totalMemory()=3829923840; htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Stream closed; at htsjdk.samtools.IndexStreamBuffer.readFully(IndexStreamBuffer.java:23); at htsjdk.samtools.IndexStreamBuffer.readInteger(IndexStreamBuffer.java:56); at htsjdk.samtools.AbstractBAMFileIndex.readIn",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-498502370:1415,clear,cleared,1415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-498502370,1,['clear'],['cleared']
Usability," operation in Genome STRiP is to ask for the sample ; associated with a read. This requires the header, I would think.; Anyway, my main point was just to stimulate thinking about the value of ; implementing an efficient way to transmit the headers.; It also seems to me that this generalizes to efficient patterns to ; transmit any widely shared data (that is referenced by many serialized ; individual data items) out-of-band. -Bob. On 9/21/15 11:42 AM, droazen wrote:. > @bhandsaker https://github.com/bhandsaker Thanks for chiming in with ; > your thoughts/concerns.; > ; > Under this proposal, the various classes in htsjdk that read and ; > return |SAMRecords| (eg., |SAMReader| & co.) would continue to put the ; > header inside of the records, so we would not be imposing an ; > additional burden on direct clients of htsjdk to check for null ; > headers any more than they do currently. The only difference is that ; > if downstream consumers of |SAMRecords| (like hellbender) choose to ; > strip the header from the records, there would be an explicit contract ; > governing the behavior of headerless |SAMRecords| (as opposed to the ; > status quo, in which the header may be null but behavior is totally ; > undocumented and in some cases inconsistent -- eg., the reference name ; > and index in a headerless |SAMRecord| can get out-of-sync in some cases).; > ; > In additional to documenting/clarifying the behavior of headerless ; > |SAMRecords| and fixing any consistency-related bugs we find when ; > operating without a header, we would also make an effort to document ; > when a class in htsjdk that consumes |SAMRecords| requires that a ; > header be present in the records (such as the various writer classes).; > ; > Does this sound reasonable? It's actually a much more conservative ; > proposal than it may have initially sounded :); > ; > —; > Reply to this email directly or view it on GitHub ; > https://github.com/broadinstitute/hellbender/issues/900#issuecomment-142020109.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-142402910:1654,undo,undocumented,1654,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-142402910,1,['undo'],['undocumented']
Usability," pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian consistency, white = reference. The second plot shows the training/truth positives used to train the model and to calculate the LL score in the validation shard. The third plot shows the ""orthogonal truth"" positives/negatives used to calculate F1. So we can see that the difficulty in deriving F1 as a function of the score along the horizontal axis to give the third plot lies in collapsing the columns in the top plot into a single condition positive or condition negative status. Again, hard to do so without some arbitrariness; I simply came up with some rules to convert various amounts of red, yellow, green, etc. in each column to a red/white/green status. If you're using a single gold-standard sample, this should definitely be more straightforward. In any case, the optimal validation LL score at ~0.02 does appear to line up quite well visually with where one might manually set a threshold. It corresponds pretty well with the transition from the yellow/red/grey junk to the clean green/white sites in the top plot. Here's the same for the test set:. ![image](https://user-images.githubusercontent.com/11076296/158385662-6693a6c9-709c-482f-9a7e-5bb7030b3383.png). Happy to chat more about how you might implement this in your WDL---should be pretty straightforward!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:2532,simpl,simply,2532,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,1,['simpl'],['simply']
Usability," publish another artifact so we can move our projects forward. The crux of what I'm trying to achieve with these changes is for our VariantQC tool; however, I'm bumping into several situations where MultiVariantWalkers need to be able to determine the FeatureInput source of the variants. I recognize that this and my original PR #6973 has taken a non-trivial amount of @cmnbroad time, but we have basically been stalled with an otherwise approved PR since Feb 8. The PRs blocking that PR are this one and the related #7021. They both involve creating a way to connect VariantContext to FeatureInput - a capability that would benefit the GATK engine and I have been told by @cmnbroad you're interested in having. It seems like the primary problem associated with these changes is ensuring tests and VariantContext comparison code still works, since VariantContexts are going to tend to report a source. I dont know your internal conversations, so I'm guessing based on what's written in github. As I've said, I'd like to do whatever I can to get these changes into a form that takes as little of your effort as possible. . While this particular PR seems close, there is clearly some cleanup needed from what's there now, including code review from @lbergelson that no one ever fixed. Would it help if I put together #4571 and #7021 into a new branch where I also work through associated test changes and try to get this into one concise piece of code to review? Basically try to put everything together to be the minimal amount of work needed on your side? I havent heard anything one way or the other from GATK staff as to whether this would actually be helpful or not. Are there higher order design decisions that need to be considered that I'm not seeing? . I completely understand your time constraints - I'm just trying to figure out some solution that let's this go forward. Again, if we cant unblock this soon I'm going to probably fork GATK and need to start working off that project. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4571#issuecomment-822691252:1358,clear,clearly,1358,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4571#issuecomment-822691252,1,['clear'],['clearly']
Usability," segmentation cost given all the candidate changepoints, calculate the cost change for removing each of the changepoints individually, remove the changepoint with the minimum cost change, and repeat. This gives the global cost as a function of the number of changepoints _C_.; 6) Add a penalty _a C + b C log(N / C)_ to the global cost and find the minimum to determine the number of changepoints. For the above simulated data, _a = 2_ and _b = 2_ works well, recovering all of the changepoints in the above example with no false positives:; ![6](https://user-images.githubusercontent.com/11076296/29582517-fbd604be-874a-11e7-8ef7-7bd727f65dcb.png). ![7](https://user-images.githubusercontent.com/11076296/29582518-fddf015c-874a-11e7-89e4-87250d2a52ab.png). In contrast, CBS produces two false positives (around the third and seventh of the true changepoints):. ![8](https://user-images.githubusercontent.com/11076296/29582545-18875126-874b-11e7-9166-9061bb120e43.png). We can change the penalty factor to smooth out less significant segments (which may be due to systematic noise, GC waves, etc.). Setting _a = 10, b = 10_ gives:; ![9](https://user-images.githubusercontent.com/11076296/29582598-515dffe0-874b-11e7-93c4-59422cd43b54.png); ![10](https://user-images.githubusercontent.com/11076296/29583130-0fe5316c-874d-11e7-8504-43618928cf68.png). (Note that the DNAcopy implementation of CBS does not allow for such simple control of the ""false-positive rate,"" as even setting the relevant p-value thresholds to zero still yields segments.). Although the above procedure has a number of parameters that need to be chosen, in practice they are all straightforward and relatively easy to understand. Being a combination of local and global methods, it allows for multiscale sensitivity to small events while still allowing for sensible control of the final number of segments via the BIC-like penalty on the global cost. All algorithms used are linear complexity and are straightforward to implement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:3824,simpl,simple,3824,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586,1,['simpl'],['simple']
Usability," some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we could move to a BitSet, but I'm not sure what the performance hit will be. EDIT: It was trivial to switch over to a BitSet, which seems to let us get away with -Xmx8g instead of -Xmx12g. Calling:; - I've ported over the naive `ReCapSegCaller` wholesale. This can take in the output of `ModelSegments`, so we can take advantage of the improved segmentation as before, but we still don't use the modeled minor-allele fractions when making calls. The method for copy-ratio calling is also extremely naive, with hardcoded bounds for identifying the copy-neutral state.; - [ ] @MartonKN is going to work on an improved caller for his next project. This caller should also make simple calls (not full allelic copy number, but just `0`, `+`, `-`), but should also take advantage of the copy-ratio and minor-allele fraction posteriors estimated by `ModelSegments` to generate quality scores. Plotting:; - Other than the allele-fraction model, the limiting factor was the original plotting code (some plotting runs originally took several hours for a single WGS sample...) We can now make plotting much faster with the ordering enforced by `TSVLocatableCollection` (see below).; - There are now two plotting tools, `PlotDenoisedCopyRatios` and `PlotModeledSegments`. This is in contrast to the old `PlotSegmentedCopyRatio` and `PlotACNVResults`.; - Because `ModelSegments` optionally takes denoised copy ratio and/or allelic counts, `PlotModeledSegments` outputs only the corresponding plots appropriately.; - I added a dependency on the R package `data.table` to slightly speed up the reading of input files.; - Setting `pch="".""` also sped up the generation of scatter plots.; - Plotting now takes a couple of minutes, most of which is I/O (#3554).; - AAF (rather than MA",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:8299,simpl,simple,8299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['simpl'],['simple']
Usability," the count likelihood is actually misspecified there. As an example, consider trying to fit a Poisson to data that is actually zero-inflated Poisson---fitting the histogram will actually result in a more robust estimate for the mean. Another benefit is that truncated data (as we have here) is straightforwardly handled in an unbiased way. In the special case of complete, trivially-binned data, the full, unbinned likelihood is recovered. I think this sort of histogram fitting is pretty standard in the astro/particle community. We can certainly change up the model to include strictly quantized + free-floating states as you describe (rather than the ""fuzzily quantized"" states I use here), but I just wanted to avoid having another level of mixtures/logsumexps for this quick prototype. However, note that modeling mosaicism on the autosomes is desirable, but there we also want the strong diploid prior to nail down the depth and per-contig bias. So we will have to be a little careful about how we introduce free-floating states. Also, since I was not using gcnvkernel, I had to integrate out all discrete parameters. It may be that we can write down a nice model with discrete parameters if we use your inference framework. Finally, I did not further bin the counts here (or rather, the bin size is 1), which already yields the maximum information, but I did use a maximum-count cutoff. If we use the same cutoff for all samples, this allows us to simply pass a non-ragged matrix from Java (with dimensions of samples x contigs x maximum count) as a TSV. However, we may run into trouble if we hit a case sample with very high depth. So some sort of sparse representation of the histogram might indeed be desirable, but I think it should be an exact representation of the full histogram. This would require us to sync up code to emit and consume the representation in both Java and python, so I'd like to avoid it if possible---I think I'd prefer just emitting the ragged matrix, in that case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522:1899,simpl,simply,1899,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522,1,['simpl'],['simply']
Usability," walker. -The HaplotypeCaller does a post-hoc downsampling pass on the reads in the active region in finalizeActiveRegion() to a hardcoded (!!!) and completely arbitrary depth of 1000, ignoring dcov. -The HaplotypeCaller does realignment of reads to the haplotypes, potentially causes the depth of coverage to vary at the locus in question. -The GenotypingEngine then computes DP based on the reads that still overlap the locus post-realignment. The end result is that DP for the HaplotypeCaller represents the undownsampled depth of coverage at the locus in question, subject to the hardcoded cap of 1000 and realignment to the haplotypes. In this particular case, the actual depth at locus 1:14464 is 561 (with no downsampling), and the DP value is 546. The difference is likely due to realignment of reads to the haplotypes by the walker. So, we basically have two options:; 1. Change the documentation for the DP annotation to mention that for ActiveRegion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlying craziness); 2. Change the ActiveRegion traversal so that it respects the dcov value (could be hard -- the LIBS downsampling process discards reads on-the-fly from previous loci when moving to a new locus, but an active region involves data for multiple loci. The potential performance win for the HC is huge, though, if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads in memory is actually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Wi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:3937,undo,undownsampled,3937,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['undo'],['undownsampled']
Usability,"![IMG_9960](https://user-images.githubusercontent.com/11076296/95899038-ee88e280-0d5d-11eb-86bf-272687eb9ac0.jpg). Decided to just sit down and go through the exercise of threading all of the parameter sets by hand after biffing it once. Reproducing above; might be helpful for the reviewer if this goes in, but they may want to independently check it. (Is there a way I could've gotten IntelliJ to do this for me?). I would hope that we could do some refactoring to simplify this a bit, if not model ablation or consolidation of parameters, but I won't attempt it for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-707919816:467,simpl,simplify,467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-707919816,1,['simpl'],['simplify']
Usability,"![screenshot 2018-08-01 11 42 49](https://user-images.githubusercontent.com/2604962/43532426-0ddde2a0-9580-11e8-90a2-1e867c05b13a.png). After seeing plots like the one above, where we have lots of false negative SNPs and lots of false positive indels, it became clear that we should have separate tranche sensitivites for the two variant types. I've added that into this PR. I made the filter removal opt-in as suggested and it now shares the argument with VariantFiltration. @cmnbroad back to you, sorry for moving the goal line for this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5042#issuecomment-409624017:262,clear,clear,262,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5042#issuecomment-409624017,1,['clear'],['clear']
Usability,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2378?src=pr&el=h1) Report; > Merging [#2378](https://codecov.io/gh/broadinstitute/gatk/pull/2378?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/9d82097641f160e00fa1ef4236d9bcdccbfa38b0?src=pr&el=desc) will **not impact** coverage. ```diff; @@ Coverage Diff @@; ## master #2378 +/- ##; =========================================; Coverage 76.378% 76.378% ; =========================================; Files 748 748 ; Lines 39315 39315 ; Branches 6847 6847 ; =========================================; Hits 30028 30028 ; Misses 6693 6693 ; Partials 2594 2594; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2378?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [...roadinstitute/hellbender/utils/tsv/TableUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...30b7f8dc5646d760cd7d8b42513538b30f803d33?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90c3YvVGFibGVVdGlscy5qYXZh) | `85% <ø> (ø)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2378?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2378?src=pr&el=footer). Last update [9d82097...30b7f8d](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...30b7f8dc5646d760cd7d8b42513538b30f803d33?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2378#issuecomment-276484501:1274,learn,learn,1274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2378#issuecomment-276484501,1,['learn'],['learn']
Usability,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2387?src=pr&el=h1) Report; > Merging [#2387](https://codecov.io/gh/broadinstitute/gatk/pull/2387?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/14f73e217970a1c53092dee88c409f8a6cdb6e87?src=pr&el=desc) will **increase** coverage by `-0.002%`. ```diff; @@ Coverage Diff @@; ## master #2387 +/- ##; ===============================================; - Coverage 76.379% 76.377% -0.002% ; - Complexity 0 10849 +10849 ; ===============================================; Files 748 748 ; Lines 39325 39347 +22 ; Branches 6849 6851 +2 ; ===============================================; + Hits 30036 30052 +16 ; - Misses 6695 6703 +8 ; + Partials 2594 2592 -2; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2387?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...ce8d93ca83ab90330776d2f46fea691af347349d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `78.065% <54.167%> (-0.883%)` | `20 <ø> (+20)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2387?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2387?src=pr&el=footer). Last update [14f73e2...ce8d93c](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...ce8d93ca83ab90330776d2f46fea691af347349d?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2387#issuecomment-277112231:1427,learn,learn,1427,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2387#issuecomment-277112231,1,['learn'],['learn']
Usability,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2403?src=pr&el=h1) Report; > Merging [#2403](https://codecov.io/gh/broadinstitute/gatk/pull/2403?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/30365e7bea2d081204a11e7d916026cb3494961f?src=pr&el=desc) will **increase** coverage by `0.003%`. ```diff; @@ Coverage Diff @@; ## master #2403 +/- ##; ===============================================; + Coverage 76.133% 76.135% +0.003% ; - Complexity 10785 10786 +1 ; ===============================================; Files 748 748 ; Lines 39372 39372 ; Branches 6856 6856 ; ===============================================; + Hits 29975 29976 +1 ; Misses 6791 6791 ; + Partials 2606 2605 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2403?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...a51febdea00d0e15069996b5b8a492587d6d220b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <ø> (+1.429%)` | `24% <ø> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2403?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2403?src=pr&el=footer). Last update [30365e7...a51febd](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...a51febdea00d0e15069996b5b8a492587d6d220b?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2403#issuecomment-279082175:1399,learn,learn,1399,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2403#issuecomment-279082175,1,['learn'],['learn']
Usability,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2407?src=pr&el=h1) Report; > Merging [#2407](https://codecov.io/gh/broadinstitute/gatk/pull/2407?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/f45f6a52d69fbf01541099cf737a0fc5391d584e?src=pr&el=desc) will **increase** coverage by `0.005%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2407 +/- ##; ===============================================; + Coverage 76.201% 76.206% +0.005% ; - Complexity 10808 10812 +4 ; ===============================================; Files 750 750 ; Lines 39417 39417 ; Branches 6858 6858 ; ===============================================; + Hits 30036 30038 +2 ; + Misses 6775 6773 -2 ; Partials 2606 2606; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2407?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/f45f6a52d69fbf01541099cf737a0fc5391d584e...9d14cf8831c2f51e6ca75d560343f35411b15c5b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `90.476% <ø> (+1.587%)` | `61% <ø> (+2%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2407?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2407?src=pr&el=footer). Last update [f45f6a5...9d14cf8](https://codecov.io/gh/broadinstitute/gatk/compare/f45f6a52d69fbf01541099cf737a0fc5391d584e...9d14cf8831c2f51e6ca75d560343f35411b15c5b?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2407#issuecomment-279825441:1434,learn,learn,1434,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2407#issuecomment-279825441,1,['learn'],['learn']
Usability,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2411?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@a49f0b3`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2411 +/- ##; ==========================================; Coverage ? 76.206% ; Complexity ? 10814 ; ==========================================; Files ? 750 ; Lines ? 39421 ; Branches ? 6859 ; ==========================================; Hits ? 30041 ; Misses ? 6773 ; Partials ? 2607; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2411?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2411?src=pr&el=footer). Last update [a49f0b3...00efddd](https://codecov.io/gh/broadinstitute/gatk/compare/a49f0b30b69eb3de3263cc976f976cd528721cc5...00efddd232b43006ad4f33e51d9387f507efe6ae?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2411#issuecomment-280715503:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2411#issuecomment-280715503,2,['learn'],['learn']
Usability,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@dc15e61`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `75%`. ```diff; @@ Coverage Diff @@; ## master #2431 +/- ##; ==========================================; Coverage ? 42.757% ; Complexity ? 5801 ; ==========================================; Files ? 750 ; Lines ? 39425 ; Branches ? 6885 ; ==========================================; Hits ? 16857 ; Misses ? 20600 ; Partials ? 1968; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...s/recalibration/covariates/ReadGroupCovariate.java](https://codecov.io/gh/broadinstitute/gatk/compare/dc15e61a728ccd4e61b139332e48c05b57e4e88c...0d5d1b12d611725c80fa572e5ffba3bf8ea45ee4?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWNhbGlicmF0aW9uL2NvdmFyaWF0ZXMvUmVhZEdyb3VwQ292YXJpYXRlLmphdmE=) | `86.667% <100%> (ø)` | `11 <0> (?)` | |; | [...dinstitute/hellbender/utils/report/GATKReport.java](https://codecov.io/gh/broadinstitute/gatk/compare/dc15e61a728ccd4e61b139332e48c05b57e4e88c...0d5d1b12d611725c80fa572e5ffba3bf8ea45ee4?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZXBvcnQvR0FUS1JlcG9ydC5qYXZh) | `40.196% <66.667%> (ø)` | `16 <0> (?)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=footer). Last update [dc15e61...0d5d1b1](https://codecov.io/gh/broadinstitute/gatk/compare/dc15e61a728c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2431#issuecomment-283404250:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2431#issuecomment-283404250,1,['learn'],['learn']
Usability,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2435?src=pr&el=h1) Report; > Merging [#2435](https://codecov.io/gh/broadinstitute/gatk/pull/2435?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/92cb86051b59acb6b18115135a5b5db99b617d22?src=pr&el=desc) will **decrease** coverage by `-0.008%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2435 +/- ##; ===============================================; - Coverage 76.231% 76.223% -0.008% ; Complexity 10822 10822 ; ===============================================; Files 750 750 ; Lines 39425 39425 ; Branches 6885 6885 ; ===============================================; - Hits 30054 30051 -3 ; - Misses 6754 6757 +3 ; Partials 2617 2617; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2435?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...f615b91329aaa84fff4fb4c22660820e2ed0dcb0?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.611% <0%> (-2.083%)` | `36% <0%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2435?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2435?src=pr&el=footer). Last update [92cb860...f615b91](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...f615b91329aaa84fff4fb4c22660820e2ed0dcb0?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-284289466:1411,learn,learn,1411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-284289466,1,['learn'],['learn']
Usability,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2456?src=pr&el=h1) Report; > Merging [#2456](https://codecov.io/gh/broadinstitute/gatk/pull/2456?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/dfa9cf1a420490285b7be7917082222a07e2b042?src=pr&el=desc) will **increase** coverage by `0.003%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2456 +/- ##; ===============================================; + Coverage 76.254% 76.256% +0.003% ; - Complexity 10861 10862 +1 ; ===============================================; Files 750 750 ; Lines 39556 39556 ; Branches 6914 6914 ; ===============================================; + Hits 30163 30164 +1 ; Misses 6775 6775 ; + Partials 2618 2617 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2456?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...988bc45a8ccfe0ae3884f0c8401015ce053f45bb?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2456?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2456?src=pr&el=footer). Last update [dfa9cf1...988bc45](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...988bc45a8ccfe0ae3884f0c8401015ce053f45bb?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2456#issuecomment-285972823:1432,learn,learn,1432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2456#issuecomment-285972823,1,['learn'],['learn']
Usability,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2513?src=pr&el=h1) Report; > Merging [#2513](https://codecov.io/gh/broadinstitute/gatk/pull/2513?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/9c1d1fb2cc1aeb171e01764ee69c1544698e796d?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2513 +/- ##; ===========================================; Coverage 76.256% 76.256% ; Complexity 10864 10864 ; ===========================================; Files 750 750 ; Lines 39543 39543 ; Branches 6915 6915 ; ===========================================; Hits 30154 30154 ; Misses 6771 6771 ; Partials 2618 2618; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2513?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2513?src=pr&el=footer). Last update [9c1d1fb...7fc08f1](https://codecov.io/gh/broadinstitute/gatk/compare/9c1d1fb2cc1aeb171e01764ee69c1544698e796d...7fc08f1c4ac1def9789665bd56448220d7ba774a?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2513#issuecomment-288454418:870,learn,learn,870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2513#issuecomment-288454418,1,['learn'],['learn']
Usability,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2544?src=pr&el=h1) Report; > Merging [#2544](https://codecov.io/gh/broadinstitute/gatk/pull/2544?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/8b4122cfb8268dcd86cca6bd8d6b3b4b6e1ed5a6?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2544 +/- ##; ===========================================; Coverage 76.282% 76.282% ; Complexity 10892 10892 ; ===========================================; Files 752 752 ; Lines 39590 39590 ; Branches 6925 6925 ; ===========================================; Hits 30200 30200 ; Misses 6768 6768 ; Partials 2622 2622; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2544?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2544?src=pr&el=footer). Last update [8b4122c...df921e4](https://codecov.io/gh/broadinstitute/gatk/pull/2544?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2544#issuecomment-290236909:870,learn,learn,870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2544#issuecomment-290236909,1,['learn'],['learn']
Usability,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=h1) Report; > Merging [#2547](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/c8ede6ef810a3d9a05c7deb8052e27ca724ce8ba?src=pr&el=desc) will **decrease** coverage by `0.005%`.; > The diff coverage is `90%`. ```diff; @@ Coverage Diff @@; ## master #2547 +/- ##; ===============================================; - Coverage 76.279% 76.275% -0.005% ; + Complexity 10891 10889 -2 ; ===============================================; Files 752 752 ; Lines 39590 39574 -16 ; Branches 6925 6922 -3 ; ===============================================; - Hits 30199 30185 -14 ; + Misses 6768 6767 -1 ; + Partials 2623 2622 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/tools/walkers/genotyper/GenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `46.226% <90%> (-2.896%)` | `39 <15> (-2)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=footer). Last update [c8ede6e...24e6497](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2547#issuecomment-290296401:1371,learn,learn,1371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2547#issuecomment-290296401,1,['learn'],['learn']
Usability,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=h1) Report; > Merging [#2568](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/6859a1202a79c1b123eac73a3f70162c6a90783c?src=pr&el=desc) will **decrease** coverage by `0.008%`.; > The diff coverage is `0%`. ```diff; @@ Coverage Diff @@; ## master #2568 +/- ##; ===============================================; - Coverage 76.386% 76.378% -0.008% ; Complexity 10898 10898 ; ===============================================; Files 754 754 ; Lines 39552 39552 ; Branches 6907 6907 ; ===============================================; - Hits 30212 30209 -3 ; - Misses 6727 6730 +3 ; Partials 2613 2613; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/tools/walkers/genotyper/GenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `47.807% <0%> (-1.316%)` | `41 <0> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=footer). Last update [6859a12...8066d14](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2568#issuecomment-291909495:1349,learn,learn,1349,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2568#issuecomment-291909495,1,['learn'],['learn']
Usability,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=h1) Report; > Merging [#2570](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/6859a1202a79c1b123eac73a3f70162c6a90783c?src=pr&el=desc) will **increase** coverage by `0.005%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2570 +/- ##; ===============================================; + Coverage 76.386% 76.391% +0.005% ; Complexity 10898 10898 ; ===============================================; Files 754 754 ; Lines 39552 39552 ; Branches 6907 6907 ; ===============================================; + Hits 30212 30214 +2 ; + Misses 6727 6725 -2 ; Partials 2613 2613; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=footer). Last update [6859a12...b9b665a](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2570#issuecomment-291915451:1349,learn,learn,1349,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2570#issuecomment-291915451,1,['learn'],['learn']
Usability,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2576?src=pr&el=h1) Report; > Merging [#2576](https://codecov.io/gh/broadinstitute/gatk/pull/2576?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/7a3d966f08a205f0961eebf73d89ed8b69be185d?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2576 +/- ##; ========================================; Coverage 76.4% 76.4% ; Complexity 10922 10922 ; ========================================; Files 755 755 ; Lines 39674 39674 ; Branches 6927 6927 ; ========================================; Hits 30311 30311 ; Misses 6740 6740 ; Partials 2623 2623; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2576?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2576?src=pr&el=footer). Last update [7a3d966...49bbaba](https://codecov.io/gh/broadinstitute/gatk/pull/2576?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2576#issuecomment-292395135:857,learn,learn,857,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2576#issuecomment-292395135,1,['learn'],['learn']
Usability,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=h1) Report; > Merging [#2580](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/d054e7aa910767c9f8d1b1a780435779d389080d?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2580 +/- ##; ===========================================; Coverage 76.036% 76.036% ; Complexity 11010 11010 ; ===========================================; Files 768 768 ; Lines 39952 39952 ; Branches 6956 6956 ; ===========================================; Hits 30378 30378 ; Misses 6943 6943 ; Partials 2631 2631; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `40.469% <ø> (ø)` | `28 <0> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=footer). Last update [d054e7a...c1d2a60](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2580#issuecomment-292624127:1318,learn,learn,1318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2580#issuecomment-292624127,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3041?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@9ca461c`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3041 +/- ##; ==========================================; Coverage ? 79.996% ; Complexity ? 16751 ; ==========================================; Files ? 1139 ; Lines ? 60989 ; Branches ? 9443 ; ==========================================; Hits ? 48789 ; Misses ? 8403 ; Partials ? 3797; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3041#issuecomment-306570934:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3041#issuecomment-306570934,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3044?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@6f5bab9`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## master #3044 +/- ##; ==========================================; Coverage ? 80.026% ; Complexity ? 16934 ; ==========================================; Files ? 1142 ; Lines ? 61616 ; Branches ? 9594 ; ==========================================; Hits ? 49309 ; Misses ? 8476 ; Partials ? 3831; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3044?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3044?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `71.622% <100%> (ø)` | `34 <0> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3044#issuecomment-306604461:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3044#issuecomment-306604461,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3158?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@8ab015d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3158 +/- ##; =========================================; Coverage ? 9.901% ; Complexity ? 2034 ; =========================================; Files ? 1145 ; Lines ? 61641 ; Branches ? 9606 ; =========================================; Hits ? 6103 ; Misses ? 54604 ; Partials ? 934; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3158#issuecomment-310720489:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3158#issuecomment-310720489,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3159?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@8ab015d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## master #3159 +/- ##; ==========================================; Coverage ? 62.624% ; Complexity ? 12641 ; ==========================================; Files ? 1145 ; Lines ? 61646 ; Branches ? 9606 ; ==========================================; Hits ? 38605 ; Misses ? 19096 ; Partials ? 3945; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3159?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ender/engine/spark/datasources/ReadsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/3159?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NpbmsuamF2YQ==) | `68.224% <100%> (ø)` | `22 <0> (?)` | |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/3159?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `69.231% <100%> (ø)` | `10 <0> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-310752620:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-310752620,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3817?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@1baf195`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3817 +/- ##; ==========================================; Coverage ? 63.755% ; Complexity ? 13569 ; ==========================================; Files ? 1164 ; Lines ? 64241 ; Branches ? 9815 ; ==========================================; Hits ? 40957 ; Misses ? 19088 ; Partials ? 4196; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3817#issuecomment-343290782:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3817#issuecomment-343290782,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4288?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@e955657`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## master #4288 +/- ##; ========================================; Coverage ? 79.1% ; Complexity ? 16614 ; ========================================; Files ? 1048 ; Lines ? 59579 ; Branches ? 9730 ; ========================================; Hits ? 47127 ; Misses ? 8675 ; Partials ? 3777; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4288?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...pynumber/gcnv/GermlineCNVPostprocessingEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/4288/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2djbnYvR2VybWxpbmVDTlZQb3N0cHJvY2Vzc2luZ0VuZ2luZS5qYXZh) | `97.143% <100%> (ø)` | `15 <0> (?)` | |; | [.../tools/copynumber/PostprocessGermlineCNVCalls.java](https://codecov.io/gh/broadinstitute/gatk/pull/4288/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL1Bvc3Rwcm9jZXNzR2VybWxpbmVDTlZDYWxscy5qYXZh) | `81.159% <100%> (ø)` | `10 <2> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4288#issuecomment-361348945:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4288#issuecomment-361348945,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4431?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@7838ffd`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## master #4431 +/- ##; ==========================================; Coverage ? 79.062% ; Complexity ? 16456 ; ==========================================; Files ? 1047 ; Lines ? 59194 ; Branches ? 9675 ; ==========================================; Hits ? 46800 ; Misses ? 8635 ; Partials ? 3759; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4431?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...kers/variantutils/CalculateGenotypePosteriors.java](https://codecov.io/gh/broadinstitute/gatk/pull/4431/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9DYWxjdWxhdGVHZW5vdHlwZVBvc3RlcmlvcnMuamF2YQ==) | `92.308% <100%> (ø)` | `14 <0> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4431#issuecomment-367163802:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4431#issuecomment-367163802,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4571?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@4416fd5`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `11.765%`. ```diff; @@ Coverage Diff @@; ## master #4571 +/- ##; ==========================================; Coverage ? 17.915% ; Complexity ? 8536 ; ==========================================; Files ? 1943 ; Lines ? 146209 ; Branches ? 16146 ; ==========================================; Hits ? 26194 ; Misses ? 117360 ; Partials ? 2655; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4571?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...e/hellbender/engine/FeatureDataSourceUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4571/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2VVbml0VGVzdC5qYXZh) | `1.488% <0%> (ø)` | `2 <0> (?)` | |; | [...institute/hellbender/engine/FeatureDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4571/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2UuamF2YQ==) | `57.246% <66.667%> (ø)` | `32 <0> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4571#issuecomment-454184426:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4571#issuecomment-454184426,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4947?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@39a9d13`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `6.58%`. ```diff; @@ Coverage Diff @@; ## master #4947 +/- ##; ==========================================; Coverage ? 13.338% ; Complexity ? 6396 ; ==========================================; Files ? 2016 ; Lines ? 151745 ; Branches ? 16269 ; ==========================================; Hits ? 20240 ; Misses ? 129234 ; Partials ? 2271; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4947?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ute/hellbender/utils/variant/GATKVCFConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4947/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWQ0ZDb25zdGFudHMuamF2YQ==) | `50% <ø> (ø)` | `2 <0> (?)` | |; | [...iantutils/PosteriorProbabilitiesUtilsUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4947/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9Qb3N0ZXJpb3JQcm9iYWJpbGl0aWVzVXRpbHNVbml0VGVzdC5qYXZh) | `4.386% <ø> (ø)` | `1 <0> (?)` | |; | [...te/hellbender/utils/variant/writers/TLODBlock.java](https://codecov.io/gh/broadinstitute/gatk/pull/4947/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L3dyaXRlcnMvVExPREJsb2NrLmphdmE=) | `0% <ø> (ø)` | `0 <0> (?)` | |; | [...notyper/GenotypeCalculationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4947/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwZUNhbGN1bGF0aW9uQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `100% <ø> (ø)` | `2 <0> (?)` | |; | [...ender/uti,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-400384235:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-400384235,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5026?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@cbbbb7a`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `0%`. ```diff; @@ Coverage Diff @@; ## master #5026 +/- ##; ==========================================; Coverage ? 80.255% ; Complexity ? 27182 ; ==========================================; Files ? 1779 ; Lines ? 132169 ; Branches ? 14721 ; ==========================================; Hits ? 106072 ; Misses ? 20971 ; Partials ? 5126; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5026?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...walkers/bqsr/AnalyzeCovariatesIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5026/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQW5hbHl6ZUNvdmFyaWF0ZXNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `12.308% <0%> (ø)` | `2 <0> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-598916467:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-598916467,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5116?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@43750e9`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `93.478%`. ```diff; @@ Coverage Diff @@; ## master #5116 +/- ##; ==========================================; Coverage ? 86.707% ; Complexity ? 29097 ; ==========================================; Files ? 1810 ; Lines ? 134816 ; Branches ? 14939 ; ==========================================; Hits ? 116895 ; Misses ? 12521 ; Partials ? 5400; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5116?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...iscovery/TestUtilsForAssemblyBasedSVDiscovery.java](https://codecov.io/gh/broadinstitute/gatk/pull/5116/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvVGVzdFV0aWxzRm9yQXNzZW1ibHlCYXNlZFNWRGlzY292ZXJ5LmphdmE=) | `95.522% <ø> (ø)` | `13 <0> (?)` | |; | [...e/hellbender/tools/spark/sv/utils/SVFileUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5116/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi91dGlscy9TVkZpbGVVdGlscy5qYXZh) | `24.691% <ø> (ø)` | `4 <0> (?)` | |; | [...tsFromContigAlignmentsSAMSparkIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5116/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9pbnRlZ3JhdGlvbi9EaXNjb3ZlclZhcmlhbnRzRnJvbUNvbnRpZ0FsaWdubWVudHNTQU1TcGFya0ludGVncmF0aW9uVGVzdC5qYXZh) | `97.561% <ø> (ø)` | `6 <0> (?)` | |; | [...der/tools/spark/sv/discovery/SvDiscoveryUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5116/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvU3ZEaXNjb3ZlcnlVdGlscy5,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5116#issuecomment-413255793:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5116#issuecomment-413255793,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5321?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@8c696a4`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `98.765%`. ```diff; @@ Coverage Diff @@; ## master #5321 +/- ##; ==========================================; Coverage ? 86.906% ; Complexity ? 30311 ; ==========================================; Files ? 1849 ; Lines ? 140500 ; Branches ? 15475 ; ==========================================; Hits ? 122103 ; Misses ? 12788 ; Partials ? 5609; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5321?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...er/tools/funcotator/FuncotatorIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5321/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3JJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `85.968% <0%> (ø)` | `111 <0> (?)` | |; | [...Sources/gencode/DataProviderForPik3caTestData.java](https://codecov.io/gh/broadinstitute/gatk/pull/5321/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL2dlbmNvZGUvRGF0YVByb3ZpZGVyRm9yUGlrM2NhVGVzdERhdGEuamF2YQ==) | `98.684% <100%> (ø)` | `3 <0> (?)` | |; | [...dataSources/gencode/GencodeFuncotationFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/5321/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL2dlbmNvZGUvR2VuY29kZUZ1bmNvdGF0aW9uRmFjdG9yeS5qYXZh) | `86.294% <100%> (ø)` | `187 <0> (?)` | |; | [...Sources/gencode/DataProviderForMuc16IndelData.java](https://codecov.io/gh/broadinstitute/gatk/pull/5321/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL2dlbmNvZGUvRGF0Y,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5321#issuecomment-432877680:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5321#issuecomment-432877680,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5576?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@d7d62d4`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## master #5576 +/- ##; =========================================; Coverage ? 87.05% ; Complexity ? 31450 ; =========================================; Files ? 1921 ; Lines ? 144996 ; Branches ? 16064 ; =========================================; Hits ? 126219 ; Misses ? 12934 ; Partials ? 5843; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5576?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ine/GATKPlugin/GATKAnnotationPluginDescriptor.java](https://codecov.io/gh/broadinstitute/gatk/pull/5576/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS0Fubm90YXRpb25QbHVnaW5EZXNjcmlwdG9yLmphdmE=) | `76.582% <ø> (ø)` | `57 <0> (?)` | |; | [...ine/GATKPlugin/GATKReadFilterPluginDescriptor.java](https://codecov.io/gh/broadinstitute/gatk/pull/5576/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yLmphdmE=) | `83.594% <100%> (ø)` | `49 <1> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5576#issuecomment-454236513:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5576#issuecomment-454236513,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5787?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@d9fd22f`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `12.5%`. ```diff; @@ Coverage Diff @@; ## master #5787 +/- ##; ==========================================; Coverage ? 44.104% ; Complexity ? 19589 ; ==========================================; Files ? 1973 ; Lines ? 147147 ; Branches ? 16215 ; ==========================================; Hits ? 64898 ; Misses ? 77129 ; Partials ? 5120; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5787?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...utils/activityprofile/ActivityProfileUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5787/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9hY3Rpdml0eXByb2ZpbGUvQWN0aXZpdHlQcm9maWxlVW5pdFRlc3QuamF2YQ==) | `0.442% <0%> (ø)` | `1 <0> (?)` | |; | [...ils/optimization/PersistenceOptimizerUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5787/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL29wdGltaXphdGlvbi9QZXJzaXN0ZW5jZU9wdGltaXplclVuaXRUZXN0LmphdmE=) | `2% <0%> (ø)` | `1 <0> (?)` | |; | [...utils/downsampling/DownsamplingMethodUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5787/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9kb3duc2FtcGxpbmcvRG93bnNhbXBsaW5nTWV0aG9kVW5pdFRlc3QuamF2YQ==) | `3.448% <0%> (ø)` | `1 <0> (?)` | |; | [...yper/StandardCallerArgumentCollectionUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5787/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9TdGFuZGFyZENhbGxlckFyZ3VtZW50Q29sbGVjdGlvblVuaXRUZXN0LmphdmE,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-471963750:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-471963750,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5808?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@70d4303`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `57.895%`. ```diff; @@ Coverage Diff @@; ## master #5808 +/- ##; ==========================================; Coverage ? 87.005% ; Complexity ? 32113 ; ==========================================; Files ? 1974 ; Lines ? 147249 ; Branches ? 16218 ; ==========================================; Hits ? 128114 ; Misses ? 13228 ; Partials ? 5907; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5808?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...walkers/genotyper/GenotypingGivenAllelesUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5808/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nR2l2ZW5BbGxlbGVzVXRpbHMuamF2YQ==) | `75% <ø> (ø)` | `5 <0> (?)` | |; | [...kers/haplotypecaller/AssemblyBasedCallerUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5808/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9Bc3NlbWJseUJhc2VkQ2FsbGVyVXRpbHMuamF2YQ==) | `88.961% <ø> (ø)` | `115 <0> (?)` | |; | [...utils/variant/GATKVariantContextUtilsUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5808/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWYXJpYW50Q29udGV4dFV0aWxzVW5pdFRlc3QuamF2YQ==) | `85.885% <100%> (ø)` | `163 <4> (?)` | |; | [...lbender/utils/variant/GATKVariantContextUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5808/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWYXJpYW50Q29udGV4dFV0aWxzLmphdmE=) | `84.892% <44.186%> (ø)` |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5808#issuecomment-474081532:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5808#issuecomment-474081532,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5810?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@70d4303`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #5810 +/- ##; ==========================================; Coverage ? 80.318% ; Complexity ? 30474 ; ==========================================; Files ? 1974 ; Lines ? 147194 ; Branches ? 16197 ; ==========================================; Hits ? 118224 ; Misses ? 23270 ; Partials ? 5700; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5810?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...kers/haplotypecaller/AssemblyBasedCallerUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5810/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9Bc3NlbWJseUJhc2VkQ2FsbGVyVXRpbHMuamF2YQ==) | `88.961% <ø> (ø)` | `115 <0> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5810#issuecomment-474086066:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5810#issuecomment-474086066,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5823?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@06df7e8`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `75.41%`. ```diff; @@ Coverage Diff @@; ## master #5823 +/- ##; ==========================================; Coverage ? 86.834% ; Complexity ? 32337 ; ==========================================; Files ? 1994 ; Lines ? 149405 ; Branches ? 16492 ; ==========================================; Hits ? 129735 ; Misses ? 13654 ; Partials ? 6016; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5823?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ls/copynumber/gcnv/GermlineCNVNamingConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/5823/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2djbnYvR2VybWxpbmVDTlZOYW1pbmdDb25zdGFudHMuamF2YQ==) | `0% <ø> (ø)` | `0 <0> (?)` | |; | [...ons/CopyNumberPosteriorDistributionCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/5823/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2Zvcm1hdHMvY29sbGVjdGlvbnMvQ29weU51bWJlclBvc3RlcmlvckRpc3RyaWJ1dGlvbkNvbGxlY3Rpb24uamF2YQ==) | `73.684% <0%> (ø)` | `6 <0> (?)` | |; | [...mats/collections/BaselineCopyNumberCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/5823/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2Zvcm1hdHMvY29sbGVjdGlvbnMvQmFzZWxpbmVDb3B5TnVtYmVyQ29sbGVjdGlvbi5qYXZh) | `63.636% <100%> (ø)` | `3 <0> (?)` | |; | [...ls/copynumber/formats/records/LinearCopyRatio.java](https://codecov.io/gh/broadinstitute/gatk/pull/5823/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2Zvcm1hdHMvcmVjb3,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5823#issuecomment-475397100:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5823#issuecomment-475397100,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5831?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@d27692d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `91.207%`. ```diff; @@ Coverage Diff @@; ## master #5831 +/- ##; ==========================================; Coverage ? 80.122% ; Complexity ? 30691 ; ==========================================; Files ? 1993 ; Lines ? 149366 ; Branches ? 16486 ; ==========================================; Hits ? 119675 ; Misses ? 23893 ; Partials ? 5798; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5831?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...typecaller/PairHMMLikelihoodCalculationEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5831/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9QYWlySE1NTGlrZWxpaG9vZENhbGN1bGF0aW9uRW5naW5lLmphdmE=) | `87.662% <ø> (ø)` | `38 <0> (?)` | |; | [...alkers/genotyper/GenotypeLikelihoodCalculator.java](https://codecov.io/gh/broadinstitute/gatk/pull/5831/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwZUxpa2VsaWhvb2RDYWxjdWxhdG9yLmphdmE=) | `91.667% <ø> (ø)` | `46 <0> (?)` | |; | [...oadinstitute/hellbender/utils/pairhmm/PairHMM.java](https://codecov.io/gh/broadinstitute/gatk/pull/5831/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9wYWlyaG1tL1BhaXJITU0uamF2YQ==) | `78.417% <ø> (ø)` | `24 <0> (?)` | |; | [...hellbender/utils/pairhmm/VectorLoglessPairHMM.java](https://codecov.io/gh/broadinstitute/gatk/pull/5831/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9wYWlyaG1tL1ZlY3RvckxvZ2xlc3NQYWlySE1NLmphdmE=) | `86.842% <ø> (ø)` | `12 <0> (?)` | |; | [...nder/,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5831#issuecomment-475962016:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5831#issuecomment-475962016,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5887?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@dcff818`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `92.708%`. ```diff; @@ Coverage Diff @@; ## master #5887 +/- ##; ==========================================; Coverage ? 86.825% ; Complexity ? 32305 ; ==========================================; Files ? 1991 ; Lines ? 149187 ; Branches ? 16484 ; ==========================================; Hits ? 129531 ; Misses ? 13641 ; Partials ? 6015; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5887?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...titute/hellbender/utils/IntervalUtilsUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5887/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9JbnRlcnZhbFV0aWxzVW5pdFRlc3QuamF2YQ==) | `91.906% <100%> (ø)` | `146 <0> (?)` | |; | [...nstitute/hellbender/utils/IntervalMergingRule.java](https://codecov.io/gh/broadinstitute/gatk/pull/5887/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9JbnRlcnZhbE1lcmdpbmdSdWxlLmphdmE=) | `100% <100%> (ø)` | `1 <0> (?)` | |; | [...broadinstitute/hellbender/utils/IntervalUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5887/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9JbnRlcnZhbFV0aWxzLmphdmE=) | `92.083% <100%> (ø)` | `192 <4> (?)` | |; | [...rgumentcollections/IntervalArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/5887/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL2FyZ3VtZW50Y29sbGVjdGlvbnMvSW50ZXJ2YWxBcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `89.063% <88.636%> (ø)` | `24 <1> (?)` | |; | [...entcollections/IntervalArgumentCollectionTest.java](,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5887#issuecomment-483884075:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5887#issuecomment-483884075,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5913?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@00f1e43`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `73.997%`. ```diff; @@ Coverage Diff @@; ## master #5913 +/- ##; ==========================================; Coverage ? 78.979% ; Complexity ? 30649 ; ==========================================; Files ? 2003 ; Lines ? 150459 ; Branches ? 16657 ; ==========================================; Hits ? 118831 ; Misses ? 25832 ; Partials ? 5796; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5913?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...lkers/coverage/DepthOfCoverageIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5913/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2NvdmVyYWdlL0RlcHRoT2ZDb3ZlcmFnZUludGVncmF0aW9uVGVzdC5qYXZh) | `0.758% <0.758%> (ø)` | `1 <1> (?)` | |; | [...nstitute/hellbender/utils/IntervalMergingRule.java](https://codecov.io/gh/broadinstitute/gatk/pull/5913/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9JbnRlcnZhbE1lcmdpbmdSdWxlLmphdmE=) | `100% <100%> (ø)` | `1 <0> (?)` | |; | [...org/broadinstitute/hellbender/utils/BaseUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5913/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9CYXNlVXRpbHMuamF2YQ==) | `88.462% <100%> (ø)` | `59 <3> (?)` | |; | [...itute/hellbender/engine/LocusWalkerByInterval.java](https://codecov.io/gh/broadinstitute/gatk/pull/5913/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvTG9jdXNXYWxrZXJCeUludGVydmFsLmphdmE=) | `100% <100%> (ø)` | `7 <7> (?)` | |; | [...llbender/engine/LocusWalkerByIntervalUnitTest.java](https://codecov.i,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5913#issuecomment-489247144:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5913#issuecomment-489247144,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5949?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@8e78dc6`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `71.429%`. ```diff; @@ Coverage Diff @@; ## master #5949 +/- ##; ==========================================; Coverage ? 80.152% ; Complexity ? 31063 ; ==========================================; Files ? 2016 ; Lines ? 151429 ; Branches ? 16623 ; ==========================================; Hits ? 121373 ; Misses ? 24201 ; Partials ? 5855; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5949?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/tools/spark/pipelines/ReadsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/5949/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `0% <0%> (ø)` | `0 <0> (?)` | |; | [...rk/pipelines/BQSRPipelineSparkIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5949/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQlFTUlBpcGVsaW5lU3BhcmtJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `2.381% <0%> (ø)` | `1 <0> (?)` | |; | [...ender/tools/spark/pipelines/BQSRPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/5949/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQlFTUlBpcGVsaW5lU3BhcmsuamF2YQ==) | `100% <100%> (ø)` | `5 <0> (?)` | |; | [...ender/tools/ApplyBQSRUniqueArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/5949/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9BcHBseUJRU1JVbmlxdWVBcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `100% <100%> (ø)` | `2 <0> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5949#issuecomment-493932361:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5949#issuecomment-493932361,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5966?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@1a290ae`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #5966 +/- ##; ==========================================; Coverage ? 86.934% ; Complexity ? 32787 ; ==========================================; Files ? 2014 ; Lines ? 151468 ; Branches ? 16642 ; ==========================================; Hits ? 131677 ; Misses ? 13729 ; Partials ? 6062; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5966#issuecomment-495791725:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5966#issuecomment-495791725,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/6004?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@20190cb`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `18.142%`. ```diff; @@ Coverage Diff @@; ## master #6004 +/- ##; ==========================================; Coverage ? 72.222% ; Complexity ? 27019 ; ==========================================; Files ? 2017 ; Lines ? 151440 ; Branches ? 16623 ; ==========================================; Hits ? 109373 ; Misses ? 36760 ; Partials ? 5307; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/6004?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...titute/hellbender/tools/walkers/GenotypeGVCFs.java](https://codecov.io/gh/broadinstitute/gatk/pull/6004/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0dlbm90eXBlR1ZDRnMuamF2YQ==) | `0% <0%> (ø)` | `0 <0> (?)` | |; | [...ellbender/tools/walkers/GenotypeGVCFsUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/6004/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0dlbm90eXBlR1ZDRnNVbml0VGVzdC5qYXZh) | `100% <100%> (ø)` | `21 <3> (?)` | |; | [.../hellbender/tools/walkers/GenotypeGVCFsEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/6004/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0dlbm90eXBlR1ZDRnNFbmdpbmUuamF2YQ==) | `15.094% <15.094%> (ø)` | `17 <17> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6004#issuecomment-502738494:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6004#issuecomment-502738494,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/6011?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@41db9df`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `0.714%`. ```diff; @@ Coverage Diff @@; ## master #6011 +/- ##; =========================================; Coverage ? 7.002% ; Complexity ? 2962 ; =========================================; Files ? 1998 ; Lines ? 150096 ; Branches ? 16654 ; =========================================; Hits ? 10509 ; Misses ? 138811 ; Partials ? 776; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/6011?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/6011/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvR0FUS1Rvb2wuamF2YQ==) | `59.259% <ø> (ø)` | `65 <0> (?)` | |; | [...rgumentcollections/IntervalArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/6011/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL2FyZ3VtZW50Y29sbGVjdGlvbnMvSW50ZXJ2YWxBcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `47.917% <ø> (ø)` | `11 <0> (?)` | |; | [...roadinstitute/hellbender/utils/SimpleInterval.java](https://codecov.io/gh/broadinstitute/gatk/pull/6011/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9TaW1wbGVJbnRlcnZhbC5qYXZh) | `40.909% <ø> (ø)` | `17 <0> (?)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/6011/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `21.333% <ø> (ø)` | `9 <0> (?)` | |; | [...ute/hellbender/utils/variant/GATKVCFConstants.java](https://codecov.io/gh/broadinstitute/gatk,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6011#issuecomment-520059698:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6011#issuecomment-520059698,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/6033?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@1a290ae`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `96.804%`. ```diff; @@ Coverage Diff @@; ## master #6033 +/- ##; ==========================================; Coverage ? 87.212% ; Complexity ? 32721 ; ==========================================; Files ? 2017 ; Lines ? 150990 ; Branches ? 16122 ; ==========================================; Hits ? 131681 ; Misses ? 13704 ; Partials ? 5605; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/6033?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...otator/dataSources/gencode/GencodeFuncotation.java](https://codecov.io/gh/broadinstitute/gatk/pull/6033/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL2dlbmNvZGUvR2VuY29kZUZ1bmNvdGF0aW9uLmphdmE=) | `74% <ø> (ø)` | `185 <0> (?)` | |; | [...tools/funcotator/DataSourceFuncotationFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/6033/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0RhdGFTb3VyY2VGdW5jb3RhdGlvbkZhY3RvcnkuamF2YQ==) | `80.303% <ø> (ø)` | `24 <0> (?)` | |; | [...dataSources/gencode/GencodeFuncotationBuilder.java](https://codecov.io/gh/broadinstitute/gatk/pull/6033/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL2dlbmNvZGUvR2VuY29kZUZ1bmNvdGF0aW9uQnVpbGRlci5qYXZh) | `96.825% <ø> (ø)` | `30 <0> (?)` | |; | [...ces/gencode/GencodeFuncotationFactoryUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/6033/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL2dlbmNvZGUvR2VuY29kZUZ1bmNvdGF0aW9uRmFj,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6033#issuecomment-511019437:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6033#issuecomment-511019437,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/6039?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@f499656`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `81.513%`. ```diff; @@ Coverage Diff @@; ## master #6039 +/- ##; ==========================================; Coverage ? 87.572% ; Complexity ? 36801 ; ==========================================; Files ? 2044 ; Lines ? 166417 ; Branches ? 19264 ; ==========================================; Hits ? 145735 ; Misses ? 14365 ; Partials ? 6317; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/6039?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...itute/hellbender/utils/report/GATKReportTable.java](https://codecov.io/gh/broadinstitute/gatk/pull/6039/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZXBvcnQvR0FUS1JlcG9ydFRhYmxlLmphdmE=) | `70.522% <100%> (ø)` | `68 <1> (?)` | |; | [...lbender/tools/walkers/varianteval/VariantEval.java](https://codecov.io/gh/broadinstitute/gatk/pull/6039/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnRldmFsL1ZhcmlhbnRFdmFsLmphdmE=) | `90.769% <100%> (ø)` | `144 <3> (?)` | |; | [.../varianteval/AlleleFrequencyQCIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/6039/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnRldmFsL0FsbGVsZUZyZXF1ZW5jeVFDSW50ZWdyYXRpb25UZXN0LmphdmE=) | `100% <100%> (ø)` | `3 <3> (?)` | |; | [...nder/metrics/analysis/AlleleFrequencyQCMetric.java](https://codecov.io/gh/broadinstitute/gatk/pull/6039/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9tZXRyaWNzL2FuYWx5c2lzL0FsbGVsZUZyZXF1ZW5jeVFDTWV0cmljLmphdmE=) | `100% <100%> (ø)` | `1 <1> (?)` | |; | [...s/varianteva,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6039#issuecomment-511868112:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6039#issuecomment-511868112,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/6042?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@8d88f6e`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #6042 +/- ##; ==========================================; Coverage ? 87.013% ; Complexity ? 32636 ; ==========================================; Files ? 2011 ; Lines ? 150967 ; Branches ? 16134 ; ==========================================; Hits ? 131361 ; Misses ? 14021 ; Partials ? 5585; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6042#issuecomment-515177272:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6042#issuecomment-515177272,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/6043?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@1f31a80`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `0%`. ```diff; @@ Coverage Diff @@; ## master #6043 +/- ##; ==========================================; Coverage ? 44.078% ; Complexity ? 20088 ; ==========================================; Files ? 2011 ; Lines ? 151051 ; Branches ? 16160 ; ==========================================; Hits ? 66580 ; Misses ? 79486 ; Partials ? 4985; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/6043?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...er/tools/AnalyzeSaturationMutagenesisUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/6043/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9BbmFseXplU2F0dXJhdGlvbk11dGFnZW5lc2lzVW5pdFRlc3QuamF2YQ==) | `0.794% <0%> (ø)` | `2 <0> (?)` | |; | [...hellbender/tools/AnalyzeSaturationMutagenesis.java](https://codecov.io/gh/broadinstitute/gatk/pull/6043/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9BbmFseXplU2F0dXJhdGlvbk11dGFnZW5lc2lzLmphdmE=) | `4.601% <0%> (ø)` | `0 <0> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6043#issuecomment-511968615:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6043#issuecomment-511968615,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/6054?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@5862989`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `92.032%`. ```diff; @@ Coverage Diff @@; ## master #6054 +/- ##; ==========================================; Coverage ? 87.219% ; Complexity ? 32747 ; ==========================================; Files ? 2013 ; Lines ? 151200 ; Branches ? 16144 ; ==========================================; Hits ? 131875 ; Misses ? 13707 ; Partials ? 5618; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/6054?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [.../hellbender/utils/read/ArtificialReadIterator.java](https://codecov.io/gh/broadinstitute/gatk/pull/6054/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL0FydGlmaWNpYWxSZWFkSXRlcmF0b3IuamF2YQ==) | `89.655% <100%> (ø)` | `11 <0> (?)` | |; | [...g/broadinstitute/hellbender/engine/ReadWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/6054/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZFdhbGtlci5qYXZh) | `91.176% <57.143%> (ø)` | `16 <1> (?)` | |; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/6054/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvR0FUS1Rvb2wuamF2YQ==) | `87.719% <75%> (ø)` | `103 <1> (?)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/6054/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `90.21% <78.947%> (ø)` | `64 <14> (?)` | |; | [...RecordAlignmentStartIntervalFilteringIterator.java](https://codecov.io/gh/broadinstitute/gatk/pull/,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6054#issuecomment-514429017:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6054#issuecomment-514429017,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/6055?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@74418c3`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `0%`. ```diff; @@ Coverage Diff @@; ## master #6055 +/- ##; =========================================; Coverage ? 7.015% ; Complexity ? 2991 ; =========================================; Files ? 2011 ; Lines ? 150930 ; Branches ? 16124 ; =========================================; Hits ? 10588 ; Misses ? 139581 ; Partials ? 761; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/6055?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/utils/genotyper/UnfilledReadsLikelihoods.java](https://codecov.io/gh/broadinstitute/gatk/pull/6055/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nZW5vdHlwZXIvVW5maWxsZWRSZWFkc0xpa2VsaWhvb2RzLmphdmE=) | `0% <ø> (ø)` | `0 <0> (?)` | |; | [...institute/hellbender/utils/haplotype/EventMap.java](https://codecov.io/gh/broadinstitute/gatk/pull/6055/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9oYXBsb3R5cGUvRXZlbnRNYXAuamF2YQ==) | `0% <ø> (ø)` | `0 <0> (?)` | |; | [...otypecaller/HaplotypeCallerArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/6055/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJBcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `0% <ø> (ø)` | `0 <0> (?)` | |; | [...ender/utils/genotyper/ReadLikelihoodsUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/6055/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nZW5vdHlwZXIvUmVhZExpa2VsaWhvb2RzVW5pdFRlc3QuamF2YQ==) | `2.731% <0%> (ø)` | `1 <0> (?)` | |; | [...plotypecaller/HaplotypeCallerGenotyping,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6055#issuecomment-514504406:180,learn,learn,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6055#issuecomment-514504406,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7742?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@e4c4bfc`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7742 +/- ##; ================================================; Coverage ? 86.282% ; Complexity ? 35195 ; ================================================; Files ? 2170 ; Lines ? 164904 ; Branches ? 17787 ; ================================================; Hits ? 142283 ; Misses ? 16296 ; Partials ? 6325 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7742#issuecomment-1125434428:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7742#issuecomment-1125434428,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7756?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@75b5115`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7756 +/- ##; ================================================; Coverage ? 86.305% ; Complexity ? 35190 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17775 ; ================================================; Hits ? 142262 ; Misses ? 16253 ; Partials ? 6322 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7756#issuecomment-1111607163:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7756#issuecomment-1111607163,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7769?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@2381a09`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7769 +/- ##; ================================================; Coverage ? 86.308% ; Complexity ? 35194 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17775 ; ================================================; Hits ? 142267 ; Misses ? 16248 ; Partials ? 6322 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7769#issuecomment-1109234666:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7769#issuecomment-1109234666,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7774?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@ba7a26c`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7774 +/- ##; ================================================; Coverage ? 86.294% ; Complexity ? 35196 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17774 ; ================================================; Hits ? 142244 ; Misses ? 16266 ; Partials ? 6327 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7774#issuecomment-1110190208:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7774#issuecomment-1110190208,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7787?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@75b5115`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7787 +/- ##; ================================================; Coverage ? 86.291% ; Complexity ? 35190 ; ================================================; Files ? 2170 ; Lines ? 164877 ; Branches ? 17780 ; ================================================; Hits ? 142274 ; Misses ? 16281 ; Partials ? 6322 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7787#issuecomment-1109095953:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7787#issuecomment-1109095953,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7804?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@f09b162`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head d6347af differs from pull request most recent head 0322dbb. Consider uploading reports for the commit 0322dbb to get more accurate results. ```diff; @@ Coverage Diff @@; ## ah_var_store #7804 +/- ##; ================================================; Coverage ? 86.282% ; Complexity ? 35192 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17775 ; ================================================; Hits ? 142224 ; Misses ? 16282 ; Partials ? 6331 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7804#issuecomment-1108503194:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7804#issuecomment-1108503194,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7807?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@614a0f7`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7807 +/- ##; ================================================; Coverage ? 86.295% ; Complexity ? 35191 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17775 ; ================================================; Hits ? 142246 ; Misses ? 16265 ; Partials ? 6326 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7807#issuecomment-1108994517:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7807#issuecomment-1108994517,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7812?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@d51a4e5`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7812 +/- ##; ================================================; Coverage ? 86.303% ; Complexity ? 35194 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17774 ; ================================================; Hits ? 142260 ; Misses ? 16254 ; Partials ? 6323 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7812#issuecomment-1109734272:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7812#issuecomment-1109734272,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7813?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@2381a09`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7813 +/- ##; ================================================; Coverage ? 86.295% ; Complexity ? 35192 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17775 ; ================================================; Hits ? 142246 ; Misses ? 16265 ; Partials ? 6326 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7813#issuecomment-1109871129:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7813#issuecomment-1109871129,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7814?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@2381a09`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7814 +/- ##; ================================================; Coverage ? 86.305% ; Complexity ? 35192 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17775 ; ================================================; Hits ? 142263 ; Misses ? 16250 ; Partials ? 6324 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7814#issuecomment-1109953501:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7814#issuecomment-1109953501,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7821?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@d51a4e5`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7821 +/- ##; ================================================; Coverage ? 86.305% ; Complexity ? 35194 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17774 ; ================================================; Hits ? 142262 ; Misses ? 16252 ; Partials ? 6323 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7821#issuecomment-1113595459:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7821#issuecomment-1113595459,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7822?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@d51a4e5`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7822 +/- ##; ================================================; Coverage ? 86.280% ; Complexity ? 35189 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17775 ; ================================================; Hits ? 142221 ; Misses ? 16286 ; Partials ? 6330 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7822#issuecomment-1113604463:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7822#issuecomment-1113604463,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7823?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@d77ebf5`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7823 +/- ##; ================================================; Coverage ? 51.397% ; Complexity ? 26413 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17775 ; ================================================; Hits ? 84721 ; Misses ? 74715 ; Partials ? 5401 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7823#issuecomment-1113783540:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7823#issuecomment-1113783540,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7827?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@c1c8154`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7827 +/- ##; ================================================; Coverage ? 86.305% ; Complexity ? 35190 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17775 ; ================================================; Hits ? 142262 ; Misses ? 16253 ; Partials ? 6322 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7827#issuecomment-1118672823:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7827#issuecomment-1118672823,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7828?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@d51a4e5`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7828 +/- ##; ================================================; Coverage ? 86.305% ; Complexity ? 35195 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17774 ; ================================================; Hits ? 142262 ; Misses ? 16253 ; Partials ? 6322 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7828#issuecomment-1118553978:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7828#issuecomment-1118553978,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7829?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@02cbbf1`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7829 +/- ##; ================================================; Coverage ? 86.303% ; Complexity ? 35189 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17775 ; ================================================; Hits ? 142260 ; Misses ? 16254 ; Partials ? 6323 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7829#issuecomment-1117949846:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7829#issuecomment-1117949846,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7830?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@02cbbf1`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7830 +/- ##; ================================================; Coverage ? 86.305% ; Complexity ? 35190 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17775 ; ================================================; Hits ? 142262 ; Misses ? 16253 ; Partials ? 6322 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7830#issuecomment-1117943226:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7830#issuecomment-1117943226,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7831?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@9363f15`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7831 +/- ##; ================================================; Coverage ? 86.290% ; Complexity ? 35196 ; ================================================; Files ? 2170 ; Lines ? 164869 ; Branches ? 17784 ; ================================================; Hits ? 142266 ; Misses ? 16275 ; Partials ? 6328 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7831#issuecomment-1118747922:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7831#issuecomment-1118747922,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7832?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@c1c8154`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 9a902a7 differs from pull request most recent head 89b0c4a. Consider uploading reports for the commit 89b0c4a to get more accurate results. ```diff; @@ Coverage Diff @@; ## ah_var_store #7832 +/- ##; ================================================; Coverage ? 86.303% ; Complexity ? 35194 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17774 ; ================================================; Hits ? 142260 ; Misses ? 16254 ; Partials ? 6323 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7832#issuecomment-1118881781:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7832#issuecomment-1118881781,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7834?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@9363f15`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7834 +/- ##; ================================================; Coverage ? 86.303% ; Complexity ? 35189 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17775 ; ================================================; Hits ? 142260 ; Misses ? 16254 ; Partials ? 6323 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7834#issuecomment-1118979716:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7834#issuecomment-1118979716,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7841?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@17a5e5e`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7841 +/- ##; ================================================; Coverage ? 86.293% ; Complexity ? 35189 ; ================================================; Files ? 2170 ; Lines ? 164876 ; Branches ? 17784 ; ================================================; Hits ? 142277 ; Misses ? 16276 ; Partials ? 6323 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7841#issuecomment-1122504396:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7841#issuecomment-1122504396,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7843?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@900651f`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head dad538e differs from pull request most recent head 42bfa0b. Consider uploading reports for the commit 42bfa0b to get more accurate results. ```diff; @@ Coverage Diff @@; ## ah_var_store #7843 +/- ##; ================================================; Coverage ? 86.304% ; Complexity ? 35190 ; ================================================; Files ? 2170 ; Lines ? 164844 ; Branches ? 17783 ; ================================================; Hits ? 142267 ; Misses ? 16254 ; Partials ? 6323 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7843#issuecomment-1122596759:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7843#issuecomment-1122596759,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7844?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@4e7b1f8`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7844 +/- ##; ================================================; Coverage ? 86.296% ; Complexity ? 35196 ; ================================================; Files ? 2170 ; Lines ? 164876 ; Branches ? 17783 ; ================================================; Hits ? 142282 ; Misses ? 16270 ; Partials ? 6324 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7844#issuecomment-1122778803:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7844#issuecomment-1122778803,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7845?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@bfccaf6`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 96b1120 differs from pull request most recent head 3dc3916. Consider uploading reports for the commit 3dc3916 to get more accurate results. ```diff; @@ Coverage Diff @@; ## ah_var_store #7845 +/- ##; ================================================; Coverage ? 86.305% ; Complexity ? 35190 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17775 ; ================================================; Hits ? 142263 ; Misses ? 16251 ; Partials ? 6323 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7845#issuecomment-1122825571:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7845#issuecomment-1122825571,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7848?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@4e7b1f8`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7848 +/- ##; ================================================; Coverage ? 86.304% ; Complexity ? 35189 ; ================================================; Files ? 2170 ; Lines ? 164837 ; Branches ? 17775 ; ================================================; Hits ? 142261 ; Misses ? 16252 ; Partials ? 6324 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7848#issuecomment-1125359698:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7848#issuecomment-1125359698,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7850?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@6767947`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 0ad3c70 differs from pull request most recent head 5b1eb60. Consider uploading reports for the commit 5b1eb60 to get more accurate results. ```diff; @@ Coverage Diff @@; ## ah_var_store #7850 +/- ##; ================================================; Coverage ? 86.296% ; Complexity ? 35197 ; ================================================; Files ? 2170 ; Lines ? 164876 ; Branches ? 17783 ; ================================================; Hits ? 142282 ; Misses ? 16270 ; Partials ? 6324 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7850#issuecomment-1126317411:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7850#issuecomment-1126317411,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7852?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@f58e9b2`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7852 +/- ##; ================================================; Coverage ? 86.293% ; Complexity ? 35194 ; ================================================; Files ? 2170 ; Lines ? 164876 ; Branches ? 17783 ; ================================================; Hits ? 142277 ; Misses ? 16276 ; Partials ? 6323 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7852#issuecomment-1127727087:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7852#issuecomment-1127727087,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7853?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@6767947`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7853 +/- ##; ================================================; Coverage ? 86.296% ; Complexity ? 35191 ; ================================================; Files ? 2170 ; Lines ? 164876 ; Branches ? 17784 ; ================================================; Hits ? 142282 ; Misses ? 16270 ; Partials ? 6324 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7853#issuecomment-1129091045:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7853#issuecomment-1129091045,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7855?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@1dc9776`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7855 +/- ##; ================================================; Coverage ? 86.293% ; Complexity ? 35189 ; ================================================; Files ? 2170 ; Lines ? 164876 ; Branches ? 17784 ; ================================================; Hits ? 142277 ; Misses ? 16276 ; Partials ? 6323 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7855#issuecomment-1129408216:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7855#issuecomment-1129408216,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7856?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@7388851`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7856 +/- ##; ================================================; Coverage ? 86.282% ; Complexity ? 35188 ; ================================================; Files ? 2170 ; Lines ? 164876 ; Branches ? 17784 ; ================================================; Hits ? 142258 ; Misses ? 16292 ; Partials ? 6326 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7856#issuecomment-1130338054:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7856#issuecomment-1130338054,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7857?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@8781b56`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7857 +/- ##; ================================================; Coverage ? 86.240% ; Complexity ? 35196 ; ================================================; Files ? 2173 ; Lines ? 165018 ; Branches ? 17793 ; ================================================; Hits ? 142311 ; Misses ? 16380 ; Partials ? 6327 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7857#issuecomment-1130350019:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857#issuecomment-1130350019,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7860?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@e7539d5`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7860 +/- ##; ================================================; Coverage ? 86.296% ; Complexity ? 35197 ; ================================================; Files ? 2170 ; Lines ? 164877 ; Branches ? 17783 ; ================================================; Hits ? 142282 ; Misses ? 16271 ; Partials ? 6324 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7860#issuecomment-1130663595:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7860#issuecomment-1130663595,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7862?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@4d30135`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7862 +/- ##; ================================================; Coverage ? 86.290% ; Complexity ? 35190 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17786 ; ================================================; Hits ? 142282 ; Misses ? 16281 ; Partials ? 6325 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7862#issuecomment-1132259223:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7862#issuecomment-1132259223,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7868?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@a4ac264`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7868 +/- ##; ================================================; Coverage ? 86.296% ; Complexity ? 35191 ; ================================================; Files ? 2170 ; Lines ? 164876 ; Branches ? 17784 ; ================================================; Hits ? 142281 ; Misses ? 16271 ; Partials ? 6324 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7868#issuecomment-1136246725:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7868#issuecomment-1136246725,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7870?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@91c33df`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7870 +/- ##; ================================================; Coverage ? 86.291% ; Complexity ? 35191 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17786 ; ================================================; Hits ? 142284 ; Misses ? 16280 ; Partials ? 6324 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7870#issuecomment-1136413584:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7870#issuecomment-1136413584,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7874?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@91c33df`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7874 +/- ##; ================================================; Coverage ? 86.297% ; Complexity ? 35196 ; ================================================; Files ? 2170 ; Lines ? 164876 ; Branches ? 17783 ; ================================================; Hits ? 142283 ; Misses ? 16270 ; Partials ? 6323 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7874#issuecomment-1140129247:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7874#issuecomment-1140129247,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7878?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@00e7d57`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7878 +/- ##; ================================================; Coverage ? 86.290% ; Complexity ? 35195 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17785 ; ================================================; Hits ? 142282 ; Misses ? 16281 ; Partials ? 6325 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7878#issuecomment-1142724637:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7878#issuecomment-1142724637,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7879?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@91c33df`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7879 +/- ##; ================================================; Coverage ? 86.290% ; Complexity ? 35190 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17786 ; ================================================; Hits ? 142282 ; Misses ? 16281 ; Partials ? 6325 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7879#issuecomment-1143880023:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7879#issuecomment-1143880023,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7880?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@b48282e`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7880 +/- ##; ================================================; Coverage ? 86.290% ; Complexity ? 35190 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17786 ; ================================================; Hits ? 142282 ; Misses ? 16281 ; Partials ? 6325 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7880#issuecomment-1145111544:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7880#issuecomment-1145111544,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7881?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@db90162`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7881 +/- ##; ================================================; Coverage ? 86.291% ; Complexity ? 35196 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17785 ; ================================================; Hits ? 142284 ; Misses ? 16280 ; Partials ? 6324 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7881#issuecomment-1146203150:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7881#issuecomment-1146203150,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7883?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@00e7d57`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7883 +/- ##; ================================================; Coverage ? 85.961% ; Complexity ? 35055 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17785 ; ================================================; Hits ? 141740 ; Misses ? 16868 ; Partials ? 6280 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7883#issuecomment-1147742409:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7883#issuecomment-1147742409,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7888?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@0be4453`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7888 +/- ##; ================================================; Coverage ? 86.291% ; Complexity ? 35196 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17785 ; ================================================; Hits ? 142284 ; Misses ? 16280 ; Partials ? 6324 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7888#issuecomment-1151455175:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7888#issuecomment-1151455175,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7891?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@727c1da`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7891 +/- ##; ================================================; Coverage ? 86.290% ; Complexity ? 35195 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17785 ; ================================================; Hits ? 142282 ; Misses ? 16281 ; Partials ? 6325 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7891#issuecomment-1154070191:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7891#issuecomment-1154070191,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7894?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@8e84f0a`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7894 +/- ##; ================================================; Coverage ? 86.288% ; Complexity ? 35194 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17785 ; ================================================; Hits ? 142278 ; Misses ? 16288 ; Partials ? 6322 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7894#issuecomment-1155931969:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7894#issuecomment-1155931969,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7896?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@727c1da`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7896 +/- ##; ================================================; Coverage ? 86.291% ; Complexity ? 35191 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17786 ; ================================================; Hits ? 142284 ; Misses ? 16280 ; Partials ? 6324 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7896#issuecomment-1154463035:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7896#issuecomment-1154463035,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7899?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@727c1da`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7899 +/- ##; ================================================; Coverage ? 86.290% ; Complexity ? 35190 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17786 ; ================================================; Hits ? 142282 ; Misses ? 16281 ; Partials ? 6325 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7899#issuecomment-1155041780:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7899#issuecomment-1155041780,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7901?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@727c1da`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7901 +/- ##; ================================================; Coverage ? 86.291% ; Complexity ? 35191 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17786 ; ================================================; Hits ? 142284 ; Misses ? 16280 ; Partials ? 6324 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7901#issuecomment-1155619910:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7901#issuecomment-1155619910,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7902?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@89a7d5d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7902 +/- ##; ================================================; Coverage ? 86.289% ; Complexity ? 35190 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17786 ; ================================================; Hits ? 142280 ; Misses ? 16283 ; Partials ? 6325 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7902#issuecomment-1155780693:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7902#issuecomment-1155780693,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7903?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@586f3f7`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7903 +/- ##; ================================================; Coverage ? 84.757% ; Complexity ? 34663 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17786 ; ================================================; Hits ? 139754 ; Misses ? 18943 ; Partials ? 6191 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7903#issuecomment-1156612074:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7903#issuecomment-1156612074,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7905?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@00f07e9`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7905 +/- ##; ================================================; Coverage ? 86.291% ; Complexity ? 35192 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17786 ; ================================================; Hits ? 142283 ; Misses ? 16281 ; Partials ? 6324 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7905#issuecomment-1157642181:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7905#issuecomment-1157642181,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7906?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@8e84f0a`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7906 +/- ##; ================================================; Coverage ? 86.291% ; Complexity ? 35191 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17786 ; ================================================; Hits ? 142283 ; Misses ? 16281 ; Partials ? 6324 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7906#issuecomment-1157763522:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7906#issuecomment-1157763522,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7912?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@00f07e9`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7912 +/- ##; ================================================; Coverage ? 86.288% ; Complexity ? 35188 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17786 ; ================================================; Hits ? 142278 ; Misses ? 16287 ; Partials ? 6323 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7912#issuecomment-1163414469:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7912#issuecomment-1163414469,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7913?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@8781b56`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7913 +/- ##; ================================================; Coverage ? 86.286% ; Complexity ? 35188 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17786 ; ================================================; Hits ? 142276 ; Misses ? 16289 ; Partials ? 6323 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7913#issuecomment-1163720997:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7913#issuecomment-1163720997,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7915?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@32a6106`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7915 +/- ##; ================================================; Coverage ? 86.288% ; Complexity ? 35194 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17785 ; ================================================; Hits ? 142278 ; Misses ? 16288 ; Partials ? 6322 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7915#issuecomment-1164475910:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7915#issuecomment-1164475910,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7917?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@8781b56`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head d353628 differs from pull request most recent head 46e7b4c. Consider uploading reports for the commit 46e7b4c to get more accurate results. ```diff; @@ Coverage Diff @@; ## ah_var_store #7917 +/- ##; ================================================; Coverage ? 86.288% ; Complexity ? 35194 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17785 ; ================================================; Hits ? 142278 ; Misses ? 16288 ; Partials ? 6322 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7917#issuecomment-1165634658:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7917#issuecomment-1165634658,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7919?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@4b2cf4b`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head cb38cde differs from pull request most recent head 4710dfe. Consider uploading reports for the commit 4710dfe to get more accurate results. ```diff; @@ Coverage Diff @@; ## ah_var_store #7919 +/- ##; ================================================; Coverage ? 16.934% ; Complexity ? 4702 ; ================================================; Files ? 1375 ; Lines ? 82064 ; Branches ? 13014 ; ================================================; Hits ? 13897 ; Misses ? 66106 ; Partials ? 2061 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7919#issuecomment-1167839916:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7919#issuecomment-1167839916,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7923?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@586f3f7`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7923 +/- ##; ================================================; Coverage ? 86.286% ; Complexity ? 35188 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17786 ; ================================================; Hits ? 142276 ; Misses ? 16289 ; Partials ? 6323 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7923#issuecomment-1169381087:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7923#issuecomment-1169381087,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7924?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@586f3f7`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7924 +/- ##; ================================================; Coverage ? 85.943% ; Complexity ? 35050 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17785 ; ================================================; Hits ? 141710 ; Misses ? 16895 ; Partials ? 6283 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7924#issuecomment-1170065408:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7924#issuecomment-1170065408,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7925?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@874d615`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7925 +/- ##; ================================================; Coverage ? 86.288% ; Complexity ? 35189 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17786 ; ================================================; Hits ? 142278 ; Misses ? 16288 ; Partials ? 6322 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7925#issuecomment-1170374559:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7925#issuecomment-1170374559,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7927?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@28ed209`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7927 +/- ##; ================================================; Coverage ? 86.247% ; Complexity ? 35200 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17793 ; ================================================; Hits ? 142321 ; Misses ? 16368 ; Partials ? 6327 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7927#issuecomment-1172484433:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7927#issuecomment-1172484433,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7929?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@874d615`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7929 +/- ##; ================================================; Coverage ? 86.288% ; Complexity ? 35194 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17785 ; ================================================; Hits ? 142278 ; Misses ? 16288 ; Partials ? 6322 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7929#issuecomment-1176794770:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7929#issuecomment-1176794770,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7931?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@4b2cf4b`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7931 +/- ##; ================================================; Coverage ? 86.286% ; Complexity ? 35188 ; ================================================; Files ? 2170 ; Lines ? 164888 ; Branches ? 17786 ; ================================================; Hits ? 142276 ; Misses ? 16289 ; Partials ? 6323 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7931#issuecomment-1178275700:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7931#issuecomment-1178275700,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7932?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`sl_sklearnvarianttrain_scalable@16e686c`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head a894c2a differs from pull request most recent head 93eebb0. Consider uploading reports for the commit 93eebb0 to get more accurate results. ```diff; @@ Coverage Diff @@; ## sl_sklearnvarianttrain_scalable #7932 +/- ##; ===================================================================; Coverage ? 87.031% ; Complexity ? 37304 ; ===================================================================; Files ? 2238 ; Lines ? 175124 ; Branches ? 18897 ; ===================================================================; Hits ? 152412 ; Misses ? 16010 ; Partials ? 6702 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7932#issuecomment-1179274550:312,learn,learn,312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7932#issuecomment-1179274550,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7934?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@0f9780a`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7934 +/- ##; ================================================; Coverage ? 86.241% ; Complexity ? 35198 ; ================================================; Files ? 2173 ; Lines ? 165003 ; Branches ? 17793 ; ================================================; Hits ? 142300 ; Misses ? 16377 ; Partials ? 6326 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7934#issuecomment-1180482832:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7934#issuecomment-1180482832,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7937?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@63108be`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7937 +/- ##; ================================================; Coverage ? 86.240% ; Complexity ? 35192 ; ================================================; Files ? 2173 ; Lines ? 165003 ; Branches ? 17794 ; ================================================; Hits ? 142298 ; Misses ? 16378 ; Partials ? 6327 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7937#issuecomment-1182288691:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7937#issuecomment-1182288691,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7939?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@d3f63e5`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #7939 +/- ##; ================================================; Coverage ? 86.241% ; Complexity ? 35201 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17792 ; ================================================; Hits ? 142311 ; Misses ? 16378 ; Partials ? 6327 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7939#issuecomment-1182575332:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7939#issuecomment-1182575332,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7940?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@63108be`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7940 +/- ##; ================================================; Coverage ? 86.247% ; Complexity ? 35200 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17793 ; ================================================; Hits ? 142321 ; Misses ? 16368 ; Partials ? 6327 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7940#issuecomment-1182715238:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7940#issuecomment-1182715238,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7942?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@201df7f`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7942 +/- ##; ================================================; Coverage ? 86.240% ; Complexity ? 35197 ; ================================================; Files ? 2173 ; Lines ? 165003 ; Branches ? 17793 ; ================================================; Hits ? 142298 ; Misses ? 16378 ; Partials ? 6327 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7942#issuecomment-1183531096:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7942#issuecomment-1183531096,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7943?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@c00e54b`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7943 +/- ##; ================================================; Coverage ? 86.240% ; Complexity ? 35192 ; ================================================; Files ? 2173 ; Lines ? 165003 ; Branches ? 17794 ; ================================================; Hits ? 142298 ; Misses ? 16378 ; Partials ? 6327 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7943#issuecomment-1184764712:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7943#issuecomment-1184764712,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7946?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@8781b56`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7946 +/- ##; ================================================; Coverage ? 86.241% ; Complexity ? 35201 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17792 ; ================================================; Hits ? 142311 ; Misses ? 16378 ; Partials ? 6327 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7946#issuecomment-1185630152:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7946#issuecomment-1185630152,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7953?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@210a6ae`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7953 +/- ##; ================================================; Coverage ? 86.247% ; Complexity ? 35205 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17792 ; ================================================; Hits ? 142321 ; Misses ? 16368 ; Partials ? 6327 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7953#issuecomment-1190703955:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7953#issuecomment-1190703955,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7965?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@fc2b7a8`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7965 +/- ##; ================================================; Coverage ? 86.235% ; Complexity ? 35200 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17793 ; ================================================; Hits ? 142302 ; Misses ? 16385 ; Partials ? 6329 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7965#issuecomment-1198518836:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7965#issuecomment-1198518836,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7969?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@8dd4541`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7969 +/- ##; ================================================; Coverage ? 51.377% ; Complexity ? 26423 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17793 ; ================================================; Hits ? 84780 ; Misses ? 74830 ; Partials ? 5406 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7969#issuecomment-1201465963:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7969#issuecomment-1201465963,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7970?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@3e62331`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7970 +/- ##; ================================================; Coverage ? 79.221% ; Complexity ? 33311 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17793 ; ================================================; Hits ? 130727 ; Misses ? 28117 ; Partials ? 6172 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7970#issuecomment-1203044007:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7970#issuecomment-1203044007,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7971?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@3e62331`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7971 +/- ##; ================================================; Coverage ? 86.247% ; Complexity ? 35200 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17793 ; ================================================; Hits ? 142321 ; Misses ? 16368 ; Partials ? 6327 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7971#issuecomment-1203189860:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7971#issuecomment-1203189860,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7972?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@0f7e2fd`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7972 +/- ##; ================================================; Coverage ? 86.247% ; Complexity ? 35200 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17793 ; ================================================; Hits ? 142321 ; Misses ? 16368 ; Partials ? 6327 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7972#issuecomment-1203827609:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7972#issuecomment-1203827609,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7974?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@798d4e8`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7974 +/- ##; ================================================; Coverage ? 79.218% ; Complexity ? 33309 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17793 ; ================================================; Hits ? 130723 ; Misses ? 28119 ; Partials ? 6174 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7974#issuecomment-1204152000:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7974#issuecomment-1204152000,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7981?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@798d4e8`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7981 +/- ##; ================================================; Coverage ? 86.247% ; Complexity ? 35205 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17792 ; ================================================; Hits ? 142321 ; Misses ? 16368 ; Partials ? 6327 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7981#issuecomment-1209321346:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7981#issuecomment-1209321346,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7985?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@42a9382`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7985 +/- ##; ================================================; Coverage ? 86.243% ; Complexity ? 35203 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17792 ; ================================================; Hits ? 142315 ; Misses ? 16376 ; Partials ? 6325 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7985#issuecomment-1211297727:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7985#issuecomment-1211297727,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7989?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@0130bb8`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7989 +/- ##; ================================================; Coverage ? 86.241% ; Complexity ? 35201 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17792 ; ================================================; Hits ? 142311 ; Misses ? 16378 ; Partials ? 6327 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7989#issuecomment-1215361098:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7989#issuecomment-1215361098,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7993?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@0130bb8`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #7993 +/- ##; ================================================; Coverage ? 77.030% ; Complexity ? 21708 ; ================================================; Files ? 1375 ; Lines ? 82251 ; Branches ? 13121 ; ================================================; Hits ? 63358 ; Misses ? 13764 ; Partials ? 5129 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7993#issuecomment-1218457451:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7993#issuecomment-1218457451,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7994?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@bed8af2`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7994 +/- ##; ================================================; Coverage ? 86.241% ; Complexity ? 35196 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17793 ; ================================================; Hits ? 142311 ; Misses ? 16378 ; Partials ? 6327 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7994#issuecomment-1218491668:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7994#issuecomment-1218491668,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7995?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@bed8af2`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## ah_var_store #7995 +/- ##; ================================================; Coverage ? 86.241% ; Complexity ? 35201 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17792 ; ================================================; Hits ? 142311 ; Misses ? 16378 ; Partials ? 6327 ; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7995#issuecomment-1218680553:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7995#issuecomment-1218680553,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7998?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@187fe60`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #7998 +/- ##; ================================================; Coverage ? 86.241% ; Complexity ? 35196 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17793 ; ================================================; Hits ? 142311 ; Misses ? 16378 ; Partials ? 6327 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7998#issuecomment-1223005785:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7998#issuecomment-1223005785,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/7999?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@187fe60`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #7999 +/- ##; ================================================; Coverage ? 86.242% ; Complexity ? 35197 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17793 ; ================================================; Hits ? 142313 ; Misses ? 16377 ; Partials ? 6326 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7999#issuecomment-1223117817:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7999#issuecomment-1223117817,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8000?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@187fe60`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8000 +/- ##; ================================================; Coverage ? 86.241% ; Complexity ? 35201 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17792 ; ================================================; Hits ? 142311 ; Misses ? 16378 ; Partials ? 6327 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8000#issuecomment-1226089548:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8000#issuecomment-1226089548,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8001?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@aff1c48`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8001 +/- ##; ================================================; Coverage ? 86.242% ; Complexity ? 35202 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17792 ; ================================================; Hits ? 142313 ; Misses ? 16377 ; Partials ? 6326 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8001#issuecomment-1226096473:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8001#issuecomment-1226096473,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8002?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@0d914c5`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8002 +/- ##; ================================================; Coverage ? 86.242% ; Complexity ? 35202 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17792 ; ================================================; Hits ? 142313 ; Misses ? 16377 ; Partials ? 6326 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8002#issuecomment-1227340828:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8002#issuecomment-1227340828,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8003?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@0d914c5`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8003 +/- ##; ================================================; Coverage ? 86.242% ; Complexity ? 35196 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17793 ; ================================================; Hits ? 142313 ; Misses ? 16376 ; Partials ? 6327 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8003#issuecomment-1228797455:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8003#issuecomment-1228797455,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8006?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@76c969d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8006 +/- ##; ================================================; Coverage ? 86.243% ; Complexity ? 35201 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17791 ; ================================================; Hits ? 142304 ; Misses ? 16373 ; Partials ? 6327 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8006#issuecomment-1230953418:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8006#issuecomment-1230953418,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8008?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@d52f05d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8008 +/- ##; ================================================; Coverage ? 86.243% ; Complexity ? 35196 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17792 ; ================================================; Hits ? 142304 ; Misses ? 16373 ; Partials ? 6327 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8008#issuecomment-1233705189:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8008#issuecomment-1233705189,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8009?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@91c9c9c`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8009 +/- ##; ================================================; Coverage ? 42.466% ; Complexity ? 23462 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17791 ; ================================================; Hits ? 70070 ; Misses ? 89631 ; Partials ? 5303 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8009#issuecomment-1234678104:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8009#issuecomment-1234678104,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8010?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@d3f63e5`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8010 +/- ##; ================================================; Coverage ? 86.241% ; Complexity ? 35196 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17793 ; ================================================; Hits ? 142311 ; Misses ? 16378 ; Partials ? 6327 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8010#issuecomment-1238195271:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8010#issuecomment-1238195271,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8011?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@a1a8e57`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head db85dfc differs from pull request most recent head 43247b7. Consider uploading reports for the commit 43247b7 to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8011 +/- ##; ================================================; Coverage ? 86.241% ; Complexity ? 35196 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17793 ; ================================================; Hits ? 142311 ; Misses ? 16378 ; Partials ? 6327 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8011#issuecomment-1238575906:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8011#issuecomment-1238575906,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8012?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@51387f1`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head e267ca5 differs from pull request most recent head 39de64e. Consider uploading reports for the commit 39de64e to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8012 +/- ##; ================================================; Coverage ? 86.242% ; Complexity ? 35196 ; ================================================; Files ? 2173 ; Lines ? 165016 ; Branches ? 17793 ; ================================================; Hits ? 142313 ; Misses ? 16376 ; Partials ? 6327 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8012#issuecomment-1238610329:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8012#issuecomment-1238610329,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8014?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@ff05126`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8014 +/- ##; ================================================; Coverage ? 86.243% ; Complexity ? 35201 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17791 ; ================================================; Hits ? 142304 ; Misses ? 16373 ; Partials ? 6327 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8014#issuecomment-1239801973:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8014#issuecomment-1239801973,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8016?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@08c1ad7`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8016 +/- ##; ================================================; Coverage ? 86.243% ; Complexity ? 35196 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17792 ; ================================================; Hits ? 142304 ; Misses ? 16373 ; Partials ? 6327 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8016#issuecomment-1241258915:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8016#issuecomment-1241258915,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8018?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@3b74d0a`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8018 +/- ##; ================================================; Coverage ? 86.226% ; Complexity ? 35201 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17792 ; ================================================; Hits ? 142277 ; Misses ? 16393 ; Partials ? 6334 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8018#issuecomment-1246697846:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8018#issuecomment-1246697846,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8019?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@08c1ad7`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8019 +/- ##; ================================================; Coverage ? 86.244% ; Complexity ? 35197 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17792 ; ================================================; Hits ? 142306 ; Misses ? 16372 ; Partials ? 6326 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8019#issuecomment-1247078773:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8019#issuecomment-1247078773,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8020?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@0ef7433`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8020 +/- ##; ================================================; Coverage ? 86.244% ; Complexity ? 35197 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17792 ; ================================================; Hits ? 142306 ; Misses ? 16372 ; Partials ? 6326 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8020#issuecomment-1248635546:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8020#issuecomment-1248635546,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8022?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@53cf8a7`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8022 +/- ##; ================================================; Coverage ? 86.244% ; Complexity ? 35202 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17791 ; ================================================; Hits ? 142306 ; Misses ? 16372 ; Partials ? 6326 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8022#issuecomment-1249814365:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8022#issuecomment-1249814365,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8023?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@0ef7433`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8023 +/- ##; ================================================; Coverage ? 42.707% ; Complexity ? 23947 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17792 ; ================================================; Hits ? 70468 ; Misses ? 89424 ; Partials ? 5112 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8023#issuecomment-1251113593:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8023#issuecomment-1251113593,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8024?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@0ef7433`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8024 +/- ##; ================================================; Coverage ? 86.240% ; Complexity ? 35196 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17792 ; ================================================; Hits ? 142300 ; Misses ? 16376 ; Partials ? 6328 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8024#issuecomment-1251325952:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8024#issuecomment-1251325952,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8026?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@0ef7433`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8026 +/- ##; ================================================; Coverage ? 86.244% ; Complexity ? 35202 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17791 ; ================================================; Hits ? 142306 ; Misses ? 16372 ; Partials ? 6326 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8026#issuecomment-1252821765:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8026#issuecomment-1252821765,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8029?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@55668c3`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8029 +/- ##; ================================================; Coverage ? 86.219% ; Complexity ? 35201 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17791 ; ================================================; Hits ? 142265 ; Misses ? 16405 ; Partials ? 6334 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8029#issuecomment-1255077969:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8029#issuecomment-1255077969,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8034?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@0735df7`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8034 +/- ##; ================================================; Coverage ? 43.628% ; Complexity ? 21865 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17792 ; ================================================; Hits ? 71988 ; Misses ? 87583 ; Partials ? 5433 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8034#issuecomment-1258126509:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8034#issuecomment-1258126509,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8038?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@55668c3`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8038 +/- ##; ================================================; Coverage ? 86.250% ; Complexity ? 35201 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17792 ; ================================================; Hits ? 142316 ; Misses ? 16361 ; Partials ? 6327 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8038#issuecomment-1259612166:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8038#issuecomment-1259612166,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8039?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@953f68c`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8039 +/- ##; ================================================; Coverage ? 86.190% ; Complexity ? 35202 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17791 ; ================================================; Hits ? 142217 ; Misses ? 16455 ; Partials ? 6332 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8039#issuecomment-1260048714:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8039#issuecomment-1260048714,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8042?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@9994658`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8042 +/- ##; ================================================; Coverage ? 86.249% ; Complexity ? 35200 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17792 ; ================================================; Hits ? 142314 ; Misses ? 16362 ; Partials ? 6328 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8042#issuecomment-1261458954:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8042#issuecomment-1261458954,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8044?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@c7df760`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8044 +/- ##; ================================================; Coverage ? 49.580% ; Complexity ? 25392 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17792 ; ================================================; Hits ? 81809 ; Misses ? 77736 ; Partials ? 5459 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8044#issuecomment-1265577285:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8044#issuecomment-1265577285,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8046?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@b088a5c`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8046 +/- ##; ================================================; Coverage ? 86.249% ; Complexity ? 35205 ; ================================================; Files ? 2173 ; Lines ? 165004 ; Branches ? 17791 ; ================================================; Hits ? 142314 ; Misses ? 16362 ; Partials ? 6328 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8046#issuecomment-1268749154:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8046#issuecomment-1268749154,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8047?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@01b2880`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 38e90c3 differs from pull request most recent head 07c6b83. Consider uploading reports for the commit 07c6b83 to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8047 +/- ##; ================================================; Coverage ? 16.953% ; Complexity ? 4702 ; ================================================; Files ? 1375 ; Lines ? 82247 ; Branches ? 13121 ; ================================================; Hits ? 13943 ; Misses ? 66245 ; Partials ? 2059 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8047#issuecomment-1270103549:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8047#issuecomment-1270103549,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8051?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@5f1f998`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8051 +/- ##; ================================================; Coverage ? 86.248% ; Complexity ? 35205 ; ================================================; Files ? 2173 ; Lines ? 165012 ; Branches ? 17791 ; ================================================; Hits ? 142319 ; Misses ? 16365 ; Partials ? 6328 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8051#issuecomment-1276533038:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8051#issuecomment-1276533038,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8052?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@1c5c486`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8052 +/- ##; ================================================; Coverage ? 86.242% ; Complexity ? 35201 ; ================================================; Files ? 2173 ; Lines ? 165041 ; Branches ? 17794 ; ================================================; Hits ? 142334 ; Misses ? 16379 ; Partials ? 6328 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8052#issuecomment-1276585423:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8052#issuecomment-1276585423,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8055?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@b338cc9`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 04f2d5a differs from pull request most recent head 6b737da. Consider uploading reports for the commit 6b737da to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8055 +/- ##; ================================================; Coverage ? 86.233% ; Complexity ? 35201 ; ================================================; Files ? 2173 ; Lines ? 165012 ; Branches ? 17791 ; ================================================; Hits ? 142294 ; Misses ? 16388 ; Partials ? 6330 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8055#issuecomment-1279486249:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8055#issuecomment-1279486249,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8056?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@1c5c486`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8056 +/- ##; ================================================; Coverage ? 86.189% ; Complexity ? 35197 ; ================================================; Files ? 2173 ; Lines ? 165041 ; Branches ? 17794 ; ================================================; Hits ? 142248 ; Misses ? 16464 ; Partials ? 6329 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8056#issuecomment-1277830813:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8056#issuecomment-1277830813,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8058?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@4c8abaa`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8058 +/- ##; ================================================; Coverage ? 84.277% ; Complexity ? 34793 ; ================================================; Files ? 2191 ; Lines ? 166324 ; Branches ? 17898 ; ================================================; Hits ? 140173 ; Misses ? 19933 ; Partials ? 6218 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8058#issuecomment-1281360109:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8058#issuecomment-1281360109,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8061?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@2a8c210`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8061 +/- ##; ================================================; Coverage ? 77.048% ; Complexity ? 21714 ; ================================================; Files ? 1375 ; Lines ? 82250 ; Branches ? 13121 ; ================================================; Hits ? 63372 ; Misses ? 13749 ; Partials ? 5129 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8061#issuecomment-1282735799:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8061#issuecomment-1282735799,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8062?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@bde383b`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8062 +/- ##; ================================================; Coverage ? 16.944% ; Complexity ? 4702 ; ================================================; Files ? 1375 ; Lines ? 82077 ; Branches ? 13014 ; ================================================; Hits ? 13907 ; Misses ? 66109 ; Partials ? 2061 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8062#issuecomment-1282984387:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8062#issuecomment-1282984387,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8065?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@f2dcc68`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8065 +/- ##; ================================================; Coverage ? 86.249% ; Complexity ? 35201 ; ================================================; Files ? 2173 ; Lines ? 165041 ; Branches ? 17794 ; ================================================; Hits ? 142346 ; Misses ? 16369 ; Partials ? 6326 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8065#issuecomment-1285833953:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8065#issuecomment-1285833953,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8066?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@116db44`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8066 +/- ##; ================================================; Coverage ? 86.250% ; Complexity ? 35205 ; ================================================; Files ? 2173 ; Lines ? 165041 ; Branches ? 17793 ; ================================================; Hits ? 142348 ; Misses ? 16367 ; Partials ? 6326 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8066#issuecomment-1286206647:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8066#issuecomment-1286206647,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8072?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@4c8abaa`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8072 +/- ##; ================================================; Coverage ? 86.251% ; Complexity ? 35201 ; ================================================; Files ? 2173 ; Lines ? 165041 ; Branches ? 17794 ; ================================================; Hits ? 142350 ; Misses ? 16366 ; Partials ? 6325 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8072#issuecomment-1289703869:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8072#issuecomment-1289703869,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8073?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@116db44`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8073 +/- ##; ================================================; Coverage ? 44.264% ; Complexity ? 24497 ; ================================================; Files ? 2173 ; Lines ? 165041 ; Branches ? 17793 ; ================================================; Hits ? 73053 ; Misses ? 86738 ; Partials ? 5250 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8073#issuecomment-1291001159:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8073#issuecomment-1291001159,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8077?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@290fd23`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8077 +/- ##; ================================================; Coverage ? 86.203% ; Complexity ? 35186 ; ================================================; Files ? 2173 ; Lines ? 165045 ; Branches ? 17794 ; ================================================; Hits ? 142274 ; Misses ? 16439 ; Partials ? 6332 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8077#issuecomment-1294050838:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8077#issuecomment-1294050838,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8078?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@e6d736b`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8078 +/- ##; ================================================; Coverage ? 86.226% ; Complexity ? 35205 ; ================================================; Files ? 2173 ; Lines ? 165041 ; Branches ? 17793 ; ================================================; Hits ? 142309 ; Misses ? 16399 ; Partials ? 6333 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8078#issuecomment-1295426924:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8078#issuecomment-1295426924,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8079?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@18fa298`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8079 +/- ##; ================================================; Coverage ? 86.191% ; Complexity ? 35200 ; ================================================; Files ? 2173 ; Lines ? 165041 ; Branches ? 17794 ; ================================================; Hits ? 142251 ; Misses ? 16460 ; Partials ? 6330 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8079#issuecomment-1295446038:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8079#issuecomment-1295446038,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8082?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@31e8cb7`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8082 +/- ##; ================================================; Coverage ? 85.912% ; Complexity ? 35063 ; ================================================; Files ? 2173 ; Lines ? 165041 ; Branches ? 17793 ; ================================================; Hits ? 141790 ; Misses ? 16969 ; Partials ? 6282 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8082#issuecomment-1297544446:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8082#issuecomment-1297544446,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8085?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@7dd6ede`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8085 +/- ##; ================================================; Coverage ? 84.371% ; Complexity ? 34536 ; ================================================; Files ? 2173 ; Lines ? 165041 ; Branches ? 17793 ; ================================================; Hits ? 139246 ; Misses ? 19641 ; Partials ? 6154 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8085#issuecomment-1305698326:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8085#issuecomment-1305698326,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8086?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@d1907a8`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 308c703 differs from pull request most recent head bd65357. Consider uploading reports for the commit bd65357 to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8086 +/- ##; ================================================; Coverage ? 86.240% ; Complexity ? 35200 ; ================================================; Files ? 2173 ; Lines ? 165041 ; Branches ? 17794 ; ================================================; Hits ? 142331 ; Misses ? 16382 ; Partials ? 6328 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8086#issuecomment-1305784256:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086#issuecomment-1305784256,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8093?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@dfe7b7e`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8093 +/- ##; ================================================; Coverage ? 19.702% ; Complexity ? 10708 ; ================================================; Files ? 2173 ; Lines ? 164868 ; Branches ? 17686 ; ================================================; Hits ? 32482 ; Misses ? 129238 ; Partials ? 3148 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8093#issuecomment-1314262080:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8093#issuecomment-1314262080,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8096?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@aa97a09`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8096 +/- ##; ================================================; Coverage ? 55.872% ; Complexity ? 15813 ; ================================================; Files ? 1375 ; Lines ? 82251 ; Branches ? 13123 ; ================================================; Hits ? 45955 ; Misses ? 32140 ; Partials ? 4156 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8096#issuecomment-1316030315:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8096#issuecomment-1316030315,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8101?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@08048f1`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8101 +/- ##; ================================================; Coverage ? 86.139% ; Complexity ? 35079 ; ================================================; Files ? 2173 ; Lines ? 164868 ; Branches ? 17686 ; ================================================; Hits ? 142015 ; Misses ? 16519 ; Partials ? 6334 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8101#issuecomment-1322778840:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8101#issuecomment-1322778840,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8104?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@31c3a02`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 8166c60 differs from pull request most recent head 9533722. Consider uploading reports for the commit 9533722 to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8104 +/- ##; ================================================; Coverage ? 86.246% ; Complexity ? 35202 ; ================================================; Files ? 2173 ; Lines ? 165045 ; Branches ? 17793 ; ================================================; Hits ? 142345 ; Misses ? 16375 ; Partials ? 6325 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8104#issuecomment-1325273610:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8104#issuecomment-1325273610,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8108?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@335a1b1`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8108 +/- ##; ================================================; Coverage ? 86.210% ; Complexity ? 35186 ; ================================================; Files ? 2173 ; Lines ? 165041 ; Branches ? 17794 ; ================================================; Hits ? 142282 ; Misses ? 16426 ; Partials ? 6333 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8108#issuecomment-1331122539:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8108#issuecomment-1331122539,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8109?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@335a1b1`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8109 +/- ##; ================================================; Coverage ? 86.171% ; Complexity ? 35132 ; ================================================; Files ? 2173 ; Lines ? 165041 ; Branches ? 17794 ; ================================================; Hits ? 142218 ; Misses ? 16477 ; Partials ? 6346 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8109#issuecomment-1331159839:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8109#issuecomment-1331159839,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8110?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@335a1b1`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8110 +/- ##; ================================================; Coverage ? 86.240% ; Complexity ? 35194 ; ================================================; Files ? 2173 ; Lines ? 165041 ; Branches ? 17794 ; ================================================; Hits ? 142331 ; Misses ? 16383 ; Partials ? 6327 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8110#issuecomment-1331168379:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8110#issuecomment-1331168379,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8113?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@aa97a09`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8113 +/- ##; ================================================; Coverage ? 86.170% ; Complexity ? 35132 ; ================================================; Files ? 2173 ; Lines ? 165045 ; Branches ? 17794 ; ================================================; Hits ? 142220 ; Misses ? 16479 ; Partials ? 6346 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8113#issuecomment-1331332637:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8113#issuecomment-1331332637,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8116?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@1412b4e`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8116 +/- ##; ================================================; Coverage ? 86.137% ; Complexity ? 35078 ; ================================================; Files ? 2173 ; Lines ? 164868 ; Branches ? 17686 ; ================================================; Hits ? 142013 ; Misses ? 16520 ; Partials ? 6335 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8116#issuecomment-1332758896:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8116#issuecomment-1332758896,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8117?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@116db44`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8117 +/- ##; ================================================; Coverage ? 79.143% ; Complexity ? 33242 ; ================================================; Files ? 2173 ; Lines ? 165045 ; Branches ? 17794 ; ================================================; Hits ? 130622 ; Misses ? 28231 ; Partials ? 6192 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8117#issuecomment-1334429236:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8117#issuecomment-1334429236,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8119?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`rc-vs-651-vat-from-vds@1ae6f4a`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## rc-vs-651-vat-from-vds #8119 +/- ##; ==========================================================; Coverage ? 62.890% ; Complexity ? 17159 ; ==========================================================; Files ? 1375 ; Lines ? 82251 ; Branches ? 13123 ; ==========================================================; Hits ? 51728 ; Misses ? 25370 ; Partials ? 5153 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8119#issuecomment-1337575778:303,learn,learn,303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8119#issuecomment-1337575778,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8133?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@080d66a`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8133 +/- ##; ================================================; Coverage ? 86.241% ; Complexity ? 35194 ; ================================================; Files ? 2173 ; Lines ? 165045 ; Branches ? 17794 ; ================================================; Hits ? 142336 ; Misses ? 16384 ; Partials ? 6325 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8133#issuecomment-1355614082:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8133#issuecomment-1355614082,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8135?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@080d66a`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8135 +/- ##; ================================================; Coverage ? 86.241% ; Complexity ? 35194 ; ================================================; Files ? 2173 ; Lines ? 165045 ; Branches ? 17794 ; ================================================; Hits ? 142336 ; Misses ? 16384 ; Partials ? 6325 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8135#issuecomment-1358492963:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8135#issuecomment-1358492963,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/8137?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) Report; > :exclamation: No coverage uploaded for pull request base (`ah_var_store@ea3408b`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## ah_var_store #8137 +/- ##; ================================================; Coverage ? 86.242% ; Complexity ? 35199 ; ================================================; Files ? 2173 ; Lines ? 165045 ; Branches ? 17793 ; ================================================; Hits ? 142338 ; Misses ? 16383 ; Partials ? 6324 ; ```. </details>,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8137#issuecomment-1363384269:293,learn,learn,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8137#issuecomment-1363384269,1,['learn'],['learn']
