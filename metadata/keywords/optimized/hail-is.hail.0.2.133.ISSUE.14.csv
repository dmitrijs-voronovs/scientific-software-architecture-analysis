quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Performance,"This allows us to insert fields elsewhere in the struct, and; is necessary for a forthcoming Python IR generation change, which will fix the BGEN allocation performance problem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5103:157,perform,performance,157,https://hail.is,https://github.com/hail-is/hail/pull/5103,1,['perform'],['performance']
Performance,This avoids confusing Docker cache behavior by baking the verison number into; the RUN command string.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10282:29,cache,cache,29,https://hail.is,https://github.com/hail-is/hail/pull/10282,1,['cache'],['cache']
Performance,"This branch:; ```; In [2]: %timeit vds.split_multi().count(); 1 loop, best of 3: 18.2 s per loop; ```. Master:; ```; In [2]: %timeit vds.split_multi().count(); 1 loop, best of 3: 27.7 s per loop; ```. I predict the performance improvement will increase with large datasets.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1363:215,perform,performance,215,https://hail.is,https://github.com/hail-is/hail/pull/1363,1,['perform'],['performance']
Performance,"This can probably be better optimized, but it is part of a larger problem with the notify children not happening atomically. Working on a better fix, but that will take some time. Figured this is an improvement.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6355:28,optimiz,optimized,28,https://hail.is,https://github.com/hail-is/hail/pull/6355,1,['optimiz'],['optimized']
Performance,"This change adds an nginx sidecar to the batch-driver pod for terminating TLS. TLS negotiation has proven a major bottleneck to scheduling performance, as the batch-driver currently spends up to 60% of its CPU time in handshakes with workers. Moving TLS termination into a sidecar that can leverage additional cores both reduced CPU pressure on the driver and allowed for a 3-4x increase in job-scheduling throughput. ## Benchmarking; Below are before-and-after profiles of the same benchmark (30,000 1s jobs) under the proposed higher rate limit, showing CPU time:; <img width=""1889"" alt=""Screen Shot 2022-03-21 at 5 10 13 PM"" src=""https://user-images.githubusercontent.com/24440116/159364769-6fd60840-5745-40ab-802e-68b8d4f32078.png"">; <img width=""1885"" alt=""Screen Shot 2022-03-21 at 5 12 33 PM"" src=""https://user-images.githubusercontent.com/24440116/159364787-ca7ec307-877d-479c-9c19-8746b5e82eab.png"">. Looking at Wall time, the before profile is nearly identical because at the current rate limit the driver uses 100% of its CPU shares under this benchmark. On this branch, CPU utilization drops to 40-60%, giving the following wall time profile:; <img width=""1879"" alt=""Screen Shot 2022-03-21 at 5 30 39 PM"" src=""https://user-images.githubusercontent.com/24440116/159367182-0830d6ff-3b6f-4fa7-8004-0fc43283ec4a.png"">. So we can be confident that driver CPU is no longer a bottleneck even in the increased rate limit. ## So what's the bottleneck now?; Since the higher rate limit still leaves the driver plenty of CPU room (I've seen it peak at 60% of a vCPU), why not crank it higher? Well, we're increasing concurrency so latent deadlocks start to be a bigger issue again. We start to see tens of deadlocks per second in the proposed rate limit and hundreds at higher rate limits. As a result, we're spending more cycles repeating queries instead of actually scheduling faster. Next steps should focus on eliminating deadlocks before we can continue to max out CPU use. ## Miscellaneous; We'v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11638:114,bottleneck,bottleneck,114,https://hail.is,https://github.com/hail-is/hail/pull/11638,3,"['bottleneck', 'perform', 'throughput']","['bottleneck', 'performance', 'throughput']"
Performance,"This change anticipates the ContextRDD change wherein `RVD.rdd` will not; be an RDD. Moreover, enforcing an abstraction barrier at the level of; `RVD` will ease changes to the implementation of `RVD`. There are two remaining types of calls that I cannot eliminate:. - uses in BlockMatrix and OrderedRDD2: these two classes are building; new RDDs based on the RVD's rdd, these classes should be considered; within the implementation of the RVD abstraction. Because these two; classes are outside of `is.hail.rvd`, I cannot enforce an access; modifier on `RVD.rdd`. - uses by methods:. - LDPrune: it seems we need a ""GeneralRVD"". - Skat: it seems like some of this could be moved to python actually;; but there is some matrix math that cannot be moved until the expr; lang has efficient small-matrix ops. - MatrixTable.same: I could probably move this if I re-implemented; forall in terms of RVD.aggregate?. - MatrixTable.annotateRowsIntervalTable: really not sure about this; one, this seems like a performance optimization that purposely; reaches through the abstraction to do Smart Things",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3186:998,perform,performance,998,https://hail.is,https://github.com/hail-is/hail/pull/3186,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"This change is split out from a larger refactoring effort on the various Backend; implementations. The goals of this effort are to provide query-level; configuration to the backend that's currently tied to the lifetime of a backend,; reduce code duplication and reduce state duplication. In this change, I'm removing blockmatrix persist/unpersist from the `Backend`; interface by adding `BlockMatrixCache: mutable.Map[String, BlockMatrix]` to; `ExecuteContext`. The various reader/writer implementations simply fetch the ; block matrix from this cache. For the spark backend, this is backed by a cache; whose lifetime is tied to the spark backend. Since block matrices are not; supported in the local and service backends, the cache is an empty map. Note that block matrix persist is broken in python (#14689)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14690:546,cache,cache,546,https://hail.is,https://github.com/hail-is/hail/pull/14690,3,['cache'],['cache']
Performance,"This commit introduces the SCode hierarchy. How are SCodes different; from PCodes? SCodes are what we want PCodes to be. They don't have; the methods `code / tcode`, which gives us a way to implement new; performance-improving types like SStackStruct with well-defined; boundaries around that functionality. Currently, the `asPCode` and `IEmitCode.typecast` methods break this; boundary, and I added these as a short-term mechanism to avoid another; very spicy meatball. This commit is entirely reorganizational, with no semantic changes; to the compiler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9729:205,perform,performance-improving,205,https://hail.is,https://github.com/hail-is/hail/pull/9729,1,['perform'],['performance-improving']
Performance,"This doesn't actually hugely improve performance:. Master:; ```; 2019-09-12 16:25:06,282: INFO: [1/1] Running import_bgen_force_count_all...; 2019-09-12 16:26:26,427: INFO: burn in: 80.14s; 2019-09-12 16:27:46,816: INFO: run 1: 80.39s; 2019-09-12 16:29:10,637: INFO: run 2: 83.82s; 2019-09-12 16:30:33,742: INFO: run 3: 83.11s; 2019-09-12 16:31:53,968: INFO: run 4: 80.22s; 2019-09-12 16:33:18,855: INFO: run 5: 84.89s; ```. PR:; ```; 2019-09-12 16:46:20,424: INFO: [1/1] Running import_bgen_force_count_all...; 2019-09-12 16:47:39,550: INFO: burn in: 79.12s; 2019-09-12 16:48:57,259: INFO: run 1: 77.71s; 2019-09-12 16:50:15,457: INFO: run 2: 78.20s; 2019-09-12 16:51:32,472: INFO: run 3: 77.01s; 2019-09-12 16:52:49,160: INFO: run 4: 76.68s; 2019-09-12 16:54:05,504: INFO: run 5: 76.34s; ```. However, it does make the generated much easier to profile, and; would make much more of difference if we support BGEN 1.3 with ZSTD.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7059:37,perform,performance,37,https://hail.is,https://github.com/hail-is/hail/pull/7059,1,['perform'],['performance']
Performance,This encapsulates the type dispatch necessary to load and store; annotations.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2396:49,load,load,49,https://hail.is,https://github.com/hail-is/hail/pull/2396,1,['load'],['load']
Performance,"This feature is not widely supported (only on MatrixTable & SparkBackend); and is not well-tested in CI (we only test that matrix writes run and return; the correct result with the _checkpoint_file parameter, not the performance; semantics). I've played around with this code on my laptop and the performance semantics; are what I expect. Logging messages provide some transparency too:. ```. In [2]: mt = hl.utils.range_matrix_table(100000, 10, 500). In [3]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:06 Hail: INFO: creating new checkpoint at /tmp/mt_checkpoint; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [4]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:14 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 192/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [5]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:22 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 300/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [6]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:29 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 372/500 partitions written; 	2021-03-23 14:50:36 Hail: INFO: wrote matrix table with 100000 rows and 10 columns in 500 partitions to /tmp/mt_temp4.mt; 	 Total size: 391.55 KiB; 	 * Rows/entries: 391.51 KiB; 	 * Columns: 31.00 B; 	 * Globals: 11.00 B; 	 * Smallest partition: 200 rows (505.00 B); 	 * Largest partition: 200 rows (835.00 B). In [7]: mt_r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10215:217,perform,performance,217,https://hail.is,https://github.com/hail-is/hail/pull/10215,2,['perform'],['performance']
Performance,"This fixes a bug in joins where:; * The join key is a prefix of the left key; * Values of the join key in the left table span multiple partitions. For example, say the left table is `{[(0, 0), (0, 1)], [(0, 2), (1, 0)]}`, where the two lists of tuples are the two partitions, and the key is the entire tuple, and the join key is just the first field. Then in the old code; ```; val loweredLeft = lower(left).strictify(); val leftKeyToRightKeyMap = left.typ.keyType.fieldNames.zip(right.typ.keyType.fieldNames).toMap; val newRightPartitioner = loweredLeft.partitioner.coarsen(commonKeyLength); .rename(leftKeyToRightKeyMap); val loweredRight = lower(right).repartitionNoShuffle(newRightPartitioner); ```; the left partitioner is coarsened to `[(0), (0)], [(0), (1)]` in the third line, and the `repartitionNoShuffle` in the fourth line fails because rows with key `(0)` are split across both partitions. One possible fix would be to strictify the coarsened partitioner, which in this case would force the left table to one partition. But this seems dangerous performance-wise. Instead, this PR makes the lowered behavior match the old behavior, which is to ""repartition"" the right to the invalid partitioner `[(0), (0)], [(0), (1)]`, meaning rows with key `(0)` get duplicated into both partitions. This is exactly the intended semantics of `alignAndZipPartitions` as described in the code:; > The partitioner of the result will be the left partitioner. Each partition will be computed by 'joiner', with corresponding partition of 'this' as first iterator, and with all rows of 'that' whose 'joinKey' might match something in partition as the second iterator. This behavior is implemented with a flag `allowDuplication` on `repartitionNoShuffle`. This simply omits the assertion on the new partitioner that guarantees each incoming row ends up in at most one result partition.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11556:1058,perform,performance-wise,1058,https://hail.is,https://github.com/hail-is/hail/pull/11556,1,['perform'],['performance-wise']
Performance,"This fixes one mistake (right join does not set fields in the right table to missing), and makes the join table to be more precise. Keys are arrays of columns/fields of the table. They need to be present (they must be of same length and type for a join to be performed). The values are what are considered. The description also doesn't give a clear idea that left/right join is really about returning all rows from left/right table, and then joining the right/left table's fields depending on matches.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8467:259,perform,performed,259,https://hail.is,https://github.com/hail-is/hail/pull/8467,1,['perform'],['performed']
Performance,"This fixes two bugs:; 1. The container logs weren't being cached. This made the logs ""disappear"" for previous tasks while the job was still running. FYI @konradjk . 2. My job got stuck in ""running"" even though the job was deleted from the worker because writing the status to GCS timed out and we didn't actually mark the job complete. I'm not sure if we should always try to retry writing the status rather than failing on non-transient errors. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; ra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8054:58,cache,cached,58,https://hail.is,https://github.com/hail-is/hail/pull/8054,1,['cache'],['cached']
Performance,"This flips the pca algorithm, so that we're locally collecting a Krylov space basis on the column side (presumably the smaller side). This leads to a few simplifications, and allows for an optimization in the `compute_loadings=False` case. Once we have a complete TSQR implementation, we can optimize the `compute_loadings=True` case as well. With that, there should be no memory limitation to the number of rows (no local value is O(rows)).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10264:189,optimiz,optimization,189,https://hail.is,https://github.com/hail-is/hail/pull/10264,2,['optimiz'],"['optimization', 'optimize']"
Performance,"This gets ld_prune on the `get_1kg` data down to around 37s. That's still ~1000 times slower than plink.; ```; mt = hl.read_matrix_table('repartitioned.mt'); pruned_tbl = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 1000000, memory_per_core = 1000); pruned_tbl.write(""pruned_tbl.ht"", overwrite=True); ```. Performance Wins:; - local ld prune returns an unkeyed, unsorted dataset, and `ld_prune` collects the relatively small number of variants locally instead of trying to do table joins (I'm doing the broadcast join optimization manually); - avoid `key_by` (and thus sort) of output of MIS, again we do a broadcast join; - two unnecessary writes removed (at the cost of no debugging output); - `maximal_independent_set` no longer keys by, thus avoiding a sort. Minor Changes:. - I don't set env vars anymore, so I need an easy way to pip install hail, so I added a gradle task for that and an associated file that does almost the same thing as deploy.sh. you should complain and make me consolidate these two files. ---; ## Big Data Test. I'm running a test on profile225 right now. ---. resolves #4506",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5078:307,Perform,Performance,307,https://hail.is,https://github.com/hail-is/hail/pull/5078,2,"['Perform', 'optimiz']","['Performance', 'optimization']"
Performance,"This has no discernable effect on latency but it makes the LIR much easier to read because, for example, `IfX IFNE L1 L2 ...` will have `L1` (in printed LIR and JVM bytecode) immediately after it rather than `L2`. The labels also tend to appear in sequential order, which I find helps me navigate the LIR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13779:34,latency,latency,34,https://hail.is,https://github.com/hail-is/hail/pull/13779,1,['latency'],['latency']
Performance,"This installs our fully-pinned requirements deep in the docker image and then installs the hail wheel without dependencies on top. This will be a lot more consistent (and docker cache friendly) than the current approach, which will install all dependencies with the wheel and possibly upgrade some of them. Also did the same to the hail-pip-installed images used for testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12433:178,cache,cache,178,https://hail.is,https://github.com/hail-is/hail/pull/12433,1,['cache'],['cache']
Performance,"This introduces a new version of the batch worker instance: one without `docker`. Instead we bring in `podman` to cover the functions of pulling images, extracting expanding filesystems from those images, and running the worker container. `podman` by default uses `crun` as its low-level runtime so we can get rid of the independent `crun` installation in the worker image. `podman` is daemonless and can be run rootless. For the most part you can't tell the difference, except this makes `podman` easy to run under multiple users with different caches per user. So if you ssh into a worker, be sure to `sudo` before any `podman` (or `crun`) command or else you might think nothing is running when in fact the worker is running under root's podman configuration. The podman/crun state directories are now shared between the host and the worker so `sudo crun list` on the worker should reveal the running job containers without having to exec into the worker first. For the most part, `podman` is a drop-in replacement for `docker`, but there are a handful of inconsistencies that comprise most of this PR. One notable change is that we no longer persist any GCR credentials in the worker VM image so we authenticate again on start up. cc: @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10693:546,cache,caches,546,https://hail.is,https://github.com/hail-is/hail/pull/10693,1,['cache'],['caches']
Performance,"This is a minor architectural change (cc'ing @cseed @tpoterba) that I hope will improve maintainability of `batch`. It foreshadows the DAG functionality. There may be shared data structures between the server and the client. At the very least, the client sends structured data to the server (e.g. a pod spec and metadata about the job). Often, the server parses this data into an object or series of objects which contain methods for performing the server's job (e.g. `batch/server/job.py`). I think this architecture is more or less a different way of defining the API (see `batch/api.py`). I think defining the API via data objects is appealing because; - it centralizes serialization and deserialization for each data structure in one class,; - it enables sharing (via object composition) of that basic data structure between potentially complex client and server objects that implement algorithms on that data structure (I want to do this with the forthcoming DAG stuff), and; - the client has objects representing its ideas (i.e. ""a job"") and those objects can have `__str__`'s and `__repr__`'s facilitating debugging of the client. Moreover, this change pushes the use of k8s' swagger models everywhere possible. This means it's harder for us to make code mistakes because pylint will notice when we, for example, misspell a parameter to a k8s model.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4804:434,perform,performing,434,https://hail.is,https://github.com/hail-is/hail/pull/4804,1,['perform'],['performing']
Performance,"This is a mitigation and does not solve the issue where we have existing duplicated billing project names. I'm not entirely sure adding BINARY in front of the search value is correct, but I was going off of this post. Apparently, if you use `BINARY name = %s` instead of `name = BINARY %s`, then you pay a performance hit because you cannot use the index any longer. https://stackoverflow.com/questions/5629111/how-can-i-make-sql-case-sensitive-string-comparison-on-mysql",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12275:306,perform,performance,306,https://hail.is,https://github.com/hail-is/hail/pull/12275,1,['perform'],['performance']
Performance,"This is a multi-stage overhaul of our Kubernetes load balancers / service discovery. This involves moving off of NGINX onto Envoy, but more importantly involves better control of what namespaces and services are active in our cluster at a given point in time. TL;DR Switching from NGINX to Envoy with CI acting as the ""control plane"" for our internal networking allows us to more easily dynamically configure our Kubernetes networking and achieve proper connection pooling/load-balancing over TLS, which translates to less resource consumption and lower request latencies. ## Motivation; This is primarily a performance-motivated change, and one largely based on our (ab)use of NGINX in order to work with our dynamically-generated Kubernetes test namespaces. Currently, we configure NGINX by creating server blocks that dynamically resolve and dispatch requests based on matching regular expressions on the host and path headers. This is in large part due that at gateway deploy time we do not statically know all of the namespaces and namespace-service combinations that will exist in the cluster in the future. This is true for `default`, but not test namespaces, and NGINX will refuse to start with statically-configured clusters that it cannot reach. Making the server blocks make the routing decisions dynamically circumvents this limitation. However, this prevents usage of NGINX [upstream](http://nginx.org/en/docs/http/ngx_http_upstream_module.html) blocks that provide connection pooling, at least in the community edition, and as a result the gateways will create and terminate a TCP connection per http request. This likely causes minor delays on the front-end through gateway, but this hampers performance greatly in job scheduling. The batch driver is forced to establish a new TCP connection and do an SSL handshake with the internal-gateway multiple times per job, which is expensive and slow. We currently have to dedicate a 2-core NGINX sidecar for the batch-driver just to terminate",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:49,load,load,49,https://hail.is,https://github.com/hail-is/hail/pull/12095,3,"['load', 'perform']","['load', 'load-balancing', 'performance-motivated']"
Performance,"This is a name to IP address and port service. GKE exposes pod IPs onto our VDC; network. As such, regular Google Cloud VMs can access pods by IP. GKE cannot; expose our services as IPs on our VDC because the way services load balance; traffic is more complex than DNS can handle. We acknowledge and accept the; limitations of client-side load balancing. In particular, if there are not many; clients and clients re-use address-port-pairs traffic will likely be; unbalanced. This is not a problem for the planned Shuffle service because the; clients are intended to be numerous (consider all the workers in a Query or; Batch pipeline). The big change is that deploy config now has an `addresses` function which will; return a list of address-port pairs. Deploy config also now has `address` which; randomly chooses one of the address-port pairs. I have included a simple test. Please review both code and overall design, considering how it fits in the wider system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9129:222,load,load,222,https://hail.is,https://github.com/hail-is/hail/pull/9129,2,['load'],['load']
Performance,"This is a performance improvement (and also returns explosion after the right number of iterations which is more logical). It is only necessary to check the first element because I'm confident NaN appears nowhere or everywhere in deltaB (and certainly if it appears somewhere in one iteration, it spreads to everywhere in the next).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5050:10,perform,performance,10,https://hail.is,https://github.com/hail-is/hail/pull/5050,1,['perform'],['performance']
Performance,"This is a simple refactoring of `lir.Emit` to directly use the core visitor based interface of ASM, rather than the higher level `tree` interface. This should have a small performance benefit, as we aren't building the in-memory tree representation only to immediately walk it with a visitor. But I also find this version of `Emit` slightly cleaner. For reference, you can find the documentation for ASM 5.1 [here](https://javadoc.io/doc/org.ow2.asm/asm/5.1/index.html).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9511:172,perform,performance,172,https://hail.is,https://github.com/hail-is/hail/pull/9511,1,['perform'],['performance']
Performance,"This is a temporary fix for the sporadic copy failures. The problem is that this code cancels a task managed by the online bounded pool, and the pool treats that cancellation as an exception that it propagates up. I need to think through the details of the bounded gather with respect to cancellation, and that's going to take a few days. We could put this back when that's done, but honestly, it doesn't seem like an important optimization (given how rarely this failure comes up), so I'll probably just leave it out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10260:428,optimiz,optimization,428,https://hail.is,https://github.com/hail-is/hail/pull/10260,1,['optimiz'],['optimization']
Performance,"This is a total overhaul of our docker images. Though very verbose, I tried to stick to these main tenets:. - Any docker image has exactly 1 layer in it (all the way down to ubuntu) that installs pip dependencies. This primarily aims to protect the cache for this particularly large layer and also avoids a later layer silently upgrading the version of a dependency installed in an earlier layer. This pairs nicely with the following goal; - We only ever use 1 version of a dependency across the monorepo. Liberal use of pip's [constraint files](https://pip.pypa.io/en/stable/user_guide/#constraints-files) to ensure that the dependencies for a service must be compatible with dependencies from hail. The `install-dev-dependencies` target which install all our pinned requirements files would tell you if there's any incompatible versions of transitive dependencies across the repo; - The image graph is shallow and images don't contain more than they need. In order to have a single layer with requirements and hail code on top, I moved the service images to just be based on hail-ubuntu. This shortens the critical path and therefore reduces total image building time by reducing the number of times our image data needs to be downloaded and re-uploaded to the registry. I also removed a lot of unnecessary cruft like gcloud in places it wasn't used anymore, some unused/unnecessary pip requirements, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578:249,cache,cache,249,https://hail.is,https://github.com/hail-is/hail/pull/12578,1,['cache'],['cache']
Performance,"This is an initial implementation of the Scala Region as a reference to a C++ off-heap object. The C++ Region allocates ""small"" blocks out of 64KB chunks, and ""large"" blocks using; malloc() directly. There's a clear_but_keep_mem() which reuses all the 64KB chunks,; and the largest few individual allocations. The usefulness of this strategy is TBD. Currently all allocations are done with a JNI call to the C++, but fields of the object are; directly accessible so it's theoretically possible to try to write optimized Scala code; for the case of a small allocation which can fit in the current chunk. The other changes are mostly consequences of using absolute addresses rather than; offset-in-contiguous-buffer, and the change in the semantics of appendFoo() when a; Region's memory is in non-contiguous chunks - things which need to be located together,; such as the length of a string and its contents, now have to be within memory from a; single allocate() call.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3718:510,optimiz,optimized,510,https://hail.is,https://github.com/hail-is/hail/pull/3718,1,['optimiz'],['optimized']
Performance,This is causing unbearable performance slowdowns when big literals are added. Randomly assigned @chrisvittal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4907:27,perform,performance,27,https://hail.is,https://github.com/hail-is/hail/issues/4907,1,['perform'],['performance']
Performance,"This is currently just dead code, although it could conceivably be useful in the future; I want to remove this for now as it's pretty simple to add back in at a later date (follows the pattern of Serialize/Deserialize Aggs pretty much exactly) and makes for less code to keep refactoring as we optimize the aggregator stuff.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6813:294,optimiz,optimize,294,https://hail.is,https://github.com/hail-is/hail/pull/6813,1,['optimiz'],['optimize']
Performance,"This is half code cleanup and half guardrail. The key assertion here is: if the request authenticates using a cookie and is attempting a state-changing HTTP method, it should pass a CSRF check. I tested this with the following:; 1. dev deployed and loaded the billing projects page; 2. Deleted the hidden csrf input from one of the forms; 3. Submitted the form; 4. Got a 401",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13604:249,load,loaded,249,https://hail.is,https://github.com/hail-is/hail/pull/13604,1,['load'],['loaded']
Performance,"This is in implementation of `linear_regression_rows` that does not rely on any `MatrixToTableApply` nodes. Once `TableKeyBy` is lowered, this should be executable on the service. There are lingering issues:. 1. `TableGroupWithinPartitions` is likely not the right abstraction. It forgets about keying, which forces me to rekey and scan the table even though it's already in order. 2. I don't support chained linear regression (the situation where `y` is a list of lists of phenotypes). I just throw an error there for now. . 3. It's not as fast as the current `linear_regression_rows` (addressing problem 1 should help with this). 4. I don't yet support the `pass_through` field. I want to PR this now because I would like to get the benchmark in so I can continue to measure how this performs in comparison to the current version of `linear_regression_rows`. The tests of this method also serve as useful integration tests for lots of NDArray functionality. Additionally, it'll make it easier to make a smaller PR in the future that adds the new `TableIR` that will hopefully be more suitable than `TableGroupWithinPartitions`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8757:786,perform,performs,786,https://hail.is,https://github.com/hail-is/hail/pull/8757,1,['perform'],['performs']
Performance,This is still happening. I think particularly under heavy batch load it might take a while for this to propagate. https://ci.azure.hail.is/batches/34523/jobs/102,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11480:64,load,load,64,https://hail.is,https://github.com/hail-is/hail/pull/11480,1,['load'],['load']
Performance,This is sufficiently large to permit the transmission of the Hail JAR; which is about 38 MB. I will use this to test and eventually normally; use the Gradle build cache server.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7801:163,cache,cache,163,https://hail.is,https://github.com/hail-is/hail/pull/7801,1,['cache'],['cache']
Performance,"This is the beginning of a series of changes to support export of VDS to VCF 4.5, the version of VCF that contains the standardized form of our work that culminated in SVCR/VDS. Reference blocks were standardized with a LEN rather than an END. So, now, by default, add LEN to all VDS reads and drop END in favor of LEN on all VDS writes. Our optimizer will be able to take care of pruning away the dead field in pipelines that don't use it. We make sure that all VDS creation (other than the combiner), such as read_vds and from_merged_representation, contains both LEN and END preserving user code that depends on the presence of the END field. Furthermore, this change contains necessary combiner updates to prefer LEN over END, and to use LEN in the combiner itself.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14675:342,optimiz,optimizer,342,https://hail.is,https://github.com/hail-is/hail/pull/14675,1,['optimiz'],['optimizer']
Performance,"This is the current state of the C++ support. If you look at the tests in src/test/is/hail/nativecode/NativeCodeSuite.scala that should give a; quick overview of how it works, viz. 1. Generate C++ source code as a Scala String, then create a NativeModule which handles; the grunt work of getting it compiled, linked, and loaded, and allows you to look up functions; by name, and get a callable Scala object corresponding to the C++ function. 2. The NativeModule also allows the binary of the DLL to be passed around and instantiated; on other cluster nodes (but note that those nodes will need to have the correct versions of; the C++ runtime shared libraries in the right directories to allow symbols in the DLL to be; correctly resolved). This is not tested yet. 3. I have been using llvm-6.0.0 on Mac, and llvm-5.0 on linux. It makes a half-hearted attempt; to use whatever other compiler you have, but that may not work. We probably need to figure; out a standardized and automated way to get the right tools installed in the right place (and; get the right shared libraries on worker nodes). 4. Data which needs to be accessible to both Scala and C++ is held in C++ objects inheriting; from NativeObj, with lifetimes managed by std::shared_ptr, i.e. reference-counted. There's; some dirty under-the-hood plumbing to allow a shared_ptr to be smuggled into a Scala; object derived from NativeBase. These Scala-side object references must be managed; carefully using copyAssign/moveAssign/close in order to maintain the off-heap ref-counts. 5. There are some gnarly differences between Linux and MacOSX in the linker and dynamic; loading. I think I'm converging on the right compile/link options for each, but in getting; Linux to work it's possible that Mac is temporarily broken ... Not really expecting that we'll merge this right away, but I wanted to put it out there to get the; review process started before it grows any bigger.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3461:321,load,loaded,321,https://hail.is,https://github.com/hail-is/hail/pull/3461,2,['load'],"['loaded', 'loading']"
Performance,"This is the first chunk of the C++ support, giving Scala ref-counted pointers to C++ objects. On its own, this doesn't do anything very useful. But we need this infrastructure to support; off-heap Region. And a later PR will support compilation/loading of Scala-generated C++; functions (which also needs this infrastructure).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3595:245,load,loading,245,https://hail.is,https://github.com/hail-is/hail/pull/3595,1,['load'],['loading']
Performance,"This is the first step to removing batch workers' reliance on the docker daemon and docker in general, in favor of a lower level of abstraction that gives us finer control over resources on the worker like overlays and network namespaces, allowing us to shortcut and pre-configure some of the overhead that goes into running a job. ## What this does differently; Currently, the high-level process for running a job involves communicating with the docker daemon to:; 1. Pull an image for a job; 2. Start a container from that image; 3. Run the container; 4. Delete the container and its associated resources. We offload some of these responsibilities into the worker code and onto [crun](https://github.com/containers/crun), a lightweight low-level runtime with the same API as `runc`, what docker uses to run containers. Once docker has retrieved an image, if we see that the pulled image has a new digest from one we currently have cached on the worker, we extract the image's filesystem into a directory on the worker's disk. We then:. - use `mount` to create an overlay on top of the image that the container will use as its rootfs; - use `xfs_quota` to limit the container's storage in the overlay; - invoke `crun` to run a container with the overlay as its root filesystem and an appropriate network namespace that we set up at worker-start time. Since we control the overlay, we can set the XFS quota before creating the container. So what was separate calls to docker create/start/run/delete is just a single `crun run`. Fewer steps, less back and forth with a single daemon, and pre-configuring the networks gives some sizable performance gains reliable, as well as reliable and consistent performance. ## What this doesn't solve; - Docker is still running the worker container. I don't see any real challenge to this it's just a matter of translating the docker parameters; - Still using docker to pull images and extract filesystems / environment variables from them. I don't have a substitu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10376:933,cache,cached,933,https://hail.is,https://github.com/hail-is/hail/pull/10376,1,['cache'],['cached']
Performance,"This is the plan for the new Hail CI (tentatively: Hephaestus aka h8s [but Hodor is also in the running, see CI software name in Zulip for the real big questions of our time]). # Expected Repo Structure; Every repository to be tested has at least two files: `hail-ci-build-image` and `hail-ci-build.sh`. The former contains a docker image in a publicly accessible repository. The latter is a shell script that exits with 0 if this branch passes the tests, otherwise it exists with a non-zero code. The logs of this shell script will be shared publicly via the GH PR Status. This script will be executed in the image referenced by `hail-ci-build-image`. # Dockerfile.pr-builder; I carefully wrote a docker file to cache as much gradle crap as possible. # gitHash in Gradle; I pushed `gitHash`'s definition into the `doLast` blocks of the gradle steps that actually need it. `doLast` is only run when the task is actually requested. This allows me to run `downloadDependencies` without creating a dependency on the entire `.git` directory (which changes with each commit, thus invalidating the cache'd docker image).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4066:713,cache,cache,713,https://hail.is,https://github.com/hail-is/hail/pull/4066,2,['cache'],['cache']
Performance,"This is used in lowering `MatrixAnnotateColsTable`, and will allow; us to optimize away unnecessary keying shuffles that happen in column; annotation table manipulations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5860:74,optimiz,optimize,74,https://hail.is,https://github.com/hail-is/hail/pull/5860,1,['optimiz'],['optimize']
Performance,"This is why copying is so slow:. ```; ==> NOTE: You are uploading one or more large file(s), which would run; significantly faster if you enable parallel composite uploads. This; feature can be enabled by editing the; ""parallel_composite_upload_threshold"" value in your .boto; configuration file. However, note that if you do this large files will; be uploaded as `composite objects; <https://cloud.google.com/storage/docs/composite-objects>`_,which; means that any user who downloads such objects will need to have a; compiled crcmod installed (see ""gsutil help crcmod""). This is because; without a compiled crcmod, computing checksums on composite objects is; so slow that gsutil disables downloads of composite objects. / [1/1 files][ 4.1 GiB/ 4.1 GiB] 100% Done 45.8 MiB/s ETA 00:00:00; Operation completed over 1 objects/4.1 GiB.; ```. We can also set this with -o GSUtil:parallel_composite_upload_threshold on the command line. https://cloud.google.com/storage/docs/gsutil/commands/cp. We currently use `-m` which is parallel per-file:. If you have a large number of files to transfer you might want to use the; gsutil -m option, to perform a parallel (multi-threaded/multi-processing); copy:. gsutil -m cp -r dir gs://my-bucket",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7024:1139,perform,perform,1139,https://hail.is,https://github.com/hail-is/hail/pull/7024,2,"['multi-thread', 'perform']","['multi-threaded', 'perform']"
Performance,"This issue could have been an RFC, but that felt too heavy. I can move this to a formal RFC if desired, but otherwise feedback and/or questions welcome in the discussion here. # Idea; For any key type, create an encoding to variable-length byte arrays, which preserves the key ordering. That way, algorithms and data structures which use key comparisons can be written monomorphically, with `memcmp` as the only comparison function needed. Idea inspired by [Fast and Memory Efficient Multi-Column Sorts in Apache Arrow Rust](https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-2/) blog post. But while they've optimized for vectorized encoding (which we currently can't do), I've preferred simplicity and smaller encodings. # Design; Type encoders can emit three kinds of output to a byte array buffer:; - byte - simply add a byte to the result, first padding an incomplete byte if necessary; - bit - add a bit to the result, possibly leaving an incomplete byte. We must know statically how many bits are used in the byte.; - pad - add `0`s to pad the last incomplete byte. This is safe (prefix-free) because the number of used bits is a (statically known) constant. We use this to ensure the number of used bits is known statically.; 	; Types:; - missingness; - treat as a type constructor `optional<T>`, i.e. base types don't encode missingness. Emits a single bit in the encoding. Can invert this bit to control whether missing values come first or last in the ordering. If missing, nothing is emitted after.; - sort-order; - treat reversing the default ordering as a type constructor `reverse<T>`; - simply inverts the encoding bitwise; - primitive types; - same as in datafusion, encoding has same size as original type; - signed integers - flip the sign bit; - floating point numbers - if sign bit is set, invert all bits, otherwise only flip the sign bit; - arrays; - before each element and after last element, emit continuation bit (0 if no more elements); - pad be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14396:637,optimiz,optimized,637,https://hail.is,https://github.com/hail-is/hail/issues/14396,1,['optimiz'],['optimized']
Performance,This lets me run tests in SBT. SBT sets up a class loader that it; uses to load freshly compiled test clases and execute tests. This; makes the code-compile-test loop less time consuming. I will check it against a cluster to ensure that it does not; introduce new class loader issues when code is shipped.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3707:51,load,loader,51,https://hail.is,https://github.com/hail-is/hail/pull/3707,3,['load'],"['load', 'loader']"
Performance,"This leverages the Indeed LSM tree. It implements this API:; - `start(...)`; - `put(x1, ...)` (keys are extracted from the records themselves); - `get(l, r)` which takes two key records and retrieves the values in `[l, r)`. There's a server (`ShuffleServer.scala`) and a client (`ShuffleClient.scala`). They communicate over TLS-secured TCP/IP sockets on a configurable port. The server has one thread per client socket. The client is currently single-threaded. I had to add a `log4j.properties` because I don't start a HailContext and log4j gets upset when you don't configure it. Files; - `HailLSM.scala` - This wraps the Indeed LSM tree with some shims so that we encoders and decoders use `InputStream` and `OutputStream` instead of these were `Data...` interfaces.; - `HailSSLContext.scala` - This implements creation of an actually secure `SSLContext` from a key store and a trust store. It requires clients to identify themselves with a trusted certificate.; - `ShuffleClient.scala` - Self-explanatory.; - `ShuffleServer.scala` - Three classes: `Handler` corresponds to a client connection. It has its own thread. `Shuffle` owns the `Region` , the LSM tree, and the encoder/decoders. `ShuffleServer` waits for connections and spawns threads. It owns the executor service.; - `ShuffleUtils.scala` - Odds and ends.; - `Wire.scala` - Serializers and deserializers for various things. Includes renames that help me keep everything sensible (e.g. for every X I use, I have ""writeX"" and ""readX"").; - `ShuffleSuite.scala` - One test: write 1,000,000 randomly ordered numbers into the LSM tree and read them all back in order. Takes about 1 minute. Obviously we need to dramatically improve the performance of that (I think this should take not longer than one second).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8361:1694,perform,performance,1694,https://hail.is,https://github.com/hail-is/hail/pull/8361,1,['perform'],['performance']
Performance,"This means we don't need out Emit params to take a region as arg to load, which was super messy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12249:68,load,load,68,https://hail.is,https://github.com/hail-is/hail/pull/12249,1,['load'],['load']
Performance,"This needs tests as well as I think I need to fix indexing (so I don't blow memory on the full genome), but I wanted to share what I've been up to with y'all. Also, caitlin can use this branch to run an analysis if it comes to that. I would also appreciate some feedback on the approach. It would be much more ideal to just make joins of variant tables against BGENs smarter, but I think the infrastructure necessary for that is big. cc: @cseed. List of changes:. - added `_variants_per_file` limits the loaded variants to variants at the given array of indexes (0-indexed, same order as on disk). - ~added `row_fields` which prevents reading and allocation of LID and RSID (also improved python-type-checking for `row_fields` and `entry_fields`)~ Moved to #3779 and #3778. - ~fixed table-table joins to _not_ always coerce (thus computing partition keys of) the right-hand table~ Moved to #3723 . - ~added a check that prevents globals and sample annotations copying when they're not used in the body of a MatrixMapCols~ Moved to #3751. - ~fixed a bug in `IndexBTree` wherein if the number of elements was a multiple of 1024, an unnecessary 1024 elements were added to the end of the index file (which I believe breaks the reading process which expects the number of bytes to correspond to the size of the tree)~ Moved to #3750. - ~added `IndexBTree2` which is just an in-memory list of the variant start positions. This is a fair bit of data. Chromosome 1 has about 250 million bases, so in the worst case this is 250 * 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3727:504,load,loaded,504,https://hail.is,https://github.com/hail-is/hail/pull/3727,1,['load'],['loaded']
Performance,"This node encapsulates a pattern we use in several places, and lets us generate IR that we can optimize more effectively.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5986:95,optimiz,optimize,95,https://hail.is,https://github.com/hail-is/hail/pull/5986,1,['optimiz'],['optimize']
Performance,"This option controls parsing and loading of the `rsid` and `varid`; BGEN row fields. When (in a future PR) the reader does not decompress; the genotypes (nor decode them), these row field imports become; a substantial portion of the time necessary to load just the row; keys. broken out from: https://github.com/hail-is/hail/pull/3727",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3779:33,load,loading,33,https://hail.is,https://github.com/hail-is/hail/pull/3779,2,['load'],"['load', 'loading']"
Performance,This performs the operations described in the `NOTE` at the end of `vcf_combiner.py`. The only thing that I feel may be confusing is that we recompute the `DP` info fields as the sum of the `DP` entry fields. We do this as `densify` (currently a placeholder) may populate a previously missing entry.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5280:5,perform,performs,5,https://hail.is,https://github.com/hail-is/hail/pull/5280,1,['perform'],['performs']
Performance,This permits shuffle-free load of files where; multiallelics are split across partitions.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/787:26,load,load,26,https://hail.is,https://github.com/hail-is/hail/pull/787,2,['load'],['load']
Performance,"This prevents decorators like `@rest_authenticated_users_only` from making an additional request to `auth` (which includes a database query for the `userinfo` endpoint) on every API request. I also realized that auth wasn't getting scraped by prometheus so I added the `grafanak8sapp` label to fix that. auth's `userinfo` endpoint should probably also have a cache, but it felt worth it to also implement it this way to avoid constant communication with the auth service from batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12122:359,cache,cache,359,https://hail.is,https://github.com/hail-is/hail/pull/12122,1,['cache'],['cache']
Performance,"This probably needs a little cleanup. What's this for? Well, in another branch I have a bunch of IR rewrite optimizations. Those rewrite rules (1) want to test types (e.g. eliminate a cast of a type to itself), and they (2) also want to create new IRs which therefore need well-formed types. Calculating all the intermediate types explicitly (or calling Infer) everywhere both seem like non-starters. Therefore, I changed the IR nodes to compute their own types. I repurposed Infer, but it is no longer recursive. This meant that In, InAgg and Ref needed to carry their types, becuase, in the old, Infer-based way, they were dependent on the environment to type themselves, but that's no longer possible. I also repurposed Infer as a recursive type checker. This created two subtle problems: (1) the IR code uses rvRowType everywhere in stead of rowType (so it can reuse pointers to the full row) and (2) toIR needs to set the Ref type from the symbol table, but the symbol table strips out all missing bits, so the types on Ref terms disagreed with the actual values flowing around. I resolved this in two ways: (1) va now refers to the full rvRowType in all eval contexts, everywhere. (This is closer to the existing IR behavior.) (2) the symbol table no longer strips missingness, but it is stripped by Ref when the symbol is referenced. Ref also records the unstripped type which is used by toIR. The sooner we can kill AST, the better.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3332:108,optimiz,optimizations,108,https://hail.is,https://github.com/hail-is/hail/pull/3332,1,['optimiz'],['optimizations']
Performance,"This provides necessary performance improvements when creating; an array of empty structs, which should have o(1) cost, not; O(n) cost.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3773:24,perform,performance,24,https://hail.is,https://github.com/hail-is/hail/pull/3773,1,['perform'],['performance']
Performance,"This reduced the number of logs in ""worker.log"" by over 80% from 515K lines to 77K for an hour with some load. https://console.cloud.google.com/logs/query;query=resource.type%3D%22gce_instance%22%0AlogName:%22worker%22%0ANOT%20labels.namespace%3D%22default%22%0ANOT%20%22crun%20process%22%20AND%20NOT%20%22crun%20run%20process%22%20AND%20NOT%20%22marking%20complete%22%20AND%20NOT%20%22initializing%22%20AND%20NOT%20%22running%20input%22%20AND%20NOT%20%22input:%22%20AND%20NOT%20%22running%20main%22%20AND%20NOT%20%22main:%22%20AND%20NOT%20%22running%20output%22%20AND%20NOT%20%22output:%22%20AND%20NOT%20%22cleaning%20up%22%20AND%20NOT%20%22downloading%20JAR%22%20AND%20NOT%20%22running%20jvm%20process%22%20AND%20NOT%20%22uploading%20log%22%20and%20NOT%20%22Obtained%20writer%22%20AND%20NOT%20%22finished%20normally%22%20AND%20NOT%20%22was%20cancelled%22%20AND%20NOT%20%22user%20exception%20encountered%22%20AND%20NOT%20%22:%20execute%22%20AND%20NOT%20%22JVM-%22%20AND%20NOT%20%22healthcheck%22%20AND%20NOT%20%22%2Fapi%2Fv1alpha%2Fbatches%2Fjobs%2Fcreate%22;timeRange=2022-06-06T17:00:59.759Z%2F2022-06-06T18:00:59.759Z;cursorTimestamp=2022-06-06T17:53:15.865608694Z?project=hail-vdc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11898:105,load,load,105,https://hail.is,https://github.com/hail-is/hail/pull/11898,1,['load'],['load']
Performance,This removes the name conflict with the `key` attribute of a table. I also changed the test `count` to `_force_count` to be sure they aren't ever optimized.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4257:146,optimiz,optimized,146,https://hail.is,https://github.com/hail-is/hail/pull/4257,1,['optimiz'],['optimized']
Performance,"This represents a small redesign of ValueWriter as well. Add deserializers, subclasses of ValueReader and have ReadValue use them. A ValueReader deserializes a single hail value from an input stream. This change also alters the semantics of ValueWriters. Both readers and writers no longer manage their own I/O resources. It is the responsibility of 'callers' to do so instead. However programmers must be careful as we need to create input/output buffers to perform native serialization/deserialization. For InputBuffers in particular, the underlying stream may be left in an unusable state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12948:459,perform,perform,459,https://hail.is,https://github.com/hail-is/hail/pull/12948,1,['perform'],['perform']
Performance,"This should always force the temporary table to never be materialized for `billing_project_users`. > STRAIGHT_JOIN is similar to JOIN, except that the left table is always read before the right table. This can be used for those (few) cases for which the join optimizer processes the tables in a suboptimal order.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12057:259,optimiz,optimizer,259,https://hail.is,https://github.com/hail-is/hail/pull/12057,1,['optimiz'],['optimizer']
Performance,This should be a big performance boost.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4769:21,perform,performance,21,https://hail.is,https://github.com/hail-is/hail/pull/4769,1,['perform'],['performance']
Performance,This should lighten the database load a bit by avoiding a couple joins and a CTE.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12651:33,load,load,33,https://hail.is,https://github.com/hail-is/hail/pull/12651,1,['load'],['load']
Performance,"This should make computing the loadings better, since it uses a checkpointed variants table instead of accidentally recomputing the incoming MatrixTable. Also made a change to avoid accidentally clobbering a field name if someone had a field named `idx`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10201:31,load,loadings,31,https://hail.is,https://github.com/hail-is/hail/pull/10201,1,['load'],['loadings']
Performance,This should play well with [the optimizer](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/Relational.scala#L189-L192),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2285:32,optimiz,optimizer,32,https://hail.is,https://github.com/hail-is/hail/pull/2285,1,['optimiz'],['optimizer']
Performance,"This should speed up the scheduler a bit. I tested it with some log statements to ensure there were cache hits. I tested the cleanup loop works before I added a try, except wrapper.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7825:100,cache,cache,100,https://hail.is,https://github.com/hail-is/hail/pull/7825,1,['cache'],['cache']
Performance,This simplifies the code and means that region bookeeping for the prune; queue is unnecessary.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12076:73,queue,queue,73,https://hail.is,https://github.com/hail-is/hail/pull/12076,1,['queue'],['queue']
Performance,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6547:805,latency,latency,805,https://hail.is,https://github.com/hail-is/hail/issues/6547,1,['latency'],['latency']
Performance,"This was a good one :). I havent measured the performance difference, but clearly the bug defeated the purpose of using a union-find data structure in the first place, which was to reduce the complexity of `unify` from quadratic to linear. While I was here, I made a separate simplification. Now that the sets are being unioned as intended, each set contains exactly one block that doesnt start with a `GotoX`, and that block is the final target of all blocks in the set. That observation allows a simplification when computing the `rootFinalTarget` map.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9475:47,perform,performance,47,https://hail.is,https://github.com/hail-is/hail/pull/9475,1,['perform'],['performance']
Performance,"This was identified as a cost center during my compiler investigations. The effect is not measurable on the standard benchmark [1]. More investigation is needed. As we improve the performance of the expression language, I suspect this will constitute a more significant fraction of execution time. [1]:; ```; filtergenotypes -c ' g.dp > 400 ||; (g.isHomRef && (g.ad[0] / g.dp < 0.9 || g.gq < 20)) ||; (g.isHomVar && (g.ad[1] / g.dp < 0.9 || g.pl[0] < 20)) ||; (g.isHet && ( (g.ad[0] + g.ad[1]) / g.dp < 0.9 || g.ad[1] / g.dp < 0.20 || g.pl[0] < 20 ))' --keep; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1292:180,perform,performance,180,https://hail.is,https://github.com/hail-is/hail/pull/1292,1,['perform'],['performance']
Performance,This was preventing use of cache when building the notebook leader image. The `$*` variable is for use with [Pattern Rules](https://www.gnu.org/software/make/manual/html_node/Automatic-Variables.html#Automatic-Variables). Still note sure why the build is failing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5226:27,cache,cache,27,https://hail.is,https://github.com/hail-is/hail/pull/5226,1,['cache'],['cache']
Performance,"This was unused, but was a performance nightmare -- the reconstruction of a single type triggered a full reallocation of the nested structure. ![image](https://user-images.githubusercontent.com/10562794/127706520-a3202b3f-b478-407d-b64f-496a4f08f68b.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10734:27,perform,performance,27,https://hail.is,https://github.com/hail-is/hail/pull/10734,1,['perform'],['performance']
Performance,This will hopefully make it harder to accidentally build in debug mode when doing performance testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14306:82,perform,performance,82,https://hail.is,https://github.com/hail-is/hail/pull/14306,1,['perform'],['performance']
Performance,"This will make `TableHead` ~and `TableTail`~ lowered implementation performant enough to actually use. EDIT: I was doing something wrong with `TableTail`, decided to just leave it for a follow up PR instead of having this sit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10719:68,perform,performant,68,https://hail.is,https://github.com/hail-is/hail/pull/10719,1,['perform'],['performant']
Performance,"Timing:. ```; mysql> SELECT job_id, spec, cores_mcpu FROM jobs WHERE batch_id = 10 AND state = 'Ready' AND always_run = 0 AND `cancelled` = 0 LIMIT 1;; +---------+------------------------------------------------------------------+------------+; | job_id | spec | cores_mcpu |; +---------+------------------------------------------------------------------+------------+; | 1606431 | [[[""batch-pods"", ""konradk-gsa-key"", ""/gsa-key"", 1]], null, 1, 1] | 1000 |; +---------+------------------------------------------------------------------+------------+; 1 row in set (1.22 sec). mysql> SELECT job_id, spec, cores_mcpu FROM jobs FORCE INDEX(jobs_batch_id_state_always_run_cancelled) WHERE batch_id = 10 AND state = 'Ready' AND always_run = 0 AND `cancelled` = 0 LIMIT 1;; +---------+------------------------------------------------------------------+------------+; | job_id | spec | cores_mcpu |; +---------+------------------------------------------------------------------+------------+; | 1606431 | [[[""batch-pods"", ""konradk-gsa-key"", ""/gsa-key"", 1]], null, 1, 1] | 1000 |; +---------+------------------------------------------------------------------+------------+; 1 row in set (0.00 sec); ```. Explain verifies that the original query was performing a full batch scan.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8093:1240,perform,performing,1240,https://hail.is,https://github.com/hail-is/hail/pull/8093,1,['perform'],['performing']
Performance,"Tiny tiny tiny point about the API docs: query_variants says, ""Performs aggregation queries over variants and variant annotations, and returns python object(s) and type(s)."" But should end after ""object(s)"". It would've been faster for me to clone the repo, make this changes and submit a pull request. . Sorry.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1522:63,Perform,Performs,63,https://hail.is,https://github.com/hail-is/hail/issues/1522,1,['Perform'],['Performs']
Performance,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. Package Info:; Name: hail; Version: 0.2.93; Summary: Scalable library for exploring and analyzing genomic data.; Home-page: https://hail.is; Author: Hail Team; Author-email: hail@broadinstitute.org; License: UNKNOWN; Location: /Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages; Requires: dill, bokeh, scipy, azure-storage-blob, janus, parsimonious, botocore, google-cloud-storage, tabulate, Jinja2, python-json-logger, plotly, avro, azure-identity, PyJWT, orjson, tqdm, aiohttp-session, google-auth, nest-asyncio, uvloop, humanize, hurry.filesize, decorator, requests, Deprecated, aiohttp, asyncinit, numpy, pyspark, sortedcontainers, boto3, pandas. -----------------------------------------------------------------------------. Importing hail via the IPython console in Spyder causes the following error:. Python 3.8.12 (default, Oct 12 2021, 06:23:56) ; IPython 8.2.0 -- An enhanced Interactive Python. In [1]: `import hail`. > [SpyderKernelApp] ERROR | Exception in message handler:; > Traceback (most recent call last):; > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages/spyder_kernels/comms/frontendcomm.py"", line 164, in poll_one; > asyncio.run(handler(out_stream, ident, msg)); > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages/nest_asyncio.py"", line 36, in run; > task = asyncio.ensure_future(main); > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/asyncio/tasks.py"", line 684, in ensure_future; > raise TypeError('An asyncio.Future, a coroutine or an awaitable is '; > TypeError: An asyncio.Future, a coroutine or an awaitable is required; > [SpyderKernelApp] ERROR | Exception in message handler:; > Traceback (most recent call last):; > File ""/Us",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11758:346,Scalab,Scalable,346,https://hail.is,https://github.com/hail-is/hail/issues/11758,1,['Scalab'],['Scalable']
Performance,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; Hello, I am having problems to read vcf file using hail. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz'); py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply. : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1; Any help? Best, Zillur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6747:525,load,load,525,https://hail.is,https://github.com/hail-is/hail/issues/6747,1,['load'],['load']
Performance,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible.; version 0.2.46-6ef64c08b000. Script is trying to merge 22 vcf.gz into a matrix table.; ```python3; import hail as hl; import sys. hl.init(default_reference='GRCh38'); vcf=""/project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz""; mt=""/project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt""; print(""Converting vcf ""+vcf+"" to mt ""+ mt); hl.import_vcf(vcf,force_bgz=True).write(mt); ```; -----------------------------------------------------------------------------; ```; $ submit ./vcf2mt_all.py; Loading modules; /share/pkg.7/gcc/8.3.0/install/lib64:/share/pkg.7/gcc/8.3.0/install/lib:/share/pkg.7/python3/3.7.7/install/lib:/usr/hdp/2.6.5.0-292/hadoop/lib/native/; Export env vars; Submitting Spark job; 20/08/17 23:24:46 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to /restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/hail-20200817-2324-0.2.46-6ef64c08b000.log; Converting vcf /project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz to mt /project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:736,Load,Loading,736,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['Load'],['Loading']
Performance,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2/devel hash eb1e04205793. ### What you did:; I'm trying to take PC loadings generated using Hail in one sample and project another sample into that PC space to generate scores for them. I've been using @danking's tricks developed for my PRS pipeline to speed things along and am now trying to feed the relevant inputs into gnomad Hail's [pc_project()](https://github.com/macarthur-lab/gnomad_hail/blob/master/utils/generic.py#L164) function. I've written out the function in script format for debugging purposes. Full code is below. . ```import hail as hl; import pickle; import time. generate_pcloadings_table = True; pcloadings_table_location = 'gs://ukbb_prs/sibdiff/keytables/ukb-pca-locus-allele-keyed.kt'; generate_contig_row_dict = True; contig_row_dict_location = 'gs://ukbb_prs/sibdiff/keytables/contig_row_dict-UKB'; output_location = 'gs://ukbb_prs/sibdiff/UKB_sibloadings.txt'; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}; bgen_files = 'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}_v3.bgen'. # large block size because we read very little data (due to filtering & ignoring genotypes); hl.init(branching_factor=10, min_block_size=2000). ### set up the pcloadings table; if (generate_pcloadings_table):; pcloadings = hl.import_table('gs://phenotype_31063/ukb31063.gwas.pca_loadings.tsv.gz', impute=True); pcloadings = pcloadings.annotate(locus=hl.parse_locus(hl.str(pcloadings.chr) + "":"" + hl.str(pcloadings.pos)),; alleles=[pcloadings.ref,pcloadings.alt]).key_by('locus','alleles'). pcloadings.write(pcloadings_table_location, overwrite=True). pcloadings = hl.read_table(pcloadings_table_location). ### determine the file locations of the pca ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3953:323,load,loadings,323,https://hail.is,https://github.com/hail-is/hail/issues/3953,1,['load'],['loadings']
Performance,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-ccaf3640241f. ### What you did:. ```; ht = hl.import_table('gs://gnomad/annotations/hail-0.2/ht/genomes/score_rankings/gnomad.sites.RF.newStats24.txt.bgz', types={'chrom': hl.tstr}, impute=True, min_partitions=100).cache(); ht.export('gs://gnomad-tmp/genomes_rf.txt.bgz', parallel=True); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; Py4JError Traceback (most recent call last); <ipython-input-17-671d2e9c22c8> in <module>(); 1 #ht = hl.import_table('gs://gnomad/annotations/hail-0.2/ht/genomes/score_rankings/gnomad.sites.RF.newStats24.txt.bgz', types={'chrom': hl.tstr}, impute=True, min_partitions=100).cache(); ----> 2 ht.export('gs://gnomad-tmp/genomes_rf.txt.bgz', parallel=True). /home/hail/hail.zip/hail/table.py in export(self, output, types_file, header, parallel); 994 """"""; 995 ; --> 996 self._jt.export(output, types_file, header, Env.hail().utils.ExportType.getExportType(parallel)); 997 ; 998 def group_by(self, *exprs, **named_exprs) -> 'GroupedTable':. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 186 import pyspark; 187 try:; --> 188 return f(*args, **kwargs); 189 except py4j.protocol.Py4JJavaError as e:; 190 s = e.java_exception.toString(). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 321 raise Py4JE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4033:474,cache,cache,474,https://hail.is,https://github.com/hail-is/hail/issues/4033,1,['cache'],['cache']
Performance,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-92bbe4b. ### What you did:; I used `join` to combine two VDS. Empty sample schema, different variant schema, no overlapping samples, hardcalls. ### What went wrong (all error messages here, including the full java stack trace):; Got an AssertionError:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 677, mycluster3-w-1.c.ccdg-wgs.internal): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadInt(Region.scala:36); at is.hail.expr.types.TContainer$.loadLength(TContainer.scala:9); at is.hail.expr.types.TContainer.loadLength(TContainer.scala:27); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1702); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1685); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:661); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:655); ```. I can make it work by copying the variant annotation from one VDS to the other before calling `join`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2763:835,load,loadInt,835,https://hail.is,https://github.com/hail-is/hail/issues/2763,3,['load'],"['loadInt', 'loadLength']"
Performance,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. Sorry for the awful formatting!! I'm not familiar with how to properly format for GitHub, and clearly some of my code got picked up as formatting.... . EDIT: Fixed the formatting :). ### Hail version: 0.2 / devel. ### What you did: ; ```; import. hail as hl. def flip_text(base):; """"""; :param StringExpression base: Expression of a single base; :return: StringExpression of flipped base; :rtype: StringExpression; """"""; return hl.cond(base == 'A', 'T',; hl.cond(base == 'T', 'A',; hl.cond(base == 'C', 'G',; hl.cond(base == 'G', 'C', base)))). # load ldpred sumstats file; sumstats = hl.import_table('gs://ukbb_prs/sumstats/UKB_SCZ_LDPred.txt', delimiter='\s+', impute=True). # create locus and alleles columns and key by locus; sumstats = (sumstats.annotate(locus=hl.parse_locus(sumstats.chrom[6:] + "":"" + hl.str(sumstats.pos)),; alleles=[sumstats.nt1,sumstats.nt2]); .key_by('locus'); .order_by('locus')). # repartition sumstats to save time; sumstats = sumstats.repartition(100). # write the sumstats table; sumstats.write('gs://ukbb_prs/sumstats/UKB_SCZ_LDPred.kt', overwrite=True). # read the sumstats table; sumstats = hl.read_table('gs://ukbb_prs/sumstats/UKB_SCZ_LDPred.kt'). # remove leading zeros from contigs; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}. # import variants; variants = hl.methods.import_bgen('gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{22}_v3.bgen',; [],; sample_file='gs://phenotype_31063/ukb31063.imputed_v3.autosomes.sample',; contig_recoding=contigs). # merge sumstats on bgen matrixtable; variants = variants.annotate_rows(ss = sumstats[variants.locus]). # handle strand/allele flips; variants = variants.annotate_rows(beta = hl.case(); .when(((variants.alleles[0] == variants.ss.nt1) &; (variants.al",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508:780,load,load,780,https://hail.is,https://github.com/hail-is/hail/issues/3508,1,['load'],['load']
Performance,Tracks latency and status code metrics on endpoints so we can see those too in the batch grafana dashboard.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10225:7,latency,latency,7,https://hail.is,https://github.com/hail-is/hail/pull/10225,1,['latency'],['latency']
Performance,"Tried running ; `hail read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf`. got the following; ```; hail: info: running: read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds; [Stage 1:======================================================>(255 + 1) / 256]hail: info: running: exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf; [Stage 2:=====> (2117 + 256) / 19042]hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2137 in stage 2.0 failed 4 times, most recent failure: Lost task 2137.3 in stage 2.0 (TID 3028, nid00013.urika.com): java.lang.IllegalArgumentException: Self-suppression not permitted; 	at java.lang.Throwable.addSuppressed(Throwable.java:1043); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:88); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```. Full error and log below:. [error.txt](https://github.com/hail-is/hail/files/652656/error.txt); [hail.log.txt](https://github.com/hail-is/hail/files/652665/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1185:1333,concurren,concurrent,1333,https://hail.is,https://github.com/hail-is/hail/issues/1185,2,['concurren'],['concurrent']
Performance,"Tried to load 1kg public VCF using newest version of hail:; ```; tgp = hl.import_vcf('gs://genomics-public-data/1000-genomes-phase-3/vcf-20150220/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf'); tgp.describe(); tgp.rows().show(); ```; Getting:; ```; hail.utils.java.FatalError: NoSuchElementException: key not found: GT. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 104, pca-w-1.c.daly-ibd.internal, executor 2): is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:9,load,load,9,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['load'],['load']
Performance,Tune write rows split files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5509:0,Tune,Tune,0,https://hail.is,https://github.com/hail-is/hail/pull/5509,1,['Tune'],['Tune']
Performance,Two big changes. Catch any errors and release the semaphore. Restart failed workers in the concurrent worker pool.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6804:91,concurren,concurrent,91,https://hail.is,https://github.com/hail-is/hail/pull/6804,1,['concurren'],['concurrent']
Performance,"Under ideal conditions, I observed this code complete 12 jobs per second in a 1000 job; batch. However, it's usually closer to 6 jobs per second. There's still a deadlock in; `mark_job_complete` which hamstrings performance. I haven't thought carefully about why my metric; (jobs per second) is much lower than our usual metric (MJC per second). I will eventually squash; the `mark_job_complete` deadlock, but I haven't the time currently. The two main issues were the `batches` field `cancelled` and the `instances` field; `free_cores_mcpu`. We frequently locked the entire batch row just to prevent a batch from being; cancelled while other work was done. This effectively serialized all operation on jobs in the same; batch. We had the dual issue with free_cores_mcpu: we lock the entire instance record everywhere to; prevent the instance changing state while we update job information. This serialized updates to; free_cores_mcpu which serialized job operation for any job on the same instance. We also had some unnecessarily lock-heavy code inside `add_attempt`. This block of code is trying to; insert a new attempt record and update the associated instance's free cores. I also fixed some out of date values and bad syntax in `estimated-current.sql`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10985:212,perform,performance,212,https://hail.is,https://github.com/hail-is/hail/pull/10985,1,['perform'],['performance']
Performance,"Unused and incorrect -- if this method were used, it would not optimize; any IRs in the DAG, which may contain relational or value IRs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5834:63,optimiz,optimize,63,https://hail.is,https://github.com/hail-is/hail/pull/5834,1,['optimiz'],['optimize']
Performance,"Updated db.py to require user to specify region as shown below, so that data will be accessed from the requester pays bucket in the region specified by the user, available regions are `'us'` and `'eu'`. . `db = hl.experimental.DB(region='us')`; `mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics')`. Entries in the `annotation_db.json` file were modified to the following format:. ```; ""dataset_name"": { ""description"": ""some description here"",; ""key_properties"": [],; ""url"": ""https://www.someurlhere.com"",; ""versions"": [{""url"": {""eu"": ""gs://hail-datasets-eu/dataset"",; ""us"": ""gs://hail-datasets-us/dataset""},; ""version"": ""one_version""},; {""url"": {""eu"": ""gs://hail-datasets-eu/dataset"",; ""us"": ""gs://hail-datasets-us/dataset""},; ""version"": ""another_version""}]}; ```. The `annotation_db.json` file is now used by the `load_dataset()` function in `datasets.py` as well, any dataset in the JSON file should now be able to be loaded this way. Made changes to the following:; - `DB` class now requires a `region` parameter.; - `Dataset.from_name_and_json()` has had a `custom_config` parameter added that indicates whether or not the user has supplied their own `config` or `url`. `Dataset.from_name_and_json()` now calls `DatasetVersion.get_region()` method to retrieve the dataset from the bucket in the selected region if `custom_config` is `False`. ; - The `DatasetVersion.get_region()` method takes the dataset `name`, a list of `DatasetVersion` objects, and a `region`, and returns a list of the versions that are available for that region. This method calls the instance method `in_region()` to check if the dataset is available in the requested region.; - If `in_region()` determines the desired region is not available for some dataset that otherwise is available in another region, it will raise a warning. If user still tries to call `db.annotate_rows_db()` using a dataset unavailable in their region, then it will get caught by the `_check_availability` instance method in the `DB` class and rai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9496:920,load,loaded,920,https://hail.is,https://github.com/hail-is/hail/pull/9496,1,['load'],['loaded']
Performance,"Updates the requirements on [astroid](https://github.com/PyCQA/astroid) to permit the latest version.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/PyCQA/astroid/blob/main/ChangeLog"">astroid's changelog</a>.</em></p>; <blockquote>; <h1>What's New in astroid 2.10.0?</h1>; <p>Release date: 2022-02-27</p>; <ul>; <li>; <p>Fixed inference of <code>self</code> in binary operations in which <code>self</code>; is part of a list or tuple.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/4826"">PyCQA/pylint#4826</a></p>; </li>; <li>; <p>Fixed builtin inference on <code>property</code> calls not calling the <code>postinit</code> of the new node, which; resulted in instance arguments missing on these nodes.</p>; </li>; <li>; <p>Fixed a crash on <code>Super.getattr</code> when the attribute was previously uninferable due to a cache; limit size. This limit can be hit when the inheritance pattern of a class (and therefore of the; <code>__init__</code> attribute) is very large.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5679"">PyCQA/pylint#5679</a></p>; </li>; <li>; <p>Inlcude names of keyword-only arguments in <code>astroid.scoped_nodes.Lambda.argnames</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5771"">PyCQA/pylint#5771</a></p>; </li>; <li>; <p>Fixed a crash inferring on a <code>NewType</code> named with an f-string.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5770"">PyCQA/pylint#5770</a></p>; </li>; <li>; <p>Add support for <a href=""https://github.com/python-attrs/attrs/releases/tag/21.3.0"">attrs v21.3.0</a> which; added a new <code>attrs</code> module alongside the existing <code>attr</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1330"">#1330</a></p>; </li>; <li>; <p>Use the <code>end_lineno</code> attribute for the <code>NodeNG.tolineno</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11463:902,cache,cache,902,https://hail.is,https://github.com/hail-is/hail/pull/11463,2,['cache'],['cache']
Performance,"Updates the requirements on [janus](https://github.com/aio-libs/janus) to permit the latest version.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/janus/releases"">janus's releases</a>.</em></p>; <blockquote>; <h2>janus 1.0.0 release</h2>; <ul>; <li>Dropped Python 3.6 support</li>; <li>Janus is marked as stable, no API changes was made for years</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/janus/blob/master/CHANGES.rst"">janus's changelog</a>.</em></p>; <blockquote>; <h2>1.0.0 (2021-12-17)</h2>; <ul>; <li>Drop Python 3.6 support</li>; </ul>; <h2>0.7.0 (2021-11-24)</h2>; <ul>; <li>Add SyncQueue and AsyncQueue Protocols to provide type hints for sync and async queues <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/374"">#374</a></li>; </ul>; <h2>0.6.2 (2021-10-24)</h2>; <ul>; <li>Fix Python 3.10 compatibility <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/358"">#358</a></li>; </ul>; <h2>0.6.1 (2020-10-26)</h2>; <ul>; <li>; <p>Raise RuntimeError on queue.join() after queue closing. <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/295"">#295</a></p>; </li>; <li>; <p>Replace <code>timeout</code> type from <code>Optional[int]</code> to <code>Optional[float]</code> <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/267"">#267</a></p>; </li>; </ul>; <h2>0.6.0 (2020-10-10)</h2>; <ul>; <li>; <p>Drop Python 3.5, the minimal supported version is Python 3.6</p>; </li>; <li>; <p>Support Python 3.9</p>; </li>; <li>; <p>Refomat with <code>black</code></p>; </li>; </ul>; <h2>0.5.0 (2020-04-23)</h2>; <ul>; <li>Remove explicit loop arguments and forbid creating queues outside event loops <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/246"">#246</a></li>; </ul>; <h2>0.4.0 (2018-07-28)</h2>; <ul>; <li>; <p>Add <code>py.typed</code> macro <a ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11466:815,queue,queues,815,https://hail.is,https://github.com/hail-is/hail/pull/11466,1,['queue'],['queues']
Performance,"Updates the requirements on [scipy](https://github.com/scipy/scipy) to permit the latest version.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/scipy/scipy/releases"">scipy's releases</a>.</em></p>; <blockquote>; <h1>SciPy 1.8.0 Release Notes</h1>; <p>SciPy <code>1.8.0</code> is the culmination of <code>6</code> months of hard work. It contains; many new features, numerous bug-fixes, improved test coverage and better; documentation. There have been a number of deprecations and API changes; in this release, which are documented below. All users are encouraged to; upgrade to this release, as there are a large number of bug-fixes and; optimizations. Before upgrading, we recommend that users check that; their own code does not use deprecated SciPy functionality (to do so,; run your code with <code>python -Wd</code> and check for <code>DeprecationWarning</code> s).; Our development attention will now shift to bug-fix releases on the; 1.8.x branch, and on adding new features on the master branch.</p>; <p>This release requires Python <code>3.8+</code> and <code>NumPy 1.17.3</code> or greater.</p>; <p>For running on PyPy, PyPy3 <code>6.0+</code> is required.</p>; <h1>Highlights of this release</h1>; <ul>; <li>A sparse array API has been added for early testing and feedback; this; work is ongoing, and users should expect minor API refinements over; the next few releases.</li>; <li>The sparse SVD library PROPACK is now vendored with SciPy, and an interface; is exposed via <code>scipy.sparse.svds</code> with <code>solver='PROPACK'</code>. It is currently; default-off due to potential issues on Windows that we aim to; resolve in the next release, but can be optionally enabled at runtime for; friendly testing with an environment variable setting of <code>USE_PROPACK=1</code>.</li>; <li>A new <code>scipy.stats.sampling</code> submodule that leverages the <code>UNU.RAN</code> C; library to sample from arbitrary univariate non-uniform c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11538:687,optimiz,optimizations,687,https://hail.is,https://github.com/hail-is/hail/pull/11538,1,['optimiz'],['optimizations']
Performance,Use NodeJS to compile docs for faster load times,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/800:38,load,load,38,https://hail.is,https://github.com/hail-is/hail/pull/800,1,['load'],['load']
Performance,"Use Sanic + ujson instead of Flask + flask.json, cache more, and fix missing favicon in HTML templates. Sanic should be 3+x faster than Flask, and ujson ~2-3x faster than the standard json lib. I also optimize away the unnecessary re-generation of user_data (via get_users()) and json equivalent for /json. This is deployed currently on scorecard.hail.is, and improves performance by ~20%, even for 1 single connection, and for that simple workload (response time from ~50ms to ~40ms). Besides performance ( https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/ ) Sanic also builds in a production-oriented web server, so need for WSGI or aWSGI, Gunicorn, etc. Can easily run multiple workers if desired. ```python; app.run(host='0.0.0.0', port=5000, workers=4); ```. This serves as a demonstration or migration to faster web frameworks, and in particular to uvloop-based asyncio implementations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242:49,cache,cache,49,https://hail.is,https://github.com/hail-is/hail/pull/5242,5,"['cache', 'optimiz', 'perform', 'response time']","['cache', 'optimize', 'performance', 'response time']"
Performance,Use class loader from asm4s instead of System,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3707:10,load,loader,10,https://hail.is,https://github.com/hail-is/hail/pull/3707,1,['load'],['loader']
Performance,Use set_message in CI and Batch to give users feedback after performing POST operations. I added set_message while working on notebook. Any message set will show up at the top of the next page loaded styled according to the message type.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7160:61,perform,performing,61,https://hail.is,https://github.com/hail-is/hail/pull/7160,2,"['load', 'perform']","['loaded', 'performing']"
Performance,Use this to fully optimize `hl.read_matrix_table().count_cols()`. Stacked on #5041,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5043:18,optimiz,optimize,18,https://hail.is,https://github.com/hail-is/hail/pull/5043,1,['optimiz'],['optimize']
Performance,Used streamed buffered aggregate to optimize in lowering ofTableKeyByAndAggregate,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11395:36,optimiz,optimize,36,https://hail.is,https://github.com/hail-is/hail/pull/11395,1,['optimiz'],['optimize']
Performance,"Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3002,cache,cached,3002,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"Using index expressions to represent abstract transformations on tensors. E.g. broadcasts from scalars and vectors to matrices would be represented by. ```; O(i, j) <- A(); O(i, j) <- A(i); O(i, j) <- A(j); ```; where A is the input tensor and O is the output tensor. Similarly, transpose and identity look like. ```; O(i, j) <- A(j, i); O(i, j) <- A(i, j); ```. Since the `O(i,j)` is the same in all of these, we only need to look at the ""in"" index expressions for broadcasts to distinguish what operation needs to be performed. There's also a Simplify rule to cut out identity broadcasts. ## Workaround; This method of index expressions treats scalars and vectors as 0-dimensional and 1-dimensional tensors, respectively. As such, their shapes in the IR are 0 and 1-dimensional. However, BlockMatrix (Py API and Scala backend) assumes that all BlockMatrices are 2-dimensional, with the potential to have dimensions of length 1. To satisfy the current user API while maintaining a tensor-like mindset in the IR, the BlockMatrixType now has a `isRowVector` flag to help the front-end BlockMatrix discern whether a 1-tensor IR should be treated as a row or column vector. As BlockMatrix generalizes to n-tensors this flag can be removed. The IR should not rely on this flag for any execution logic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5338:519,perform,performed,519,https://hail.is,https://github.com/hail-is/hail/pull/5338,1,['perform'],['performed']
Performance,"Version:; Apache Spark version 2.4.3; Hail version 0.2.19-c6ec8b76eb26. When exporting a table, the following error occurs when running on a yarn cluster. It does not occur when running locally. Any suggestions on this?. ```; Container exited with a non-zero exit code 127. Error file: prelaunch.err.; Last 4096 bytes of prelaunch.err :; Last 4096 bytes of stderr :; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/data04/hadoop/yarn/local/usercache/farrell/filecache/291/__spark_libs__4347827829503170766.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.5.0-292/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 19/09/06 15:54:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 19/09/06 15:54:30 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; /usr/java/default/bin/java: symbol lookup error: /data01/hadoop/yarn/local/usercache/farrell/appcache/application_1565788829616_0098/container_e2451_1565788829616_0098_01_000011/tmp/jniloader3452911880890326608netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. ```; -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7008:960,load,load,960,https://hail.is,https://github.com/hail-is/hail/issues/7008,2,['load'],"['load', 'loaded']"
Performance,"WIP pull request. - [x] Write tests to replicate existing errors. - [ ] Write test to replicate stalled request. - [ ] Resolve stalled request. However, I think we should consider writing a more complete solution to this. As far as I can tell, our use of threads is fragile; following Flask recommendations w.r.t reliance on production-ready WSGI server seems a good idea. Happy to take that on. I'd also like to move away from Flask for API stuff. While not likely to be a bottleneck for many moons, there are solutions rumored to be far faster (Falcon, esp using Cpython).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5065:474,bottleneck,bottleneck,474,https://hail.is,https://github.com/hail-is/hail/pull/5065,1,['bottleneck'],['bottleneck']
Performance,"We are interested in the performance of `hl.sample_qc` on a hard calls (`GT` only) dataset. In particular, are there computations that dominate when the data is small but are obscured when the input data is large? This is relevant to the analysis of 100k whole genomes (~600k variants).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4525:25,perform,performance,25,https://hail.is,https://github.com/hail-is/hail/issues/4525,1,['perform'],['performance']
Performance,"We are not permitted more than 10,000 parts. Moreover a tiny copy part size; seems to have negative consequences on throughput.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10938:116,throughput,throughput,116,https://hail.is,https://github.com/hail-is/hail/pull/10938,1,['throughput'],['throughput']
Performance,"We currently cannot run untrusted code on our cluster and guarantee that malicious code in one pod does not leak into other pods, or affect the entire cluster. This proposal outlines a solution to this problem. *This is a work in progress*. ### TL;DR; Use Kata + CRI-Containerd runtime to sandbox pods, at a low performance cost. [Jessie Frazelles Blog: Hard Multi-Tenancy in Kubernetes](https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/). ### Roadmap; I would like to implement a test cluster that uses this system, and begin migrating our existing workloads to it asap. . *TODO*. ### Rationale; 1. We want resource preemption across users., running multiple user containers on a single cluster.; 2. This means sandboxing at the cluster level is out.; 3. Therefore we must sandbox at the pod (or container) level. Kata + CRI-Containerd chosen for performance and maturity reasons.; CRI-Containerd is much faster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5111:312,perform,performance,312,https://hail.is,https://github.com/hail-is/hail/issues/5111,2,['perform'],['performance']
Performance,"We currently have a `@monitor_endpoint` decorator that we use to wrap aiohttp endpoints and produce prometheus metrics, but the same result can be achieved with fewer lines of code by using essentially the same wrapper as an aiohttp middleware. . Separately, I fixed the wrapper to correctly catch and report on endpoints that raise exceptions, as well as keeping track of the number of concurrent connections.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10600:387,concurren,concurrent,387,https://hail.is,https://github.com/hail-is/hail/pull/10600,1,['concurren'],['concurrent']
Performance,"We don't want this in PR namespaces when running `test_ci`. ```; retry buildctl-daemonless.sh build --frontend dockerfile.v0 --local context=/io/repo/ --local dockerfile=/home/user --output 'type=image,""name=gcr.io/hail-vdc/ci-intermediate:kbqu6modc66u,gcr.io/hail-vdc/service-base:cache"",push=true' --export-cache type=inline --import-cache type=registry,ref=gcr.io/hail-vdc/service-base:cache --trace=/home/user/trace; cat /home/user/trace; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907:282,cache,cache,282,https://hail.is,https://github.com/hail-is/hail/pull/11907,4,['cache'],['cache']
Performance,"We had a couple PRs fail because the database reached its [max connections](https://portal.azure.com/#@haildev.onmicrosoft.com/resource/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/haildev/providers/Microsoft.DBforMySQL/servers/db-393222c4/metrics). We should probably be resilient to this, but I figured we should be able to handle our normal PR load. I also checked GCP and their default is 4k. Azure sets its cap at 1250. I haven't applied any terraform since you've made your changes. Is that safe to do?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11329:368,load,load,368,https://hail.is,https://github.com/hail-is/hail/pull/11329,1,['load'],['load']
Performance,"We have persist / cache, but no unpersist",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1852:18,cache,cache,18,https://hail.is,https://github.com/hail-is/hail/issues/1852,1,['cache'],['cache']
Performance,"We keep an in-memory cache on the driver for which job ranges correspond to which job spec token. Currently, we cache the open range [N, None) which corresponds to the last bunch in the batch. This is fine when the batch is all submitted at once, but can become stale if there are updates to the batch. Here, we just use the range [N, n_jobs), which even if the number of jobs in the batch changes should still be correct. Ultimately, I think we should just add the end id to the batch_bunches table instead of doing this subquery and join against the batches table. This is a bit of a hack.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12146:21,cache,cache,21,https://hail.is,https://github.com/hail-is/hail/pull/12146,2,['cache'],['cache']
Performance,"We need to configure Grafana with an SMTP server so that it can send us alert emails. I'd like to have alerts for things like disks getting dangerously close to filling up, abnormally high latency, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6697:189,latency,latency,189,https://hail.is,https://github.com/hail-is/hail/issues/6697,1,['latency'],['latency']
Performance,"We should already have measures in place (like firewall rules) that prevent untrusted code from reaching the Batch Worker server, but this provides an extra layer of protection through which we can enforce that only the Batch front end and the Batch Driver can use the endpoints on the Batch Worker. This, along with #14581 are the final pieces to ensure that every endpoint in our system, both internal and external, uses HTTPS and performs the appropriate auth checks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14622:433,perform,performs,433,https://hail.is,https://github.com/hail-is/hail/pull/14622,1,['perform'],['performs']
Performance,We should be able to check something in netlib to determine if it loaded the natives or the reference library.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5040:66,load,loaded,66,https://hail.is,https://github.com/hail-is/hail/issues/5040,1,['load'],['loaded']
Performance,"We tried loading a large bgen file (chr21 - 950k Variants, 150k samples) from UK Biobank. It seemed to proceed ok and generate a VDS file that was loadable, but upon looking at variantqc, it appears that at the large majority of sites, all homref calls are missing. There were no 'tolerance too low' messages while loading, and this property seems to be retained across several runs with difference tolerances.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/714:9,load,loading,9,https://hail.is,https://github.com/hail-is/hail/issues/714,3,['load'],"['loadable', 'loading']"
Performance,"We use Q twice if someone wants to compute the loadings, so we want to save it to avoid redoing full QR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10223:47,load,loadings,47,https://hail.is,https://github.com/hail-is/hail/pull/10223,1,['load'],['loadings']
Performance,"We use `nest_asyncio` to allow synchronous blocking on a coroutine execution inside an async context. For example, if a user is using the synchronous interface of hail inside a jupyter cell (which runs an event loop). `nest_asyncio` achieves this by patching the `asyncio` event loop to make it reentrant, and with that there are footguns. This allows us to do things like create 100 tasks that all concurrently invoke `run_until_complete`, each of which will add stack frames to the event loop stack that can pile up and trigger a cryptic `RecursionError`. But internally we never *need* to make concurrent calls to `run_until_complete`, and more broadly we should never have `async / sync / async` inside hail code. This change exposes an asynchronous `validate_file` so that asynchronous methods in `hailtop` can use it directly instead of inserting a synchronous layer (`async_to_blocking`) between them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14576:399,concurren,concurrently,399,https://hail.is,https://github.com/hail-is/hail/pull/14576,2,['concurren'],"['concurrent', 'concurrently']"
Performance,"We use a readiness probe with a generous timeout to ensure the browser; loads the relevant files into a cache before any users see the website. This; slows deployment because we wait about 30s before sending any user traffic (in; the meantime, users will see 502s). On the bright side, after start-up, users will; always have a face experience, even if the pod was restarted since someone last; visited the site. I also codified some of the file update steps and clarified the README wrt Duncan's; GitHub repo.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8899:72,load,loads,72,https://hail.is,https://github.com/hail-is/hail/pull/8899,2,"['cache', 'load']","['cache', 'loads']"
Performance,"We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous small clean ups like unifying `RVD.rdd` and adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3392:375,Load,LoadVCF,375,https://hail.is,https://github.com/hail-is/hail/pull/3392,1,['Load'],['LoadVCF']
Performance,"We want to track Hail's performance with every release for a number of reasons, including but not limited to: ; - Measure how we are doing in delivering value to scientists; - Measure the effect of changes, test our intuition and learn how to improve the product. ; - Compare our solution with others; - Catch unexpected regressions. As of the time of writing, benchmarks are run rarely and have rotted somewhat. There's a bit of work required to get them going again. There's also some work in getting them running in CI and capturing the results. Very roughly, I think work can broken down as follows:; - [ ] get benchmarks passing; - [ ] organise trials with learnings from https://www.zora.uzh.ch/id/eprint/170445/1/emse_smb_cloud.pdf; - [x] run bechmarks in ci on deploy and store the results somewhere appropriate, fail if there's something really awful ; - [ ] visualise results on some appropriate cadance for trends. Might be nice to have a graphic on our github page. . I think many of these can be done in parallel.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14221:24,perform,performance,24,https://hail.is,https://github.com/hail-is/hail/issues/14221,1,['perform'],['performance']
Performance,"We're currently emitting the explicit node (without optimization!).; This design is incrementally better, and lets us do ptyping more easily. The right solution is to do generate a method as the node suggests, but; there are some issues to sort out here, like how to return a missing; value. We may need to return a (possibly null) pointer to an allocated; value, which could be inefficient. Pushing ptypes/requiredness fully; through the system would let us avoid this in many cases. Stacked on #8084, don't review until that goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8085:52,optimiz,optimization,52,https://hail.is,https://github.com/hail-is/hail/pull/8085,1,['optimiz'],['optimization']
Performance,"We're currently emitting the explicit node (without optimization!).; This design is incrementally better, and lets us do ptyping more easily. The right solution is to do generate a method as the node suggests, but; there are some issues to sort out here, like how to return a missing; value. We may need to return a (possibly null) pointer to an allocated; value, which could be inefficient. Pushing ptypes/requiredness fully; through the system would let us avoid this in many cases. cc @catoverdrive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8067:52,optimiz,optimization,52,https://hail.is,https://github.com/hail-is/hail/pull/8067,1,['optimiz'],['optimization']
Performance,"When accessing two VDS files in `gs://hail-common/`, I got two errors. 1. gs://hail-common/all_coding_plus_minus_50bp_vep.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683:349,load,loading,349,https://hail.is,https://github.com/hail-is/hail/issues/1683,2,['load'],['loading']
Performance,"When loading a plink binary file, if the delimiter is incorrect it currently fails with the following cryptic error:. caught scala.MatchError: [Ljava.lang.String;@459f703f (of class [Ljava.lang.String;). This was a file with spaces (where the default is tab). As space seems to be the default for plink2, which was used to make these files, it may be worth allowing either by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/709:5,load,loading,5,https://hail.is,https://github.com/hail-is/hail/issues/709,1,['load'],['loading']
Performance,"When the number of fields in `x` is really huge, this optimization creates huge chains of `Let` bindings which cause stack size issues in downstream IR analysis passes. (we can remove this cap once our passes will no longer run into stack size issues on very deep IRs.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7726:54,optimiz,optimization,54,https://hail.is,https://github.com/hail-is/hail/pull/7726,1,['optimiz'],['optimization']
Performance,"While implementing IBD, I added two type-classes to make implementing a fuzzy-comparable slightly easier. I'm not sure this is the best approach. Unfortunately, we didn't heavily discuss this choice because IBD was moving into master rather quickly after the performance issues were resolved. The files are: [AbsoluteFuzzyComparable](https://github.com/hail-is/hail/blob/master/src/main/scala/org/broadinstitute/hail/utils/AbsoluteFuzzyComparable.scala) and [RelativeFuzzyComparable](https://github.com/hail-is/hail/blob/master/src/main/scala/org/broadinstitute/hail/utils/RelativeFuzzyComparable.scala). Absolute means `(x - y) < `. Relative means `(x - y) < max(abs(x), abs(y)) * ` (i.e. `D_==`). Any type for which these type classes has been defined may be compared. For example:. ``` scala; (x: T, y: T) => AbsoluteFuzzyComparable.absoluteEq(epsilon, x, y); ```. I gave a type-class generator for `Map[K, V]` where `V` is comparable, so arbitrarily nested maps may be compared in this way.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/876:259,perform,performance,259,https://hail.is,https://github.com/hail-is/hail/issues/876,1,['perform'],['performance']
Performance,"Will merge cleanly when https://github.com/hail-is/hail/pull/3560 lands. I needed to remove `RegionValue.copy` and `Region.copy` because they necessarily create regions that aren't managed by an `RVDContext`. `RegionValue.copy` is only used in three places. . - `Table.toMatrixTable`: Here, I took the somewhat inefficient choice of creating `SafeRow`s. If `toMatrixTable` is a performance bottleneck, we might want to reconsider this. It's not totally obvious how to do this. I think I'd need to explicitly serialize/deserialize these values and modify `reduceByKey` to explicitly provide the `RVDContext`. Anyway, this works and I don't think it's _that_ slow. (I guess I should check that). - `OrderedRVD.localKeySort` & `LocalLDPrune.pruneLocal`: in both cases we need keep a handful of region values around per-partition. This does not lend itself to region-based-allocation. I solve this with two copies and a fresh region per value. Putting a value into `localKeySort`'s queue requires copying it into a fresh region. Taking a value out of the queue requires copying it into the consumer's region and closing/freeing the region it was living in. There fresh region is alive as long as the value is in the queue. I had to modify `RVDContext` to track `Region`s that get closed early. This seems a bit inefficient. Maybe I should track children as a `Set`?. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3579:378,perform,performance,378,https://hail.is,https://github.com/hail-is/hail/pull/3579,5,"['bottleneck', 'perform', 'queue']","['bottleneck', 'performance', 'queue']"
Performance,"With good optimization too:. ```; hl.eval(hl.utils.range_table(10).annotate(x=[1,2,3]).explode('x').annotate(y=10).index_globals()); ```. ```; 2018-12-05 18:33:33 root: INFO: optimize: before:; (TableGetGlobals; (TableMapRows; (TableExplode x; (TableMapRows; (TableRange 10 12); (InsertFields; (Ref row); (x; (Literal Array[Int32] ""[1,2,3]""))))); (InsertFields; (Ref row); (y; (I32 10))))); 2018-12-05 18:33:33 root: INFO: optimize: after:; (MakeStruct); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4902:10,optimiz,optimization,10,https://hail.is,https://github.com/hail-is/hail/pull/4902,3,['optimiz'],"['optimization', 'optimize']"
Performance,"Without `--cache-repo`, Kaniko uses `{image_name}/cache` (e.g. `gcr.io/hail-vdc/batch-worker/cache`). That works particularly poorly for all the test images which are `gcr.io/hail-vdc/ci-intermediate` and therefore don't share a cache with the default images.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10396:11,cache,cache-repo,11,https://hail.is,https://github.com/hail-is/hail/pull/10396,4,['cache'],"['cache', 'cache-repo']"
Performance,"Wonder what your thoughts are on this. I think that we have too deep an image hierarchy and that it's only stalling our time-to-test in CI. I propose here getting rid of the run-tests image and just putting plink in the hail-run image. I also remove the dependency on service-base and just install hailtop in the hail-run image -- I don't think anything hail/hail should depend on services code. This change actually revealed an accidental dependency where there was a gear (services) test in the hail/python tests. I moved that over to the batch suite as batch is the primary user of that class and we don't have a gear suite atm. Ultimately, I think we should have a pretty flat docker image hierarchy. Each of our Dockerfiles does a decent job of following the docker mantra of dependencies first, code later. But if you consider the fact that we stack images on top of each other then you can see that in reality that doesn't translate to our images! We instead get an interleaving of hail code and dependencies such that it's very easy to break the cache for images like this one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12396:1054,cache,cache,1054,https://hail.is,https://github.com/hail-is/hail/pull/12396,1,['cache'],['cache']
Performance,"Working in cluster. Fixes the below error, the origin of which I'm not quite sure: does it happen because wherever CI builds this has spark2.4 installed, or is spark2.4 pulled by gradlew shadowJar (I don't see where this happens, but I also haven't looked very carefully). ```; install-hail-locally:; 	rm -rf build; 	(cd ../hail && GRADLE_OPTS=-Xmx2048m ./gradlew shadowJar --gradle-user-home /gradle-cache); 	mkdir -p build/hail/jars; 	mkdir -p build/hail/python; 	cp -a ../hail/build/libs/hail-all-spark.jar build/hail/jars; 	cp -a ../hail/python/hail build/hail/python. build-hail-base: build-spark-base install-hail-locally; ```. <img width=""814"" alt=""Screenshot 2019-04-10 13 22 01"" src=""https://user-images.githubusercontent.com/5543229/55902941-79ea4c00-5b9a-11e9-9899-8e37311c4d06.png"">. ; Only issue I see is; """"""; 2019-04-10 18:00:59 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN""; """""""". not sure if that's new, but googling around suggests the typical solution is warning suppression. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5850:401,cache,cache,401,https://hail.is,https://github.com/hail-is/hail/pull/5850,2,"['cache', 'load']","['cache', 'load']"
Performance,"Would it be possible to add an option to annotate with --tab and --pick_allele, as below:; This would be super helpful!. <hail.vep.perl>; <hail.vep.location>; --format vcf; --tab; --pick_allele; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.cache_dir>/homo_sapiens/81_GRCh37/Homo_sapiens.GRCh37.75.dna.primary_assembly.fa; --minimal; --assembly GRCh37; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/489:240,cache,cache,240,https://hail.is,https://github.com/hail-is/hail/issues/489,1,['cache'],['cache']
Performance,"Write loading lines from text files in a way that can be used by Spark or table lowering. This will be to lower the VCF and text file importers. This handles three cases:; - an (splittable) uncompressed file,; - a (splittable) bgzip compressed file, and; - an non-splittable compressed file compressed by some other codec. In this case, it will be loaded as a single partition. I test each of the three cases. I also ported the prexisting partition test that generates random splittings and tests that in the bgzip case. I tested with the number of tests turned up to 10,000. Two ideas here:. Each line has a fixed offset depending on the case (file offset, virtual offset, or decompressed offset). The split defines a partition of the offset spaces. For a partition (start, end), and line with offset x, the line belongs in the partition if x lies in the range (start, end] or, if it is the first partition, [start, end]. This is because, if at the beginning of a split, you can't tell if the line started earlier or not. The second idea is to copy the blocks one at a time into buf. bgzip blocks are max 64KB. `read` on a BGzipInputStream doesn't return data that spans blocks. So buf always contains the entire data for a block. This is used to track the offset o the beginning of the line.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8662:6,load,loading,6,https://hail.is,https://github.com/hail-is/hail/pull/8662,2,['load'],"['loaded', 'loading']"
Performance,XmYSkP5TA/specs BlockBlob Hot 1264 application/octet-stream 2023-06-09T12:43:37+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/dK3o5ZfXmYSkP5TA/specs.idx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:37+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/eOrFpVrN98GBIizi/specs BlockBlob Hot 1264 application/octet-stream 2023-06-09T12:43:34+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/eOrFpVrN98GBIizi/specs.idx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:34+00:00; ```. I looked at the status:. ```; az storage blob download --account-name haildevtest --container test --name batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json | jq '.' | less; ```. which contained an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source); 	at sun.reflect.DelegatingMeth,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:3914,concurren,concurrent,3914,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance,"YAML has an arbitrary extension mechanism. pyYAML defines a python extension that lets you create arbitrary python objets. This is clearly a huge security vulnerability. Apparently, pyYAML, by default, enables this extension (rather than just parsing vanilla YAML, ). `safe_load` loads vanilla YAML without the gaping security hole. I was getting warnings about this when testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5825:284,load,loads,284,https://hail.is,https://github.com/hail-is/hail/pull/5825,1,['load'],['loads']
Performance,"You can now say:. ```; async with db.start() as tx:; await tx.just_execute(sql); row = await tx.execute_fetchone(sql, args); ...; ```. Transactions support all the database utility functions. If the transaction context manager exists with an exception, the transaction is rolled back, otherwise it is committed. You also can explicitly rollback or commit the transaction, although it can't be used again after that. I also added an execute_many function. I use this in the front end instead of dropping down to aiomysql to create explicit transactions. Note on internal changes: I no longer use autocommit now that transaction boundaries are explicit. You can start a read only transaction, and I do that by default for execute_and_fetch{one, all}, although maybe those should be renamed select_and_fetch{one, all} to make their read-only nature apparent (MySQL throws an error if you try to modify something in a read-only transaction). This follows mysql best transaction performance recommendations as described here: https://dev.mysql.com/doc/refman/5.6/en/optimizing-innodb-transaction-management.html and https://dev.mysql.com/doc/refman/5.6/en/innodb-performance-ro-txn.html. FYI @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7641:974,perform,performance,974,https://hail.is,https://github.com/hail-is/hail/pull/7641,3,"['optimiz', 'perform']","['optimizing-innodb-transaction-management', 'performance', 'performance-ro-txn']"
Performance,You were right about the cache'ing. Thanks @jbloom22 !,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5305:25,cache,cache,25,https://hail.is,https://github.com/hail-is/hail/pull/5305,1,['cache'],['cache']
Performance,[QOB] Parameterize code cache by backend (don't cache on service),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12618:24,cache,cache,24,https://hail.is,https://github.com/hail-is/hail/pull/12618,2,['cache'],['cache']
Performance,"[SIG Node]</li>; <li>Kubelet: add '--logging-format' flag to support structured logging (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91532"">kubernetes/kubernetes#91532</a>, <a href=""https://github.com/afrouzMashaykhi""><code>@afrouzMashaykhi</code></a>) [SIG API Machinery, Cluster Lifecycle, Instrumentation and Node]</li>; <li>Kubernetes is now built with golang 1.15.0-rc.1.; <ul>; <li>The deprecated, legacy behavior of treating the CommonName field on X.509 serving certificates as a host name when no Subject Alternative Names are present is now disabled by default. It can be temporarily re-enabled by adding the value x509ignoreCN=0 to the GODEBUG environment variable. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/93264"">kubernetes/kubernetes#93264</a>, <a href=""https://github.com/justaugustus""><code>@justaugustus</code></a>) [SIG API Machinery, Auth, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation, Network, Node, Release, Scalability, Storage and Testing]</li>; </ul>; </li>; <li>Promote Immutable Secrets/ConfigMaps feature to Beta and enable the feature by default.; This allows to set <code>Immutable</code> field in Secrets or ConfigMap object to mark their contents as immutable. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89594"">kubernetes/kubernetes#89594</a>, <a href=""https://github.com/wojtek-t""><code>@wojtek-t</code></a>) [SIG Apps and Testing]</li>; <li>Remove <code>BindTimeoutSeconds</code> from schedule configuration <code>KubeSchedulerConfiguration</code> (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91580"">kubernetes/kubernetes#91580</a>, <a href=""https://github.com/cofyc""><code>@cofyc</code></a>) [SIG Scheduling and Testing]</li>; <li>Remove kubescheduler.config.k8s.io/v1alpha1 (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89298"">kubernetes/kubernetes#89298</a>, <a href=""https://github.com/gavi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:11074,Scalab,Scalability,11074,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['Scalab'],['Scalability']
Performance,"[VCF version 4.5](https://samtools.github.io/hts-specs/VCFv4.5.pdf) contains the changes we developed as part of our work developing the [Scalable Variant Call Representation](https://www.biorxiv.org/content/10.1101/2024.01.09.574205v1). As the developers and drivers of these changes, we should fully support v4.5 via import to VDS and export from VDS. Current checklist. May be extended over time:. - [x] Prefer `LEN` over `END` for reference blocks. (Begins in #14675); - [x] Update the combiner to convert to `LEN` from INFO `END` (Part of #14675).; - [x] Update `to_dense_mt` to use `LEN` (we think it may be more efficient).; - [ ] Add VDS to VCF export. (#14743); - [ ] Add Sparse VCF to VDS import. (#14743); - [x] ~'Official' non-ref genotype `<*>` support?~ (not part of this issue); - [ ] Make sure that we output well formed VCF 4.5, this includes things like VCF 4.4's phased haploid calls (this will also require updates to our parser)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14655:138,Scalab,Scalable,138,https://hail.is,https://github.com/hail-is/hail/issues/14655,1,['Scalab'],['Scalable']
Performance,[`orjson`](https://github.com/ijl/orjson#serialize) is a fast JSON serialize/deserialize library. I found that it; improved the performance of the copy tool substantially. I suspect we will see a low-level improvement across all; services. We can't use aiohttp's normal json library overrides because aiohttp stubbornly refuses to support a JSON interface that doesn't (unnecessarily) return strings (which are then decoded to utf-8 bytes anyway). cc: @daniel-goldstein,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10982:128,perform,performance,128,https://hail.is,https://github.com/hail-is/hail/pull/10982,1,['perform'],['performance']
Performance,[aiotools][batch][memory] Fix the secret cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11040:41,cache,cache,41,https://hail.is,https://github.com/hail-is/hail/pull/11040,1,['cache'],['cache']
Performance,[asyncfs] improvements to give better performance on S3,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10752:38,perform,performance,38,https://hail.is,https://github.com/hail-is/hail/pull/10752,1,['perform'],['performance']
Performance,[auth] Cache user sessions in memory,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12122:7,Cache,Cache,7,https://hail.is,https://github.com/hail-is/hail/pull/12122,2,['Cache'],['Cache']
Performance,[auth] Dont load session_id in the UserData dict,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13618:12,load,load,12,https://hail.is,https://github.com/hail-is/hail/pull/13618,1,['load'],['load']
Performance,[azure] Optimize file status operations,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13368:8,Optimiz,Optimize,8,https://hail.is,https://github.com/hail-is/hail/pull/13368,1,['Optimiz'],['Optimize']
Performance,[batch.aioclient] Optimize batch.wait for QoB,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13177:18,Optimiz,Optimize,18,https://hail.is,https://github.com/hail-is/hail/pull/13177,1,['Optimiz'],['Optimize']
Performance,[batch2] Fix queue starvation problem,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7336:13,queue,queue,13,https://hail.is,https://github.com/hail-is/hail/pull/7336,1,['queue'],['queue']
Performance,[batch2] optimize scheduler query,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7634:9,optimiz,optimize,9,https://hail.is,https://github.com/hail-is/hail/pull/7634,1,['optimiz'],['optimize']
Performance,[batch2] scalable job search/batch page,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7635:9,scalab,scalable,9,https://hail.is,https://github.com/hail-is/hail/pull/7635,1,['scalab'],['scalable']
Performance,[batch] Add image cache and remove old images,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8445:18,cache,cache,18,https://hail.is,https://github.com/hail-is/hail/pull/8445,1,['cache'],['cache']
Performance,[batch] Add k8s secret/sa cache and retry transient errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7850:26,cache,cache,26,https://hail.is,https://github.com/hail-is/hail/pull/7850,1,['cache'],['cache']
Performance,"[batch] Add kubernetes secret, service account cache",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7825:47,cache,cache,47,https://hail.is,https://github.com/hail-is/hail/pull/7825,1,['cache'],['cache']
Performance,[batch] Cache tokens for job bunches on the driver to avoid db queries,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12023:8,Cache,Cache,8,https://hail.is,https://github.com/hail-is/hail/pull/12023,1,['Cache'],['Cache']
Performance,[batch] Create pool scheduling loops after loading existing instances,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11766:43,load,loading,43,https://hail.is,https://github.com/hail-is/hail/pull/11766,1,['load'],['loading']
Performance,[batch] Dont include open-ended job ranges in token cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12146:52,cache,cache,52,https://hail.is,https://github.com/hail-is/hail/pull/12146,1,['cache'],['cache']
Performance,[batch] Have Batch Worker perform auth checks on requests from Batch,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14622:26,perform,perform,26,https://hail.is,https://github.com/hail-is/hail/pull/14622,1,['perform'],['perform']
Performance,[batch] Improve SQL performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5945:20,perform,performance,20,https://hail.is,https://github.com/hail-is/hail/issues/5945,1,['perform'],['performance']
Performance,[batch] Miscellaneous performance improvements,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5991:22,perform,performance,22,https://hail.is,https://github.com/hail-is/hail/pull/5991,1,['perform'],['performance']
Performance,[batch] Mitigate possible corrupt containerd image cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12758:51,cache,cache,51,https://hail.is,https://github.com/hail-is/hail/pull/12758,1,['cache'],['cache']
Performance,[batch] Mitigate siwei bug by removing cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13399:39,cache,cache,39,https://hail.is,https://github.com/hail-is/hail/pull/13399,1,['cache'],['cache']
Performance,[batch] Optimize SQL query generated for listing jobs / batches / job groups,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14410:8,Optimiz,Optimize,8,https://hail.is,https://github.com/hail-is/hail/issues/14410,1,['Optimiz'],['Optimize']
Performance,[batch] Orphaned attempts loop performs full scan of instances table,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14460:31,perform,performs,31,https://hail.is,https://github.com/hail-is/hail/issues/14460,1,['perform'],['performs']
Performance,[batch] Remove unused file cache from worker,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9860:27,cache,cache,27,https://hail.is,https://github.com/hail-is/hail/pull/9860,1,['cache'],['cache']
Performance,[batch] Use a simulated job queue to estimate the ready cores in the control loop,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12212:28,queue,queue,28,https://hail.is,https://github.com/hail-is/hail/pull/12212,1,['queue'],['queue']
Performance,[batch] active_instances_only name-token cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11346:41,cache,cache,41,https://hail.is,https://github.com/hail-is/hail/pull/11346,1,['cache'],['cache']
Performance,[batch] add start jobs queue,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6485:23,queue,queue,23,https://hail.is,https://github.com/hail-is/hail/pull/6485,1,['queue'],['queue']
Performance,[batch] always use throttler to delete pods,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6807:19,throttle,throttler,19,https://hail.is,https://github.com/hail-is/hail/pull/6807,1,['throttle'],['throttler']
Performance,[batch] cache files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9087:8,cache,cache,8,https://hail.is,https://github.com/hail-is/hail/pull/9087,1,['cache'],['cache']
Performance,[batch] cache input files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9095:8,cache,cache,8,https://hail.is,https://github.com/hail-is/hail/pull/9095,1,['cache'],['cache']
Performance,[batch] cache more hail images on the worker VMs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13913:8,cache,cache,8,https://hail.is,https://github.com/hail-is/hail/issues/13913,1,['cache'],['cache']
Performance,[batch] create load test to stress the batch-driver at a maximum sche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11226:15,load,load,15,https://hail.is,https://github.com/hail-is/hail/pull/11226,1,['load'],['load']
Performance,[batch] easy performance improvements,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6487:13,perform,performance,13,https://hail.is,https://github.com/hail-is/hail/pull/6487,1,['perform'],['performance']
Performance,[batch] easy performance improvements for create_batch,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6455:13,perform,performance,13,https://hail.is,https://github.com/hail-is/hail/pull/6455,1,['perform'],['performance']
Performance,[batch] fix get_jobs performance issue,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5981:21,perform,performance,21,https://hail.is,https://github.com/hail-is/hail/pull/5981,1,['perform'],['performance']
Performance,[batch] fix revert input file cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9267:30,cache,cache,30,https://hail.is,https://github.com/hail-is/hail/pull/9267,1,['cache'],['cache']
Performance,[batch] fix search bar -- load js directory,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6472:26,load,load,26,https://hail.is,https://github.com/hail-is/hail/pull/6472,1,['load'],['load']
Performance,[batch] fix throttler bugs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6959:12,throttle,throttler,12,https://hail.is,https://github.com/hail-is/hail/pull/6959,1,['throttle'],['throttler']
Performance,[batch] input file cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9074:19,cache,cache,19,https://hail.is,https://github.com/hail-is/hail/pull/9074,1,['cache'],['cache']
Performance,[batch] k8s cache fails for unclear reasons,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13909:12,cache,cache,12,https://hail.is,https://github.com/hail-is/hail/issues/13909,1,['cache'],['cache']
Performance,[batch] limit concurrent pods to 3k,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6802:14,concurren,concurrent,14,https://hail.is,https://github.com/hail-is/hail/pull/6802,1,['concurren'],['concurrent']
Performance,[batch] load credentials from file once instead of thrice,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11453:8,load,load,8,https://hail.is,https://github.com/hail-is/hail/pull/11453,1,['load'],['load']
Performance,[batch] make tests resilient to concurrent batches,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10928:32,concurren,concurrent,32,https://hail.is,https://github.com/hail-is/hail/pull/10928,1,['concurren'],['concurrent']
Performance,[batch] more concurrency in refresh loop,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6801:13,concurren,concurrency,13,https://hail.is,https://github.com/hail-is/hail/pull/6801,1,['concurren'],['concurrency']
Performance,[batch] more reslience in pod throttler,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6804:30,throttle,throttler,30,https://hail.is,https://github.com/hail-is/hail/pull/6804,1,['throttle'],['throttler']
Performance,[batch] revert some of file cache changes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9266:28,cache,cache,28,https://hail.is,https://github.com/hail-is/hail/pull/9266,1,['cache'],['cache']
Performance,"[batch] separate eviction from missing pod, check queue before rescheduling",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6799:50,queue,queue,50,https://hail.is,https://github.com/hail-is/hail/pull/6799,1,['queue'],['queue']
Performance,[batch] throttle instance pool on too many free cores,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8020:8,throttle,throttle,8,https://hail.is,https://github.com/hail-is/hail/pull/8020,1,['throttle'],['throttle']
Performance,[batch] throttle number of pods,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6675:8,throttle,throttle,8,https://hail.is,https://github.com/hail-is/hail/pull/6675,1,['throttle'],['throttle']
Performance,[batch] throttle pod creation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6694:8,throttle,throttle,8,https://hail.is,https://github.com/hail-is/hail/pull/6694,1,['throttle'],['throttle']
Performance,[batch] use safe load in batch client,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5825:17,load,load,17,https://hail.is,https://github.com/hail-is/hail/pull/5825,1,['load'],['load']
Performance,[benchmark] Fix `make_ndarray_bench` to avoid optimizations that obvi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10717:46,optimiz,optimizations,46,https://hail.is,https://github.com/hail-is/hail/pull/10717,1,['optimiz'],['optimizations']
Performance,[bugfix] Fix PCA loadings containing extra fields,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5691:17,load,loadings,17,https://hail.is,https://github.com/hail-is/hail/pull/5691,1,['load'],['loadings']
Performance,[bugfix] Optimize optional variantsTable IR in import_bgen,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5730:9,Optimiz,Optimize,9,https://hail.is,https://github.com/hail-is/hail/pull/5730,1,['Optimiz'],['Optimize']
Performance,[bugfix] fix LoadVCF warning logic,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6096:13,Load,LoadVCF,13,https://hail.is,https://github.com/hail-is/hail/pull/6096,1,['Load'],['LoadVCF']
Performance,[ci2] concurrency improvements,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5921:6,concurren,concurrency,6,https://hail.is,https://github.com/hail-is/hail/pull/5921,1,['concurren'],['concurrency']
Performance,[ci] 8 concurrent builds,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7728:7,concurren,concurrent,7,https://hail.is,https://github.com/hail-is/hail/pull/7728,1,['concurren'],['concurrent']
Performance,[ci] Bring max number of concurrent PRs down to three,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11762:25,concurren,concurrent,25,https://hail.is,https://github.com/hail-is/hail/pull/11762,1,['concurren'],['concurrent']
Performance,[ci] Dont try to json.loads the namespace string,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12631:22,load,loads,22,https://hail.is,https://github.com/hail-is/hail/pull/12631,1,['load'],['loads']
Performance,[ci] Fix bug where we pushed test images to the main image cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907:59,cache,cache,59,https://hail.is,https://github.com/hail-is/hail/pull/11907,1,['cache'],['cache']
Performance,[ci] Reduce max number of concurrent PRs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11426:26,concurren,concurrent,26,https://hail.is,https://github.com/hail-is/hail/pull/11426,1,['concurren'],['concurrent']
Performance,[ci] Use image and PR-specific cache tags for image builds,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11999:31,cache,cache,31,https://hail.is,https://github.com/hail-is/hail/pull/11999,1,['cache'],['cache']
Performance,[ci] all builds share the same cache repo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10396:31,cache,cache,31,https://hail.is,https://github.com/hail-is/hail/pull/10396,1,['cache'],['cache']
Performance,[ci] increase max concurrent PRs to 3,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11506:18,concurren,concurrent,18,https://hail.is,https://github.com/hail-is/hail/pull/11506,1,['concurren'],['concurrent']
Performance,[ci] reduce PR test latency to ten minutes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14003:20,latency,latency,20,https://hail.is,https://github.com/hail-is/hail/issues/14003,1,['latency'],['latency']
Performance,[compiler] Add partition planning optimizer,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12747:34,optimiz,optimizer,34,https://hail.is,https://github.com/hail-is/hail/pull/12747,2,['optimiz'],['optimizer']
Performance,[compiler] Chunk cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10618:17,cache,cache,17,https://hail.is,https://github.com/hail-is/hail/pull/10618,1,['cache'],['cache']
Performance,"[compiler] Fix bad performance in parseFloat{32, 64}",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10507:19,perform,performance,19,https://hail.is,https://github.com/hail-is/hail/pull/10507,1,['perform'],['performance']
Performance,[compiler] Lots of compiler and code size performance improvements,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10746:42,perform,performance,42,https://hail.is,https://github.com/hail-is/hail/pull/10746,1,['perform'],['performance']
Performance,[compiler] Minor Requiredness Performance Enchancements,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13991:30,Perform,Performance,30,https://hail.is,https://github.com/hail-is/hail/pull/13991,1,['Perform'],['Performance']
Performance,[compiler] Print after each pass rather than before/after optimize,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12603:58,optimiz,optimize,58,https://hail.is,https://github.com/hail-is/hail/pull/12603,1,['optimiz'],['optimize']
Performance,[compiler] Remove SType.loadFrom,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10488:24,load,loadFrom,24,https://hail.is,https://github.com/hail-is/hail/pull/10488,1,['load'],['loadFrom']
Performance,[compiler] Stream iota optimizations,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10787:23,optimiz,optimizations,23,https://hail.is,https://github.com/hail-is/hail/pull/10787,1,['optimiz'],['optimizations']
Performance,[compiler] explicitly describe performance issue of BMAgg,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12657:31,perform,performance,31,https://hail.is,https://github.com/hail-is/hail/pull/12657,1,['perform'],['performance']
Performance,[compiler] explicitly provide type infos for array loads,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11331:51,load,loads,51,https://hail.is,https://github.com/hail-is/hail/pull/11331,1,['load'],['loads']
Performance,[docker] Use updated cache tags in Makefiles,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12015:21,cache,cache,21,https://hail.is,https://github.com/hail-is/hail/pull/12015,1,['cache'],['cache']
Performance,[fs] Improve Azure read performance by using buffered copies,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12488:24,perform,performance,24,https://hail.is,https://github.com/hail-is/hail/pull/12488,1,['perform'],['performance']
Performance,[gear] Dont cache the db ssl context in a global,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13652:12,cache,cache,12,https://hail.is,https://github.com/hail-is/hail/pull/13652,1,['cache'],['cache']
Performance,[git][docker] ignore mypy cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9270:26,cache,cache,26,https://hail.is,https://github.com/hail-is/hail/pull/9270,1,['cache'],['cache']
Performance,[gke] optimize utilization autoscaling and shrink nodes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11985:6,optimiz,optimize,6,https://hail.is,https://github.com/hail-is/hail/pull/11985,1,['optimiz'],['optimize']
Performance,[hail/ptypes] Die should perform deepInnerRequired,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7843:25,perform,perform,25,https://hail.is,https://github.com/hail-is/hail/pull/7843,1,['perform'],['perform']
Performance,[hail/ptypes] PStruct: make loadString an instance method,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7754:28,load,loadString,28,https://hail.is,https://github.com/hail-is/hail/issues/7754,1,['load'],['loadString']
Performance,"[hail/ptypes] rename loadField, loadElement to loadFieldAddress, loadElementAddress",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7829:21,load,loadField,21,https://hail.is,https://github.com/hail-is/hail/issues/7829,4,['load'],"['loadElement', 'loadElementAddress', 'loadField', 'loadFieldAddress']"
Performance,[hail] All calls to optimize are tracked with ExecuteContext,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7541:20,optimiz,optimize,20,https://hail.is,https://github.com/hail-is/hail/pull/7541,1,['optimiz'],['optimize']
Performance,[hail] Do not cache in interactive describe.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7245:14,cache,cache,14,https://hail.is,https://github.com/hail-is/hail/pull/7245,1,['cache'],['cache']
Performance,[hail] Fix EXTREME performance regression in column scans.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7771:19,perform,performance,19,https://hail.is,https://github.com/hail-is/hail/pull/7771,1,['perform'],['performance']
Performance,[hail] Fix `mt.entry.take` performance regression,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7764:27,perform,performance,27,https://hail.is,https://github.com/hail-is/hail/pull/7764,1,['perform'],['performance']
Performance,[hail] Fix memory leak in BM and make cache size configurable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9501:38,cache,cache,38,https://hail.is,https://github.com/hail-is/hail/pull/9501,1,['cache'],['cache']
Performance,[hail] Fix performance problems in aggregators by binding arguments,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6969:11,perform,performance,11,https://hail.is,https://github.com/hail-is/hail/pull/6969,1,['perform'],['performance']
Performance,[hail] Fix up usages of `Type.physicalType` in `LoadVCF`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7693:48,Load,LoadVCF,48,https://hail.is,https://github.com/hail-is/hail/pull/7693,1,['Load'],['LoadVCF']
Performance,[hail] Improve performance of IR copying,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7355:15,perform,performance,15,https://hail.is,https://github.com/hail-is/hail/pull/7355,1,['perform'],['performance']
Performance,[hail] Improve split_multi_hts performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6980:31,perform,performance,31,https://hail.is,https://github.com/hail-is/hail/pull/6980,1,['perform'],['performance']
Performance,[hail] Introduce better TableFilterIntervals optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7171:45,optimiz,optimization,45,https://hail.is,https://github.com/hail-is/hail/pull/7171,1,['optimiz'],['optimization']
Performance,[hail] LoadPlink ptype,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6421:7,Load,LoadPlink,7,https://hail.is,https://github.com/hail-is/hail/pull/6421,1,['Load'],['LoadPlink']
Performance,[hail] Log the size of the associative combiner queue,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6755:48,queue,queue,48,https://hail.is,https://github.com/hail-is/hail/pull/6755,1,['queue'],['queue']
Performance,[hail] Table.union with almost-sorted could perform as well as unkeyed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8110:44,perform,perform,44,https://hail.is,https://github.com/hail-is/hail/issues/8110,1,['perform'],['perform']
Performance,[hail] [bugfix] Fix correctness bug in Table.order_by optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6757:54,optimiz,optimization,54,https://hail.is,https://github.com/hail-is/hail/pull/6757,1,['optimiz'],['optimization']
Performance,[hail] [high-prio] Don't optimize in ApplyIR,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6353:25,optimiz,optimize,25,https://hail.is,https://github.com/hail-is/hail/pull/6353,1,['optimiz'],['optimize']
Performance,[hail] cache encoder and decoder functions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7528:7,cache,cache,7,https://hail.is,https://github.com/hail-is/hail/pull/7528,1,['cache'],['cache']
Performance,"[hail] clean up BlockMatrixMap IRs, delegate optimization of broadcasting for scalars to Simplify pass",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7566:45,optimiz,optimization,45,https://hail.is,https://github.com/hail-is/hail/pull/7566,1,['optimiz'],['optimization']
Performance,[hail] improve import_gtf performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8887:26,perform,performance,26,https://hail.is,https://github.com/hail-is/hail/pull/8887,1,['perform'],['performance']
Performance,[hail] improve performance of import_plink,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6984:15,perform,performance,15,https://hail.is,https://github.com/hail-is/hail/pull/6984,1,['perform'],['performance']
Performance,[hail] optimize flatten,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7719:7,optimiz,optimize,7,https://hail.is,https://github.com/hail-is/hail/pull/7719,1,['optimiz'],['optimize']
Performance,[hail] optimizer breaks correctness of scans,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6408:7,optimiz,optimizer,7,https://hail.is,https://github.com/hail-is/hail/issues/6408,1,['optimiz'],['optimizer']
Performance,[hail] remove load/store/append from Region methods,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7092:14,load,load,14,https://hail.is,https://github.com/hail-is/hail/pull/7092,1,['load'],['load']
Performance,[hail][bugfix] Fix optimizer bug in PruneDeadFields for MatrixUnionRows,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7965:19,optimiz,optimizer,19,https://hail.is,https://github.com/hail-is/hail/pull/7965,1,['optimiz'],['optimizer']
Performance,[hail][bugfix] Resolve performance bug in variant_qc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7989:23,perform,performance,23,https://hail.is,https://github.com/hail-is/hail/pull/7989,1,['perform'],['performance']
Performance,[hail][bugfix] Table/MatrixTable read should grab references and load into Python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6921:65,load,load,65,https://hail.is,https://github.com/hail-is/hail/pull/6921,1,['load'],['load']
Performance,[hail][bugfix]: fix out of bounds read in LoadVCF,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7182:42,Load,LoadVCF,42,https://hail.is,https://github.com/hail-is/hail/pull/7182,1,['Load'],['LoadVCF']
Performance,[hail][performance] Deduplicate inlined IRs in `annotate(**thing)`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6506:7,perform,performance,7,https://hail.is,https://github.com/hail-is/hail/pull/6506,1,['perform'],['performance']
Performance,[hail][performance] Fix literals lazy decoding,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6605:7,perform,performance,7,https://hail.is,https://github.com/hail-is/hail/pull/6605,1,['perform'],['performance']
Performance,[hailctl batch client] Add ability to cache batch status.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9510:38,cache,cache,38,https://hail.is,https://github.com/hail-is/hail/pull/9510,1,['cache'],['cache']
Performance,[hailctl] only load deploy metadata when needed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6513:15,load,load,15,https://hail.is,https://github.com/hail-is/hail/pull/6513,1,['load'],['load']
Performance,[hailtop-auth] allow tokens to be loaded from user-specified file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9585:34,load,loaded,34,https://hail.is,https://github.com/hail-is/hail/pull/9585,1,['load'],['loaded']
Performance,[hailtop] Prevent event loop RecursionError during highly concurrent file uploads,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14576:58,concurren,concurrent,58,https://hail.is,https://github.com/hail-is/hail/pull/14576,1,['concurren'],['concurrent']
Performance,[hailtop] substantially improve the performance of copy,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11079:36,perform,performance,36,https://hail.is,https://github.com/hail-is/hail/pull/11079,1,['perform'],['performance']
Performance,[hailtop][auth] log file when successfully loading tokens,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9132:43,load,loading,43,https://hail.is,https://github.com/hail-is/hail/pull/9132,1,['load'],['loading']
Performance,[k8s] Steps towards optimizing k8s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11900:20,optimiz,optimizing,20,https://hail.is,https://github.com/hail-is/hail/pull/11900,1,['optimiz'],['optimizing']
Performance,"[memory,query] Cache job function/context/outputs in memory service",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10315:15,Cache,Cache,15,https://hail.is,https://github.com/hail-is/hail/pull/10315,1,['Cache'],['Cache']
Performance,[memory] add simple read-only cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9349:30,cache,cache,30,https://hail.is,https://github.com/hail-is/hail/pull/9349,2,['cache'],['cache']
Performance,[performance] Unkey table before collectAsDict,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5860:1,perform,performance,1,https://hail.is,https://github.com/hail-is/hail/pull/5860,1,['perform'],['performance']
Performance,[performance] fix split_multi_hts to only map split rows,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6554:1,perform,performance,1,https://hail.is,https://github.com/hail-is/hail/issues/6554,1,['perform'],['performance']
Performance,[qob] Invalidate batch cache on error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12519:23,cache,cache,23,https://hail.is,https://github.com/hail-is/hail/pull/12519,1,['cache'],['cache']
Performance,[qob] Support loading RGs from FASTA files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736:14,load,loading,14,https://hail.is,https://github.com/hail-is/hail/pull/12736,1,['load'],['loading']
Performance,[qob] replace JVMJobs with DockerJobs which start a JVM using a JAR and a pre-warmed cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13675:85,cache,cache,85,https://hail.is,https://github.com/hail-is/hail/issues/13675,1,['cache'],['cache']
Performance,[query-service] concurrently use multiple Hail Query JAR versions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:16,concurren,concurrently,16,https://hail.is,https://github.com/hail-is/hail/pull/10279,3,['concurren'],['concurrently']
Performance,[query/shuffler] somewhat tune branching factor to dataset & log phases,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11784:26,tune,tune,26,https://hail.is,https://github.com/hail-is/hail/pull/11784,1,['tune'],['tune']
Performance,[query] Add CastToArray optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9265:24,optimiz,optimization,24,https://hail.is,https://github.com/hail-is/hail/pull/9265,1,['optimiz'],['optimization']
Performance,[query] Add a persist to improve performance of blanczos_pca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9498:33,perform,performance,33,https://hail.is,https://github.com/hail-is/hail/pull/9498,1,['perform'],['performance']
Performance,[query] Cache header lines instead of recomputing per row field in `import_matrix_table`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12038:8,Cache,Cache,8,https://hail.is,https://github.com/hail-is/hail/pull/12038,1,['Cache'],['Cache']
Performance,[query] Cache table coercers per driver JVM to avoid scans on every q,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12099:8,Cache,Cache,8,https://hail.is,https://github.com/hail-is/hail/pull/12099,1,['Cache'],['Cache']
Performance,[query] Cache unkeyed interval instead of allocating for each partition,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11845:8,Cache,Cache,8,https://hail.is,https://github.com/hail-is/hail/pull/11845,1,['Cache'],['Cache']
Performance,[query] Call-Cache `CollectDistributedArray` (rfc-0000),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12954:13,Cache,Cache,13,https://hail.is,https://github.com/hail-is/hail/pull/12954,1,['Cache'],['Cache']
Performance,[query] Clean up PNDArray / Improve loadElement performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9883:36,load,loadElement,36,https://hail.is,https://github.com/hail-is/hail/pull/9883,2,"['load', 'perform']","['loadElement', 'performance']"
Performance,[query] Clear the bdist build cache before building the wheel,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12008:30,cache,cache,30,https://hail.is,https://github.com/hail-is/hail/pull/12008,1,['cache'],['cache']
Performance,[query] Don't broadcast unnecessarily in LoadVCF,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9350:41,Load,LoadVCF,41,https://hail.is,https://github.com/hail-is/hail/pull/9350,1,['Load'],['LoadVCF']
Performance,[query] Dont compile code cache function three times,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13796:26,cache,cache,26,https://hail.is,https://github.com/hail-is/hail/pull/13796,1,['cache'],['cache']
Performance,[query] Fix PCanonicalIntervalValue loadStart and loadEnd,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9032:36,load,loadStart,36,https://hail.is,https://github.com/hail-is/hail/pull/9032,2,['load'],"['loadEnd', 'loadStart']"
Performance,[query] Fix performance of parse_locus_interval,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11216:12,perform,performance,12,https://hail.is,https://github.com/hail-is/hail/pull/11216,1,['perform'],['performance']
Performance,[query] Fix subtle hamming performance regression/bug,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8497:27,perform,performance,27,https://hail.is,https://github.com/hail-is/hail/pull/8497,1,['perform'],['performance']
Performance,"[query] Hail uses more than 2.7 gigabytes of RAM to load ~100,000 export VCF results",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13533:52,load,load,52,https://hail.is,https://github.com/hail-is/hail/issues/13533,1,['load'],['load']
Performance,[query] Hana / SEQR need support optimizing Hail Query code,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882:33,optimiz,optimizing,33,https://hail.is,https://github.com/hail-is/hail/issues/13882,1,['optimiz'],['optimizing']
Performance,[query] Improve GVCF/VCF import performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8535:32,perform,performance,32,https://hail.is,https://github.com/hail-is/hail/pull/8535,1,['perform'],['performance']
Performance,[query] Improve `hl.Struct` performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10968:28,perform,performance,28,https://hail.is,https://github.com/hail-is/hail/pull/10968,1,['perform'],['performance']
Performance,[query] Load each missing byte once during struct decoding.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14406:8,Load,Load,8,https://hail.is,https://github.com/hail-is/hail/pull/14406,1,['Load'],['Load']
Performance,"[query] NDArray based PCA Performance Fix. Discard R, persist Q",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10223:26,Perform,Performance,26,https://hail.is,https://github.com/hail-is/hail/pull/10223,1,['Perform'],['Performance']
Performance,[query] Optimize GoogleFS listStatus operation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13390:8,Optimiz,Optimize,8,https://hail.is,https://github.com/hail-is/hail/pull/13390,1,['Optimiz'],['Optimize']
Performance,[query] Parameterize Hail Query Class Loading by ClassLoader,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11212:38,Load,Loading,38,https://hail.is,https://github.com/hail-is/hail/pull/11212,1,['Load'],['Loading']
Performance,[query] Perform a shufflerectomy (remove the shuffle service and code),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10744:8,Perform,Perform,8,https://hail.is,https://github.com/hail-is/hail/pull/10744,1,['Perform'],['Perform']
Performance,[query] Refactor LoweringPipeline so that optimization is a pass,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9030:42,optimiz,optimization,42,https://hail.is,https://github.com/hail-is/hail/pull/9030,1,['optimiz'],['optimization']
Performance,"[query] Remove Region.{load,store}Primitive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10094:23,load,load,23,https://hail.is,https://github.com/hail-is/hail/pull/10094,1,['load'],['load']
Performance,[query] Remove optimize overload,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9800:15,optimiz,optimize,15,https://hail.is,https://github.com/hail-is/hail/pull/9800,1,['optimiz'],['optimize']
Performance,[query] Some new optimizations for StreamLen,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10334:17,optimiz,optimizations,17,https://hail.is,https://github.com/hail-is/hail/pull/10334,1,['optimiz'],['optimizations']
Performance,[query] Use `loadFromNested` rather than matching on fundamental type,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9989:13,load,loadFromNested,13,https://hail.is,https://github.com/hail-is/hail/pull/9989,1,['load'],['loadFromNested']
Performance,[query] Use a larger buffer size in scala FS to reduce request load,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12071:63,load,load,63,https://hail.is,https://github.com/hail-is/hail/pull/12071,1,['load'],['load']
Performance,[query] VDS combiner will not load a successfully completed plan causing combiner to be rerun when a successful run has been completed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14079:30,load,load,30,https://hail.is,https://github.com/hail-is/hail/issues/14079,1,['load'],['load']
Performance,[query] Work around concurrency issues in FASTAReader,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9435:20,concurren,concurrency,20,https://hail.is,https://github.com/hail-is/hail/pull/9435,1,['concurren'],['concurrency']
Performance,[query] add functionality to `hardy_weinberg_test` to perform one-sided test of excess heterozygosity,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10989:54,perform,perform,54,https://hail.is,https://github.com/hail-is/hail/pull/10989,1,['perform'],['perform']
Performance,[query] blanczos_pca dont do extra loading work,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10201:35,load,loading,35,https://hail.is,https://github.com/hail-is/hail/pull/10201,1,['load'],['loading']
Performance,[query] cache RVD specs to avoid lots of file reads,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12086:8,cache,cache,8,https://hail.is,https://github.com/hail-is/hail/pull/12086,1,['cache'],['cache']
Performance,[query] eliminate optimization that can blow RAM,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13619:18,optimiz,optimization,18,https://hail.is,https://github.com/hail-is/hail/pull/13619,1,['optimiz'],['optimization']
Performance,"[query] farewell to `Region.{load,store}IRIntermediate`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10021:29,load,load,29,https://hail.is,https://github.com/hail-is/hail/pull/10021,1,['load'],['load']
Performance,[query] fix loading of reference genomes when GCS object versions are used,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10175:12,load,loading,12,https://hail.is,https://github.com/hail-is/hail/pull/10175,1,['load'],['loading']
Performance,[query] fix performance bug in lir.SimplifyControl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9475:12,perform,performance,12,https://hail.is,https://github.com/hail-is/hail/pull/9475,1,['perform'],['performance']
Performance,[query] load each missingness byte once when decoding a struct,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13811:8,load,load,8,https://hail.is,https://github.com/hail-is/hail/issues/13811,1,['load'],['load']
Performance,[query] minor performance improvement,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13044:14,perform,performance,14,https://hail.is,https://github.com/hail-is/hail/pull/13044,1,['perform'],['performance']
Performance,[query] optimize in CompileAndEvalutate consistently,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9236:8,optimiz,optimize,8,https://hail.is,https://github.com/hail-is/hail/pull/9236,1,['optimiz'],['optimize']
Performance,[query] pca simplification/optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10264:27,optimiz,optimization,27,https://hail.is,https://github.com/hail-is/hail/pull/10264,2,['optimiz'],['optimization']
Performance,[query] substantially reduce single core latency for force-count,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13776:41,latency,latency,41,https://hail.is,https://github.com/hail-is/hail/pull/13776,1,['latency'],['latency']
Performance,[query] synchronize the code cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10309:29,cache,cache,29,https://hail.is,https://github.com/hail-is/hail/pull/10309,1,['cache'],['cache']
Performance,[query] use null (0) pointer as cached node value instead of -1L,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12308:32,cache,cached,32,https://hail.is,https://github.com/hail-is/hail/pull/12308,1,['cache'],['cached']
Performance,[svcr] improve performance of both sample_qc methods,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10727:15,perform,performance,15,https://hail.is,https://github.com/hail-is/hail/pull/10727,1,['perform'],['performance']
Performance,[various] Use load/dump over loads/dumps as appropriate,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9492:14,load,load,14,https://hail.is,https://github.com/hail-is/hail/pull/9492,2,['load'],"['load', 'loads']"
Performance,[vds/combiner] Fix attribute assignment on combiner plan load,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12843:57,load,load,57,https://hail.is,https://github.com/hail-is/hail/pull/12843,1,['load'],['load']
Performance,[wip] perform simple CSE during python IR serialization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6688:6,perform,perform,6,https://hail.is,https://github.com/hail-is/hail/pull/6688,2,['perform'],['perform']
Performance,[www/docs] don't modify url on load,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7385:31,load,load,31,https://hail.is,https://github.com/hail-is/hail/pull/7385,1,['load'],['load']
Performance,[www] cache bust 2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7056:6,cache,cache,6,https://hail.is,https://github.com/hail-is/hail/pull/7056,1,['cache'],['cache']
Performance,[www] cache bust css,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6997:6,cache,cache,6,https://hail.is,https://github.com/hail-is/hail/pull/6997,2,['cache'],['cache']
Performance,"\; 1318 proto.END_COMMAND_PART; 1320 answer = self.gateway_client.send_command(command); -> 1321 return_value = get_return_value(; 1322 answer, self.gateway_client, self.target_id, self.name); 1324 for temp_arg in temp_args:; 1325 temp_arg._detach(). File /private/tmp/hail/hail/python/hail/backend/py4j_backend.py:35, in handle_java_exception.<locals>.deco(*args, **kwargs); 33 tpl = Env.jutils().handleForPython(e.java_exception); 34 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 35 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 36 except pyspark.sql.utils.CapturedException as e:; 37 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 38 'Hail version: %s\n'; 39 'Error summary: %s' % (e.desc, e.stackTrace, hail.__version__, e.desc)) from None. FatalError: ClassCastException: class is.hail.types.virtual.TStruct cannot be cast to class is.hail.types.virtual.TIterable (is.hail.types.virtual.TStruct and is.hail.types.virtual.TIterable are in unnamed module of loader 'app'). Java stack trace:; java.lang.RuntimeException: typ: inference failure:; 	at is.hail.expr.ir.IR.typ(IR.scala:38); 	at is.hail.expr.ir.IR.typ$(IR.scala:33); 	at is.hail.expr.ir.ToStream.typ(IR.scala:300); 	at is.hail.expr.ir.IRParser$.$anonfun$ir_value_expr_1$81(Parser.scala:1111); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_value_ir$1(Parser.scala:2157); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:2153); 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:2157); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_value_ir$2(SparkBackend.scala:691); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.ut",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13699:3416,load,loader,3416,https://hail.is,https://github.com/hail-is/hail/issues/13699,1,['load'],['loader']
Performance,"]; 		at is.hail.backend.service.Worker$.main(Worker.scala:164) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main$.main(Main.scala:14) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 		at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 		at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; 	Caused by: com.google.api.client.http.HttpResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:25179,concurren,concurrent,25179,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Col",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6250,cache,cached,6250,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"_From @cseed on August 26, 2015 14:20_; - fix representation for variant information; - support upstream deletion allele; - normalize on import: left-align, split complex. Should be done concurrently with:; https://github.com/cseed/k3/issues/5. _Copied from original issue: cseed/hail#9_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10:187,concurren,concurrently,187,https://hail.is,https://github.com/hail-is/hail/issues/10,1,['concurren'],['concurrently']
Performance,"_From @cseed on August 26, 2015 14:23_. Requires loading `.ped`/`.fam` files. _Copied from original issue: cseed/hail#10_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11:49,load,loading,49,https://hail.is,https://github.com/hail-is/hail/issues/11,1,['load'],['loading']
Performance,"_From @cseed on August 26, 2015 14:51_. Waiting on suitable machines (Intel spark cluster, cloud access, etc.); - measure size of stored data; - compressed vs uncompressed (gzip parquet, lz4 in SparkyVSM, etc.); - compute cost (or at least compute-hrs); - compare best-case (e.g. `gzip -cd file.vcf.gz | wc -l` vs `LoadVCF`). _Copied from original issue: cseed/hail#18_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/19:315,Load,LoadVCF,315,https://hail.is,https://github.com/hail-is/hail/issues/19,1,['Load'],['LoadVCF']
Performance,"_From @tpoterba on October 23, 2015 17:9_. Array data only includes a genotype call, so >80% of the memory and unpacking cpu for genotypes is wasted on missing data. We could implement multiple types of Genotype instances, if there is a way to do this without sequencing performance suffering. _Copied from original issue: cseed/hail#91_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/55:271,perform,performance,271,https://hail.is,https://github.com/hail-is/hail/issues/55,1,['perform'],['performance']
Performance,"__call__(self, *args); 1302 ; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: UnsupportedFileSystemException: No FileSystem for scheme ""gs"". Java stack trace:; org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""gs""; 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.isDir(FS.scala:175); 	at is.hail.io.fs.FS.isDir$(FS.scala:173); 	at is.hail.io.fs.HadoopFS.isDir(HadoopFS.scala:70); 	at is.hail.expr.ir.RelationalSpec$.readMetadata(AbstractMatrixTableSpec.scala:30); 	at is.hail.expr.ir.RelationalSpec$.readReferences(AbstractMatrixTableSpec.scala:68); 	at is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:596); 	at is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10530:3630,Cache,Cache,3630,https://hail.is,https://github.com/hail-is/hail/issues/10530,1,['Cache'],['Cache']
Performance,"_catch:. /opt/conda/lib/python3.7/site-packages/hail/__init__.py in <module>; 32 # E402 module level import not at top of file; ---> 33 from .table import Table, GroupedTable, asc, desc # noqa: E402; 34 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402. /opt/conda/lib/python3.7/site-packages/hail/table.py in <module>; 4 import numpy as np; ----> 5 import pyspark; 6 from typing import Optional, Dict, Callable, Sequence. ModuleNotFoundError: No module named 'pyspark'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); /tmp/ipykernel_233/4275665471.py in <module>; ----> 1 combined_pandas = pd.read_pickle(gwas_pandas_file). /opt/conda/lib/python3.7/site-packages/pandas/io/pickle.py in read_pickle(filepath_or_buffer, compression, storage_options); 220 # ""No module named 'pandas.core.sparse.series'""; 221 # ""Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib""; --> 222 return pc.load(handles.handle, encoding=None); 223 except UnicodeDecodeError:; 224 # e.g. can occur for files written in py27; see GH#28645 and GH#31988. /opt/conda/lib/python3.7/site-packages/pandas/compat/pickle_compat.py in load(fh, encoding, is_verbose); 272 up.is_verbose = is_verbose; 273 ; --> 274 return up.load(); 275 except (ValueError, TypeError):; 276 raise. /opt/conda/lib/python3.7/pickle.py in load(self); 1086 raise EOFError; 1087 assert isinstance(key, bytes_types); -> 1088 dispatch[key[0]](self); 1089 except _Stop as stopinst:; 1090 return stopinst.value. /opt/conda/lib/python3.7/pickle.py in load_stack_global(self); 1383 if type(name) is not str or type(module) is not str:; 1384 raise UnpicklingError(""STACK_GLOBAL requires str""); -> 1385 self.append(self.find_class(module, name)); 1386 dispatch[STACK_GLOBAL[0]] = load_stack_global; 1387 . /opt/conda/lib/python3.7/site-packages/pandas/compat/pickle_compat.py in find_class(self, module, name); 204 key = (module, name); 205 module, name = _",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14004:2711,load,load,2711,https://hail.is,https://github.com/hail-is/hail/issues/14004,1,['load'],['load']
Performance,"_per_month); ```. This works out to 143 USD to run a 10,000 VM cluster 24 hours a day for 30 days. I suspect our average VM count in a month is closer to 10 which is within the free tier (340 MiB). I; might be wrong abou the connections per vm per aggregation interval, but this is straightforward to; monitor once we have the logs. For a sense of the cost landscape, these are all free:. 1. 1000 VMs.; 2. 500 VMs, with a sampling rate of 1.; 3. 200 VMs, with a sampling rate of 1, with an interval of 5 minutes.; 4. 10 VMs, with a sampling rate of 1, with an interval of 30 seconds. It's all linear, so if we need to halve the interval we can either change the sampling rate, reasses; our expected number of VM-hours, or adjust the service fee accordingly. We can also assess the landscape of fees necessary to cover costs (ignoring the free 50 GiB):. 1. 15 minute intervals, 0.5 sampling rate, 100 expected connections per vm per interval: 0.0000008; USD per core per hour. 2. 30 second intervals, 1.0 sampling rate, 100 expected connections per vm per interval: 0.00005 USD; per core per hour. 2. 5 second intervals, 1.0 sampling rate, 100 expected connections per vm per interval: 0.0003 USD; per core per hour. 2. 5 second intervals, 1.0 sampling rate, 1000 expected connections per vm per interval (1000 unique; connections per second honestly seems to me quite remarkable performance): 0.003 USD per core per; hour. ```; USD_per_core_per_hour = bytes_per_hour / vms / 1024. / 1024 / 1024 * 0.5 / 16. print(USD_per_core_per_hour); ```. ---. # Conclusion. I think we're safe to enable this with the parameters in this PR (15 minute intervals, 50%; sampling). We can assess unknown parameters, like connections per vm, and get comfortable looking at; these logs. Security constraints or observability demands may push us towards desiring more logs. If that; occurs, we can assess the need for a new fee. Regardless, this fee appears to be small relative to; the current cost of preemptible cores.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12883:4666,perform,performance,4666,https://hail.is,https://github.com/hail-is/hail/pull/12883,1,['perform'],['performance']
Performance,"_required=False); ```. ---. ### What happened?. https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/checkpoint.20with.20missing.20fields. ```; is.hail.utils.HailException: gs://jn-vcf-cleanup-central1/McCarroll-Macosko-UM1-BICAN-Express-WGS-2023-0626/McCarroll-Macosko-UM1-BICAN-Express-WGS-2023-0626.vcf.gz:offset 1344376382: error while parsing line; chr1	10403	.	ACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	A,ACCCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	.	LowQual	AC=1,1;AF=0.250,0.250;AN=4;AS_QUALapprox=0|23|45;AS_VQSLOD=.,.;AS_YNG=.,.;QUALapprox=45	GT:AD:GQ:RGQ	./.	0/1:23,7,0:20:23	./.	./.	./.	0/2:6,0,4:35:45	./.	./.	./.	./.	./.	./.	./.	./.	./.	./.	./.	./. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:21); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:21); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1934); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1922); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C2005collect_distributed_array_matrix_native_writer.apply_region1_27(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:52); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:51); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:751); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13346:1142,Load,LoadVCF,1142,https://hail.is,https://github.com/hail-is/hail/issues/13346,1,['Load'],['LoadVCF']
Performance,"_rows(mt.locus < hl.Locus('1', 1)).show(); ```. Output:; ```; 2019-06-24 19:12:05 WARN Utils:66 - Your hostname, wp086-661 resolves to a loopback address: 127.0.1.1; using 10.1.8.50 instead (on interface wlp2s0); 2019-06-24 19:12:05 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2019-06-24 19:12:06 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/backend/backend.py"", line 108, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/BROAD.M",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:1139,cache,cache,1139,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['cache'],['cache']
Performance,"`. activate NAME` might silently fail if `NAME` does not exist or `conda` is not configured. `. ./loadconda` tries to find conda in a variety of places and configure it (meaning source `conda.sh`). After this, `conda activate NAME` will work correctly. ---. This is already in my batch dag PR, but that's getting bogged down in test issues, and this is blocking @akotlar 's https://github.com/hail-is/hail/pull/5065 PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5066:98,load,loadconda,98,https://hail.is,https://github.com/hail-is/hail/pull/5066,1,['load'],['loadconda']
Performance,`NetworkAllocator.reserve` creates a bunch of network namespaces on startup and adds them to an `asyncio.Queue`. There's no reason to wait for all of them before accepting jobs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11251:105,Queue,Queue,105,https://hail.is,https://github.com/hail-is/hail/pull/11251,1,['Queue'],['Queue']
Performance,"`Table._select` got way too complicated (mostly my fault) when key changing moved from `TableMapRows` to `TableKeyBy`. Making `_select` a simple wrapper around `TableMapRows`, and moving the key logic to `key_by`, made both way simpler. I then realized the `key_by` code could be even simpler by adding some rules to the optimizer to clean up the case where all new keys are existing fields. I actually think some things had gotten broken in the old `_select` (performance wise). In particular, in `split_multi`, in the `split_rows` function with `rekey=false`, I think it's supposed to extend the key from `['locus']` to `['locus', 'alleles']`, but that wasn't happening. I changed `key_by` to no longer accept `key_by(None)` or `key_by([])`, both of which should now be `key_by()`, which is more consistent with the rest of our interface, but is a breaking change. Is it worth the disruption? Should I add a warning? Or just continue to accept both?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4455:321,optimiz,optimizer,321,https://hail.is,https://github.com/hail-is/hail/pull/4455,2,"['optimiz', 'perform']","['optimizer', 'performance']"
Performance,"```; In [1]: import hail as hl; In [2]: hl.hadoop_ls(""gs://bw2/bla""); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-3-ca1bc15ebb3c> in <module>; ----> 1 hl.hadoop_ls(""gs://bw2/bla""). /Library/Python/3.7/site-packages/hail/utils/hadoop_utils.py in hadoop_ls(path); 212 :obj:`list` [:obj:`dict`]; 213 """"""; --> 214 return Env.fs().ls(path); 215; 216. /Library/Python/3.7/site-packages/hail/fs/hadoop_fs.py in ls(self, path); 40; 41 def ls(self, path: str) -> List[Dict]:; ---> 42 return json.loads(self._utils_package_object.ls(self._jfs, path)); 43; 44 def mkdir(self, path: str) -> None:. /Library/Python/3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258; 1259 for temp_arg in temp_args:. /Library/Python/3.7/site-packages/hail/backend/spark_backend.py in deco(*args, **kwargs); 49 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 50 'Hail version: %s\n'; ---> 51 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None; 52 except pyspark.sql.utils.CapturedException as e:; 53 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NullPointerException: null. Java stack trace:; java.lang.NullPointerException: null; 	at scala.collection.mutable.ArrayOps$ofRef$.length$extension(ArrayOps.scala:192); 	at scala.collection.mutable.ArrayOps$ofRef.length(ArrayOps.scala:192); 	at scala.collection.SeqLike$class.size(SeqLike.scala:106); 	at scala.collection.mutable.ArrayOps$ofRef.size(ArrayOps.scala:186); 	at scala.collection.mutable.Builder$class.sizeHint(Builder.scala:69); 	at scala.collection.mutable.ArrayBuilder.sizeHint(ArrayBuilder.scala:22); 	at scala.collection.TraversableLike$class.builder$1(TraversableLike.scala:230); 	at scala.collection.TraversableLike$class.map(TraversableLike",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9600:581,load,loads,581,https://hail.is,https://github.com/hail-is/hail/issues/9600,1,['load'],['loads']
Performance,"```; Lots of local cleanups.; Use `' for quoting inside error messages.; Added SuperCommand, ToplevelCommands. Use for annotatevariants and annotatesamples.; Try to make multiple instances of (essentially) same error message consistent.; Added option to LoadVCF, ImportVCF to skip genotypes. Use in importannotations.; Fixed some bugs in FatalException handling.; Moved Type.parse to trait Parsable.; Added Parser.parseAnnotationTypes.; Added `type_expr' non-terminal to Parser.; Prefer `import ...expr._' to `import ...expr' and `expr.Foo'.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/228:254,Load,LoadVCF,254,https://hail.is,https://github.com/hail-is/hail/pull/228,1,['Load'],['LoadVCF']
Performance,"```; Traceback (most recent call last):; File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/sandbox.py"", line 17, in <module>; pprint(hc.read('%s/variantqc/exacv2_rf.vds' % root, sites_only=True).filter_variants_intervals('gs://exac2/temp').head()); File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/utils.py"", line 201, in head; return json.loads(self.variants_keytable().to_dataframe().toJSON().first()); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1328, in first; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1310, in take; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py"", line 933, in runJob; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 20022, exac-sw-3pdd.c.broad-mpg-gnomad.internal): java.lang.ClassCastException: scala.Tuple2 cannot be cast to org.apache.spark.sql.Row; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1311:337,load,loads,337,https://hail.is,https://github.com/hail-is/hail/issues/1311,1,['load'],['loads']
Performance,"```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8201:873,load,loads,873,https://hail.is,https://github.com/hail-is/hail/issues/8201,1,['load'],['loads']
Performance,"```; gsutil cat gs://hail-ci-0-1/deploy/ef349a51016f\*/job-log; ```. the last few lines:. ```; + make push-batch; docker build -t batch .; time=""2018-09-26T00:14:20Z"" level=error msg=""failed to dial gRPC: cannot connect to the Docker daemon. Is 'docker daemon' running on this host?: dial unix /var/run/docker.sock: connect: permission denied""; Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.38/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&session=vhnl6wchhs00sgt8raa35j7m7&shmsize=0&t=batch&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied; time=""2018-09-26T00:14:20Z"" level=error msg=""Can't add file /hail/repo/batch/batch/server.py to tar: io: read/write on closed pipe""; Makefile:14: recipe for target 'build-batch' failed; make: *** [build-batch] Error 1; ```. this is failing all deploys of hail, which is safe, but it prevents our users from getting updates.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4443:519,cache,cachefrom,519,https://hail.is,https://github.com/hail-is/hail/issues/4443,1,['cache'],['cachefrom']
Performance,"```; import hail as hl; ds = hl.balding_nichols_model(3, 100, 100); ds.annotate_globals(x=[1,2,3]); ```; The above script breaks on devel clusters.; ```; py4j.protocol.Py4JJavaError: An error occurred while calling o64.annotateGlobalExpr.; : java.lang.NoClassDefFoundError: is/hail/asm4s/AsmFunction2; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:763); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:642); 	at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:174); 	at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:170); 	at is.hail.asm4s.package$.loadClass(package.scala:181); 	at is.hail.asm4s.FunctionBuilder$$anon$1.apply(FunctionBuilder.scala:312); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.expr.Parser$$anonfun$12$$anonfun$apply$6.apply(Parser.scala:172); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2982:599,load,loadOrDefineClass,599,https://hail.is,https://github.com/hail-is/hail/issues/2982,2,['load'],"['loadClass', 'loadOrDefineClass']"
Performance,"```; is.hail.utils.HailException: hybrid.m37m.vcf.bgz: caught htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; offending line: 3	60830534	.	M	C	40	.	.	GT:AD	1/1:0,40; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:459,Load,LoadVCF,459,https://hail.is,https://github.com/hail-is/hail/issues/3015,2,['Load'],['LoadVCF']
Performance,"```pycon; In [1]: import hail as hl. In [2]: hl.init(); 2022-03-11 14:49:23 WARN Utils:69 - Your hostname, metis resolves to a loopback address: 127.0.0.1; using 192.168.1.169 instead (on interface eth0); 2022-03-11 14:49:23 WARN Utils:69 - Set SPARK_LOCAL_IP if you need to bind to another address; 2022-03-11 14:49:23 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.1.2; SparkUI available at http://192.168.1.169:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.90-92e40ce648a8; LOGGING: writing to /home/cdv/src/hail/hail/hail-20220311-1449-0.2.90-92e40ce648a8.log. In [3]: mt = hl.import_vcf('src/test/resources/sample.vcf').filter_rows(False). In [4]: ht = mt._localize_entries('entries', 'columns'). In [5]: groups = ht.group_by(the_key=ht.key).aggregate(value=hl.agg.collect(ht.row_value)).collect(); 2022-03-11 14:50:08 Hail: INFO: Coerced sorted dataset; 2022-03-11 14:50:10 Hail: INFO: Ordering unsorted dataset with network shuffle1]. In [6]: len(groups); Out[6]: 346. In [7]: mt = mt.checkpoint('~/tmp/hail/sample.vcf.filtered.mt'); 2022-03-11 14:51:14 Hail: INFO: wrote matrix table with 0 rows and 100 columns in 0 partitions to ~/tmp/hail/sample.vcf.filtered.mt. In [8]: ht = mt._localize_entries('entries', 'columns'). In [9]: groups_native = ht.group_by(the_key=ht.key).aggregate(value=hl.agg.collect(ht.row_value)).collect(). In [10]: len(groups_native); Out[10]: 0; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11562:357,load,load,357,https://hail.is,https://github.com/hail-is/hail/issues/11562,1,['load'],['load']
Performance,"```python; import hail as hl; hl.init(); hl.utils.get_1kg('data'); mt = hl.read_matrix_table('data/1kg.mt'); mt.entries().show(10); df = mt.entries().to_pandas(); ```. ```; Hail version: 0.2.18-08ec699f0fd4; Error summary: HailException: optimization changed type!; before: Table{global:Struct{},key:[],row:Struct{`locus.contig`:String,`locus.position`:Int32,alleles:Array[String],rsid:String,qual:Float64,filters:Array[String],`info.AC`:Array[Int32],`info.AF`:Array[Float64],`info.AN`:Int32,`info.BaseQRankSum`:Float64,`info.ClippingRankSum`:Float64,`info.DP`:Int32,`info.DS`:Boolean,`info.FS`:Float64,`info.HaplotypeScore`:Float64,`info.InbreedingCoeff`:Float64,`info.MLEAC`:Array[Int32],`info.MLEAF`:Array[Float64],`info.MQ`:Float64,`info.MQ0`:Int32,`info.MQRankSum`:Float64,`info.QD`:Float64,`info.ReadPosRankSum`:Float64,`info.set`:String,s:String,`GT.alleles`:Array[Int32],`GT.phased`:Boolean,AD:Array[Int32],DP:Int32,GQ:Int32,PL:Array[Int32]}}; after: Table{global:Struct{},key:[],row:Struct{`locus.contig`:String,`locus.position`:Int32,alleles:Array[String],rsid:String,qual:Float64,filters:Array[String],`info.AC`:Array[Int32],`info.AF`:Array[Float64],`info.AN`:Int32,`info.BaseQRankSum`:Float64,`info.ClippingRankSum`:Float64,`info.DP`:Int32,`info.DS`:Boolean,`info.FS`:Float64,`info.HaplotypeScore`:Float64,`info.InbreedingCoeff`:Float64,`info.MLEAC`:Array[Int32],`info.MLEAF`:Array[Float64],`info.MQ`:Float64,`info.MQ0`:Int32,`info.MQRankSum`:Float64,`info.QD`:Float64,`info.ReadPosRankSum`:Float64,`info.set`:String,s:String,`GT.alleles`:Array[Int32],`GT.phased`:Boolean,AD:Array[+Int32],DP:Int32,GQ:Int32,PL:Array[+Int32]}}; ```. Randomly assigned @catoverdrive, cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6766:238,optimiz,optimization,238,https://hail.is,https://github.com/hail-is/hail/issues/6766,1,['optimiz'],['optimization']
Performance,"`builder.loadFields` just loads the offset of the data array, but it needs to do a deep copy otherwise all of the array aggragators will start off writing to (and overwriting) the same arraybuilder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6819:9,load,loadFields,9,https://hail.is,https://github.com/hail-is/hail/pull/6819,2,['load'],"['loadFields', 'loads']"
Performance,`f` is a thunk so it is currently being evaluated thrice before inserted into the code cache. The `compiledFunction` variable was unused so I think this is what was originally intended.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13796:87,cache,cache,87,https://hail.is,https://github.com/hail-is/hail/pull/13796,1,['cache'],['cache']
Performance,"`hl.balding_nichols_model` generates a MatrixTable representing a genetic dataset randomly drawn according to the Balding Nichols model.; ```; In [5]: hl.balding_nichols_model(2,3,3).show() ; 2019-08-15 10:38:05 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 3 samples, and 3 variants...; +---------------+------------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT |; +---------------+------------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call |; +---------------+------------+------+------+------+; | 1:1 | [""A"",""C""] | 0/0 | 0/0 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 1/1 | 1/1 |; | 1:3 | [""A"",""C""] | 1/1 | 0/1 | 0/0 |; +---------------+------------+------+------+------+. ```; These MatrixTables are useful both as examples and test datasets for genetics-related Hail code. Unfortunately, the loci are chosen sequentially starting with chromosome 1, position 1. This region of chromosome 1 is in the telomere. Many genetic annotations contain no information in this region. As a result, `hl.balding_nichols_model` is not useful when demonstrating the annotation database or third-party genetic annotations. We want to enhance `hl.balding_nichols_model` to select variants (loci-allele-array pairs) that are likely to appear in real genetic datasets. One very simple model would be to draw variants according to their alternate/minor allele frequency in the gnomAD or 1000 Genomes datasets. An additional improvement would be to generate chromosomes roughly proportionally to their true sizes. These changes should not significantly slow down the method. We may want to include a small dataset of allele frequencies with Hail for use when the requested number of variants is small, only loading the full gnomAD or 1000 Genomes allele frequencies when the requested number of variants is in the millions or tens of millions. This functionality should be enabled and disabled by a parameter to `hl.balding_nichols_model`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6880:1755,load,loading,1755,https://hail.is,https://github.com/hail-is/hail/issues/6880,1,['load'],['loading']
Performance,"`import_bgen` fails because there are no reference genomes on worker nodes. `import_bgen` needs to read the index file. Reading the index file means parsing a type. Parsing a locus type means looking up a reference genome. The error message comes from line 588 in `ReferenceGenome.scala` by way of line 70 of `IndexReader.scala`:; ```scala; val keyType = IRParser.parseType(metadata.keyType); ```. The root cause seems to be #5512, in which we [stop loading the genomes from resources](https://github.com/hail-is/hail/pull/5512/files#diff-16c24a9c4265932816e9e88806f5a2abL527).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5673:450,load,loading,450,https://hail.is,https://github.com/hail-is/hail/issues/5673,1,['load'],['loading']
Performance,"`strides` shouldn't be a public field on `PNDArray` interface, just an internal detail of `PCanonicalNDArray`. . `dimensionLength` was just loading the shape, should never have been its own method. There's a lot more interface clean up to do, these two were just easy",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9783:140,load,loading,140,https://hail.is,https://github.com/hail-is/hail/pull/9783,1,['load'],['loading']
Performance,"`tls.py` has many different functions with long names. This change reduces it to three functions:. - internal_server_ssl_context; - internal_client_ssl_context; - external_client_ssl_context. I also added `httpx.py` which contains the HTTPS-related functions that `tls.py` previously; contained. I also simplified the HTTPS-related functions to just:. - client_session; - blocking_client_session. I determine internal vs. external using the deploy config. ---. An [`ssl.SSLContext`](https://docs.python.org/3/library/ssl.html#ssl.SSLContext) defines how a; network library (such as `aiohttp`) should perform SSL/TLS. Let's look at an example:. ```python3; server_ssl_context = ssl.create_default_context(; purpose=Purpose.CLIENT_AUTH,; cafile='/incoming.cacerts'); server_ssl_context.load_cert_chain(ssl_config['cert'],; keyfile=ssl_config['key'],; password=None); server_ssl_context.verify_mode = ssl.CERT_OPTIONAL; server_ssl_context.check_hostname = False; ```. The first function call states that we are a *server* performing *client; authentication* (`Purpose.CLIENT_AUTH`). We also state that anyone who sends requests to us will be; identified by a certificate that is trusted by our certificate database: `/incoming.cacerts` (which; is a file). `load_cert_chain` states where to find the certificates and secret key that prove who we are. The; certificate and secret key together are like a property title that proves someone owns a house. The; `password=None` means that our secret key has no password. Some keys are themselves locked by a; password. `verify_mode` means what do we expect our clients to have. `CERT_OPTIONAL` means anonymous clients; are OK. This is how servers normally operate (https://google.com does not care who you are). `check_hostname` means should we verify that the client certificate matches the client's; hostname. Since we allow anonymous clients, this must be `False`. ---. `test-address.py` is a gross hack. It will disappear in subsequent PRs. For now, I push",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9862:600,perform,perform,600,https://hail.is,https://github.com/hail-is/hail/pull/9862,1,['perform'],['perform']
Performance,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6248:685,load,load,685,https://hail.is,https://github.com/hail-is/hail/pull/6248,2,['load'],"['load', 'load-main-class']"
Performance,a.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:4592,Load,LoadVCF,4592,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,a.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.for,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:11405,Load,LoadVCF,11405,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,a.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Back,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8338:11830,Optimiz,Optimize,11830,https://hail.is,https://github.com/hail-is/hail/issues/8338,1,['Optimiz'],['Optimize']
Performance,"a:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.sca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13180,Load,LoadVCF,13180,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,a:214); 	at java.lang.Thread.run(Thread.java:748)com.esotericsoftware.kryo.KryoException: sun.reflect.generics.reflectiveObjects.NotImplementedException; Serialization trace:; m (is.hail.annotations.aggregators.KeyedRegionValueAggregator); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)sun.reflect.generics.reflectiveObjects.NotImplementedException: null; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.Obje,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:6232,concurren,concurrent,6232,https://hail.is,https://github.com/hail-is/hail/issues/4215,1,['concurren'],['concurrent']
Performance,a:422); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:5925,concurren,concurrent,5925,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['concurren'],['concurrent']
Performance,"a:422); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.12-13681278eb89; Error summary: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary sea",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:12493,concurren,concurrent,12493,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['concurren'],['concurrent']
Performance,a:475); 	at is.hail.methods.CalculateConcordance$.apply(CalculateConcordance.scala:108); 	at is.hail.methods.CalculateConcordance.apply(CalculateConcordance.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:7826,load,loadInt,7826,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['load'],['loadInt']
Performance,a:90); 	at is.hail.backend.service.Worker$.$anonfun$main$9(Worker.scala:172); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:171); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.service.Worker$.main(Worker.scala:169); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.lang.NullPointerException: null; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionPutTask.call(JsonResumableSessionPutTask.java:201); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.lambda$put$0(JsonResumableSession.java:81); 	at is.hail.relocated.com.google.cloud.storage.Retrying.lambda$run$0(Retrying.java:102); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:99); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.put(JsonResumableSession.java:68); 	at is,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13937:5591,concurren,concurrent,5591,https://hail.is,https://github.com/hail-is/hail/issues/13937,1,['concurren'],['concurrent']
Performance,"a_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Basically, the ; ```; hl.utils.get_1kg('data/'); ```; ![image](https://user-images.githubusercontent.com/10011",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775:2688,Load,LoadVCF,2688,https://hail.is,https://github.com/hail-is/hail/issues/4775,1,['Load'],['LoadVCF']
Performance,"ab2e4""><code>e85d865</code></a> Simplify &quot;update order&quot; consistency test</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/7dc426c95a0c329d5514e6198d92080f1ffc1e5e""><code>7dc426c</code></a> Fix update() ordering to be more consistent with add() ordering (<a href=""https://github-redirect.dependabot.com/grantjenks/python-sortedcontainers/issues/159"">#159</a>)</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/13d30bc654eb9e6be092282ca502967fcb7f0113""><code>13d30bc</code></a> Bump version to 2.2.2</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/4997d0e849f2275d1931772a5432163ecc20e0b0""><code>4997d0e</code></a> Refactor small slice optimization in SortedList.<strong>getitem</strong></li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/6ee5d57fc8d691fbab4972b853a60348d0f922ef""><code>6ee5d57</code></a> improve SortedList.<strong>getitem</strong>() performance for small slices</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/ac80254fb6a08045ced7d9704412878ff8000fa7""><code>ac80254</code></a> suppress warning in test of deprecated function (<a href=""https://github-redirect.dependabot.com/grantjenks/python-sortedcontainers/issues/118"">#118</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/grantjenks/python-sortedcontainers/compare/v2.1.0...v2.4.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sortedcontainers&package-manager=pip&previous-version=2.1.0&new-version=2.4.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (depen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11476:3262,perform,performance,3262,https://hail.is,https://github.com/hail-is/hail/pull/11476,1,['perform'],['performance']
Performance,"able(10).collect(); Initializing Hail with default parameters...; /Users/dking/projects/hail/hail/python/hail/utils/java.py:54: UserWarning: When using the query service backend, use `await Env._async_hc()'; warnings.warn('When using the query service backend, use `await Env._async_hc()\''); Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.81-edcde5c1b324; LOGGING: writing to /Users/dking/projects/hail/query/hail-20220113-1844-0.2.81-edcde5c1b324.log; Out[1]: ; [Struct(idx=0),; Struct(idx=1),; Struct(idx=2),; Struct(idx=3),; Struct(idx=4),; Struct(idx=5),; Struct(idx=6),; Struct(idx=7),; Struct(idx=8),; Struct(idx=9)]; ```. The very first time you execute this comand, it will run four batches to generate the four default; reference genomes and download those to your local machine. Those four reference genomes are cached; per-SHA on your local machine, so future Hail pipelines will have lower latency. If you have the ""standing working"" enabled, you should expect a latency of ~8s for the above job. I; think we can approximately halve this by re-using classloaders. Ask me more about this. After running this job a few times, the Batch UI looks like this:. ![Screen Shot 2022-01-14 at 11 59 41 AM](https://user-images.githubusercontent.com/106194/149554866-221e4243-1238-4d01-8944-0a6ed0b4c28c.png). When you call `collect`, the Python-side `ServiceBackend` writes a file to cloud storage containing; the temporary directory, billing project, and the IR. In addition to the `execute` command used for; `collect`, there are command for getting the table type and references; genomes. `ServiceBackend._rpc` defines this API on the Python-side and; `is.hail.backend.service.ServiceBackendSocketAPI2` defines this API on the Scala-side. After writing to cloud storage, the ServiceBackend submits the one-job driver batch to Hail; Batch. It then waits for the driver job to complete. When the driver job is finished, it reads the; outputs from google clo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194:1886,latency,latency,1886,https://hail.is,https://github.com/hail-is/hail/pull/11194,1,['latency'],['latency']
Performance,"ache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13206,Load,LoadVCF,13206,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,ache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). 	Java stack trace:; 	java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 			at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 			at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 			at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 			at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); 			at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkB,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:5935,concurren,concurrent,5935,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['concurren'],['concurrent']
Performance,ache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:17855,concurren,concurrent,17855,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['concurren'],['concurrent']
Performance,ache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.D,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:10600,concurren,concurrent,10600,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['concurren'],['concurrent']
Performance,"ack trace):; ```; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181114-1827-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.Heartbe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:1785,load,loadClass,1785,https://hail.is,https://github.com/hail-is/hail/issues/4780,1,['load'],['loadClass']
Performance,"ackages/batch/front_end/front_end.py"", line 112, in _get_job_log; job_log = await job._read_logs(); File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 57, in _read_logs; return {k: v for k, v in await future_logs}; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 52, in _read_log_from_gcs; pod_log = await self.app['log_store'].read_gs_file(LogStore.container_log_path(self.directory, task_name)); File ""/usr/local/lib/python3.6/dist-packages/batch/log_store.py"", line 40, in read_gs_file; return await self.gcs.read_gs_file(uri); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 27, in read_gs_file; return await self._wrapped_read_gs_file(self, uri); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 37, in wrapped; **kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 53, in _read_gs_file; content = f.download_as_string(); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 697, in download_as_string; self.download_to_file(string_buffer, client=client, start=start, end=end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 638, in download_to_file; _raise_from_invalid_response(exc); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 2034, in _raise_from_invalid_response; raise exceptions.from_http_status(response.status_code, message, response=response); google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/download/storage/v1/b/hail-batch2-nru9x/o/cd50b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7412:4643,concurren,concurrent,4643,https://hail.is,https://github.com/hail-is/hail/pull/7412,1,['concurren'],['concurrent']
Performance,ackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.115-71fc978b5c22; Error summary: SocketException: Connection reset. -------------------. Some more content from the failing worker job:. ...; 2023-05-04 01:04:35.959 : INFO: executing D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e61c-4fa9-9419-2278528f3c86; 2023-05-04 01:04:37.559 : INFO: TaskReport: stage,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:21594,concurren,concurrent,21594,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['concurren'],['concurrent']
Performance,ackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:464); 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:237); 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190); 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109); 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1400); 	at sun.security.ssl.SSLSocketImpl.readApplicat,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12982:13880,concurren,concurrent,13880,https://hail.is,https://github.com/hail-is/hail/issues/12982,3,['concurren'],['concurrent']
Performance,"acy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the tru",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:4554,load,load,4554,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['load'],['load']
Performance,"adBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v localhost:32029; * Trying 127.0.0.1...; * TCP_NODELAY set; * Connected to localhost (127.0.0.1) port 32029 (#0); > GET / HTTP/1.1; > Host: localhost:32029; > User-Agent: curl/7.64.1; > Accept: */*; >; < HTTP/1.1 200 OK; < Content-Type: application/json; < Date: Wed, 05 Feb 2020 20:59:27 GMT; < Content-Length: 88; <; {; 	""service"": {; 		""namespace"": ""default"",; 		""name"": ""gateway""; 	},; 	""localEndpoints"": 1; }; ```; ```; }dking@gke-vdc-non-preemptible-pool-5-80798769-kp8n ~ $ curl -v localhost:32029; * Trying 127.0.0.1...; * TCP_NODELAY set; * Connected to localhost (127.0.0.1) port 32029 (#0); > GET / HTTP/1.1; > Host: localhost:32029; > User-Agent: curl/7.64.1; > Accept: */*; >; < HTTP/1.1 503 Service Unavailable; < Conte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8045:1747,load,loadbalancer,1747,https://hail.is,https://github.com/hail-is/hail/pull/8045,1,['load'],['loadbalancer']
Performance,add instructions on how to load `code_style.xml`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/820:27,load,load,27,https://hail.is,https://github.com/hail-is/hail/pull/820,1,['load'],['load']
Performance,add isDeterministic flag to IRFunction to help with optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3430:52,optimiz,optimization,52,https://hail.is,https://github.com/hail-is/hail/pull/3430,1,['optimiz'],['optimization']
Performance,"ading a new JAR. You can interact at ipython like this:. ```; In [1]: import hail as hl; ...: hl.utils.range_table(10).collect(); Initializing Hail with default parameters...; /Users/dking/projects/hail/hail/python/hail/utils/java.py:54: UserWarning: When using the query service backend, use `await Env._async_hc()'; warnings.warn('When using the query service backend, use `await Env._async_hc()\''); Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.81-edcde5c1b324; LOGGING: writing to /Users/dking/projects/hail/query/hail-20220113-1844-0.2.81-edcde5c1b324.log; Out[1]: ; [Struct(idx=0),; Struct(idx=1),; Struct(idx=2),; Struct(idx=3),; Struct(idx=4),; Struct(idx=5),; Struct(idx=6),; Struct(idx=7),; Struct(idx=8),; Struct(idx=9)]; ```. The very first time you execute this comand, it will run four batches to generate the four default; reference genomes and download those to your local machine. Those four reference genomes are cached; per-SHA on your local machine, so future Hail pipelines will have lower latency. If you have the ""standing working"" enabled, you should expect a latency of ~8s for the above job. I; think we can approximately halve this by re-using classloaders. Ask me more about this. After running this job a few times, the Batch UI looks like this:. ![Screen Shot 2022-01-14 at 11 59 41 AM](https://user-images.githubusercontent.com/106194/149554866-221e4243-1238-4d01-8944-0a6ed0b4c28c.png). When you call `collect`, the Python-side `ServiceBackend` writes a file to cloud storage containing; the temporary directory, billing project, and the IR. In addition to the `execute` command used for; `collect`, there are command for getting the table type and references; genomes. `ServiceBackend._rpc` defines this API on the Python-side and; `is.hail.backend.service.ServiceBackendSocketAPI2` defines this API on the Scala-side. After writing to cloud storage, the ServiceBackend submits the one-job driver batch to Hail; Batch. It the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194:1733,cache,cached,1733,https://hail.is,https://github.com/hail-is/hail/pull/11194,2,"['cache', 'latency']","['cached', 'latency']"
Performance,"ae-afc6-326f710d9a89; 2023-09-24 01:58:24.305 GoogleStorageFS$: INFO: close: gs://aou_tmp/tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89; 2023-09-24 01:58:51.513 : INFO: TaskReport: stage=0, partition=9571, attempt=0, peakBytes=4507648, peakBytesReadable=4.30 MiB, chunks requested=51, cache hits=0; 2023-09-24 01:58:51.513 : INFO: RegionPool: FREE: 4.3M allocated (2.2M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:51.515 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/aou_tmp/o?name=tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89&uploadType=resumable&upload_id=ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtA",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721:4222,concurren,concurrent,4222,https://hail.is,https://github.com/hail-is/hail/issues/13721,1,['concurren'],['concurrent']
Performance,age.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:808); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:807); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:807); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:804); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:804); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3041:4892,concurren,concurrent,4892,https://hail.is,https://github.com/hail-is/hail/issues/3041,2,['concurren'],['concurrent']
Performance,ageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed. /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:477: FatalError; ------------------------------ Captured log call -------------------------------; INFO batch_client.aioclient:aioclient.py:753 created batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:22916,concurren,concurrent,22916,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,ageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed.; ```. ### Version. 0.2.115-f6017673dbb6. ### Relevant log output. ```shell; ________________________________ test_spectra_4 ________________________________; [gw2] linux -- Python 3.8.10 /usr/bin/python3. def test_spectra_4():; > spectra_helper(spec4). test/hail/methods/test_pca.py:229: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/hail/methods/test_pca.py:172: in spectra_helper; hail_V = (np.array(scores.scores.collect()) / singulars).T; <decorator-gen-538>:2: in collect; ???; /usr/local/lib/python3.8/dist-packages/hail/typecheck/check.py:584: in wrapper; return __original_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:9298,concurren,concurrent,9298,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,"ail [master|8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.w",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1939,cache,cached,1939,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,ail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90); 	at is.hail.backend.service.Worker$.$anonfun$main$9(Worker.scala:172); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:171); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.service.Worker$.main(Worker.scala:169); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.lang.NullPointerException: null; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionPutTask.call(JsonResumableSessionPutTask.java:201); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.lambda$put$0(JsonResumableSession.java:81); 	at is.hail.relocated.com.google.cloud.storage.Retrying.lambda$run$0(Retrying.java:102); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Re,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13937:5452,concurren,concurrent,5452,https://hail.is,https://github.com/hail-is/hail/issues/13937,1,['concurren'],['concurrent']
Performance,"ail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:5273,concurren,concurrent,5273,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['concurren'],['concurrent']
Performance,"ail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:12369,concurren,concurrent,12369,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['concurren'],['concurrent']
Performance,"ailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:768); at is.hail.utils.package$.using(package.scala:575); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:3546,Load,LoadVCF,3546,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['Load'],['LoadVCF']
Performance,"ailKryoRegistrator \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; pyspark-shell '. from pyspark import SparkContext; sc=SparkContext.getOrCreate(). import hail as hl; hl.init(sc=sc); ```. - Error logs ; ```; 22/05/11 14:31:21 WARN Utils: Your hostname, spacerider.local resolves to a loopback address: 127.0.0.1; using 172.20.10.4 instead (on interface en6); 22/05/11 14:31:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/username/miniforge3/envs/hail/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 22/05/11 14:31:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Input In [2], in <cell line: 6>(); 3 sc = spark._sc; 5 import hail as hl; ----> 6 hl.init(sc=sc). File <decorator-gen-1703>:2, in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, backend, driver_cores, driver_memory). File ~/miniforge3/envs/hail/lib/python3.9/site-packages/hail/typecheck/check.py:577, in _",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11827:1716,load,load,1716,https://hail.is,https://github.com/hail-is/hail/issues/11827,1,['load'],['load']
Performance,"aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3864,cache,cached,3864,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,al(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:6163,Load,LoadPlink,6163,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,ala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:4459,concurren,concurrent,4459,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['concurren'],['concurrent']
Performance,ala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5623,optimiz,optimizeIR,5623,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['optimiz'],['optimizeIR']
Performance,ala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Ite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19034,Load,LoadVCF,19034,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,alizers.java:302); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651); at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:270); at org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:321); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:323); at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:140); at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95); at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34); at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:75); at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1539); at is.hail.backend.spark.SparkBackend.broadcast(SparkBackend.scala:411); at is.hail.io.plink.MatrixPLINKReader.executeGeneric(LoadPlink.scala:390); at is.hail.io.plink.MatrixPLINKReader.lower(LoadPlink.scala:561); at is.hail.expr.ir.TableReader.lower(TableIR.scala:663); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1062); at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:728); at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1021); at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:27); at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:11); at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:91); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:27); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:4701,Load,LoadPlink,4701,https://hail.is,https://github.com/hail-is/hail/issues/14168,3,['Load'],['LoadPlink']
Performance,"also `loadElement` instead of `elementOffset`; although they're equivalent as of now because we store structs inline, it was sematincally wrong since we need the pointer to the actual element itself and not the pointer inside the array. (this would cause problems if, e.g., we had physical types that stored the struct as a pointer.). This (TAGG thing) got caught by some tests in #3431.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3434:6,load,loadElement,6,https://hail.is,https://github.com/hail-is/hail/pull/3434,1,['load'],['loadElement']
Performance,also remove load balancing from scorecard,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4447:12,load,load,12,https://hail.is,https://github.com/hail-is/hail/pull/4447,1,['load'],['load']
Performance,"ameter so large groups only return their size rather than filling memory and killing the entire job. - Now computeGramianLargeN is used if n * m exceeds 8000 * 8000 (about 512MB of doubles) or maxSize * maxSize if maxSize is given and smaller than 8000. This seems a reasonable approach for now to prevent OOM without exposing additional memory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 200",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2248:1256,tune,tune,1256,https://hail.is,https://github.com/hail-is/hail/pull/2248,1,['tune'],['tune']
Performance,"and give the user a warning that they might want to log in. Anonymous credentials will allow us to access public buckets without authenticating, which was not possible until now. I contemplated whether we should only suggest the login if they've received a 401 but I assume most everyone using this will want to be authenticated and we already log a warning as it is. This is reasonably testable, but I couldn't find where it should go. Does `test_fs` test credential-loading at all?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11207:468,load,loading,468,https://hail.is,https://github.com/hail-is/hail/pull/11207,1,['load'],['loading']
Performance,and.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailure,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822:10904,concurren,concurrent,10904,https://hail.is,https://github.com/hail-is/hail/issues/1822,1,['concurren'],['concurrent']
Performance,"anges include:; <ul>; <li>Add support for <code>sankey</code> links with arrows</li>; <li>Add <code>selections</code>, <code>newselection</code> and <code>activeselection</code> layout attributes to have persistent and editable selections over cartesian subplots</li>; <li>Add <code>unselected.line.color</code> and <code>unselected.line.opacity</code> options to <code>parcoords</code> trace</li>; <li>Display Plotly's new logo in the modebar</li>; </ul>; </li>; </ul>; <h2>[5.9.0] - 2022-06-23</h2>; <h3>Added</h3>; <ul>; <li><code>pattern_shape</code> options now available in <code>px.timeline()</code> <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3774"">#3774</a></li>; <li><code>facet_*</code> and <code>category_orders</code> now available in <code>px.pie()</code> <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3775"">#3775</a></li>; </ul>; <h3>Performance</h3>; <ul>; <li><code>px</code> methods no longer call <code>groupby</code> on the input dataframe when the result would be a single group, and no longer groups by a lambda, for significant speedups <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3765"">#3765</a> with thanks to <a href=""https://github.com/jvdd""><code>@jvdd</code></a></li>; </ul>; <h3>Updated</h3>; <ul>; <li>Allow non-string extras in <code>flaglist</code> attributes, to support upcoming changes to <code>ax.automargin</code> in plotly.js <a href=""https://github-redirect.dependabot.com/plotly/plotly.js/pull/6193"">plotly.js#6193</a>, <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3749"">#3749</a></li>; </ul>; <h2>[5.8.2] - 2022-06-10</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed a syntax error that caused rendering issues in Databricks notebooks and likely elsewhere. <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3763"">#3763</a> with thanks to <a href=""https://github.com/fwetdb""><code>@fwetdb</code></a></li>; </ul>; <h2>[5.8.1] - 2022-06-08</h2>; <p>(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12113:2383,Perform,Performance,2383,https://hail.is,https://github.com/hail-is/hail/pull/12113,1,['Perform'],['Performance']
Performance,annelHandlerContext.java:343); 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336); 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343); 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911); 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); 	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); 	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); 	at java.lang.Thread.run(Thread.java:748); 	Error: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.; 	This stopped SparkContext was created at:; 	; 	org.apache.spark.SparkContext.getOrCreate(SparkContext.scala); 	sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	java.lang.reflect.Method.invoke(Method.java:498); 	sparklyr.Invoke.invoke(invoke.scala:139); 	sparklyr.StreamHandler.handleMethodCall(stream.scala:123); 	sparklyr.StreamHandler.read(stream.scala:66); 	sparklyr.BackendHandler.channelRead0(handler.scala:51); 	sparklyr.BackendHandler.channelRead0(handler.scala:4); 	io.netty.channel.SimpleChan,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:6500,concurren,concurrent,6500,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['concurren'],['concurrent']
Performance,annelHandlerContext.java:343); 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336); 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343); 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911); 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); 	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); 	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); 	at java.lang.Thread.run(Thread.java:748); </details>. <details>; <summary>Working hail.log</summary>. ```; 2018-10-09 15:04:33 Hail: INFO: SparkUI: http://10.32.119.167:4040; 2018-10-09 15:04:33 Hail: INFO: Running Hail version devel-17a988f2a628; 2018-10-09 15:04:33 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 15:04:33 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 15:04:33 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@16ba3696,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:13118,concurren,concurrent,13118,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['concurren'],['concurrent']
Performance,annelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerConnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.nettylerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractCt io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at io.nettyentLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.NioEventLoot io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thread.javawork.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.nettyRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelP,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:22366,concurren,concurrent,22366,https://hail.is,https://github.com/hail-is/hail/issues/8106,2,['concurren'],['concurrent']
Performance,anonfun$8.apply(RDD.scala:330); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:8279,concurren,concurrent,8279,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['concurren'],['concurrent']
Performance,anonfun$apply$22.apply(ContextRDD.scala:308); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-c8ca698; Error summary: NegativeArraySizeException: null,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3583:15570,concurren,concurrent,15570,https://hail.is,https://github.com/hail-is/hail/issues/3583,2,['concurren'],['concurrent']
Performance,"any different functions with long names. This change reduces it to three functions:. - internal_server_ssl_context; - internal_client_ssl_context; - external_client_ssl_context. I also added `httpx.py` which contains the HTTPS-related functions that `tls.py` previously; contained. I also simplified the HTTPS-related functions to just:. - client_session; - blocking_client_session. I determine internal vs. external using the deploy config. ---. An [`ssl.SSLContext`](https://docs.python.org/3/library/ssl.html#ssl.SSLContext) defines how a; network library (such as `aiohttp`) should perform SSL/TLS. Let's look at an example:. ```python3; server_ssl_context = ssl.create_default_context(; purpose=Purpose.CLIENT_AUTH,; cafile='/incoming.cacerts'); server_ssl_context.load_cert_chain(ssl_config['cert'],; keyfile=ssl_config['key'],; password=None); server_ssl_context.verify_mode = ssl.CERT_OPTIONAL; server_ssl_context.check_hostname = False; ```. The first function call states that we are a *server* performing *client; authentication* (`Purpose.CLIENT_AUTH`). We also state that anyone who sends requests to us will be; identified by a certificate that is trusted by our certificate database: `/incoming.cacerts` (which; is a file). `load_cert_chain` states where to find the certificates and secret key that prove who we are. The; certificate and secret key together are like a property title that proves someone owns a house. The; `password=None` means that our secret key has no password. Some keys are themselves locked by a; password. `verify_mode` means what do we expect our clients to have. `CERT_OPTIONAL` means anonymous clients; are OK. This is how servers normally operate (https://google.com does not care who you are). `check_hostname` means should we verify that the client certificate matches the client's; hostname. Since we allow anonymous clients, this must be `False`. ---. `test-address.py` is a gross hack. It will disappear in subsequent PRs. For now, I pushed all the; gr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9862:1019,perform,performing,1019,https://hail.is,https://github.com/hail-is/hail/pull/9862,1,['perform'],['performing']
Performance,apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DA,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9552,Load,LoadVCF,9552,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.Mappark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iteratoadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartiti288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scat org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at ) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.rception: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.jnnel.AbstractChannelHandlerContext",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:20114,concurren,concurrent,20114,https://hail.is,https://github.com/hail-is/hail/issues/8106,2,['concurren'],['concurrent']
Performance,apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); 	at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:318); 	at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:305); 	at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:6908,Optimiz,OptimizePass,6908,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['OptimizePass']
Performance,"ar_url, self.argv); File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2629, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:224); at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:186); at is.hail.JVMEntryway.main(JVMEntryway.java:156); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at is.hail.JVMEntryway$1.run(JVMEntryway.java:107); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); ... 7 more; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/rwalters-hail-tmp/o?name=merged_round2_sumstats.fix_lowconf.mt/entries/rows/parts/part-15801-2fde3786-67cb-42ed-8aac-f900cfcc4c00&up",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:1608,concurren,concurrent,1608,https://hail.is,https://github.com/hail-is/hail/issues/12950,1,['concurren'],['concurrent']
Performance,"are only provided for Python; 3.8 and 3.9 on Windows, all other wheels are 64 bits on account of; Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.; All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix; the occasional problems encountered by folks using truly huge arrays.</p>; <h2>Expired deprecations</h2>; <h3>Deprecated numeric style dtype strings have been removed</h3>; <p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,; and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>; <p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19539"">gh-19539</a>)</p>; <h3>Expired deprecations for <code>loads</code>, <code>ndfromtxt</code>, and <code>mafromtxt</code> in npyio</h3>; <p><code>numpy.loads</code> was deprecated in v1.15, with the recommendation that; users use <code>pickle.loads</code> instead. <code>ndfromtxt</code> and <code>mafromtxt</code> were both; deprecated in v1.17 - users should use <code>numpy.genfromtxt</code> instead with; the appropriate value for the <code>usemask</code> parameter.</p>; <p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19615"">gh-19615</a>)</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/numpy/numpy/commit/4adc87dff15a247e417d50f10cc4def8e1c17a03""><code>4adc87d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20685"">#20685</a> from charris/prepare-for-1.22.0-release</li>; <li><a href=""https://github.com/numpy/numpy/commit/fd66547557f57c430d41be2fc0764f74a62e8ccf""><code>fd66547</code></a> REL: Prepare for the NumPy 1.22.0 release.</li>; <li><a href=""https://github.com/numpy/numpy/commit/125304b035effcd82e366e601b102e7347eaa9ba""><code>125304b</code></a> wip</li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11939:2526,load,loads,2526,https://hail.is,https://github.com/hail-is/hail/pull/11939,2,['load'],['loads']
Performance,"ark.SparkException: Job aborted due to stage failure: Task 3 in stage 8.0 failed 20 times, most recent failure: Lost task 3.19 in stage 8.0 (TID 54368) (leo-test-w-8.australia-southeast1-a.c.ourdna-browser.internal executor 14): java.lang.ClassFormatError: Too many arguments in method signature in class file __C2866stream; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:635); 	at is.hail.asm4s.HailClassLoader.liftedTree1$1(HailClassLoader.scala:10); 	at is.hail.asm4s.HailClassLoader.loadOrDefineClass(HailClassLoader.scala:6); 	at is.hail.asm4s.ClassesBytes.$anonfun$load$1(ClassBuilder.scala:64); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1234); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(Contex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:4093,load,load,4093,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['load'],['load']
Performance,ark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:2491,Load,LoadVCF,2491,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,"array""></a> PArray. An abstract class for immutable ordered collections where all elements are of a single type. Does not contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offset into arrayAddress, otherwise returns that address. ## <a name=""parray""></a> PCanonicalArray. A growable array that is accessed by a pointer. ### Structure. Starting at `arrayAddress`:. [`4-byte length`, `n/8 byte missigness data`, `n * elementByteSize byte element data`]. # <a name=""parray""></a> PSet. An abstract class for immutable ordered collections where all elements are unique. ## Core Methods. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:4852,load,loadLength,4852,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['load'],['loadLength']
Performance,"artition=9571, attempt=0, peakBytes=4507648, peakBytesReadable=4.30 MiB, chunks requested=51, cache hits=0; 2023-09-24 01:58:51.513 : INFO: RegionPool: FREE: 4.3M allocated (2.2M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:51.515 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/aou_tmp/o?name=tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89&uploadType=resumable&upload_id=ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guploader-uploadid: ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PL",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721:4483,concurren,concurrent,4483,https://hail.is,https://github.com/hail-is/hail/issues/13721,1,['concurren'],['concurrent']
Performance,"ary>; <p><em>Sourced from <a href=""https://github.com/googleapis/google-auth-library-python/releases"">google-auth's releases</a>.</em></p>; <blockquote>; <h2>v2.6.0</h2>; <h3>Features</h3>; <ul>; <li>ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/962"">#962</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/52c8ef90058120d7d04d3d201adc111664be526c"">52c8ef9</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>revert &quot;feat: add api key support (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/826"">#826</a>)&quot; (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/964"">#964</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/f9f23f4370f2a7a5b2c66ee56a5e700ef03b5b06"">f9f23f4</a>)</li>; </ul>; <h2>v2.5.0</h2>; <h3>Features</h3>; <ul>; <li>ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/956"">#956</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/a8eb4c8693055a3420cfe9c3420aae2bc8cd465a"">a8eb4c8</a>)</li>; </ul>; <h2>v2.4.1</h2>; <h3>Bug Fixes</h3>; <ul>; <li>urllib3 import (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/953"">#953</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/c8b5cae3da5eb9d40067d38dac51a4a8c1e0763e"">c8b5cae</a>)</li>; </ul>; <h2>v2.4.0</h2>; <h3>Features</h3>; <ul>; <li>add 'py.typed' declaration (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/919"">#919</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/c99350455d0f7fd3aab950ac47b43000c73dd312"">c993504</a>)</li>; <li>add api key support (<a href=""https://github-redirect.dependabot.co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546:1138,load,load,1138,https://hail.is,https://github.com/hail-is/hail/pull/11546,1,['load'],['load']
Performance,"ase-3.1.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/1e68ba86177504bb6404288610608b855eab93fa""><code>1e68ba8</code></a> release version 3.1.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/8efee35092404ba67ede8316566be4f430e7b61d""><code>8efee35</code></a> pre-commit updates latest release branch</li>; <li><a href=""https://github.com/pallets/jinja/commit/a24df26d54fa2ccbe9bdaa0bb9419075a00e2699""><code>a24df26</code></a> ignore new mypy finding</li>; <li><a href=""https://github.com/pallets/jinja/commit/9faee281ea75694e28c33e2878879b322359d411""><code>9faee28</code></a> update requirements</li>; <li><a href=""https://github.com/pallets/jinja/commit/b802b5a6ad9deea082c16d9adb6417eda1a184d8""><code>b802b5a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1655"">#1655</a> from dvitek/dvitek/issue1654</li>; <li><a href=""https://github.com/pallets/jinja/commit/746bb95780c17687b27b6d1bf4df1216f0da972c""><code>746bb95</code></a> Fix race conditions in FileSystemBytecodeCache</li>; <li><a href=""https://github.com/pallets/jinja/commit/466a200ea40642b674db77588d13889abbad55f5""><code>466a200</code></a> update requirements</li>; <li><a href=""https://github.com/pallets/jinja/commit/990602f719b4086540287e95f601baefd830d790""><code>990602f</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1647"">#1647</a> from Tom-Brouwer/202204/add-missing-overlay-options</li>; <li><a href=""https://github.com/pallets/jinja/commit/5d3d2414710c1439105d84efc58e4aba8e453cb3""><code>5d3d241</code></a> fix flake8-bugbear finding</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/3.0.3...3.1.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=3.0.3&new-version=3.1.2)](https://docs.github.com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12173:5795,race condition,race conditions,5795,https://hail.is,https://github.com/hail-is/hail/pull/12173,1,['race condition'],['race conditions']
Performance,"aseStruct___iruid_8616Spills.__f2352null Z; 31039 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31040 25791 (LdcX 3 I))))); 31041 (ReturnX). # Elsewhere, this split method is called, then the resulting field is loaded and written to the output buffer. 11325 (MethodStmtX INVOKEVIRTUAL __C1527collect_distributed_array.__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616_region0_0 (L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)V; 11326 (LoadX arg:0 L__C1527collect_distributed_array;); 11327 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11328 25772 (MethodStmtX INVOKEINTERFACE is/hail/io/OutputBuffer.writeByte (B)Vinterface; 11329 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2354null Lis/hail/io/OutputBuffer;; 11330 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11331 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 11332 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;))); ```. # Pervasiveness. There is absolutely nothing about this bug that is whole-stage-codegen-specific, but I suspect the much larger single IRs compiled in whole stage code generation made it exponentially more likely for this corner case to occur. I imagine it would be possible to construct a failing pipeline with whole stage code generation turned off. # Testing. This is super hard to reproduce using small/public examples, and any unit tests to capture this *crazy edge case* are pretty much meaningless. John suggested we programmatically check the TypeInfo inference against some JVM reference, and I agree that's the best bet, but don't want to block this critical fix on that project. I fixed BALOAD for the same reason",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:6292,Load,LoadX,6292,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['Load'],['LoadX']
Performance,ask 55.0 in stage 3.0 (TID 1197); java.lang.NullPointerException; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:515); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.create(GoogleCloudStorageFileSystem.java:261); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.createChannel(GoogleHadoopOutputStream.java:82); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.<init>(GoogleHadoopOutputStream.java:74); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.create(GoogleHadoopFileSystemBase.java:797); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:91); at is.hail.io.fs.HadoopFS.unsafeWriter(HadoopFS.scala:445); at is.hail.linalg.WriteBlocksRDD$$anonfun$63.apply(BlockMatrix.scala:1840); at is.hail.linalg.WriteBlocksRDD$$anonfun$63.apply(BlockMatrix.scala:1833); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1833); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8239:2715,concurren,concurrent,2715,https://hail.is,https://github.com/hail-is/hail/issues/8239,2,['concurren'],['concurrent']
Performance,"ask.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:922); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:915); at is.hail.utils.package$.using(package.scala:577); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:915); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:12175,Load,LoadVCF,12175,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['Load'],['LoadVCF']
Performance,"aster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infrastructure/3868.htm); * 10-30% cost for networking-heavy operations. ### Example implementations of sandboxed containers; https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md. [CRI installation - Kubernetes](https://kubernetes.io/docs/setup/cri/). ### References:; [Kata Containers - Why Kata Containers doesnt replace Kubernetes: A Kata Containers explainer](https://katacontainers.io/posts/why-kata-containers-doesnt-replace-kubernetes/). [Kubernetes Container Runtimes - kubedex.com](https://kubedex.com/kubernetes-container-runtimes/). [GitHub - containerd/cri: Containerd Plugin for Kubernetes Container Runtime Interface](https://github.com/containerd/cri). https://github.com/kubern",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5111:1900,Perform,Performance,1900,https://hail.is,https://github.com/hail-is/hail/issues/5111,2,"['Perform', 'perform']","['Performance', 'performance']"
Performance,"async def _read_output(self, ir: Optional[BaseIR], output_uri: str) -> bytes:; assert self._batch; ; try:; driver_output = await self._async_fs.open(output_uri); except FileNotFoundError as exc:; raise FatalError('Hail internal error. Please contact the Hail team and provide the following information.\n\n' + yamlx.dump({; 'service_backend_debug_info': self.debug_info(),; 'batch_debug_info': await self._batch.debug_info(); })) from exc; ; async with driver_output as outfile:; success = await read_bool(outfile); if success:; return await read_bytes(outfile); ; short_message = await read_str(outfile); expanded_message = await read_str(outfile); error_id = await read_int(outfile); ; reconstructed_error = fatal_error_from_java_error_triplet(short_message, expanded_message, error_id); if ir is None:; raise reconstructed_error; > raise reconstructed_error.maybe_user_error(ir); E hail.utils.java.FatalError: RuntimeException: Stream is already closed.; E ; E Java stack trace:; E java.util.concurrent.ExecutionException: java.lang.RuntimeException: Stream is already closed.; E 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); E 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); E 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:150); E 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:44); E 	at __C256669Compiled.__m256730split_CollectDistributedArray(Emit.scala); E 	at __C256669Compiled.__m256689split_Let(Emit.scala); E 	at __C256669Compiled.apply(Emit.scala); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:74); E 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:13705,concurren,concurrent,13705,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,"at __C2005collect_distributed_array_matrix_native_writer.apply_region1_27(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:52); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:51); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:751); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PCString; ```. Notice in particular:; ```; AS_VQSLOD=.,.;AS_YNG=.,.; ```; These fields are array fields containing missing values. By default, Hail errors when parsing these due to the inherent ambiguity of a single dot: is it a missing array or an array with one, missing, element. The error message should suggest that the user try using array_elements_required. The docs for `import_vcf` should provide enough information for the user to understand what this does. We should also consider making this the default. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13346:2431,concurren,concurrent,2431,https://hail.is,https://github.com/hail-is/hail/issues/13346,2,['concurren'],['concurrent']
Performance,at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$4(LocalBackend.scala:150); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$3(LocalBackend.scala:150); E 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:1038); E 	at is.hail.CancellingExecutorService.$anonfun$newTaskFor$2(package.scala:1090); E 	at is.hail.CancellingExecutorService$CancellingTask.run(package.scala:1067); E 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515); E 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264); E 	at is.hail.relocated.com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:327); E 	at is.hail.CancellingExecutorService.execute(package.scala:1111); E 	at java.base/java.util.concurrent.ExecutorCompletionService.submit(ExecutorCompletionService.java:184); E 	at is.hail.utils.package$.$anonfun$runAll$1(package.scala:1038); E 	at scala.collection.Iterator.foreach(Iterator.scala:943); E 	at scala.collection.Iterator.foreach$(Iterator.scala:943); E 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); E 	at scala.collection.IterableLike.foreach(IterableLike.scala:74); E 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73); E 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56); E 	at is.hail.utils.package$.runAll(package.scala:1038); E 	at is.hail.utils.package$.$anonfun$runAllKeepFirstError$3(package.scala:1054); E 	at is.hail.backend.local.LocalBackend.paralleli,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14705:4204,concurren,concurrent,4204,https://hail.is,https://github.com/hail-is/hail/issues/14705,1,['concurren'],['concurrent']
Performance,at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.Mat,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5611,Load,LoadPlink,5611,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,at scala.collection.AbstractTraversable.map(Traversable.scala:108); at is.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:113); at is.hail.expr.ir.Pretty.header(Pretty.scala:405); at is.hail.expr.ir.Pretty.pretty$1(Pretty.scala:463); at is.hail.expr.ir.Pretty.$anonfun$sexprStyle$4(Pretty.scala:453); at scala.collection.Iterator$$anon$10.next(Iterator.scala:459); at scala.collection.Iterator$ConcatIterator.next(Iterator.scala:230); at is.hail.utils.richUtils.RichIterator$$anon$3.next(RichIterator.scala:67); at is.hail.utils.prettyPrint.Doc$.advance$1(PrettyPrintWriter.scala:68); at is.hail.utils.prettyPrint.Doc$.render(PrettyPrintWriter.scala:139); at is.hail.utils.prettyPrint.Doc.render(PrettyPrintWriter.scala:163); at is.hail.utils.prettyPrint.Doc.render(PrettyPrintWriter.scala:167); at is.hail.expr.ir.Pretty.sexprStyle(Pretty.scala:466); at is.hail.expr.ir.Pretty.apply(Pretty.scala:429); at is.hail.expr.ir.Pretty$.apply(Pretty.scala:22); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:45); at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.fo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046:2753,Optimiz,Optimize,2753,https://hail.is,https://github.com/hail-is/hail/issues/13046,1,['Optimiz'],['Optimize']
Performance,"ated (698.0K blocks / 410.0K chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:19.984 : INFO: RegionPool: REPORT_THRESHOLD: 2.0M allocated (1.0M blocks / 1010.0K chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:24.240 : INFO: RegionPool: REPORT_THRESHOLD: 4.3M allocated (2.2M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:24.240 GoogleStorageFS$: INFO: createNoCompression: gs://aou_tmp/tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89; 2023-09-24 01:58:24.305 GoogleStorageFS$: INFO: close: gs://aou_tmp/tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89; 2023-09-24 01:58:51.513 : INFO: TaskReport: stage=0, partition=9571, attempt=0, peakBytes=4507648, peakBytesReadable=4.30 MiB, chunks requested=51, cache hits=0; 2023-09-24 01:58:51.513 : INFO: RegionPool: FREE: 4.3M allocated (2.2M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:51.515 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721:3582,cache,cache,3582,https://hail.is,https://github.com/hail-is/hail/issues/13721,1,['cache'],['cache']
Performance,"ates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_client_ssl_context` which should only be used in; publicly consumable tools (*never* in a service). This function allows the same; tool to be used inside and outside the cluster. It will load the correct certs; for your environment (it will load public certs if you're outside the cluster,; it will load in-cluster-only certs if you're in the cluster). I also added types to `tls.py` and fixed some type errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9120:2188,load,load,2188,https://hail.is,https://github.com/hail-is/hail/pull/9120,3,['load'],['load']
Performance,"ative (not AnyIO) cancellation with just the right timing, leaving the; next acquiring task waiting forever (<code>[#398](https://github.com/agronholm/anyio/issues/398) &lt;https://github.com/agronholm/anyio/issues/398&gt;</code>_)</li>; <li>Added workaround for bpo-46313_ to enable compatibility with OpenSSL 3.0</li>; </ul>; <p>.. _bpo-46313: <a href=""https://bugs.python.org/issue46313"">https://bugs.python.org/issue46313</a></p>; <p><strong>3.4.0</strong></p>; <ul>; <li>; <p>Added context propagation to/from worker threads in <code>to_thread.run_sync()</code>,; <code>from_thread.run()</code> and <code>from_thread.run_sync()</code>; (<code>[#363](https://github.com/agronholm/anyio/issues/363) &lt;https://github.com/agronholm/anyio/issues/363&gt;</code>_; partially based on a PR by Sebastin; Ramrez)</p>; <p><strong>NOTE</strong>: Requires Python 3.7 to work properly on asyncio!</p>; </li>; <li>; <p>Fixed race condition in <code>Lock</code> and <code>Semaphore</code> classes when a task waiting on <code>acquire()</code>; is cancelled while another task is waiting to acquire the same primitive; (<code>[#387](https://github.com/agronholm/anyio/issues/387) &lt;https://github.com/agronholm/anyio/issues/387&gt;</code>_)</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/agronholm/anyio/commit/787cb0c2e53c2a3307873d202fbd49dc5eac4e96""><code>787cb0c</code></a> Pinned trio to &lt; 0.22 on AnyIO 3.x</li>; <li><a href=""https://github.com/agronholm/anyio/commit/7cc3cf8cdb58f3b6df6c4fe21e9daa11537c1844""><code>7cc3cf8</code></a> Updated pre-commit modules</li>; <li>See full diff in <a href=""https://github.com/agronholm/anyio/compare/3.6.1...3.6.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=anyio&package-manager=pip&previous-version=3.6.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12362:2859,race condition,race condition,2859,https://hail.is,https://github.com/hail-is/hail/pull/12362,1,['race condition'],['race condition']
Performance,"ator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.31-6060f9c971cc; Error summary: HailException: Hail only supports 8-bit probabilities, found 16. How can I solve it? Or why is it happening?. Thank you very much!. Kind regards,; Catarina",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8545:19620,concurren,concurrent,19620,https://hail.is,https://github.com/hail-is/hail/issues/8545,2,['concurren'],['concurrent']
Performance,automated performance testing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/19:10,perform,performance,10,https://hail.is,https://github.com/hail-is/hail/issues/19,1,['perform'],['performance']
Performance,"ava:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.115-71fc978b5c22; Error summary: SocketException: Connection reset. -------------------. Some more content from the failing worker job:. ...; 2023-05-04 01:04:35.959 : INFO: executing D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e61c-4fa9-9419-2278528f3c86; 2023-05-04 01:04:37.559 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=132096, peakBytesReadable=129.00 KiB, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.560 : INFO: RegionPool: FREE: 129.0K allocated (129.0K blocks / 0 chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 0",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:21880,concurren,concurrent,21880,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['concurren'],['concurrent']
Performance,ava:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:464); 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:237); 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190); 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109); 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1400); 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1368); 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73); 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:962); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284); 	at java.io.BufferedInputStream,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12982:14166,concurren,concurrent,14166,https://hail.is,https://github.com/hail-is/hail/issues/12982,3,['concurren'],['concurrent']
Performance,"ava:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf spark.executor.memory=40G \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/backend/hail-all-spark.jar\"" \; --conf spark.executor.extraCl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:18640,load,load,18640,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['load'],['load']
Performance,ava:566); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plin,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5530,Load,LoadPlink,5530,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,ava:748)org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'; 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:247); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Unsupported/Unknown Elasticsearch version 6.0.0; 	at org.elasticsearch.hadoop.util.EsMajorVersion.parse(EsMajorVersion.java:79); 	at org.elasticsearch.hadoop.rest.RestClient.remoteEsVersion(RestClient.java:613); 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:240); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4138:6722,concurren,concurrent,6722,https://hail.is,https://github.com/hail-is/hail/issues/4138,1,['concurren'],['concurrent']
Performance,"avro) to permit the latest version.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/apache/avro/releases"">avro's releases</a>.</em></p>; <blockquote>; <h2>Apache Avro 1.11.0</h2>; <p>The Apache Avro community is pleased to announce the release of Avro 1.11.0!</p>; <p>All signed release artifacts, signatures and verification instructions can; be found here: <a href=""https://avro.apache.org/releases.html"">https://avro.apache.org/releases.html</a></p>; <p>This release includes 120 Jira issues, including some interesting features:</p>; <p>Specification: AVRO-3212 Support documentation tags for FIXED types; C#: AVRO-2961 Support dotnet framework 5.0; C#: AVRO-3225 Prevent memory errors when deserializing untrusted data; C++: AVRO-2923 Logical type corrections; Java: AVRO-2863 Support Avro core on android; Javascript: AVRO-3131 Drop support for node.js 10; Perl: AVRO-3190 Fix error when reading from EOF; Python: AVRO-2906 Improved performance validating deep record data; Python: AVRO-2914 Drop Python 2 support; Python: AVRO-3004 Drop Python 3.5 support; Ruby: AVRO-3108 Drop Ruby 2.5 support</p>; <p>For the first time, the 1.11.0 release includes experimental support for; Rust. Work is continuing on this donated SDK, but we have not versioned and; published official artifacts for this release.</p>; <p>Python: The avro package fully supports Python 3. We will no longer publish a; separate avro-python3 package</p>; <p>And of course upgraded dependencies to latest versions, CVE fixes and more:; <a href=""https://issues.apache.org/jira/issues/?jql=project%3DAVRO%20AND%20fixVersion%3D1.11.0"">https://issues.apache.org/jira/issues/?jql=project%3DAVRO%20AND%20fixVersion%3D1.11.0</a></p>; <p>The link to all fixed JIRA issues and a brief summary can be found at:; <a href=""https://github.com/apache/avro/releases/tag/release-1.11.0"">https://github.com/apache/avro/releases/tag/release-1.11.0</a></p>; <p>In addition, language-specific release ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11475:1046,perform,performance,1046,https://hail.is,https://github.com/hail-is/hail/pull/11475,1,['perform'],['performance']
Performance,"b.com/ai/nanoid/blob/main/CHANGELOG.md"">nanoid's changelog</a>.</em></p>; <blockquote>; <h1>Change Log</h1>; <p>This project adheres to <a href=""http://semver.org/"">Semantic Versioning</a>.</p>; <h2>3.2</h2>; <ul>; <li>Added <code>--size</code> and <code>--alphabet</code> arguments to binary (by Vitaly Baev).</li>; </ul>; <h2>3.1.32</h2>; <ul>; <li>Reduced <code>async</code> exports size (by Artyom Arutyunyan).</li>; <li>Moved from Jest to uvu (by Vitaly Baev).</li>; </ul>; <h2>3.1.31</h2>; <ul>; <li>Fixed collision vulnerability on object in <code>size</code> (by Artyom Arutyunyan).</li>; </ul>; <h2>3.1.30</h2>; <ul>; <li>Reduced size for project with <code>brotli</code> compression (by Anton Khlynovskiy).</li>; </ul>; <h2>3.1.29</h2>; <ul>; <li>Reduced npm package size.</li>; </ul>; <h2>3.1.28</h2>; <ul>; <li>Reduced npm package size.</li>; </ul>; <h2>3.1.27</h2>; <ul>; <li>Cleaned <code>dependencies</code> from development tools.</li>; </ul>; <h2>3.1.26</h2>; <ul>; <li>Improved performance (by Eitan Har-Shoshanim).</li>; <li>Reduced npm package size.</li>; </ul>; <h2>3.1.25</h2>; <ul>; <li>Fixed <code>browserify</code> support.</li>; </ul>; <h2>3.1.24</h2>; <ul>; <li>Fixed <code>browserify</code> support (by Artur Paikin).</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ai/nanoid/commit/23b136929a6d58f32e31b269534a3ce3f680a086""><code>23b1369</code></a> Release 3.2 version</li>; <li><a href=""https://github.com/ai/nanoid/commit/967788efce880960512f969a56f8f22f3fc20bae""><code>967788e</code></a> Remove TS test tools</li>; <li><a href=""https://github.com/ai/nanoid/commit/27eaa90cd207a7782bbcf17343092ae87dd62164""><code>27eaa90</code></a> Simplify new binary tool</li>; <li><a href=""https://github.com/ai/nanoid/commit/a9d91239931dc77506381874826d297aee71d6ef""><code>a9d9123</code></a> Update dependencies</li>; <li><a href=""https://github.com/ai/nanoid/commit/32b9bdaab1fbc28576b17de8516164ce0360f292""><code",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11284:1147,perform,performance,1147,https://hail.is,https://github.com/hail-is/hail/pull/11284,2,['perform'],['performance']
Performance,"b.com/googleapis/google-auth-library-python/commit/52c8ef90058120d7d04d3d201adc111664be526c""><code>52c8ef9</code></a> feat: ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/962"">#962</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/83b20f0b4d32b2ff1183a9c2926afd37f3baf92b""><code>83b20f0</code></a> chore: update user creds for system test (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/963"">#963</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/3c9feff3e9037a15bf07496623e3a810f117adcf""><code>3c9feff</code></a> chore(main): release 2.5.0 (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/960"">#960</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/a8eb4c8693055a3420cfe9c3420aae2bc8cd465a""><code>a8eb4c8</code></a> feat: ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/956"">#956</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/87706fd9561aeb651ef551f3576f236a73fad27a""><code>87706fd</code></a> chore: update user cred for system test (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/957"">#957</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/5a09454703bd004d23355a6f660ec8579597d981""><code>5a09454</code></a> chore(main): release 2.4.1 (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/955"">#955</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/c8b5cae3da5eb9d40067d38dac51a4a8c1e0763e""><code>c8b5cae</code></a> fix: urllib3 import (<a href=""https://github-redirect.dependabot.c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546:11252,load,load,11252,https://hail.is,https://github.com/hail-is/hail/pull/11546,1,['load'],['load']
Performance,"b_metadata/issues/300"">#300</a>: Removed compatibility shims for deprecated entry; point interfaces.</li>; </ul>; <h1>v4.13.0</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/396"">#396</a>: Added compatibility for <code>PathDistributions</code> originating; from Python 3.8 and 3.9.</li>; </ul>; <h1>v4.12.0</h1>; <ul>; <li>py-93259: Now raise <code>ValueError</code> when <code>None</code> or an empty; string are passed to <code>Distribution.from_name</code> (and other; callers).</li>; </ul>; <h1>v4.11.4</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/379"">#379</a>: In <code>PathDistribution._name_from_stem</code>, avoid including; parts of the extension in the result.</li>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/381"">#381</a>: In <code>PathDistribution._normalized_name</code>, ensure names; loaded from the stem of the filename are also normalized, ensuring; duplicate entry points by packages varying only by non-normalized; name are hidden.</li>; </ul>; <h1>v4.11.3</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/372"">#372</a>: Removed cast of path items in FastPath, not needed.</li>; </ul>; <h1>v4.11.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/369"">#369</a>: Fixed bug where <code>EntryPoint.extras</code> was returning; match objects and not the extras strings.</li>; </ul>; <h1>v4.11.1</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/367"">#367</a>: In <code>Distribution.requires</code> for egg-info, if <code>requires.txt</code>; is empty, return an empty list.</li>; </ul>; <h1>v4.11.0</h1>; <ul>; <li>bpo-46246: Added <code>__slots__</code> to <code>EntryPoints</code>.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12391:1518,load,loaded,1518,https://hail.is,https://github.com/hail-is/hail/pull/12391,1,['load'],['loaded']
Performance,"back (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2150, in run; await self.jvm.execute(local_jar_location, self.scratch, self.log_file, self.jar_url, self.argv); File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2629, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:224); at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:186); at is.hail.JVMEntryway.main(JVMEntryway.java:156); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at is.hail.JVMEntryway$1.run(JVMEntryway.java:107); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); ... 7 more; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purpos",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:1410,concurren,concurrent,1410,https://hail.is,https://github.com/hail-is/hail/issues/12950,1,['concurren'],['concurrent']
Performance,backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$4(LocalBackend.scala:150); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$3(LocalBackend.scala:150); E 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:1038); E 	at is.hail.CancellingExecutorService.$anonfun$newTaskFor$2(package.scala:1090); E 	at is.hail.CancellingExecutorService$CancellingTask.run(package.scala:1067); E 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515); E 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264); E 	at is.hail.relocated.com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:327); E 	at is.hail.CancellingExecutorService.execute(package.scala:1111); E 	at java.base/java.util.concurrent.ExecutorCompletionService.submit(ExecutorCompletionService.java:184); E 	at is.hail.utils.package$.$anonfun$runAll$1(package.scala:1038); E 	at scala.collection.Iterator.foreach(Iterator.scala:943); E 	at scala.collection.Iterator.foreach$(Iterator.scala:943); E 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); E 	at scala.collection.IterableLike.foreach(IterableLike.scala:74); E 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73); E 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56); E 	at is.hail.utils.package$.runAll(package.scala:1038); E 	at is.hail.utils.package$.$anonfun$runAllKeepFirstError$3(package.scala:1054); E 	at is.hail.backend.local.LocalBackend.parallelizeAndComputeWithIndex(LocalBackend.scala:146); E 	at is.hail.backend.BackendUtils.collectDArray,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14705:4299,concurren,concurrent,4299,https://hail.is,https://github.com/hail-is/hail/issues/14705,1,['concurren'],['concurrent']
Performance,"backend.service.Main$.main(Main.scala:14) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 		at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 		at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; 	Caused by: com.google.api.client.http.HttpResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:25350,concurren,concurrent,25350,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"basically, it now looks at the partitioner for the MatrixValues of its children when it executes, and only shuffles if rvds are non-disjoint. It removes empty rvds and separates children into non-disjoint piles, shuffles those independently, and then concatenates them together in sorted order. (I'm not sure how this behaves relative to the old behavior (coerce everything together) when we have multiple shuffles that need to happen, but at least filtering out empty RVDs helps a lot in e.g. the split-multi case where none of the variants need to be moved.). I think the (a?) better optimization would be to add smarter partitioning so that we're adjusting partition bounds and not really shuffling under most circumstances, but I thought I'd PR this first since at least there's a warning in python about unioning non-disjoint datasets (where this will avoid the scan). I also added tests for MatrixUnionRows and pulled out one of the simplify rules; TableUnion does an unsorted union (I'm not sure this is actually correct behavior now if the tables have keys) and pulling the union outside of a MatrixRowsTable can cause the ordering to be wrong.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4043:586,optimiz,optimization,586,https://hail.is,https://github.com/hail-is/hail/pull/4043,1,['optimiz'],['optimization']
Performance,"bit wheels are only provided for Python; 3.8 and 3.9 on Windows, all other wheels are 64 bits on account of; Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.; All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix; the occasional problems encountered by folks using truly huge arrays.</p>; <h2>Expired deprecations</h2>; <h3>Deprecated numeric style dtype strings have been removed</h3>; <p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,; and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>; <p>(<a href=""https://redirect.github.com/numpy/numpy/pull/19539"">gh-19539</a>)</p>; <h3>Expired deprecations for <code>loads</code>, <code>ndfromtxt</code>, and <code>mafromtxt</code> in npyio</h3>; <p><code>numpy.loads</code> was deprecated in v1.15, with the recommendation that; users use <code>pickle.loads</code> instead. <code>ndfromtxt</code> and <code>mafromtxt</code> were both; deprecated in v1.17 - users should use <code>numpy.genfromtxt</code> instead with; the appropriate value for the <code>usemask</code> parameter.</p>; <p>(<a href=""https://redirect.github.com/numpy/numpy/pull/19615"">gh-19615</a>)</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/numpy/numpy/commit/4adc87dff15a247e417d50f10cc4def8e1c17a03""><code>4adc87d</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/20685"">#20685</a> from charris/prepare-for-1.22.0-release</li>; <li><a href=""https://github.com/numpy/numpy/commit/fd66547557f57c430d41be2fc0764f74a62e8ccf""><code>fd66547</code></a> REL: Prepare for the NumPy 1.22.0 release.</li>; <li><a href=""https://github.com/numpy/numpy/commit/125304b035effcd82e366e601b102e7347eaa9ba""><code>125304b</code></a> wip</li>; <li><a href=""https:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12809:2515,load,loads,2515,https://hail.is,https://github.com/hail-is/hail/pull/12809,2,['load'],['loads']
Performance,boost concurrent builds to 4,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6040:6,concurren,concurrent,6,https://hail.is,https://github.com/hail-is/hail/pull/6040,1,['concurren'],['concurrent']
Performance,"bot.com/michel-kraemer/gradle-download-task/issues/284"">#284</a>). Thanks to <a href=""https://github.com/liblit""><code>@liblit</code></a> for testing!</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; <li>Improve documentation</li>; <li>Add integration tests for Gradle 6.9.3 and 7.6</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a0374fc7c895ae53309ea351e989571204e0ea5f""><code>a0374fc</code></a> Bump up version number to 5.3.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/612f57a382b8640cc730dc5e75d1c809e3e772bd""><code>612f57a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/291"">#291</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/53af1049f5514afe58e884d487d7c57dae47759d""><code>53af104</code></a> Bump http-cache-semantics from 4.1.0 to 4.1.1 in /screencast</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/398c14c05c6448b380ac35c6095598299c5e23c5""><code>398c14c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/15cf7eecfbc17d2466143828b9b69494c6cb6f2b""><code>15cf7ee</code></a> Bump up version number to 5.3.1-SNAPSHOT</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/e3c65ffcb49b9c5a33fde5f31fb63043dbf21134""><code>e3c65ff</code></a> Allow extensions to be created from tasks</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/34e2dd41477f18b1ae3d6d5a71dca5449d6cd1e0""><code>34e2dd4</code></a> Downgrade slf4j to fix warning on console about missing slf4j provider</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b3fa29f9ffb4d4544e13ef84601e371fb2778ddf""><code>b3fa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:1709,cache,cache-semantics,1709,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['cache'],['cache-semantics']
Performance,"but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly wi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:2099,load,load,2099,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['load'],['load']
Performance,"cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2098,cache,cached,2098,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,cala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:4374,concurren,concurrent,4374,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['concurren'],['concurrent']
Performance,"cala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:1965,Load,LoadPlink,1965,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,cala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5456,Optimiz,Optimize,5456,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Optimiz'],['Optimize']
Performance,"cate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offset into arrayAddress, otherwise returns that address. ## <a name=""parray""></a> PCanonicalArray. A growable array that is accessed by a pointer. ### Structure. Starting at `arrayAddress`:. [`4-byte length`, `n/8 byte missigness data`, `n * elementByteSize byte element data`]. # <a name=""parray""></a> PSet. An abstract class for immutable ordered collections where all elements are unique. ## Core Methods. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutable unordered collections of key-value pairs. All keys must have one PType, and all values must have one (possibly different from keys) PType. ## Core Methods. ```scala; def elementType: PStruct; ```. - ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:5041,load,loadElement,5041,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['load'],['loadElement']
Performance,"cc @mkveerapen @tpoterba . 1) Adds natural language documentation search bar with autocompletion; 2) Adds paginated search page to browse all search results; 3) Makes navbar scripts async, to allow inclusion of navbar scripts at top of file without impacting performance (since we include the navbar.html content in other templates we cannot easily locate those scripts at bottom of the includers' pages); 4) Increases hero content width on mobile.; 5) Hides sphinx search bar. http://34.207.246.132/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8955:259,perform,performance,259,https://hail.is,https://github.com/hail-is/hail/pull/8955,1,['perform'],['performance']
Performance,"cc @tpoterba . Here because as we discovered, dev forum has a relatively short editing window. Will post to dev forum when ""complete"" and ready for broader discussion. # What are Physical Types?. Physical types are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:356,perform,performance,356,https://hail.is,https://github.com/hail-is/hail/issues/7988,2,['perform'],"['perform', 'performance']"
Performance,"cc: @cseed @konradjk . This PCRelate should handle larger data sizes than the previous one by avoiding shuffles. It avoids the shuffle by writing the vds to a temporary directory in block matrix form. It then loads this BlockMatrix directly. Form that point forward, the PCRelate algorithm is just non-shuffling linear algebra (however: matrix multiplication will require each node to communicate with approximately `n+m` other nodes). I'm vaguely uncomfortable with two things:. 1. I've added some hail expr lang to mean impute missing values. This is written in python. As such, correctly mean imputing is not tested by our test system any more. The mean imputation is pretty simple, so maybe we should just verify my code is right?. 2. I noticed that at some earlier point PCA was moved outside of Java as well. This also makes me uncomfortable for the same reason. Moving the tests into python is a fair lift because, AFAIK, we don't have as robust test infrastructure over there. I'm torn between the desire to get this out for @konradjk and the desire to follow our normal testing standards. ---. I've played a bit with this locally, but have not tried it on a large cluster. @konradjk, I would be very interested in how this performs on a large dataset, if you would be so kind. Please don't try this until the CI tests and doc builds pass on this PR though. I haven't run the tests locally, so I'm not certain it passes them :P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2821:209,load,loads,209,https://hail.is,https://github.com/hail-is/hail/pull/2821,2,"['load', 'perform']","['loads', 'performs']"
Performance,"cc: @cseed @tpoterba. docker does not pull an image specified in --cache-from, so we need to; explicitly pull it before trying to use it as a cache",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4356:67,cache,cache-from,67,https://hail.is,https://github.com/hail-is/hail/pull/4356,2,['cache'],"['cache', 'cache-from']"
Performance,"cc: @cseed, this explains some weirdness with docker caches. The Docker docs are [misleading at best wrt `--cache-from`](https://github.com/moby/moby/issues/32612). `--cache-from X` means treat `X`'s layers as a cache source *and do not use the local cache*. This is a crucial misfeature for two reasons. First, you [must explicitly specify every image that may have useful cache layers](https://github.com/moby/moby/issues/33002). Second, you cannot include in the cache untagged local images. The latter is particularly an issue for local developers who might run docker builds that fail half-way through. The first issue is not relevant to us because we don't share many layers between images. The second issue is addressed with Makefile conditions that provide a different experience for local versus CI users. Each CI deploy pushes the `latest` tag so that future builds can use it as a cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5016:53,cache,caches,53,https://hail.is,https://github.com/hail-is/hail/pull/5016,8,['cache'],"['cache', 'cache-from', 'caches']"
Performance,"cc: @daniel-goldstein, this is a tricky asyncio situation which you should also keep in mind. OK, there were two problems:. 1. A timeout of 5s appears to be now too short for Google Cloud Storage. I am not sure why but we; timeout substantially more frequently. I have observed this myself on my laptop. Just this; morning I saw it happen to Daniel. 2. When using an `aiohttp.AsyncIterablePayload`, it is *critical* to always check if the coroutine; which actually writes to GCS (which is stashed in the variable `request_task`) is still; alive. In the current `main`, we do not do this which causes hangs (in particular the timeout; exceptions are never thrown ergo we never retry). To understand the second problem, you must first recall how writing works in aiogoogle. There are; two Tasks and an `asyncio.Queue`. The terms ""writer"" and ""reader"" are somewhat confusing, so let's; use left and right. The left Task has the owning reference to both the source ""file"" and the; destination ""file"". In particular, it is the *left* Task which closes both ""files"". Moreover, the; left Task reads chunks from the source file and places those chunks on the `asyncio.Queue`. The; right Task takes chunks off the queue and writes those chunks to the destination file. This situation can go awry in two ways. First, if the right Task encounters any kind of failure, it will stop taking chunks off of the; queue. When the queue (which has a size limit of one) is full, the left Task will hang. The system; is stuck. The left Task will wait forever for the right Task to empty the queue. The second scenario is exactly the same except that the left Task is trying to add the ""stop""; message to the queue rather than a chunk. In either case, it is critical that the left Task waits simultaneously on the queue operation *and*; on the right Task completing. If the right Task has died, no further writes can occur and the left; Task must raise an exception. In the first scenario, we do not observe the right Task'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11830:809,Queue,Queue,809,https://hail.is,https://github.com/hail-is/hail/pull/11830,1,['Queue'],['Queue']
Performance,"cc: @tpoterba @patrick-schultz @catoverdrive . We are not allowed to clear a region we do not own. Someone should test this doesn't blow memory on a severe filter in the cloud. ---. Prevent segfaults when joining two tables using `t1.join(t2)`. This syntax does a ""product join"", i.e., a normal join. The `t2[t1.key]` syntax takes only one matching element from `t2` for each element in `t1`. When performing a ""product join"", hail keeps a side-buffer of region values from the right-hand-side table. This side buffer *must not be cleared* by down stream operations (it is owned by the join node). Unfortunately, hail's filter method was incorrectly clearing regions it might not own. This bug only appeared as a segfault when `t1.join(t2)` was followed by a filter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5421:398,perform,performing,398,https://hail.is,https://github.com/hail-is/hail/pull/5421,1,['perform'],['performing']
Performance,cc: @tpoterba. There's a Sphinx extension that uses Python type annotations to figure out; parameter types so we don't have to write it twice. I implement that here. I; wrestled for a while with the cyclical import structure until I realized that. ```; from . import module; ```. side steps a lot of the cyclical issues (pylint still dislikes it). I also added `sphinx.ext.intersphinx` which makes every reference to a Python; standard library thing work correctly. Now when I write; `concurrent.futures.ThreadPoolExecutor` it links right to the Python docs; page. It's really quite nice.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9198:485,concurren,concurrent,485,https://hail.is,https://github.com/hail-is/hail/pull/9198,1,['concurren'],['concurrent']
Performance,"cc: the ""services team"" @cseed, @johnc1231. This fixes gateway to log the user's IP. Forthcoming PRs will fix all downstream; services. ---. There are two important pieces of which to be aware:. - The gateway pod are exposed via the gateway Service, which is the only; object modified in this PR.; - K8s fulfills our request for the gateway Service by creating a [Google TCP; LoadBalancer](https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list). Moreover,; we specify `loadBalancerIP` which is a manually (outside of k8s) allocated IP; which we expose on the public internet. When you `curl https://hail.is` this is what happens:. - Your packet travels across the internet until it reaches the Google TCP; LoadBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8045:376,Load,LoadBalancer,376,https://hail.is,https://github.com/hail-is/hail/pull/8045,6,"['Load', 'load']","['LoadBalancer', 'loadBalancerIP', 'loadBalancers', 'loadbalancing']"
Performance,"ccessfully, the job is marked as scheduled.; 4. Once all requests complete, goto 1. On the worker, what happens inside `/api/v1alpha/batches/jobs/create`:; 1. Read metadata describing the job to schedule from the request body; 2. Using that information, load the full job spec from blob storage; 3. Spawn a task to run the job asynchronously; 4. Respond to the driver with a 200. The key point relevant to this issue is that the driver currently must wait for all the requests to workers in an iteration to complete before it starts the next iteration of the scheduler. This leaves the scheduler vulnerable to problematic workers or workers that happen to be preempted during the scheduling process. So, the driver sets a [2 second timeout](https://github.com/hail-is/hail/blob/b27737f67bf9e69f1abed2fec07fc7c921790ef8/batch/batch/driver/job.py#L585) on the call to `/api/v1alpha/batches/jobs/create`. Additionally, this general design means that in the event of a request timeout or transient error, Batch cannot guarantee that there is always at most one concurrent running attempt for a given job. This ends up being a fine (and intentional) concession in practice because the idempotent design of preemptible jobs tends to cover this scenario, but it is regardless wasted compute and cost to users. Nevertheless, we strive to minimize cases where we might halt the scheduling loop or double-schedule work, and one way to do that in the current design is to minimize the variance in latency of `/api/v1alpha/batches/jobs/create`. The largest source of this latency is the request to blob storage. While GCS and ABS are relatively fast and highly available, Batch in Azure Terra requires first obtaining SAS tokens from the Terra control plane, which can introduce much higher and more variable latency. There have also been occurrences in the past of corrupted or deleted specs, which introduce unexpected failure modes that should error the job but instead disrupt the scheduling loop. Many of the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14456:1498,concurren,concurrent,1498,https://hail.is,https://github.com/hail-is/hail/issues/14456,1,['concurren'],['concurrent']
Performance,ce$1(PrettyPrintWriter.scala:68); at is.hail.utils.prettyPrint.Doc$.render(PrettyPrintWriter.scala:139); at is.hail.utils.prettyPrint.Doc.render(PrettyPrintWriter.scala:163); at is.hail.utils.prettyPrint.Doc.render(PrettyPrintWriter.scala:167); at is.hail.expr.ir.Pretty.sexprStyle(Pretty.scala:466); at is.hail.expr.ir.Pretty.apply(Pretty.scala:429); at is.hail.expr.ir.Pretty$.apply(Pretty.scala:22); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:45); at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:450); at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:486); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); at is.hail.utils.package$.using(package.scala:635); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); at is.hail.utils.pa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046:3325,Optimiz,OptimizePass,3325,https://hail.is,https://github.com/hail-is/hail/issues/13046,1,['Optimiz'],['OptimizePass']
Performance,ce.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:456); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:456); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor90.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:7825,concurren,concurrent,7825,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['concurren'],['concurrent']
Performance,ce.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.net.SocketTimeoutException: connect timed out; E 	at java.net.PlainSocketImpl.socketConnect(Native Method); E 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); E 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); E 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); E 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); E 	at java.net.Socket.connect(Socket.java:607); E 	at is.hail.relocated.org.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75); E 	at is.hail.relocated.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142); E 	at is.hail.relocated.org.apache.htt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:7539,concurren,concurrent,7539,https://hail.is,https://github.com/hail-is/hail/issues/13074,1,['concurren'],['concurrent']
Performance,ce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:7901,concurren,concurrent,7901,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['concurren'],['concurrent']
Performance,ce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:5348,concurren,concurrent,5348,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['concurren'],['concurrent']
Performance,"ce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$ano",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:11371,concurren,concurrent,11371,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['concurren'],['concurrent']
Performance,cessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:750). java.lang.ClassFormatError: Too many arguments in method signature in class file __C2866stream; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:635); 	at is.hail.asm4s.HailClassLoader.liftedTree1$1(HailClassLoader.scala:10); 	at is.hail.asm4s.HailClassLoader.loadOrDefineClass(HailClassLoader.scala:6); 	at is.hail.asm4s.ClassesBytes.$anonfun$load$1(ClassBuilder.scala:64); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$ano,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:11873,load,load,11873,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['load'],['load']
Performance,"cestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:21171,Cache,CacheDir,21171,https://hail.is,https://github.com/hail-is/hail/issues/14513,4,['Cache'],['CacheDir']
Performance,"ch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of filtered variant indices to the workers somehow, so I shipped them as base64 encoded arrays of bytes. It's pretty groady (and that's why I added the commons-codec library). I don't know how else to initialize record readers with hadoop. Generally, I think the BGEN loading code could use a clean up, and I haven't done that here, if anything I've made it more complicated. I also need to check that there are tests for or write tests for:. - indexing tables doesn't cause an extra shuffle; - ~the include lid and include raid flags~ included in #3779; - the variant list flag; - `getSplits`; - the variant filtering code; - ~loading a bgen with no entries~ exists: `BGENTests.test_import_bgen_no_entries`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3727:2820,load,loading,2820,https://hail.is,https://github.com/hail-is/hail/pull/3727,3,['load'],['loading']
Performance,"ch to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`.; - The `query` user was missing from bootstrap-create-accounts.; - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule; - Use a dummy `WorkerBackend` when we're on the worker. The worker isn't allowed to call these; meth",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:1909,load,loads,1909,https://hail.is,https://github.com/hail-is/hail/pull/10279,1,['load'],['loads']
Performance,chClient allocated.; 2024-11-05 02:43:37.207 ServiceBackendAPI$: INFO: BatchConfig parsed.; 2024-11-05 02:43:37.209 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2024-11-05 02:43:37.783 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]; 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]; 	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) [jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]; 	at java.lang.Thread.run(Thread.java:829) [?:?]; Caused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String length (20013488) exceeds the maximum length (20000000); 	at com.fasterxml.jackson.core.StreamReadConstraints.validateStringLength(StreamReadConstraints.java:324) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.ReadConstrainedTextBuffer.validateStringLength(ReadConstrainedTextBuffer.java:27) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.TextBuffer.finishCurrentSegment(TextBuffer.java:939) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._finishString2(UTF8StreamJsonParser.java:2584) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14749:2917,concurren,concurrent,2917,https://hail.is,https://github.com/hail-is/hail/issues/14749,1,['concurren'],['concurrent']
Performance,"ched gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2371,cache,cached,2371,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"ched python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2279,cache,cached,2279,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"ched requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2453,cache,cached,2453,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"ck:; E Failed: Timeout >600.0s. /usr/lib/python3.9/concurrent/futures/thread.py:162: Failed; ---------------------------- Captured log teardown -----------------------------; INFO hailtop.utils:utils.py:450 discarding exception; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 378, in rm_dir; await self.rmdir(path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 352, in rmdir; return await blocking_to_async(self._thread_pool, os.rmdir, path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 162, in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; File ""/usr/lib/python3.9/asyncio/futures.py"", line 284, in __await__; yield self # This tells Task to wait for completion.; File ""/usr/lib/python3.9/asyncio/tasks.py"", line 328, in __wakeup; future.result(); File ""/usr/lib/python3.9/asyncio/futures.py"", line 201, in result; raise self._exception; File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 58, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 163, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); OSError: [Errno 39] Directory not empty: '/tmp/JnQ2m'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 409, in rmtree; await rm_dir(pool, contents_tasks_by_dir.get(path, []), path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 387, in rm_dir; excs = [exc; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 389, in <listcomp>; for exc in [t.exception()]; File ""/usr/lib/python3.9/asyncio/futures.py"", line 214, in exception; raise exc; asyncio.exceptions.CancelledError; ```. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:13078,concurren,concurrent,13078,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,ckage.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_matrix_ir$2(SparkBackend.scala:689); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:69); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:69); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.annotations.RegionPool$.scoped(Reg,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:2700,Load,LoadPlink,2700,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,"ckages/hail/fs/hadoop_fs.py in ls(self, path); 40; 41 def ls(self, path: str) -> List[Dict]:; ---> 42 return json.loads(self._utils_package_object.ls(self._jfs, path)); 43; 44 def mkdir(self, path: str) -> None:. /Library/Python/3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258; 1259 for temp_arg in temp_args:. /Library/Python/3.7/site-packages/hail/backend/spark_backend.py in deco(*args, **kwargs); 49 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 50 'Hail version: %s\n'; ---> 51 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None; 52 except pyspark.sql.utils.CapturedException as e:; 53 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz. Java stack trace:; java.io.IOException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1284); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoke",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:1552,concurren,concurrent,1552,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,ckend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90); 	at is.hail.backend.service.Worker$.$anonfun$main$9(Worker.scala:172); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:171); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.service.Worker$.main(Worker.scala:169); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.lang.NullPointerException: null; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionPutTask.call(JsonResumableSessionPutTask.java:201); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.lambda$put$0(JsonResumableSession.java:81); 	at is.hail.relocated.com.google.cloud.storage.Retrying.lambda$run$0(Retrying.java:102); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:99); 	at is.hail.relocated.com.google.cloud.storage.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13937:5529,concurren,concurrent,5529,https://hail.is,https://github.com/hail-is/hail/issues/13937,1,['concurren'],['concurrent']
Performance,"ckend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.115-71fc978b5c22; Error summary: SocketException: Connection reset. -------------------. Some more content from the failing worker job:. ...; 2023-05-04 01:04:35.959 : INFO: executing D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e61c-4fa9-9419-2278528f3c86; 2023-05-04 01:04:37.559 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=132096, peakBytesReadable=129.00 KiB, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.560 : INFO: RegionPool: FREE: 129.0K allocated (129.0K blocks / 0 chunk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:21795,concurren,concurrent,21795,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['concurren'],['concurrent']
Performance,ckend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:464); 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:237); 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190); 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109); 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1400); 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1368); 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73); 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:962); 	at java.io.Buf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12982:14081,concurren,concurrent,14081,https://hail.is,https://github.com/hail-is/hail/issues/12982,3,['concurren'],['concurrent']
Performance,ckend.scala:458); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:456); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:456); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:456); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor90.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:7618,concurren,concurrent,7618,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['concurren'],['concurrent']
Performance,ckend.scala:460); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:459); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.net.SocketTimeoutException: connect timed out; E 	at java.net.PlainSocketImpl.socketConnect(Native Method); E 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); E 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); E 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); E 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); E 	at java.net.Socket.connect(Socket.java:607); E 	at is.hail.relocated.org.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:7332,concurren,concurrent,7332,https://hail.is,https://github.com/hail-is/hail/issues/13074,1,['concurren'],['concurrent']
Performance,cketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:456); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:456); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:456); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor90.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$paral,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:7682,concurren,concurrent,7682,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['concurren'],['concurrent']
Performance,cketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:459); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.net.SocketTimeoutException: connect timed out; E 	at java.net.PlainSocketImpl.socketConnect(Native Method); E 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); E 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); E 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); E 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); E 	at java.net.Socket.connect(Socket.java:607); E 	at is.hail.relocated.org.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75); E 	at is.hail.relocated.org.apache.h,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:7396,concurren,concurrent,7396,https://hail.is,https://github.com/hail-is/hail/issues/13074,1,['concurren'],['concurrent']
Performance,"ckquote>; <h2><!-- raw HTML omitted -->2.9.13 (2022-06-27)<!-- raw HTML omitted --></h2>; <ul>; <li>fix: /@fs/ dir traversal with escaped chars (fixes <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8498"">#8498</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8805"">#8805</a>) (<a href=""https://github.com/vitejs/vite/commit/e109d64"">e109d64</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8498"">#8498</a> <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8805"">#8805</a></li>; <li>fix(wasm): support decoding data URL in Node &lt; v16 (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8668"">#8668</a>) (<a href=""https://github.com/vitejs/vite/commit/1afc1c2"">1afc1c2</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8668"">#8668</a></li>; </ul>; <h2><!-- raw HTML omitted -->2.9.12 (2022-06-10)<!-- raw HTML omitted --></h2>; <ul>; <li>fix: outdated optimized dep removed from module graph (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8534"">#8534</a>) (<a href=""https://github.com/vitejs/vite/commit/c0d6c60"">c0d6c60</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8534"">#8534</a></li>; </ul>; <h2><!-- raw HTML omitted -->2.9.11 (2022-06-10)<!-- raw HTML omitted --></h2>; <ul>; <li>fix: respect server.headers in static middlewares (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8481"">#8481</a>) (<a href=""https://github.com/vitejs/vite/commit/ab7dc1c"">ab7dc1c</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8481"">#8481</a></li>; <li>fix(dev): avoid FOUC when swapping out link tag (fix <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7973"">#7973</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8495"">#8495</a>) (<a href=""https://github.com/vitejs/vite/commit/01fa807"">01fa807",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12142:2357,optimiz,optimized,2357,https://hail.is,https://github.com/hail-is/hail/pull/12142,2,['optimiz'],['optimized']
Performance,"code>1442e64</code></a> chore(main): release 2.6.0 (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/965"">#965</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/f9f23f4370f2a7a5b2c66ee56a5e700ef03b5b06""><code>f9f23f4</code></a> fix: revert &quot;feat: add api key support (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/826"">#826</a>)&quot; (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/964"">#964</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/3c72365d8407bb097568919123cd7232c1a49f4f""><code>3c72365</code></a> chore: update user cred for system test (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/966"">#966</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/52c8ef90058120d7d04d3d201adc111664be526c""><code>52c8ef9</code></a> feat: ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/962"">#962</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/83b20f0b4d32b2ff1183a9c2926afd37f3baf92b""><code>83b20f0</code></a> chore: update user creds for system test (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/963"">#963</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/3c9feff3e9037a15bf07496623e3a810f117adcf""><code>3c9feff</code></a> chore(main): release 2.5.0 (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/960"">#960</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/a8eb4c8693055a3420cfe9c3420aae2bc8cd465a""><code>a8eb4c8</code></a> feat: ADC can load an impersonated service account credentials. (",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546:10343,load,load,10343,https://hail.is,https://github.com/hail-is/hail/pull/11546,1,['load'],['load']
Performance,"collection/mutable/ArrayBuffer; INVOKESPECIAL is/hail/codegen/generated/C0.apply ([Ljava/lang/Object;Lscala/collection/mutable/ArrayBuffer;)Ljava/lang/Object;; ARETURN; MAXSTACK = 3; MAXLOCALS = 3; }; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-297>"", line 2, in filter_genotypes; File ""/tmp/spark-0721abd3-c72d-4439-a655-c09fddad864c/userFiles-7c41df44-c5b2-44b1-924e-8f73a9aa8148/hail.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: ClassNotFoundException: is.hail.asm4s.AsmFunction2. Java stack trace:; java.lang.NoClassDefFoundError: is/hail/asm4s/AsmFunction2; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:10774,load,loadClass,10774,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['load'],['loadClass']
Performance,com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at java.util.concurrent.AbstractExecutorService.doInvokeAny(AbstractExecutorService.java:193); 	at java.util.concurrent.AbstractExecutorService.invokeAny(AbstractExecutorService.java:215); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1282); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodA,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:2933,concurren,concurrent,2933,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,"com/googleapis/google-auth-library-python/issues/904"">#904</a>) (<a href=""https://www.github.com/googleapis/google-auth-library-python/commit/bd0ccc5fe77d55f7a19f5278d6b60587c393ee3c"">bd0ccc5</a>)</li>; </ul>; <h2>v2.3.2</h2>; <h3>Bug Fixes</h3>; <ul>; <li>add clock_skew_in_seconds to verify_token functions (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/894"">#894</a>) (<a href=""https://www.github.com/googleapis/google-auth-library-python/commit/8e95c1e458793593972b6b05a355aaeaecd31670"">8e95c1e</a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/google-auth-library-python/blob/main/CHANGELOG.md"">google-auth's changelog</a>.</em></p>; <blockquote>; <h2><a href=""https://github.com/googleapis/google-auth-library-python/compare/v2.5.0...v2.6.0"">2.6.0</a> (2022-01-31)</h2>; <h3>Features</h3>; <ul>; <li>ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/962"">#962</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/52c8ef90058120d7d04d3d201adc111664be526c"">52c8ef9</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>revert &quot;feat: add api key support (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/826"">#826</a>)&quot; (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/964"">#964</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/f9f23f4370f2a7a5b2c66ee56a5e700ef03b5b06"">f9f23f4</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/google-auth-library-python/compare/v2.4.1...v2.5.0"">2.5.0</a> (2022-01-25)</h2>; <h3>Features</h3>; <ul>; <li>ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546:5322,load,load,5322,https://hail.is,https://github.com/hail-is/hail/pull/11546,1,['load'],['load']
Performance,"command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: UnsupportedFileSystemException: No FileSystem for scheme ""gs"". Java stack trace:; org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""gs""; 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.isDir(FS.scala:175); 	at is.hail.io.fs.FS.isDir$(FS.scala:173); 	at is.hail.io.fs.HadoopFS.isDir(HadoopFS.scala:70); 	at is.hail.expr.ir.RelationalSpec$.readMetadata(AbstractMatrixTableSpec.scala:30); 	at is.hail.expr.ir.RelationalSpec$.readReferences(AbstractMatrixTableSpec.scala:68); 	at is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:596); 	at is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.Method",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10530:3707,Cache,Cache,3707,https://hail.is,https://github.com/hail-is/hail/issues/10530,1,['Cache'],['Cache']
Performance,"conda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Basically, the ; ```; hl.utils.get_1kg('data/'); ```; ![image](http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775:2649,Load,LoadVCF,2649,https://hail.is,https://github.com/hail-is/hail/issues/4775,1,['Load'],['LoadVCF']
Performance,configure apache webserver to allow navbar to be loaded from github,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1024:49,load,loaded,49,https://hail.is,https://github.com/hail-is/hail/issues/1024,1,['load'],['loaded']
Performance,"consider:; ```ht.annotate(x=5)```. Here's the IR:. ```; (TableMapRows (idx) 1; (TableRange 5 5); (Let __uid_1; (InsertFields; (SelectFields (); (Ref Struct{idx:Int32} row)); (x; (I32 5))); (InsertFields; (MakeStruct; (idx; (GetField idx; (Ref Struct{idx:Int32} row)))); (x; (GetField x; (Ref Struct{x:Int32} __uid_1)))))); ```. Cotton added some optimizer rules, but they're not always sufficient. We need to fix this",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4001:346,optimiz,optimizer,346,https://hail.is,https://github.com/hail-is/hail/issues/4001,1,['optimiz'],['optimizer']
Performance,"ct(self, _localize). /opt/conda/miniconda3/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in collect(self, _localize); 1918 e = construct_expr(rows_ir, hl.tarray(t.row.dtype)); 1919 if _localize:; -> 1920 return Env.backend().execute(e._ir); 1921 else:; 1922 return e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 96 raise HailUserError(message_and_trace) from None; 97; ---> 98 raise e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 72 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 73 try:; ---> 74 result = json.loads(self._jhc.backend().executeJSON(jir)); 75 value = ir.typ._from_json(result['value']); 76 timings = result['timings']. /usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 1302; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stag",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:5069,load,loads,5069,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['load'],['loads']
Performance,ct.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at java.util.concurrent.AbstractExecutorService.doInvokeAny(AbstractExecutorService.java:193); 	at java.util.concurrent.AbstractExecutorService.invokeAny(AbstractExecutorService.java:215); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1282); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.Met,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:3220,concurren,concurrent,3220,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,"ct/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:2322,load,load,2322,https://hail.is,https://github.com/hail-is/hail/pull/4931,2,"['load', 'perform']","['load', 'performance']"
Performance,"cting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6941,cache,cached,6941,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"curl -XPOST localhost:5000/jobs/create -H 'Content-Type: application/json' -d '{ ; ""spec"" : {; ""containers"" : [; { ""name"" : ""foobarbaz""; , ""image"" : ""alpine:3.8""; , ""command"": [""/bin/sh"", ""-c"", ""echo hi""] }] } }'; ```; 3. ; ```; curl localhost:5000/jobs; ```; 4. the job never transitions to Complete and the server log shows:; ```; (hail-batch) # make run; BATCH_USE_KUBE_CONFIG=1 python -c 'import batch.server; batch.server.serve()'; INFO	| 2018-11-13 18:12:19,124 	| server.py 	| <module>:25 | REFRESH_INTERVAL_IN_SECONDS 300; INFO	| 2018-11-13 18:12:19,125 	| server.py 	| <module>:28 | instance_id = 63aeb0cd4fa840a9864cfd909ce7f682; INFO	| 2018-11-13 18:12:19,130 	| server.py 	| run_forever:391 | run_forever: run target kube_event_loop; INFO	| 2018-11-13 18:12:19,130 	| server.py 	| run_forever:391 | run_forever: run target polling_event_loop; INFO	| 2018-11-13 18:12:19,131 	| server.py 	| run_forever:391 | run_forever: run target flask_event_loop; * Serving Flask app ""batch"" (lazy loading); * Environment: production; WARNING: Do not use the development server in a production environment.; Use a production WSGI server instead.; * Debug mode: off; INFO	| 2018-11-13 18:12:19,168 	| _internal.py 	| _log:88 | * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit); INFO	| 2018-11-13 18:12:20,141 	| server.py 	| refresh_k8s_state:360 | started k8s state refresh; INFO	| 2018-11-13 18:12:20,159 	| server.py 	| refresh_k8s_state:379 | k8s state refresh complete; INFO	| 2018-11-13 18:12:20,160 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:12:20] ""POST /refresh_k8s_state HTTP/1.1"" 204 -; INFO	| 2018-11-13 18:12:55,902 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:12:55] ""GET /jobs HTTP/1.1"" 200 -; INFO	| 2018-11-13 18:17:20,174 	| server.py 	| refresh_k8s_state:360 | started k8s state refresh; INFO	| 2018-11-13 18:17:20,179 	| server.py 	| refresh_k8s_state:379 | k8s state refresh complete; INFO	| 2018-11-13 18:17:20,179 	| _internal.py 	| _log:88 | ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4773:1118,load,loading,1118,https://hail.is,https://github.com/hail-is/hail/issues/4773,1,['load'],['loading']
Performance,"cutor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13244,Load,LoadVCF,13244,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,"d support for reading DX10 BC4 DDS images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7603"">#7603</a> [<a href=""https://github.com/sambvfx""><code>@sambvfx</code></a>]</li>; <li>Optimized ImageStat.Stat.count <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7599"">#7599</a> [<a href=""https://github.com/florath""><code>@florath</code></a>]</li>; <li>Moved error from truetype() to FreeTypeFont <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7587"">#7587</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Correct PDF palette size when saving <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7555"">#7555</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Fixed closing file pointer with olefile 0.47 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7594"">#7594</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>ruff: Minor optimizations of list comprehensions, x in set, etc. <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7524"">#7524</a> [<a href=""https://github.com/cclauss""><code>@cclauss</code></a>]</li>; <li>Build Windows wheels using cibuildwheel <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7580"">#7580</a> [<a href=""https://github.com/nulano""><code>@nulano</code></a>]</li>; <li>Raise ValueError when TrueType font size is zero or less <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7584"">#7584</a> [<a href=""https://github.com/akx""><code>@akx</code></a>]</li>; <li>Install cibuildwheel from requirements file <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7581"">#7581</a> [<a href=""https://github.com/hugovk""><code>@hugovk</code></a>]</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/python-pillow",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:8813,optimiz,optimizations,8813,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['optimiz'],['optimizations']
Performance,d.OrderedRVD.coalesce(OrderedRVD.scala:200); at is.hail.variant.MatrixTable.coalesce(MatrixTable.scala:2073); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:299); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:8560,load,loadField,8560,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['load'],['loadField']
Performance,d.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cma,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:12940,Load,LoadMatrixParser,12940,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrixParser']
Performance,"d</h3>; <ul>; <li>Fix invalid indexing in line and column number reporting in; <code>JSONDecodeError</code>.</li>; <li>Fix <code>orjson.OPT_STRICT_INTEGER</code> not raising an error on; values exceeding a 64-bit integer maximum.</li>; </ul>; <h2>3.6.5</h2>; <h3>Fixed</h3>; <ul>; <li>Fix build on macOS aarch64 CPython 3.10.</li>; <li>Fix build issue on 32-bit.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.6.7 - 2022-02-14</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of deserializing almost-empty documents.</li>; <li>Publish arm7l <code>manylinux_2_17</code> wheels to PyPI.</li>; <li>Publish amd4 <code>musllinux_1_1</code> wheels to PyPI.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix build requiring <code>python</code> on <code>PATH</code>.</li>; </ul>; <h2>3.6.6 - 2022-01-21</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing <code>datetime.datetime</code> using <code>tzinfo</code> that; are <code>zoneinfo.ZoneInfo</code>.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix invalid indexing in line and column number reporting in; <code>JSONDecodeError</code>.</li>; <li>Fix <code>orjson.OPT_STRICT_INTEGER</code> not raising an error on; values exceeding a 64-bit integer maximum.</li>; </ul>; <h2>3.6.5 - 2021-12-05</h2>; <h3>Fixed</h3>; <ul>; <li>Fix build on macOS aarch64 CPython 3.10.</li>; <li>Fix build issue on 32-bit.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ijl/orjson/commit/aee8a9fed45f84d227cf2cb7102656aa65a4890a""><code>aee8a9f</code></a> 3.6.7</li>; <li><a href=""https://github.com/ijl/orjson/commit/622cd7b1167262ffe458f6a2c15ec239f015d174""><code>622cd7b</code></a> Add special casing for deserializing empty objects, lists and strings</li>; <li><a href=""https://github.com/ijl/orjson/commit/5da14a00fed93",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11572:1790,perform,performance,1790,https://hail.is,https://github.com/hail-is/hail/pull/11572,1,['perform'],['performance']
Performance,"dAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:768); at is.hail.utils.package$.using(package.scala:575); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:11670,Load,LoadVCF,11670,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['Load'],['LoadVCF']
Performance,dConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5707,Optimiz,Optimize,5707,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,dLeftJoinDistinct$1.apply(KeyedRVD.scala:152); E 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:149); E 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$24.apply(ContextRDD.scala:316); E 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$24.apply(ContextRDD.scala:316); E 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$10$$anonfun$apply$11.apply(ContextRDD.scala:218); E 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$10$$anonfun$apply$11.apply(ContextRDD.scala:218); E 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); E 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); E 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409); E 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); E 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409); E 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409); E 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); E 	at is.hail.rvd.RVD$$anonfun$34.apply(RVD.scala:1220); E 	at is.hail.rvd.RVD$$anonfun$34.apply(RVD.scala:1219); E 	at is.hail.sparkextras.ContextRDD$$anonfun$crunJobWithIndex$1.apply(ContextRDD.scala:242); E 	at is.hail.sparkextras.ContextRDD$$anonfun$crunJobWithIndex$1.apply(ContextRDD.scala:240); E 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); E 	at org.apache.spark.scheduler.Task.run(Task.scala:121); E 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); E 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); E 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748). ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9867:9087,concurren,concurrent,9087,https://hail.is,https://github.com/hail-is/hail/issues/9867,2,['concurren'],['concurrent']
Performance,dRVD.scala:1014); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:471); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4114:3252,concurren,concurrent,3252,https://hail.is,https://github.com/hail-is/hail/issues/4114,1,['concurren'],['concurrent']
Performance,"d` an `OrderedRVD`, possibly with some non-empty key. This is consistent with the rule that the `rvd` must always have a stronger/longer key than the `TableType`.; * **small tweaks** - Now I start working through the `TableIR` nodes, rewriting them to remove explicit uses of `UnpartitionedRVD`. The general plan is to sandwich the rvd logic between `toOrderedRVD` and `toOldStyleRVD`. The first takes an `UnpartitionedRVD` to an `OrderedRVD` with empty key (and leaves `OrderedRVD`s alone), and the second takes an `OrderedRVD` to an `UnpartitionedRVD` if its key was empty, and leaves it alone otherwise. Once they're all rewritten this way, I redefine `toOldStyleRVD` to always return `OrderedRVD`, and `UnpartitionedRVD` is no longer used.; * **remove `TableUnkey`** - With `UnpartitionedRVD` going away, `TableUnkey` is no longer necessary, it's equivalent to keying by an empty key.; * **small tweaks** - these next two rewrite more `TableIR` nodes; * **Merge master** - the big one; * **tweak MatrixColsTable** - 1) Optimize `coerce` by checking if the requested key is empty, avoiding a scan in that case. 2) Optimize `sortedColsValue` by checking if the column key is empty, avoiding the sort in that case. 3) Simplify `colsRVD`, removing the case on the type of the `RVD`, just calling `coerce` and letting the previous optimizations avoid unnecessary work.; * **`distinctByKey` fix** - While looking over `TableIR` implementations, I noticed a bug in `distinctByKey`: you need to be sure no key is split across multiple partitions. To be sure the empty key edge case still works, I added a test to check that `strictify` on an empty-key partitioner will always collapse everything to one partition.; * **Flipped switch** - redifines `toOldStyleRVD` to just return the `OrderedRVD` unchanged, and asserts that `TableValue.rvd` is always an `OrderedRVD`.; * **rest of the `TableIR` tweaks** - added a factory method `OrderedRVD.unkeyed` to replace `UnpartitionedRVD.apply`.; * the rest are si",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4319:1833,Optimiz,Optimize,1833,https://hail.is,https://github.com/hail-is/hail/pull/4319,1,['Optimiz'],['Optimize']
Performance,"d`. With this infrastructure in place, I was able to implement read, write, and; matrix-multiply for DNDArray!. In addition, to the arguable hacks above, a couple pain points remain:; 1. I do not know how to rename keys in Python without triggering shuffles. If I; write `key_by(x=t.y, y=t.x)`, Hail implements this as; `TableKeyBy(TableMapRows(TableKeyBy(Array(), ...)`. The inner key by throws; the keys away so that they can be modified with TableMapRows. Unfortunately,; this completely defeats my attempts to avoid shuffles. I avoid this issue by; not using fixed names for the x and y block coordinates (their names are; stored in `x_field` and `y_field`).; 2. Hail lacks `ndarray_sum`. Instead, I convert from ndarray to array so that I; can use `array_sum`. Unfortunately, this operation seems to completely; dominate all of my time. It takes about 10x as much time as the matrix; multiplies take. I do not understand this. I should be reading the entries in; column-major order. Performance; -----------. ```; In [1]: %%time; ...: import hail as hl; ...: mt = hl.balding_nichols_model(n_populations=2,; ...: n_variants=10000,; ...: n_samples=10000,; ...: n_partitions=100); ...: mt = mt.select_entries(gt = hl.float(mt.GT.n_alt_alleles())); ...: da = hl.experimental.dnd.array(mt, 'gt'); ...: da.write('/tmp/in.da', overwrite=True); In [3]: %%time; ...: bm = hl.linalg.BlockMatrix.from_entry_expr(mt.gt); In [5]: %%time; ...: (bm @ bm.T).write('/tmp/foo.bm', overwrite=True); In [7]: %%time; ...: import hail as hl; ...: da = hl.experimental.dnd.read('/tmp/in.da'); ...: (da @ da.T).write('/tmp/out.da', overwrite=True); ```. Block matrix performed the matrix multiply in 19.3s. DNDArray performed the; matrix multiply in 37.6s. Block Matrix:; ![Screen Shot 2020-05-26 at 1 37 51 PM](https://user-images.githubusercontent.com/106194/82932367-54630200-9f56-11ea-86f4-94726c36d727.png). DNDArray:; ![Screen Shot 2020-05-26 at 1 37 08 PM](https://user-images.githubusercontent.com/106194/829323",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8864:1984,Perform,Performance,1984,https://hail.is,https://github.com/hail-is/hail/pull/8864,1,['Perform'],['Performance']
Performance,"damentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutable unordered collections of key-value pairs. All keys must have one PType, and all values must have one (possibly different from keys) PType. ## Core Methods. ```scala; def elementType: PStruct; ```. - The PStruct representation of the key/value pair. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalDict. A PCanonicalArray-backed implementation of PDict. # <a name=""parray""></a> PNDArray. An abstract class for multidimensional arrays (tensors) that have a row-major or column-major layout. ## Core Methods. ```scala; val shape: StaticallyKnownField[PTuple, Long]; val strides: StaticallyKnownField[PTuple, Long]; ```. - Defines the tensor shape. ```scala; def loadElementToIRIntermediate(indices: Array[Code[Long]], ndAddress: Code[Long], mb: MethodBuilder): Code[_]; ```. - Load the element's primitive representation, as indexed by `indices`, which specifies the element index at every dimension in the PNDArray's shape. ```scala; def linearizeIndicesRowMajor(indices: Array[Code[Long]], shapeArray: Array[Code[Long]], mb: MethodBuilder): Code[Long]; ```. - Get the off-heap index of the element (since NDArray elements are stored as a 1D series of bytes off-heap). ```scala; def unlinearizeIndexRowMajor(index: Code[Long], shapeArray: Array[Code[Long]], mb: MethodBuilder): (Code[Unit], Array[Code[Long]]); ```. - Generate the index path that represents the virtual, shape-dependent index into an arbitrary tensor. ```scala; def copyRowMajorToColumnMajor(rowMajorAddress: Code[Long], targetAddress: Code[Long], nRows: Code[Long], nCols: Code[Long], mb: MethodBuilder): Code[Unit]. def copyColumnMajorToRowMajor(colMajorAddress: Code[Long], targetAddress: Code[Long], nRows: Code[Long], nCols: Code[Long], m",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:6612,load,loadElementToIRIntermediate,6612,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['load'],['loadElementToIRIntermediate']
Performance,dd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413:3783,concurren,concurrent,3783,https://hail.is,https://github.com/hail-is/hail/issues/3413,1,['concurren'],['concurrent']
Performance,"ddition of a `save_path` and `vds_paths`. The `save_path` is used as; a filesystem or object storage location to save the state of the; combiner so that it may be resumed. The vds paths implement the long; awaited mixed hail and gvcf inputs, that was never completed for the VCF; combiner. End users should not call `VariantDatasetCombiner.__init__`; instead preferring the free function new_combiner. The; `VariantDatasetCombiner` API is fairly simple:. * @property finished; * save: serialize the current state to save_path; * step: does the next unit of work for the combiner (described in more detail below); * run: until `finished`, calls `save` then `step` followed by one last `save`. The public api is present in 2 free functions:. * new_combiner: public VariantDatasetCombiner constructor; * load_combiner. if `new_combiner` is not given a `save_path`, it will compute one based on; the SHA-256 of it's inputs. If the file at `save_path` is present, and; a valid `VariantDatasetCombiner`, it will load it and use it, unless the; `force` argument to `new_combiner` is True. This way, every combiner has; a `save_path`. ## The `step` algorithm. If there are any gvcf paths still uncombined, they are combined; `branch_factor` at a time, using `batch_size` parallel matrix table; writes to achive good parallelism. If there is only one merged vds, and; no vds arguments remaining to be combined it is written to; `output_path`. Otherwise the new vds paths are appended to the vds list; and the vds list is sorted by number of samples. The vds paths are always kept sorted by samples and new items are only; ever added to the end of the list and then sorted. Since python's sort; function is stable, this will produce a stable output for a given set of; inputs and batch size. In order to preserve the gvcf sample ordering; from the paths, we work from the start of the gvcf list. Once all the gvcfs are combined, we combine the variant datasets in; reverse order (smallest number of samples to ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10892:1383,load,load,1383,https://hail.is,https://github.com/hail-is/hail/pull/10892,1,['load'],['load']
Performance,"de></a> Prepare next development iteration</li>; <li><a href=""https://github.com/java-native-access/jna/commit/0d7499f105e4495bdea15fc21f5b1046e81ca822""><code>0d7499f</code></a> Release 5.12.0</li>; <li><a href=""https://github.com/java-native-access/jna/commit/fa86166d4f75ef4478de7ad9d7d6c0b6b6933ee0""><code>fa86166</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1445"">#1445</a> from matthiasblaesing/aix</li>; <li><a href=""https://github.com/java-native-access/jna/commit/4cca4405f7f6bc32d2a08495efb81c081b065279""><code>4cca440</code></a> Fix name mapping difference between AIX JDK 8 and Semeru JDK 18</li>; <li><a href=""https://github.com/java-native-access/jna/commit/f58b0f8f6b5c013adfe44a2cfb018ccb6ef6a688""><code>f58b0f8</code></a> Improve test stability on AIX (exclude tests that are expected to fail)</li>; <li><a href=""https://github.com/java-native-access/jna/commit/c1565fb89469cbcba67b1cc305e16d520779b270""><code>c1565fb</code></a> Handle race condition in PdhUtil#PdhEnumObjectItems (<a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1442"">#1442</a>)</li>; <li><a href=""https://github.com/java-native-access/jna/commit/99fcfa822db86b1f2ba5823dbf17efeb3d246ad5""><code>99fcfa8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1444"">#1444</a> from matthiasblaesing/update_libffi</li>; <li><a href=""https://github.com/java-native-access/jna/commit/9e473350a5ad5e04aab8b01e4018f973976e19f8""><code>9e47335</code></a> Update CHANGES.md</li>; <li>Additional commits viewable in <a href=""https://github.com/java-native-access/jna/compare/5.6.0...5.12.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=net.java.dev.jna:jna&package-manager=gradle&previous-version=5.6.0&new-version=5.12.1)](https://docs.github.com/en/github/ma",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12438:7127,race condition,race condition,7127,https://hail.is,https://github.com/hail-is/hail/pull/12438,1,['race condition'],['race condition']
Performance,"de></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/720"">jpadilla/pyjwt#720</a></li>; <li>api_jwk: Add PyJWKSet.<strong>getitem</strong> by <a href=""https://github.com/woodruffw""><code>@woodruffw</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/725"">jpadilla/pyjwt#725</a></li>; <li>Update usage.rst by <a href=""https://github.com/guneybilen""><code>@guneybilen</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/727"">jpadilla/pyjwt#727</a></li>; <li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/728"">jpadilla/pyjwt#728</a></li>; <li>fix: Update copyright information by <a href=""https://github.com/kkirsche""><code>@kkirsche</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/729"">jpadilla/pyjwt#729</a></li>; <li>Docs: mention performance reasons for reusing RSAPrivateKey when encoding by <a href=""https://github.com/dmahr1""><code>@dmahr1</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/734"">jpadilla/pyjwt#734</a></li>; <li>Fixed typo in usage.rst by <a href=""https://github.com/israelabraham""><code>@israelabraham</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/738"">jpadilla/pyjwt#738</a></li>; <li>Add detached payload support for JWS encoding and decoding by <a href=""https://github.com/fviard""><code>@fviard</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/723"">jpadilla/pyjwt#723</a></li>; <li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/740"">jpadilla/pyjwt#740</a></li>; <li>Raise DeprecationWarning for jwt.decode(verify=...) by <a href=""https://github.com/akx""><code>@akx</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11866:3669,perform,performance,3669,https://hail.is,https://github.com/hail-is/hail/pull/11866,1,['perform'],['performance']
Performance,"ded_gather`, allows it to be used recursively. In particular, suppose we had a semaphore of; 50. The outer `bounded_gather2` might need 20 slots to run its 20 paths in parallel. That leaves 30 slots of parallelism left over for its children. By passing the semaphore down, we let our children optimistically use some of that excess parallelism. 2. If we happen to have the `StatResult` for a particular object, we should never again look it up. In particular, getting the `StatResult` for every file in a directory can be done in O(1) requests. Getting the `StatResult` for each of those files individually (using their full paths) is necessarily O(N). If there was at least one glob and also there are no `suffix_components`, then we can use the `StatResult`s that we learned when checking the glog pattern. The latter point is perhaps a bit more clear with examples:. 1. `gs://foo/bar/baz`. Since there are no globs, we can make exactly one API request to list `gs://foo/bar/baz`. 2. `gs://foo/b*r/baz`. In this case, we must make one API request to list `gs://foo/`. This gives us a list of paths under that prefix. We check each path for conformance to the glob pattern `gs://foo/b*r`. For any path that matches, we must then list `<the matching path>/baz` which may itself be a directory containing files. Overall we make O(1) API requests to do the glob and then O(K) API requests to get the final `StatResult`s, where K is the number of paths matching the glob pattern. 3. `gs://foo/bar/b*z`. In this case, we must make one API request to list `gs://foo/bar/`. In `main`, we then throw away the `StatResult`s we got from that API request! Now we have to make O(K) requests to recover those `StatResult`s for all K paths that match the glob pattern. This PR just caches the `StatResult`s of the most recent globbing. If there is no suffix to later append, then we can just re-use the `StatResult`s we already have!. cc: @daniel-goldstein since you've reviewed this before. Might be of interest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13253:2059,cache,caches,2059,https://hail.is,https://github.com/hail-is/hail/pull/13253,1,['cache'],['caches']
Performance,default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.schedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:4638,Load,LoadVCF,4638,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:11451,Load,LoadVCF,11451,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,"demand. A fresh class loader for; each Hail version allows the classes to co-exist in the same JVM. We cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Sm",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:1222,load,loading,1222,https://hail.is,https://github.com/hail-is/hail/pull/10279,1,['load'],['loading']
Performance,der.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2136:1394,Load,LoadVCF,1394,https://hail.is,https://github.com/hail-is/hail/issues/2136,2,['Load'],['LoadVCF']
Performance,deredRVD$.getPartitionKeyInfo(OrderedRVD.scala:541); at is.hail.rvd.OrderedRVD.coalesce(OrderedRVD.scala:200); at is.hail.variant.MatrixTable.coalesce(MatrixTable.scala:2073); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:299); at is.hail.sparkextras.Co,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:8496,load,loadAddress,8496,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['load'],['loadAddress']
Performance,"different than any physical type arguments passed to it. PLocus. - Concrete implementations (canonical/non). PCall. - Concrete implementations (canonical/non). PInterval. - Concrete implementations (canonical/non). ## Primitive Types. While long-term these may have canonical and non-canonical types, that is outside the scope of this design document. When non-canonical primitives are introduced they will follow the strucutre outlined for non-primitive types. PFloat32. - Represents a 4 byte float. PFloat64. - Represents an 8 byte float. PInt32. - Represents a 4 byte integer. PInt64. - Represents an 8 byte integer. PVoid. <br/>. # Common methods. ```scala; def constructAtAddress(mb: MethodBuilder, addr: Code[Long], region: Code[Region], srcPType: PType, srcAddress: Code[Long], forceDeep: Boolean): Code[Unit]; def constructAtAddress(addr: Long, region: Region, srcPType: PType, srcAddress: Long, forceDeep: Boolean): Unit; ```. - Constructs a new value at `addr`, from `srcAddrss`; - Performs a deep copy when `srcPType != this`, or when `forceDeep == true`. ```scala; def copyFromType(mb: MethodBuilder, region: Code[Region], srcPType: PType, srcAddress: Code[Long], forceDeep: Boolean): Code[Long] = ...; def copyFromType(region: Region, srcPType: PType, srcAddress: Long, forceDeep: Boolean): Long = ...; ```. - Allocates a new address and calls constructAtAddress; - For operations that can be shallow, returns srcAddress, skipping construction. # <a name=""parray""></a> PArray. An abstract class for immutable ordered collections where all elements are of a single type. Does not contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:3396,Perform,Performs,3396,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['Perform'],['Performs']
Performance,ding control to the QoB Job.; 2024-11-05 02:43:37.206 ServiceBackendAPI$: INFO: BatchClient allocated.; 2024-11-05 02:43:37.207 ServiceBackendAPI$: INFO: BatchConfig parsed.; 2024-11-05 02:43:37.209 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2024-11-05 02:43:37.783 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]; 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]; 	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) [jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]; 	at java.lang.Thread.run(Thread.java:829) [?:?]; Caused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String length (20013488) exceeds the maximum length (20000000); 	at com.fasterxml.jackson.core.StreamReadConstraints.validateStringLength(StreamReadConstraints.java:324) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.ReadConstrainedTextBuffer.validateStringLength(ReadConstrainedTextBuffer.java:27) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.TextBuffer.finishCurrentSegment(TextBuffer.java:939) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._finishString2(UTF8StreamJson,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14749:2834,concurren,concurrent,2834,https://hail.is,https://github.com/hail-is/hail/issues/14749,1,['concurren'],['concurrent']
Performance,"doesn't include the ability to use user-provided expr to generate double matrixes for PCA. @cseed @tpoterba I think you had feelings on whether or not PCA should keep the current behavior where it returns an annotated vsm. . Currently, I've kept VariantSampleMatrix.pca as something that annotates and returns a vsm (using annotateVariantsTable for loadings), and then there's another method (VariantSampleMatrix.pcaResults) that just returns a tuple of (scores, loadings, eigenvalues). I could refactor them such that pca() => pcaAsAnnotations() (or something) and pcaResults() => pca(), and updating the python interface to match?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2432:349,load,loadings,349,https://hail.is,https://github.com/hail-is/hail/pull/2432,2,['load'],['loadings']
Performance,don't check if agg states are loaded,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6791:30,load,loaded,30,https://hail.is,https://github.com/hail-is/hail/pull/6791,1,['load'],['loaded']
Performance,"dre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 2524, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: SocketException: Too many open files. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(Compil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:2497,load,loads,2497,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['load'],['loads']
Performance,dropSamples plays well with optimizer,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2285:28,optimiz,optimizer,28,https://hail.is,https://github.com/hail-is/hail/pull/2285,1,['optimiz'],['optimizer']
Performance,"duler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:6002,Load,LoadVCF,6002,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,dx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:34+00:00; ```. I looked at the status:. ```; az storage blob download --account-name haildevtest --container test --name batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json | jq '.' | less; ```. which contained an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: is.hail.backend.service.EndOfInputException; 	at is.hail.backend.service.ServiceBackendSocketAPI2.read(ServiceBackend.scala:497); 	at is.hail.backend.service.ServiceBackendSocketAPI2.readInt(ServiceBackend.scala:510); 	at is.hail.backend.service.ServiceBackendSo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:4368,concurren,concurrent,4368,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance,"e -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: IN",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:1433,Optimiz,Optimize,1433,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,"e -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:1572,Optimiz,Optimize,1572,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,"e = True, center = True, normalize = True); bm_norm = BlockMatrix.read(out_dir + out_name + ""_norm"" + ""_bm""). # LD (unadjusted); starts_and_stops = hl.linalg.utils.locus_windows(mt.locus, radius = 2.1e6, _localize = False); bm_ld = (bm_norm @ bm_norm.T); bm_ld = BlockMatrix._from_java(bm_ld._jbm.filterRowIntervalsIR(Env.backend()._to_java_ir(starts_and_stops._ir), False)); bm_ld.write(out_dir + out_name + ""_LD"" + ""_bm"", overwrite = True); bm_ld = BlockMatrix.read(out_dir + out_name + ""_LD"" + ""_bm""). # Export LD matrices; list_range = [list(range(x.start_idx, x.end_idx + 1)) for x in list_meta[0:5]]; bms = [bm_ld.filter(x,x) for x in list_range]; hl.experimental.export_block_matrices(bms, out_dir + out_name + ""_tissue"" + ""_ld""). # Example image of problem:; <img width=""594"" alt=""Screen Shot 2019-06-13 at 5 36 58 PM"" src=""https://user-images.githubusercontent.com/24594616/59470325-52676800-8e05-11e9-93fe-e48c0e06e70b.png"">. If genotypes are normalized to N(0,1), then X @ X.T should never have values larger than 1 except for floating point precision. This is anecdotal, but I never had this problem when using > 100k samples, but here I'm using ~700 samples. I'm not sure what's causing this, but I had a conversation with @liameabbott a while ago about how one should normalize these matrices. His understanding was that hail normalizes by dividing by `sqrt(sum(x^2))` whereas one may prefer to divide `sd(x)`. The example he sent me to do this is below:. # Liam's example; g = BlockMatrix.read('gs://ukbb-ldsc-dev/1000_genomes.phase_3.europeans.GT.autosomes.bm'). n = g.shape[1]; m1 = g.sum(axis=1).cache(); m2 = (g**2).sum(axis=1).cache(). mean = m1 / n; stdev = ((m2-m1**2 / n) / (n-1)).sqrt(); g_std = ((g - mean) / stdev). g_std.write('gs://ukbb-ldsc-dev/1000_genomes.phase_3.europeans.GT_standardized.autosomes.bm', overwrite=True). I'll try this way of normalizing tomorrow to see if this is the root of the error and post back. Tagging @jbloom22 and @tpoterba 'cause why not :).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6351:2020,cache,cache,2020,https://hail.is,https://github.com/hail-is/hail/issues/6351,2,['cache'],['cache']
Performance,"e as we discovered, dev forum has a relatively short editing window. Will post to dev forum when ""complete"" and ready for broader discussion. # What are Physical Types?. Physical types are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non). PStruct. - Concrete imp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:976,load,loadAddress,976,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['load'],['loadAddress']
Performance,"e cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's ow",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:1335,Load,LoadSelfFirstURLClassLoader,1335,https://hail.is,https://github.com/hail-is/hail/pull/10279,1,['Load'],['LoadSelfFirstURLClassLoader']
Performance,"e copy in Batch. After this goes in, these can mostly be developed in parallel. A few principles guided the implementation of copy: perform the minimal number of system calls or API requests per copy, and only do error checking when it doesn't involve additional FS operations. For example, it is too expensive to exhaustively check if we're creating a path that is a file and a directory in Google Storage. I considered doing additional and exhaustive checking for the actual copy arguments. For example, currently, `cp -T /path/to/file /path/to/dir` will not generate an error on Google Storage. In the end, I decided to go with the current behavior and I will add an option to do a postpass to check for file-and-dir paths. To achieve this, for each transfer, I simultaneously stat the destination (if needd) to determine if it is a file, directory or doesn't exist. For each source, I simultaneously try to copy it as a file and a directory. When copying each source, we don't need to know the type of the destination until after we've stat'ed the source, so stat'ing the sources and destinations are all overlapping. This avoids dependencies where I have to e.g. stat the input, decide what to do, and then perform a second action. I approached testing two ways: First, hand test common operations and errors (copy file, copy dir, overwrite, overwrite dir with file and vice versa, the various treat_dest_as settings, large files, detecting copy-and-files on input on Google Storage, etc.) Second, I enumerated essentially all single input transfers and recorded the results. These are then tested on every pair of file systems: file to file, file to gs, gs to file, etc. To make this run reasonably, I parallelize using xdist and the full tests (1364 tests) now take about 2 minutes. (If you restrict to the just the local filesystem, the tests just take a couple seconds.) The second test verifies that all cases run without unexpected errors, and that the behavior is file system independent.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9822:2157,perform,perform,2157,https://hail.is,https://github.com/hail-is/hail/pull/9822,1,['perform'],['perform']
Performance,"e fact that certain parts of that array *must* be the JAR URL and the SHA-1 is confusing. Instead, there are now two keys in a JVM process specification:; 1. `jar_spec`, which may be either `{""type"": ""jar_url"", ""value"": ""gs://..../abc123....jar""}` or `{""type"":""git_revision"", ""value"": ""abc123...""}`.; 2. `argv`, an opaque list of strings which are passed, by the JVMEntryway, along with a few more args, to `is.hail.backend.service.Main`. The `Main` class dispatches to either `ServiceBackendSocketAPI2` or the `Worker` based on the first element of `argv`. Each class expects different contents in `argv` that suits its needs. Second, I completely eliminated the HAIL_SHA/revision from the Worker and Hail Query Java code. This was only ever used as unique name for the JAR. Instead, I just use the full JAR URL as a unique name for the JAR. If you need to defeat the cache, just create a new git commit before running `make -C query ipython`. If defeating the cache becomes a common problem, we can add a ""reload_jar"" parameter or similar to the job spec. Third, I renamed `push-jar` in `query/Makefile` to `upload-query-jar` to mirror the build.yaml step. Fourth, I embraced the use of `NAMEPSACE` in `query/Makefile` instead of relying on the minor hack that our laptop usernames match our namespace names. This does mean you need to always specify NAMESPACE when uploading a jar. Finally, a pleasant outcome of this change is the elimination of a bunch of conditional build.yaml logic in the service backend tests!. I think this will simplify the use of Hail Query by Australia et al. because I've isolated the use of hail-specific data to `query/Makefile`. If there's a way to access the relevant global-config variables from `query/Makefile`, I can also fix the `query/Makefile` to be deployment-independent. cc: @lgruen @illusional @tpoterba . [1] For our default namespace deployment, `gs://hail-query/jars/{GIT_REVISION}.jar`. In general, `{HAIL_QUERY_STORAGE_URI}{HAIL_QUERY_ACCEPTABLE_JAR_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11645:2365,cache,cache,2365,https://hail.is,https://github.com/hail-is/hail/pull/11645,1,['cache'],['cache']
Performance,"e non-empty). I also found all the function currying and comingling of fitting and testing really confusing. To be fair, the Scala code does this (and its really confusing). I think the current structure is easier to follow:. 1. Fit the null model.; 2. If wald, assume the beta for the genotypes is zero and use the rest of the parameters from the null model fit to compute the score (i.e. the gradient of the likelihood). Recall calculus: gradient near zero => value near the maximum. Return: this is the test.; 3. Otherwise, fit the full model starting at the null fit parameters.; 4. Test the ""goodness"" of this new & full fit. ---. Poisson regression is similar but with a different likelihood function and gradient thereof. Notice that I `key_cols_by()` to indicate to Hail that the order of the cols is irrelevant (the result is a locus-keyed table after all). This is necessary at least until #12753 merges. I think it's generally a good idea though: it indicates to Hail that the ordering of the columns is irrelevant, which is potentially useful information for the optimizer!. ---. Both logistic and Poisson regression can benefit from BLAS3 by running at least the score test for multiple variants at once. ---. I'll attach an image in the comments, but I spend ~6 seconds compiling this trivial model and ~140ms testing it. ```python3; import hail as hl; mt = hl.utils.range_matrix_table(1, 3); mt = mt.annotate_entries(x=hl.literal([1, 3, 10, 5])); ht = hl.poisson_regression_rows(; 'wald', y=hl.literal([0, 1, 1, 0])[mt.col_idx], x=mt.x[mt.col_idx], covariates=[1], max_iterations=2); ht.collect(); ```. I grabbed some [sample code from; scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PoissonRegressor.html) for Poisson regression (doing a score test rather than a wald test) and timed it. It takes ~8ms. So we're 3 orders of magnitude including the compiler, and ~1.2 orders of magnitude off without the compiler. Digging in a bit:; - ~65ms for cl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12793:1903,optimiz,optimizer,1903,https://hail.is,https://github.com/hail-is/hail/pull/12793,1,['optimiz'],['optimizer']
Performance,"e that dependency properly); - added a 404 page; ![screen shot 2018-12-07 at 3 33 13 pm](https://user-images.githubusercontent.com/106194/49671603-72713580-fa36-11e8-91cf-b24936257628.png); - fixed redirect rules for /docs and /hail see note below. Resolves #4919 . ---; ### On NGINX Redirects; The internet seems to think that `rewrite` for redirects is ""bad"", ergo, I ignore the deleted rule and explain the additions. ```; location = /docs/ {; return 307 $scheme://$http_host/docs/0.2;; }; location ~ ^/hail(|/.*)$ {; return 301 $scheme://$http_host/docs/0.1$1;; }; ```. The [location](http://nginx.org/en/docs/http/ngx_http_core_module.html#location) directive can match `=` exactly, `~` by regex, `~*` by case insensitive regex, and `^~` which I do not understand. Question one: does this redirect `hail.is/docs` to `/docs/0.2`? Yes, the last paragraph of the location docs:. > If a location is defined by a prefix string that ends with the slash character, and requests are processed by one of proxy_pass, fastcgi_pass, uwsgi_pass, scgi_pass, memcached_pass, or grpc_pass, then the special processing is performed. In response to a request with URI equal to this string, but without the trailing slash, a permanent redirect with the code 301 will be returned to the requested URI with the slash appended. The docs appear incomplete, though, because this is a `return` rule, but it gets the 301. Question two: does this redirect `hail.is/docs/foo` to `/docs/0.2/foo`. No, the docs redirect is an `=` or exact match so `hail.is/docs/foo` is a 404. Question three: does this redirect `/hail/overview.html` redirect to `docs/0.1/overview.html`. Yes, the regex for rules two matches `/hail`, `/hail/`, `/hail/overview.html`, etc. and redirects, replacing the hail with `docs/0.1`. One last note: I used 301 Permanent Redirect for the `/hail` since that's a dead url. I used 307 Temporary Redirect for `/docs` since that will change when versions change. Here's a test interaction. I interleaved my c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4929:1513,perform,performed,1513,https://hail.is,https://github.com/hail-is/hail/pull/4929,1,['perform'],['performed']
Performance,"e to exceed `width`; * print `body` normally, as described in the previous paragraph, allowing nested `Group`s to print either flat or normally. This pretty-printer DSL has become fairly standard, with some common enhancements that I don't think we need. It was first described in [A prettier printer](https://homepages.inf.ed.ac.uk/wadler/papers/prettier/prettier.pdf) by Wadler (though my implementation is completely different). This achieves stack safety by `Concat` taking an `Iterable`, so each contained `Doc` can be produced on demand. `render` pulls from these iterators, keeping in memory only things that might print to the current line, but where the format hasn't been decided yet. As soon as the formatting of a group is decided, as much of its body as possible is written to the `java.io.Writer`. A very quick and dirty performance comparison had the new pretty printer about 20% slower. That's paying for both the stack safety and the added smarts. And I think there's still room for optimization if it becomes necessary. Here is a snippet of the IR generated by `test_ld_score_regression`, first on master, then this PR:; ```; (InsertFields; (SelectFields (SNP A1 A2 N Z); (Ref row)); None; (chi_squared; (Apply pow () Float64; (GetField Z; (Ref row)); (ApplyIR toFloat64 () Float64; (I32 2)))); (n; (GetField N; (Ref row))); (ld_score; (GetField L2; (GetField __uid_3; (Ref row)))); (locus; (Apply Locus () Locus(GRCh37); (GetField CHR; (GetField __uid_4; (Ref row))); (GetField BP; (GetField __uid_5; (Ref row))))); (alleles; (MakeArray Array[String]; (GetField A2; (Ref row)); (GetField A1; (Ref row)))); (phenotype; (Str ""50_irnt""))))); (InsertFields; (SelectFields (locus alleles chi_squared n ld_score phenotype); (SelectFields (SNP A1 A2 N Z chi_squared n ld_score locus alleles phenotype); (Ref row))); None)); ```; ```; (InsertFields; (SelectFields (SNP A1 A2 N Z) (Ref row)); None; (chi_squared; (Apply pow () Float64; (GetField Z (Ref row)); (ApplyIR toFloat64 () Float64 (",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9652:3721,optimiz,optimization,3721,https://hail.is,https://github.com/hail-is/hail/pull/9652,1,['optimiz'],['optimization']
Performance,"e-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3962,cache,cached,3962,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"e-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5036,cache,cached,5036,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,e.cloud.hadoop.util.HadoopConfigurationProperty.getPropsWithPrefix(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsFsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:383); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1516); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1486); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:541); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:494); at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669); at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94); at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703); at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685); at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373); at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); at is.hail.io.fs.HadoopFS.is$hail$io$fs$HadoopFS$$_fileSystem(HadoopFS.scala:157); at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:244); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:226); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:225); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8343:3878,Cache,Cache,3878,https://hail.is,https://github.com/hail-is/hail/issues/8343,1,['Cache'],['Cache']
Performance,"e.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 4, localhost): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateextern",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1260:7058,concurren,concurrent,7058,https://hail.is,https://github.com/hail-is/hail/issues/1260,1,['concurren'],['concurrent']
Performance,e.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.126-ee77707f4fab; Error summary: HailException: cannot set missing field for required type +PFloat64; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:20672,concurren,concurrent,20672,https://hail.is,https://github.com/hail-is/hail/issues/14102,2,['concurren'],['concurrent']
Performance,"e/releasenotes/10.2.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/10.2.0.html</a></p>; <h2>Changes</h2>; <ul>; <li>Add <code>keep_rgb</code> option when saving JPEG to prevent conversion of RGB colorspace <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7553"">#7553</a> [<a href=""https://github.com/bgilbert""><code>@bgilbert</code></a>]</li>; <li>Trim negative glyph offsets in ImageFont.getmask() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7672"">#7672</a> [<a href=""https://github.com/nulano""><code>@nulano</code></a>]</li>; <li>Removed unnecessary &quot;pragma: no cover&quot; <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7668"">#7668</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Trim glyph size in ImageFont.getmask() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7669"">#7669</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Fix loading IPTC images and update test <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7667"">#7667</a> [<a href=""https://github.com/nulano""><code>@nulano</code></a>]</li>; <li>Allow uncompressed TIFF images to be saved in chunks <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7650"">#7650</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Concatenate multiple JPEG EXIF markers <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7496"">#7496</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Changed IPTC tile tuple to match other plugins <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7661"">#7661</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Do not assign new fp attribute when exiting context manager <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7566"">#7566</a> [<a href=""https://github.com/radarhere""><code>@",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:1317,load,loading,1317,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['load'],['loading']
Performance,"e46c2abcb856217d204812ae8bd33f3dfc21e7""><code>6ce46c2</code></a> gcsfs: drop python 3.6 (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/445"">#445</a>)</li>; <li><a href=""https://github.com/fsspec/gcsfs/commit/fd67c7d1b6ca9db83a0deadd1557470c37b0836a""><code>fd67c7d</code></a> Update changelog, deps (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/443"">#443</a>)</li>; <li><a href=""https://github.com/fsspec/gcsfs/commit/2dc256b7c0df075d44f8d4a0f3400c1b926166ee""><code>2dc256b</code></a> Make tarball creation more reproducible (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/442"">#442</a>)</li>; <li><a href=""https://github.com/fsspec/gcsfs/commit/85e2ee2abfa429f3c10cc08dcda8ed30db8ab3b5""><code>85e2ee2</code></a> update deps, changelog (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/438"">#438</a>)</li>; <li><a href=""https://github.com/fsspec/gcsfs/commit/cbf751e83785d8b7e5d7ea879e5534e32d633ae0""><code>cbf751e</code></a> Don't touch cache on find with prefix (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/437"">#437</a>)</li>; <li><a href=""https://github.com/fsspec/gcsfs/commit/7b5aee98724c7ca44a73524bf448089ac4b79b75""><code>7b5aee9</code></a> for release (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/435"">#435</a>)</li>; <li><a href=""https://github.com/fsspec/gcsfs/commit/4de170703d3f245a2e4f5e5c7abcef3ad3ec33c7""><code>4de1707</code></a> fixup references to dask/gcsfs (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/434"">#434</a>)</li>; <li><a href=""https://github.com/fsspec/gcsfs/commit/8f7115216346648bacc57f5910048cab5735b9b3""><code>8f71152</code></a> Add support to additional 'fixed-key-metadata' (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/429"">#429</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/fsspec/gcsfs/compare/2021.04.0...2022.02.0"">compare view</a></li>; </ul>; </details",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11575:1617,cache,cache,1617,https://hail.is,https://github.com/hail-is/hail/pull/11575,1,['cache'],['cache']
Performance,e5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json BlockBlob Hot 4453 application/octet-stream 2023-06-09T12:44:22+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/dK3o5ZfXmYSkP5TA/specs BlockBlob Hot 1264 application/octet-stream 2023-06-09T12:43:37+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/dK3o5ZfXmYSkP5TA/specs.idx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:37+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/eOrFpVrN98GBIizi/specs BlockBlob Hot 1264 application/octet-stream 2023-06-09T12:43:34+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/eOrFpVrN98GBIizi/specs.idx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:34+00:00; ```. I looked at the status:. ```; az storage blob download --account-name haildevtest --container test --name batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json | jq '.' | less; ```. which contained an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lan,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:3731,concurren,concurrent,3731,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance,"e; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function uint64_t vector_popcnt(uint64vector):; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline long long int _mm_popcnt_u64(long long unsigned int): target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline long long int _mm_popcnt_u64(long long unsigned int): target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 2; On-line CPU(s) list: 0,1; Thread(s) per core: 1; Core(s) per socket: 2; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 23; Model name: Intel(R) Core(TM)2 CPU P8600 @ 2.40GHz; Stepping: 10; CPU MHz: 800.000; CPU max MHz: 2401.0000; CPU min MHz: 800.0000; BogoMIPS: 4800.16; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 3072K; NUMA node0 CPU(s): 0,1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1520:2343,cache,cache,2343,https://hail.is,https://github.com/hail-is/hail/issues/1520,3,['cache'],['cache']
Performance,"e></a> release: v2.9.13</li>; <li><a href=""https://github.com/vitejs/vite/commit/e109d64331d9fa57753832762c3573c3532a6947""><code>e109d64</code></a> fix: backport <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8804"">#8804</a>, /@fs/ dir traversal with escaped chars (fixes <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8498"">#8498</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8"">#8</a>...</li>; <li><a href=""https://github.com/vitejs/vite/commit/1afc1c2370e09998f800f9067491a25e9dd463a0""><code>1afc1c2</code></a> fix(wasm): support decoding data URL in Node &lt; v16 (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8668"">#8668</a>)</li>; <li><a href=""https://github.com/vitejs/vite/commit/86a55d3cc0668eca79a55f5cf8b6034b9e3bf835""><code>86a55d3</code></a> release: v2.9.12</li>; <li><a href=""https://github.com/vitejs/vite/commit/c0d6c60b45d89e0995a5ea6bf74e9e3c023ae828""><code>c0d6c60</code></a> fix: backport outdated optimized dep removed from module graph (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8534"">#8534</a>)</li>; <li><a href=""https://github.com/vitejs/vite/commit/078a7dcabc8ffc93a06c84063fba04e0e2157f3b""><code>078a7dc</code></a> release: v2.9.11</li>; <li><a href=""https://github.com/vitejs/vite/commit/01fa8070fab5faa590fbe312d2465897a0e6c6a2""><code>01fa807</code></a> fix(dev): avoid FOUC when swapping out link tag (fix <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7973"">#7973</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8495"">#8495</a>)</li>; <li><a href=""https://github.com/vitejs/vite/commit/ab7dc1c4405ce2814ccc38d5979b51ad2f37d4e6""><code>ab7dc1c</code></a> fix: backport respect server.headers in static middlewares (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8481"">#8481</a>)</li>; <li><a href=""https://github.com/vitejs/vite/commit/ced0374b867db3c01b91027",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12142:11961,optimiz,optimized,11961,https://hail.is,https://github.com/hail-is/hail/pull/12142,2,['optimiz'],['optimized']
Performance,eBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflect,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5381,Optimiz,Optimize,5381,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Optimiz'],['Optimize']
Performance,eChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:23867,concurren,concurrent,23867,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['concurren'],['concurrent']
Performance,eContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:6047,Optimiz,Optimize,6047,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,"e[Long]; }; ```. - Construct the NDArray off-heap. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalNDArray. A PCanonicalArray-backed NDArray. # <a name=""ptuple"">PTuple</a>. An immutible, collection of ordered values, whose elements may be of different types. ## Core methods. ```scala; val _types: IndexedSeq[PTupleField]; ```. - The ordered representation of physical types that represent this collection. ```scala; def allocate(region: Region): Long; def allocate(region: Code[Region]): Code[Long]; ```. - Allocate enough memory off-heap to store the requested elements. ```scala; def initialize(address: Long, setMissing: Boolean = false): Unit; def stagedInitialize(address: Code[Long], setMissing: Boolean = false): Code[Unit]; ```; - Set element missingness and store element length. ```scala; def isFieldDefined(address: Long, fieldIdx: Int): Boolean; def isFieldDefined(address: Code[Long], fieldIdx: Code[Int]): Boolean; ```. ```scala; def setFieldMissing(address: Long, fieldIdx: Int): Unit; def setFieldMissing(address: Code[Long], fieldIdx: Int): Code[Unit]. def setFieldPresent(address: Long, fieldIdx: Int): Unit; def setFieldPresent(address: Code[Long], fieldIdx: Int): Code[Unit]; }; ```; - Set field present of missing at a given memory address. ```scala; def loadField(address: Long, fieldIdx: Int): Long; def loadField(address: Code[Long], fieldIdx: Int): Code[Long]; ```; - Load field at a given memory address. ```scala; def storeField(address: Long, fieldIdx: Int): Long; def storeField(address: Code[Long], fieldIdx: Int): Code[Long]; ```; - Store field at a given memory address; - (This does not exist yet, but should I believe). ## <a name=""ptuple"">PCanonicalTuple</a>. An immutible, fixed-length collection of ordered values (of possibly different types). Number of elements known statically, and just like PCanonicalStruct, elements are stored inline, rather than behind a pointer to a collection.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:9311,load,loadField,9311,https://hail.is,https://github.com/hail-is/hail/issues/7988,3,"['Load', 'load']","['Load', 'loadField']"
Performance,ead.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$an,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:11340,Load,LoadVCF,11340,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,"ease notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/janus/releases"">janus's releases</a>.</em></p>; <blockquote>; <h2>janus 1.0.0 release</h2>; <ul>; <li>Dropped Python 3.6 support</li>; <li>Janus is marked as stable, no API changes was made for years</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/janus/blob/master/CHANGES.rst"">janus's changelog</a>.</em></p>; <blockquote>; <h2>1.0.0 (2021-12-17)</h2>; <ul>; <li>Drop Python 3.6 support</li>; </ul>; <h2>0.7.0 (2021-11-24)</h2>; <ul>; <li>Add SyncQueue and AsyncQueue Protocols to provide type hints for sync and async queues <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/374"">#374</a></li>; </ul>; <h2>0.6.2 (2021-10-24)</h2>; <ul>; <li>Fix Python 3.10 compatibility <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/358"">#358</a></li>; </ul>; <h2>0.6.1 (2020-10-26)</h2>; <ul>; <li>; <p>Raise RuntimeError on queue.join() after queue closing. <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/295"">#295</a></p>; </li>; <li>; <p>Replace <code>timeout</code> type from <code>Optional[int]</code> to <code>Optional[float]</code> <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/267"">#267</a></p>; </li>; </ul>; <h2>0.6.0 (2020-10-10)</h2>; <ul>; <li>; <p>Drop Python 3.5, the minimal supported version is Python 3.6</p>; </li>; <li>; <p>Support Python 3.9</p>; </li>; <li>; <p>Refomat with <code>black</code></p>; </li>; </ul>; <h2>0.5.0 (2020-04-23)</h2>; <ul>; <li>Remove explicit loop arguments and forbid creating queues outside event loops <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/246"">#246</a></li>; </ul>; <h2>0.4.0 (2018-07-28)</h2>; <ul>; <li>; <p>Add <code>py.typed</code> macro <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/89"">#89</a></p>; </li>; <li>; <p>Drop python 3.4 support ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11466:1151,queue,queue,1151,https://hail.is,https://github.com/hail-is/hail/pull/11466,1,['queue'],['queue']
Performance,"eb_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,991"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; ```. Recall that pod creation happens in the background, so the `batches/9/create` and `batches/9/close` endpoints return to the client before pods are necessarily created. See batch.py:866, `Batch.close`. Likewise, the `batches/9/cancel` endpoint returns before the individual jobs are cancelled. There are now at most three concurrent threads of control interacting with the database and k8s. Eventually this sequence of log messages appears three times in quick succession. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6617:4950,concurren,concurrent,4950,https://hail.is,https://github.com/hail-is/hail/issues/6617,1,['concurren'],['concurrent']
Performance,"ec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collectin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4844,cache,cached,4844,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"ect(pca_rows.locus.contig, pca_rows.file_row_idx); contig_row_list = pca_rows.collect(); print('finished collecting'); contig_reformed = [(x['contig'], x['file_row_idx']) for x in contig_row_list]; print('reformed'); from collections import defaultdict; contig_row_dict = defaultdict(list); for k, v in contig_reformed:; contig_row_dict[k].append(v); print('dictionary created'). with hl.hadoop_open(contig_row_dict_location, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][""loadings""],; pca_af=pcloadings[sibs.row_key][""pca_af""]; ). mt = mt.filter_rows(hl.is_defined(mt.pca_loadings) & hl.is_defined(mt.pca_af) &; (mt.pca_af > 0) & (mt.pca_af < 1)). gt_norm = (mt.GT.n_alt_alleles() - 2 * mt.pca_af) / hl.sqrt(n_variants * 2 * mt.pca_af * (1 - mt.pca_af)). mt = mt.annotate_cols(scores=hl.agg.array_sum(mt.pca_loadings * gt_norm)). related_scores = mt.cols().select('scores'); ```. ### What went wrong (all",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3953:3351,load,loadings,3351,https://hail.is,https://github.com/hail-is/hail/issues/3953,1,['load'],['loadings']
Performance,"ectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:2486,Load,LoadVCF,2486,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,"ecute(local_jar_location, self.scratch, self.log_file, self.jar_url, self.argv); File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2629, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:224); at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:186); at is.hail.JVMEntryway.main(JVMEntryway.java:156); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at is.hail.JVMEntryway$1.run(JVMEntryway.java:107); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); ... 7 more; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/rwalters-hail-tmp/o?name=merged_round2_sumstats.fix_lowconf.mt/entries/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:1547,concurren,concurrent,1547,https://hail.is,https://github.com/hail-is/hail/issues/12950,1,['concurren'],['concurrent']
Performance,ecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:6007,Optimiz,Optimize,6007,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,"ecutor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:6123,Load,LoadVCF,6123,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,ecutor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182); 	at scala.Option.foreach(Option.scala:4,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9717,Load,LoadVCF,9717,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"ed PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3100,cache,cached,3100,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"ed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.10.0 - 2024-03-27</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14427:1485,perform,performance,1485,https://hail.is,https://github.com/hail-is/hail/pull/14427,1,['perform'],['performance']
Performance,"ed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14357:1264,perform,performance,1264,https://hail.is,https://github.com/hail-is/hail/pull/14357,3,['perform'],['performance']
Performance,"edge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting them from its callers. Tim Poterba: hmmm, you're right. Passing the region on load is not sufficient -- that region needs to be the owning region for the original data. Alex Kotlar: Would we want to associate an instance of a PType with a single region?. Patrick Schultz: I think we have most of the infrastructure needed to have a hail type hold a region. Then a lazily decoding PType can hold a (uniquely owned) region to decode into, invisibly to callers. Patrick Schultz: In which case I don't think the region argument to loadElement would be needed. Alex Kotlar:; In which case I don't think the region argument to loadElement would be needed; agreed. Tim Poterba: ok, I think I'm convinced.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826:1914,perform,performance,1914,https://hail.is,https://github.com/hail-is/hail/issues/7826,4,"['load', 'perform']","['load', 'loadElement', 'performance']"
Performance,"eduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminated': {'container_id': None,\n 'exit_code': 0,\n 'finished_at': None,\n 'message': None,\n 'reason': None,\n 'signal': None,\n 'started_at': None},\n 'waiting': None}}],\n 'host_ip': '10.128.0.56',\n 'init_container_statuses': None,\n 'message': None,\n 'nominated_node_name': None,\n 'phase': 'Pending',\n 'pod_ip': None,\n 'qos_class': 'Burstable',\n 'reason': None,\n 'start_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal())}}""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_clie",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6616:7990,concurren,concurrent,7990,https://hail.is,https://github.com/hail-is/hail/issues/6616,2,['concurren'],['concurrent']
Performance,"ef execute(self, ir: BaseIR, timed: bool = False) -> Any:; 217 try:; --> 218 return super().execute(ir, timed); 219 except Exception as err:; 220 if self._copy_log_on_error:. File /usr/local/lib/python3.11/dist-packages/hail/backend/backend.py:190, in Backend.execute(self, ir, timed); 188 result, timings = self._rpc(ActionTag.EXECUTE, payload); 189 except FatalError as e:; --> 190 raise e.maybe_user_error(ir) from None; 191 if ir.typ == tvoid:; 192 value = None. File /usr/local/lib/python3.11/dist-packages/hail/backend/backend.py:188, in Backend.execute(self, ir, timed); 186 payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); 187 try:; --> 188 result, timings = self._rpc(ActionTag.EXECUTE, payload); 189 except FatalError as e:; 190 raise e.maybe_user_error(ir) from None. File /usr/local/lib/python3.11/dist-packages/hail/backend/py4j_backend.py:221, in Py4JBackend._rpc(self, action, payload); 219 if resp.status_code >= 400:; 220 error_json = orjson.loads(resp.content); --> 221 raise fatal_error_from_java_error_triplet(; 222 error_json['short'], error_json['expanded'], error_json['error_id']; 223 ); 224 return resp.content, resp.headers.get('X-Hail-Timings', ''). FatalError: IllegalArgumentException: requirement failed. Java stack trace:; java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.expr.ir.lowering.TableStage.repartitionNoShuffle(LowerTableIR.scala:357); 	at is.hail.rvd.AbstractRVDSpec$.$anonfun$readZippedLowered$12(AbstractRVDSpec.scala:207); 	at is.hail.expr.ir.TableNativeZippedReader.lower(TableIR.scala:2042); 	at is.hail.expr.ir.TableReader.lower(TableIR.scala:663); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1061); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1050); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2242); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.sca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14529:5889,load,loads,5889,https://hail.is,https://github.com/hail-is/hail/issues/14529,1,['load'],['loads']
Performance,"ef=""https://redirect.github.com/python-pillow/Pillow/issues/7619"">#7619</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Import plugins relative to the module <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7576"">#7576</a> [<a href=""https://github.com/deliangyang""><code>@deliangyang</code></a>]</li>; <li>Translate encoder error codes to strings; deprecate <code>ImageFile.raise_oserror()</code> <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7609"">#7609</a> [<a href=""https://github.com/bgilbert""><code>@bgilbert</code></a>]</li>; <li>Updated readthedocs to latest version of Python <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7611"">#7611</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Support reading BC4U and DX10 BC1 images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/6486"">#6486</a> [<a href=""https://github.com/REDxEYE""><code>@REDxEYE</code></a>]</li>; <li>Optimize ImageStat.Stat.extrema <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7593"">#7593</a> [<a href=""https://github.com/florath""><code>@florath</code></a>]</li>; <li>Handle pathlib.Path in FreeTypeFont <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7578"">#7578</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Use list comprehensions to create transformed lists <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7597"">#7597</a> [<a href=""https://github.com/hugovk""><code>@hugovk</code></a>]</li>; <li>Added support for reading DX10 BC4 DDS images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7603"">#7603</a> [<a href=""https://github.com/sambvfx""><code>@sambvfx</code></a>]</li>; <li>Optimized ImageStat.Stat.count <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7599"">#7599</a> [<a href=""https://github.com/florath""><code>@florath</code></a>]</li>; <li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:7208,Optimiz,Optimize,7208,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['Optimiz'],['Optimize']
Performance,efficiently load compressed VCFs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4:12,load,load,12,https://hail.is,https://github.com/hail-is/hail/issues/4,1,['load'],['load']
Performance,egatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:10470,Load,LoadMatrix,10470,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,elocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:60); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1476); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:574); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:563); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:288); 	at is.hail.services.package$.retryTransientErrors(package.scala:163); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:286); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:26); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:239); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:235); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/wes-bipolar-tmp-4day/o/bge-wave-1-VQSR%2FparallelizeAndComputeWithIndex%2FgCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=%2Fresult.2706?alt=media; No such object: wes-bipolar-tmp-4day/bge-wave-1-VQSR/parallelizeAndComputeWithIndex/gCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=/result.2706; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118); 	at com.google.api.client.googleapis.services.json.AbstractG,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409:6631,concurren,concurrent,6631,https://hail.is,https://github.com/hail-is/hail/issues/13409,1,['concurren'],['concurrent']
Performance,"emitPackEncoder now supports requested type. I see a small (few percent) increase in performance in import_vcf/write on a gVCF file. Still, getting rid of a RVB is always a good change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5606:85,perform,performance,85,https://hail.is,https://github.com/hail-is/hail/pull/5606,1,['perform'],['performance']
Performance,"emoving batch workers' reliance on the docker daemon and docker in general, in favor of a lower level of abstraction that gives us finer control over resources on the worker like overlays and network namespaces, allowing us to shortcut and pre-configure some of the overhead that goes into running a job. ## What this does differently; Currently, the high-level process for running a job involves communicating with the docker daemon to:; 1. Pull an image for a job; 2. Start a container from that image; 3. Run the container; 4. Delete the container and its associated resources. We offload some of these responsibilities into the worker code and onto [crun](https://github.com/containers/crun), a lightweight low-level runtime with the same API as `runc`, what docker uses to run containers. Once docker has retrieved an image, if we see that the pulled image has a new digest from one we currently have cached on the worker, we extract the image's filesystem into a directory on the worker's disk. We then:. - use `mount` to create an overlay on top of the image that the container will use as its rootfs; - use `xfs_quota` to limit the container's storage in the overlay; - invoke `crun` to run a container with the overlay as its root filesystem and an appropriate network namespace that we set up at worker-start time. Since we control the overlay, we can set the XFS quota before creating the container. So what was separate calls to docker create/start/run/delete is just a single `crun run`. Fewer steps, less back and forth with a single daemon, and pre-configuring the networks gives some sizable performance gains reliable, as well as reliable and consistent performance. ## What this doesn't solve; - Docker is still running the worker container. I don't see any real challenge to this it's just a matter of translating the docker parameters; - Still using docker to pull images and extract filesystems / environment variables from them. I don't have a substitute for this at the moment.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10376:1635,perform,performance,1635,https://hail.is,https://github.com/hail-is/hail/pull/10376,2,['perform'],['performance']
Performance,"empt=0, peakBytes=656384, peakBytesReadable=641.00 KiB, chunks requested=4, cache hits=2; 2023-09-22 19:11:12.656 : INFO: RegionPool: FREE: 641.0K allocated (257.0K blocks / 384.0K chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.656 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.ob",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:9919,concurren,concurrent,9919,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"en users install hail, or hail could remove usage of setuptools & its associated modules (`pkg_resources`) at runtime, as some other projects have done: https://github.com/TDAmeritrade/stumpy/issues/950. At a glance, the cleanest thing to do here may be to move off of the deprecated `pkg_resources` and to the recommended `importlib` if it has what you need: https://setuptools.pypa.io/en/latest/pkg_resources.html. I also have to admit that I discovered this while playing around with hail on a Raspberry Pi 4, so it is possible that something else broken caused this failure, but I believe I understand what's happening. Here's my full `pip freeze` for reference:. ```; (venv) (py312) alex@rpi400:~/hail $ pip freeze; aiodns==2.0.0; aiohttp==3.9.3; aiosignal==1.3.1; attrs==23.2.0; avro==1.11.3; azure-common==1.1.28; azure-core==1.30.1; azure-identity==1.15.0; azure-mgmt-core==1.4.0; azure-mgmt-storage==20.1.0; azure-storage-blob==12.19.1; bokeh==3.4.0; boto3==1.34.73; botocore==1.34.73; cachetools==5.3.3; certifi==2024.2.2; cffi==1.16.0; charset-normalizer==3.3.2; click==8.1.7; commonmark==0.9.1; contourpy==1.2.0; cryptography==42.0.5; decorator==4.4.2; Deprecated==1.2.14; dill==0.3.8; frozenlist==1.4.1; google-auth==2.29.0; google-auth-oauthlib==0.8.0; hail==0.2.128; humanize==1.1.0; idna==3.6; isodate==0.6.1; janus==1.0.0; Jinja2==3.1.3; jmespath==1.0.1; jproperties==2.1.1; MarkupSafe==2.1.5; msal==1.28.0; msal-extensions==1.1.0; msrest==0.7.1; multidict==6.0.5; nest-asyncio==1.6.0; numpy==1.26.4; oauthlib==3.2.2; orjson==3.9.11; packaging==24.0; pandas==2.2.1; parsimonious==0.10.0; pillow==10.2.0; plotly==5.20.0; portalocker==2.8.2; protobuf==3.20.2; py4j==0.10.9.5; pyasn1==0.6.0; pyasn1_modules==0.4.0; pycares==4.4.0; pycparser==2.21; Pygments==2.17.2; PyJWT==2.8.0; pyspark==3.3.4; python-dateutil==2.9.0.post0; python-json-logger==2.0.7; pytz==2024.1; PyYAML==6.0.1; regex==2023.12.25; requests==2.31.0; requests-oauthlib==2.0.0; rich==12.6.0; rsa==4.9; s3transfer==0.10.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14428:1967,cache,cachetools,1967,https://hail.is,https://github.com/hail-is/hail/issues/14428,1,['cache'],['cachetools']
Performance,"ent call last):; File ""/opt/conda/default/lib/python3.6/site-packages/luigi/worker.py"", line 199, in run; new_deps = self._run_get_new_deps(); File ""/opt/conda/default/lib/python3.6/site-packages/luigi/worker.py"", line 141, in _run_get_new_deps; task_gen = self.task.run(); File ""/tmp/c7e0443c47b54e91b295e2bff7b554b9/seqr_loading.py"", line 54, in run; self.read_vcf_write_mt(); File ""/tmp/c7e0443c47b54e91b295e2bff7b554b9/seqr_loading.py"", line 84, in read_vcf_write_mt; mt.write(self.output().path, stage_locally=True, overwrite=True); File ""<decorator-gen-1092>"", line 2, in write; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/matrixtable.py"", line 2529, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowerin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:48470,load,loads,48470,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['load'],['loads']
Performance,eption: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/rwalters-hail-tmp/o?name=merged_round2_sumstats.fix_lowconf.mt/entries/rows/parts/part-15801-2fde3786-67cb-42ed-8aac-f900cfcc4c00&uploadType=resumable&upload_id=ADPycduMEzX6d_uX4CiP6_XItJKmP8UnUnYBfyPoselMbyLUkxs1wDLPnxWl5gXr5LnBaVntYR_i7jchyxgVsRb_5PknvcCIcfDJ; chunkOffset: 16777216; chunkLength: 0; localOffset: 0; remoteOffset: 16777216; lastChunk: false. at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:270); at is.hail.io.fs.FSPositionedOutputStream.write(FS.scala:218); at java.io.DataOutputStream.write(DataOutputStream.java:107); at is.hail.utils.richUtils.ByteTrackingOutputStream.write(ByteTrackingOutputStream.scala:19); at is.hail.io.StreamBlockOutputBuffer.writeBlock(OutputBuffers.scala:293); at is.hail.io.LZ4OutputBlockBuffer.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:3273,concurren,concurrent,3273,https://hail.is,https://github.com/hail-is/hail/issues/12950,1,['concurren'],['concurrent']
Performance,eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:7817,concurren,concurrent,7817,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['concurren'],['concurrent']
Performance,eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:5264,concurren,concurrent,5264,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['concurren'],['concurrent']
Performance,"eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:11287,concurren,concurrent,11287,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['concurren'],['concurrent']
Performance,er.java:174); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:134); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:239); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/566:3100,concurren,concurrent,3100,https://hail.is,https://github.com/hail-is/hail/issues/566,2,['concurren'],['concurrent']
Performance,er.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:11275,Load,LoadVCF,11275,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,erator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneri,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:18351,concurren,concurrent,18351,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['concurren'],['concurrent']
Performance,erator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$ex,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:8797,concurren,concurrent,8797,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['concurren'],['concurrent']
Performance,erence genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:2267,Load,LoadPlink,2267,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,eric.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NegativeArraySizeException; at java.util.Arrays.copyOf(Arrays.java:3236); at is.hail.annotations.Region.ensure(Region.scala:139); at is.hail.annotations.Region.allocate(Region.scala:152); at is.hail.annotations.Region.allocate(Region.scala:159); at is.hail.annotations.RegionValueBuilder.allocateRoot(RegionValueBuilder.scala:73); at is.hail.annotations.RegionValueBuilder.startBaseStruct(RegionValueBuilder.scala:92); at is.hail.annotations.RegionValueBuilder.startStruct(RegionValueBuilder.scala:115); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:740); ... 49 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:16076,concurren,concurrent,16076,https://hail.is,https://github.com/hail-is/hail/issues/3507,4,"['Load', 'concurren']","['LoadVCF', 'concurrent']"
Performance,"ertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31032 25785 (LdcX 1 I))); 31033 25786 (InsnX ISHL; 31034 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2350null Z; 31035 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31036 25788 (LdcX 2 I))); 31037 25789 (InsnX ISHL; 31038 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2352null Z; 31039 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31040 25791 (LdcX 3 I))))); 31041 (ReturnX). # Elsewhere, this split method is called, then the resulting field is loaded and written to the output buffer. 11325 (MethodStmtX INVOKEVIRTUAL __C1527collect_distributed_array.__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616_region0_0 (L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)V; 11326 (LoadX arg:0 L__C1527collect_distributed_array;); 11327 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11328 25772 (MethodStmtX INVOKEINTERFACE is/hail/io/OutputBuffer.writeByte (B)Vinterface; 11329 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2354null Lis/hail/io/OutputBuffer;; 11330 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11331 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 11332 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;))); ```. # Pervasiveness. There is absolutely nothing about this bug that is whole-stage-codegen-specific, but I suspect the much larger single IRs compiled in whole stage code generation made it exponentially more likely for this corner case to occur. I imagine it would be possible to con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:5896,Load,LoadX,5896,https://hail.is,https://github.com/hail-is/hail/pull/11328,2,['Load'],['LoadX']
Performance,"es: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this better; 3. Performance. NodeJS is faster than Flask, React is ~fastest JS view layer. Next makes it really easy to split app into page bundles, and (on localhost) achieves DOMContentLoaded of ~70-100ms, and faster interactivity: first loaded page (the page of the current route) is ~6-10ms.; * [Techempower]: https://www.techempower.com/benchmarks/; * [Node vs , ](https://medium.com/@mihaigeorge.c/web-rest-api-benchmark-on-a-real-life-application-ebb743a5d7a3). * React vs other client side micro bench (pay attention to ""Non-keyed""): https://krausest.github.io/js-framework-benchmark/current.html; 4. Structure, aforementioned; 5. Path to relatively performant desktop and mobile applications, via [Electron](https://getstream.io/blog/takeaways-on-building-a-react-based-app-with-electron/). [Visual Studio Code](https://github.com/Microsoft/vscode) and [Slack](https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview-2759690d9c7b) are good examples. Facebook Messenger written in React",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:3343,Perform,Performance,3343,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['Perform'],['Performance']
Performance,"es</summary>; <p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>; <blockquote>; <h2>10.3.0</h2>; <p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/10.3.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/10.3.0.html</a></p>; <h2>Changes</h2>; <ul>; <li>CVE-2024-28219: Use strncpy to avoid buffer overflow <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7928"">#7928</a> [<a href=""https://github.com/hugovk""><code>@hugovk</code></a>]</li>; <li>Use <code>functools.lru_cache</code> for <code>hopper()</code> <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7912"">#7912</a> [<a href=""https://github.com/hugovk""><code>@hugovk</code></a>]</li>; <li>Raise ValueError if seeking to greater than offset-sized integer in TIFF <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7883"">#7883</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Improve speed of loading QOI images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7925"">#7925</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Added RGB to I;16N conversion <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7920"">#7920</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Add --report argument to <strong>main</strong>.py to omit supported formats <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7818"">#7818</a> [<a href=""https://github.com/nulano""><code>@nulano</code></a>]</li>; <li>Added RGB to I;16, I;16L and I;16B conversion <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7918"">#7918</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Fix editable installation with custom build backend and configuration options <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7658"">#7658</a> [<a href=""https://g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:1125,load,loading,1125,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['load'],['loading']
Performance,"eserialized (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2153"">#2153</a>) (<a href=""https://github.com/googleapis/java-storage/commit/68ad8e7357097e3dd161c2ab5f7a42a060a3702c"">68ad8e7</a>)</li>; <li>Update grpc default metadata projection to include acl same as json (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2150"">#2150</a>) (<a href=""https://github.com/googleapis/java-storage/commit/330e795040592e5df22d44fb5216ad7cf2448e81"">330e795</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.14.0 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2151"">#2151</a>) (<a href=""https://github.com/googleapis/java-storage/commit/eba8b6a235919a27d1f6dadf770140c7d143aa1a"">eba8b6a</a>)</li>; </ul>; <h2>v2.25.0</h2>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.24.0...v2.25.0"">2.25.0</a> (2023-07-24)</h2>; <h3>Features</h3>; <ul>; <li>BlobWriteChannelV2 - same throughput less GC (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2110"">#2110</a>) (<a href=""https://github.com/googleapis/java-storage/commit/1b52a1053130620011515060787bada10c324c0b"">1b52a10</a>)</li>; <li>Update Storage.createFrom(BlobInfo, Path) to have 150% higher throughput (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2059"">#2059</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4c2f44e28a1ff19ffb2a02e3cefc062a1dd98fdc"">4c2f44e</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>Update BlobWriteChannelV2 to properly carry forward offset after incremental flush (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2125"">#2125</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c099a2f4f8ea9afa6953270876653916b021fd9f"">c099a2f</a>)</li>; <li>Update GrpcStorageImpl.createFrom(BlobInfo, Path) to use RewindableContent (<a href=""https://redirect.github.com/googleapis/java-storage/issue",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13605:3393,throughput,throughput,3393,https://hail.is,https://github.com/hail-is/hail/pull/13605,1,['throughput'],['throughput']
Performance,"et `TypeError: an integer is required (got type bytes)` immediately upon importing hail. A full transcript is below. (pyve is an alias to create a python virtual env and activate it). ---. ```; snafu$ pyve; + python3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8)  ~/sandbox/hail [master|8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1090,cache,cached,1090,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,ethod.invoke(Method.java:566); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5499,Load,LoadPlink,5499,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,ethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:10424,Load,LoadMatrix,10424,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,ethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5572,Load,LoadPlink,5572,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,etting 'es.nodes.wan.only'; 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:247); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Unsupported/Unknown Elasticsearch version 6.0.0; 	at org.elasticsearch.hadoop.util.EsMajorVersion.parse(EsMajorVersion.java:79); 	at org.elasticsearch.hadoop.rest.RestClient.remoteEsVersion(RestClient.java:613); 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:240); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4138:7756,concurren,concurrent,7756,https://hail.is,https://github.com/hail-is/hail/issues/4138,2,['concurren'],['concurrent']
Performance,eway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at org.apache.hadoop.fs.Path.initialize(Path.java:205); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.net.URI.checkPath(URI.java:1823); 	at java.net.URI.<init>(URI.java:745); 	at org.apache.hadoop.fs.Path.initialize(Path.java:202); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHado,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:5348,concurren,concurrent,5348,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,"ex;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * Solution: use arrow functions. ```js; class Something {; constructor() {; this.bar = 'foo';; } ; //Do; onSubmit = () => {; console.log(this.bar) //prints foo; }. // Don't; onSubmitBad() {; console.log(this.bar) //may be undefined; }; }. const barrer = new Something();; console.info(""good"", barrer.onSubmit());; console.info(""bad"", barrer.onSubmitBad());; ```. # Tips . ### Client-side routing; Wrap a normal anchor tag in `<Link ></Link>`; ex:; ```jsx; <Link href='/path/to/page'><a>Page Name</a></Link>; ```. This simply adds the client-side routing logic, and passes the href to <a href=. . ### Prefetching; One of the neat things about Next is how easy it makes prefetching pages. This allows perceived page loading times on the order of 5ms, even when the page requires very complex state (say a GraphQL or series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:13750,load,loading,13750,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['load'],['loading']
Performance,extRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:6010,concurren,concurrent,6010,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['concurren'],['concurrent']
Performance,"extRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.12-13681278eb89; Error summary: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; ```. --------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:12578,concurren,concurrent,12578,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['concurren'],['concurrent']
Performance,"f any way to get around this? I.e. would building hail from source work?. ### Hail version:. `0.2.3`, installed from pip. (Installed Spark 2.2.2 separately and set `SPARK_HOME` accordingly). . ### What you did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixKeyRowsBy.execute(MatrixIR.scala:1317); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733:1189,load,load,1189,https://hail.is,https://github.com/hail-is/hail/issues/4733,1,['load'],['load']
Performance,"f.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Basically, the ; ```; hl.utils.get_1kg('data/'); ```; ![image](https://user-images.githubusercontent.com/10011161/48459558-9f645c80-e798-11e8-94db-0faa2e44e985.png).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775:2744,Load,LoadVCF,2744,https://hail.is,https://github.com/hail-is/hail/issues/4775,1,['Load'],['LoadVCF']
Performance,f41a81995c53a28e375c26b.jar.jar:?]; 	at is.hail.relocated.org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:71) ~[gs:__hail-query-daaf463550_jars_4c60fddb171a52c21f41a81995c53a28e375c26b.jar.jar:?]; 	at is.hail.backend.service.ServiceBackendAPI$.$anonfun$main$5(ServiceBackend.scala:467) ~[gs:__hail-query-daaf463550_jars_4c60fddb171a52c21f41a81995c53a28e375c26b.jar.jar:?]; 	at is.hail.utils.package$.using(package.scala:673) ~[gs:__hail-query-daaf463550_jars_4c60fddb171a52c21f41a81995c53a28e375c26b.jar.jar:?]; 	at is.hail.backend.service.ServiceBackendAPI$.main(ServiceBackend.scala:467) ~[gs:__hail-query-daaf463550_jars_4c60fddb171a52c21f41a81995c53a28e375c26b.jar.jar:?]; 	at is.hail.backend.service.Main$.main(Main.scala:10) ~[gs:__hail-query-daaf463550_jars_4c60fddb171a52c21f41a81995c53a28e375c26b.jar.jar:?]; 	at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-daaf463550_jars_4c60fddb171a52c21f41a81995c53a28e375c26b.jar.jar:?]; 	... 12 more; 2024-11-05 02:43:37.787 JVMEntryway: ERROR: Exception encountered in QoB cancel thread.; org.newsclub.net.unix.SocketClosedException: Not open; 	at org.newsclub.net.unix.AFCore.validFdOrException(AFCore.java:90) ~[jvm-entryway.jar:?]; 	at org.newsclub.net.unix.AFSocketImpl$AFInputStreamImpl.read(AFSocketImpl.java:510) ~[jvm-entryway.jar:?]; 	at java.io.DataInputStream.readInt(DataInputStream.java:393) ~[?:?]; 	at is.hail.JVMEntryway$2.run(JVMEntryway.java:136) [jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]; 	at java.lang.Thread.run(Thread.java:829) [?:?]; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14749:7312,concurren,concurrent,7312,https://hail.is,https://github.com/hail-is/hail/issues/14749,6,['concurren'],['concurrent']
Performance,"f=""https://github.com/damemi""><code>@damemi</code></a>) [SIG Scheduling and Testing]</li>; <li>Kubelet should reject pods whose OS doesn't match the node's OS label. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105292"">kubernetes/kubernetes#105292</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@ravisantoshgudimetla</code></a>) [SIG Apps and Node]</li>; <li>Kubelet: turn the KubeletConfiguration v1beta1 <code>ResolverConfig</code> field from a <code>string</code> to <code>*string</code>. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104624"">kubernetes/kubernetes#104624</a>, <a href=""https://github.com/Haleygo""><code>@Haleygo</code></a>)</li>; <li>Kubernetes is now built using go 1.17. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/103692"">kubernetes/kubernetes#103692</a>, <a href=""https://github.com/justaugustus""><code>@justaugustus</code></a>)</li>; <li>Performs strict server side schema validation requests via the <code>fieldValidation=[Strict,Warn,Ignore]</code>. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105916"">kubernetes/kubernetes#105916</a>, <a href=""https://github.com/kevindelgado""><code>@kevindelgado</code></a>)</li>; <li>Promote <code>IPv6DualStack</code> feature to stable.; Controller Manager flags for the node IPAM controller have slightly changed:; <ol>; <li>When configuring a dual-stack cluster, the user must specify both <code>--node-cidr-mask-size-ipv4</code> and <code>--node-cidr-mask-size-ipv6</code> to set the per-node IP mask sizes, instead of the previous <code>--node-cidr-mask-size</code> flag.</li>; <li>The <code>--node-cidr-mask-size</code> flag is mutually exclusive with <code>--node-cidr-mask-size-ipv4</code> and <code>--node-cidr-mask-size-ipv6</code>.</li>; <li>Single-stack clusters do not need to change, but may choose to use the more specific flags. Users can use either the older <code>--node-cidr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:10068,Perform,Performs,10068,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['Perform'],['Performs']
Performance,fNZv/jHpWQ6lemx/out; 2024-11-05 02:43:37.202 JVMEntryway: INFO: Yielding control to the QoB Job.; 2024-11-05 02:43:37.206 ServiceBackendAPI$: INFO: BatchClient allocated.; 2024-11-05 02:43:37.207 ServiceBackendAPI$: INFO: BatchConfig parsed.; 2024-11-05 02:43:37.209 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2024-11-05 02:43:37.783 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]; 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]; 	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) [jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]; 	at java.lang.Thread.run(Thread.java:829) [?:?]; Caused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String length (20013488) exceeds the maximum length (20000000); 	at com.fasterxml.jackson.core.StreamReadConstraints.validateStringLength(StreamReadConstraints.java:324) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.ReadConstrainedTextBuffer.validateStringLength(ReadConstrainedTextBuffer.java:27) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.TextBuffer.finishCurrentSegment(TextBuffer.java:939) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14749:2766,concurren,concurrent,2766,https://hail.is,https://github.com/hail-is/hail/issues/14749,1,['concurren'],['concurrent']
Performance,factoring out gencode gtf load,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8264:26,load,load,26,https://hail.is,https://github.com/hail-is/hail/pull/8264,1,['load'],['load']
Performance,failing loading table with 3K columns,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4153:8,load,loading,8,https://hail.is,https://github.com/hail-is/hail/issues/4153,1,['load'],['loading']
Performance,"faultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 YarnScheduler: ERROR: Lost executor 1 on bw2-sw-dp3j.c.seqr-project.internal: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1574.1 in stage 8.0 (TID 21699, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:3310,concurren,concurrent,3310,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['concurren'],['concurrent']
Performance,ffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:3340,concurren,concurrent,3340,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['concurren'],['concurrent']
Performance,ffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:10166,concurren,concurrent,10166,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['concurren'],['concurrent']
Performance,figurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsFsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:383); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1516); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1486); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:541); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:494); at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669); at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94); at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703); at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685); at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373); at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); at is.hail.io.fs.HadoopFS.is$hail$io$fs$HadoopFS$$_fileSystem(HadoopFS.scala:157); at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:244); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:226); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:225); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:31,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8343:3954,Cache,Cache,3954,https://hail.is,https://github.com/hail-is/hail/issues/8343,1,['Cache'],['Cache']
Performance,fix --cache-from in docker build,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5670:6,cache,cache-from,6,https://hail.is,https://github.com/hail-is/hail/pull/5670,1,['cache'],['cache-from']
Performance,fix docker image cacheing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4356:17,cache,cacheing,17,https://hail.is,https://github.com/hail-is/hail/pull/4356,1,['cache'],['cacheing']
Performance,fix loadconda,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5239:4,load,loadconda,4,https://hail.is,https://github.com/hail-is/hail/pull/5239,1,['load'],['loadconda']
Performance,fix localref load and store operations to use VarInsnNode instead of ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2566:13,load,load,13,https://hail.is,https://github.com/hail-is/hail/pull/2566,1,['load'],['load']
Performance,fix optimizer to clean up mt.field.show(),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3966:4,optimiz,optimizer,4,https://hail.is,https://github.com/hail-is/hail/pull/3966,1,['optimiz'],['optimizer']
Performance,fixed problem in LoadVCF where if sample IDs in header1 is subset of ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2247:17,Load,LoadVCF,17,https://hail.is,https://github.com/hail-is/hail/pull/2247,1,['Load'],['LoadVCF']
Performance,fixed reuse code bug in loadElement in SJavaArray,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10902:24,load,loadElement,24,https://hail.is,https://github.com/hail-is/hail/pull/10902,1,['load'],['loadElement']
Performance,"fixes #13407. CHANGELOG: Resolves #13407 in which uses of `union_rows` could reduce parallelism to one partition resulting in severely degraded performance. TableUnion was always collapsing to a single partition when the key was empty. This adds a special case handling, which just concatenates partitions. The body of the resulting TableStage is a little hacky: it does a StreamMultiMerge, but where exactly one input stream is non-empty. I think that should have fine performance, and I didnt see any simpler ways to do it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13414:144,perform,performance,144,https://hail.is,https://github.com/hail-is/hail/pull/13414,2,['perform'],['performance']
Performance,"fixes #7418. The main change here is fixing #7418 by making nodes which perform aggregation aware of the nearest containing node which defines what is being aggregated over. This is done by making a new variable name---here called ""agg_capability""---which is added to the child context by any node which (re)defines the meaning of aggregation (`TableMapRows`, `AggFilter`, etc.), and which is implicitly referenced by any node which performs an aggregation (`ApplyAggOp`, `AggFilter`, etc.). This requires nodes to be able to bind variables which shadow variables already bound by parents, which it turns out wasn't handled correctly by the CSE algorithm. Fixing this required several changes:; * I moved free variable computation to a lazy value on `BaseIR`. This way, each time we see a subtree `x`, we can recompute the max depth of the binding sites of all of `x`'s free variables, since those binding sites may be different than last time we saw `x`. This also required splitting the free variables into value, agg, and scan sets, so they can be looked up in the correct context (previously context lookup was always done at the `Ref` node, at which point the variable was in the value context).; * Now, in the `CSEPrintPass`, we have to recompute the same binding depth calculation that was done in the analysis pass, so we know which binding site to look at (previously I just searched all binding sites in scope, but with shadowing handled correctly we don't have sufficient information to decide which binding site is valid). This requires maintaining contexts in the print pass, which is annoying because we are now traversing the tree of `Renderable` children, which is not exactly the same as the IR tree.; * To fix this, I duplicated all methods involving binding structure on `BaseIR`. To avoid having to write twice as many methods on concrete IR classes, I made the methods taking the index of the `Renderable` child (e.g. `renderable_bindings`) be the primary methods which are overri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7479:72,perform,perform,72,https://hail.is,https://github.com/hail-is/hail/pull/7479,2,['perform'],"['perform', 'performs']"
Performance,flection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: is.hail.asm4s.AsmFunction2; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:14046,load,loadClass,14046,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['load'],['loadClass']
Performance,flectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at org.apache.hadoop.fs.Path.initialize(Path.java:205); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.net.URI.checkPath(URI.java:1823); 	at java.net.URI.<init>(URI.java:745); 	at org.apache.hadoop.fs.Path.initialize(Path.java:202); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleH,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:5286,concurren,concurrent,5286,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,flectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:234);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2407:12183,concurren,concurrent,12183,https://hail.is,https://github.com/hail-is/hail/issues/2407,1,['concurren'],['concurrent']
Performance,fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5682,Load,LoadMatrix,5682,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"for better IR generation. This change the (`key_struct`, `value_struct`) arguments to a more general (`row`, `new_keys`) which allows individual methods more flexibility in how the row is constructed. @tpoterba this fixes the specific case in #4001, but I'm not sure if you wanted to keep that open as a general issue or just open issues with other cases as we test them? With this change I'm getting:. ```; 2018-07-30 18:11:23 root: INFO: optimize: before:; (TableMapRows (idx) 1; (TableRange 5 2); (InsertFields; (Ref Struct{idx:Int32} row); (x; (I32 5)))); 2018-07-30 18:11:23 root: INFO: optimize: after:; (TableMapRows (idx) 1; (TableRange 5 2); (InsertFields; (Ref Struct{idx:Int32} row); (x; (I32 5)))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4042:440,optimiz,optimize,440,https://hail.is,https://github.com/hail-is/hail/pull/4042,2,['optimiz'],['optimize']
Performance,"for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting them from its callers. Tim Poterba: hmmm, you're right. Passing the region on load is not sufficient -- that region needs to be the owning region for the origina",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826:1266,load,load,1266,https://hail.is,https://github.com/hail-is/hail/issues/7826,2,['load'],"['load', 'loading']"
Performance,found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRPar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:2194,Load,LoadPlink,2194,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,fqr-9h24; <p>Changed</p>; <pre><code>; - Explicit check the key for ECAlgorithm by @estin in https://github.com/jpadilla/pyjwt/pull/713; - Raise DeprecationWarning for jwt.decode(verify=...) by @akx in https://github.com/jpadilla/pyjwt/pull/742. Fixed; ~~~~~. - Don't use implicit optionals by @rekyungmin in https://github.com/jpadilla/pyjwt/pull/705; - documentation fix: show correct scope for decode_complete() by @sseering in https://github.com/jpadilla/pyjwt/pull/661; - fix: Update copyright information by @kkirsche in https://github.com/jpadilla/pyjwt/pull/729; - Don't mutate options dictionary in .decode_complete() by @akx in https://github.com/jpadilla/pyjwt/pull/743. Added; ~~~~~. - Add support for Python 3.10 by @hugovk in https://github.com/jpadilla/pyjwt/pull/699; - api_jwk: Add PyJWKSet.__getitem__ by @woodruffw in https://github.com/jpadilla/pyjwt/pull/725; - Update usage.rst by @guneybilen in https://github.com/jpadilla/pyjwt/pull/727; - Docs: mention performance reasons for reusing RSAPrivateKey when encoding by @dmahr1 in https://github.com/jpadilla/pyjwt/pull/734; - Fixed typo in usage.rst by @israelabraham in https://github.com/jpadilla/pyjwt/pull/738; - Add detached payload support for JWS encoding and decoding by @fviard in https://github.com/jpadilla/pyjwt/pull/723; - Replace various string interpolations with f-strings by @akx in https://github.com/jpadilla/pyjwt/pull/744; - Update CHANGELOG.rst by @hipertracker in https://github.com/jpadilla/pyjwt/pull/751. `v2.3.0 &amp;lt;https://github.com/jpadilla/pyjwt/compare/2.2.0...2.3.0&amp;gt;`__; -----------------------------------------------------------------------. Fixed; ~~~~~. - Revert &amp;quot;Remove arbitrary kwargs.&amp;quot; `[#701](https://github.com/jpadilla/pyjwt/issues/701) &amp;lt;https://github.com/jpadilla/pyjwt/pull/701&amp;gt;`__. Added; ~~~~~. - Add exception chaining `[#702](https://github.com/jpadilla/pyjwt/issues/702) &amp;lt;https://github.com/jpadilla/pyjwt/pull/702&amp;gt;`__.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11866:10344,perform,performance,10344,https://hail.is,https://github.com/hail-is/hail/pull/11866,1,['perform'],['performance']
Performance,"from @zaczap; ```; 2018-02-21 15:30:31 Hail: INFO: interval filter loaded 89 of 1274 partitions; Traceback (most recent call last):; File ""/tmp/85fbcd66-b973-4c5c-9216-ef00b9a7f3a7/temp_vep.py"", line 34, in <module>; vds = hl.filter_intervals(vds, intervals); File ""<decorator-gen-718>"", line 2, in filter_intervals; File ""/tmp/85fbcd66-b973-4c5c-9216-ef00b9a7f3a7/hail-devel-5b95158ed055.zip/hail/utils/java.py"", line 198, in handle_py4j; hail.utils.java.FatalError: requirement failed. Java stack trace:; scala.Predef$.require(Predef.scala:212); at is.hail.rvd.OrderedRVDPartitioner.<init>(OrderedRVDPartitioner.scala:28); at is.hail.rvd.OrderedRVDPartitioner.copy(OrderedRVDPartitioner.scala:98); at is.hail.rvd.OrderedRVD.filterIntervals(OrderedRVD.scala:270); at is.hail.methods.FilterIntervals$.apply(FilterIntervals.scala:23); at is.hail.methods.FilterIntervals$.apply(FilterIntervals.scala:18); at is.hail.methods.FilterIntervals.apply(FilterIntervals.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); Hail version: devel-5b95158; Error summary: requirement failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [85fbcd66-b973-4c5c-9216-ef00b9a7f3a7] entered state [ERROR] while waiting for [DONE].; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2949:67,load,loaded,67,https://hail.is,https://github.com/hail-is/hail/issues/2949,1,['load'],['loaded']
Performance,"front_end loads them from batch2.front_end:. > setup_aiohttp_jinja2(app, 'batch.front_end'). added tests to check the ui pages load",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7270:10,load,loads,10,https://hail.is,https://github.com/hail-is/hail/pull/7270,2,['load'],"['load', 'loads']"
Performance,fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.net.URI.checkPath(URI.java:1823); 	at java.net.URI.<init>(URI.java:745); 	at org.apache.hadoop.fs.Path.initialize(Path.java:202); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.58-3f304aae6ce2; Error summary: URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:6319,concurren,concurrentGlobInternal,6319,https://hail.is,https://github.com/hail-is/hail/issues/9607,6,['concurren'],"['concurrent', 'concurrentGlobInternal']"
Performance,fun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5618,Load,LoadMatrixParser,5618,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrixParser']
Performance,"g JUnit 4 Tests using TestNG (Krishnan Mahadevan); Fixed: GITHUB-2847: Deprecate support for running JUnit tests (Krishnan Mahadevan); Fixed: GITHUB-2844: Deprecate support for running Spock Tests (Krishnan Mahadevan); Fixed: GITHUB-550: Weird <a href=""https://github.com/BeforeMethod""><code>@BeforeMethod</code></a> and <a href=""https://github.com/AfterMethod""><code>@AfterMethod</code></a> behaviour with dependsOnMethods (Krishnan Mahadevan); Fixed: GITHUB-893: TestNG should provide an Api which allow to find all dependent of a specific test (Krishnan Mahadevan); New: Added .yml file extension for yaml suite files, previously only .yaml was allowed for yaml (Steven Jubb); Fixed: GITHUB-141: regular expression in &quot;dependsOnMethods&quot; does not work (Krishnan Mahadevan); Fixed: GITHUB-2770: FileAlreadyExistsException when report is generated (melloware); Fixed: GITHUB-2825: Programmatically Loading TestNG Suite from JAR File Fails to Delete Temporary Copy of Suite File (Steven Jubb); Fixed: GITHUB-2818: Add configuration key for callback discrepancy behavior (Krishnan Mahadevan); Fixed: GITHUB-2819: Ability to retry a data provider in case of failures (Krishnan Mahadevan); Fixed: GITHUB-2308: StringIndexOutOfBoundsException in findClassesInPackage - Surefire/Maven - JDK 11 fails (Krishnan Mahadevan); Fixed: GITHUB:2788: TestResult.isSuccess() is TRUE when test fails due to expectedExceptions (Krishnan Mahadevan); Fixed: GITHUB-2800: Running Test Classes with Inherited <a href=""https://github.com/Factory""><code>@Factory</code></a> and <a href=""https://github.com/DataProvider""><code>@DataProvider</code></a> Annotated Non-Static Methods Fail (Krishnan Mahadevan); New: Ability to provide custom error message for assertThrows\expectThrows methods (Anatolii Yuzhakov); Fixed: GITHUB-2780: Use SpotBugs instead of abandoned FindBugs; Fixed: GITHUB-2801: JUnitReportReporter is too slow; Fixed: GITHUB-2807: buildStackTrace should be fail-safe (Sergey Chernov); Fixed: G",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:10988,Load,Loading,10988,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['Load'],['Loading']
Performance,"g builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/backend/backend.py"", line 108, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/BROAD.MIT.EDU/cvittal/.local/opt/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/utils/java.py"", line 221, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: locus<GRCh37> (of class is.hail.expr.types.virtual.TLocus). Java ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:1556,cache,cache,1556,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['cache'],['cache']
Performance,"g cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6775,cache,cached,6775,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"g.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:6097,Load,LoadVCF,6097,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,"gene_variant|MODIFIER|WASH7P|ENSG00000227232|Transcript|ENST00000488147|unprocessed_pseudogene|||||||||||2206|-1||HGNC|HGNC:38034|||||||||||||||||||||||||||;DP=3;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1	GT:AD:DP:GQ:PL	./.:.:.:.:.	0/1:1,2:3:37:77,0,37; ```. Getting this error message:; ```; INFO: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) running SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz, dest_path=gs://seqr-bw/merged_phased_3P5CH.mt, genome_version=38, vep_runner=VEP, reference_ht_path=gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht, clinvar_ht_path=gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht, hgmd_ht_path=None, sample_type=WGS, validate=False, dataset_type=VARIANTS, remap_path=, subset_path=, vep_config_json_path=); Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.5; SparkUI available at http://seqr-loading-cluster-m.c.seqr-project.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-914bd8a10ca2; LOGGING: writing to /tmp/c7e0443c47b54e91b295e2bff7b554b9/hail-20200405-1408-0.2.34-914bd8a10ca2.log; {'_Task__hash': -3818947167740532127,; 'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht',; 'dataset_type': 'VARIANTS',; 'decrease_running_resources': <bound method TaskStatusReporter.decrease_running_resources of <luigi.worker.TaskStatusReporter object at 0x7f0583f0f588>>,; 'dest_path': 'gs://seqr-bw/merged_phased_3P5CH.mt',; 'genome_version': '38',; 'hgmd_ht_path': None,; 'param_kwargs': {'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht',; 'dataset_type': 'VARIANTS',; 'dest_path': 'gs://seqr-bw/merged_phased_3P5CH.mt',; 'genome_version': '38',; 'hgmd_ht_path': None,; 'reference_ht_path': 'gs://seqr-reference-data/GRCh38/all_reference_data/combined_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:36767,load,loading-cluster-m,36767,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['load'],['loading-cluster-m']
Performance,"github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10007"">#10007</a>: Drop <code>setuptools</code></li>; </ul>; <h2>Features added</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9075"">#9075</a>: autodoc: Add a config variable :confval:<code>autodoc_typehints_format</code>; to suppress the leading module names of typehints of function signatures (ex.; <code>io.StringIO</code> -&gt; <code>StringIO</code>)</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9831"">#9831</a>: Autosummary now documents only the members specified in a module's; <code>__all__</code> attribute if :confval:<code>autosummary_ignore_module_all</code> is set to; <code>False</code>. The default behaviour is unchanged. Autogen also now supports; this behavior with the <code>--respect-module-all</code> switch.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9555"">#9555</a>: autosummary: Improve error messages on failure to load target object</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9800"">#9800</a>: extlinks: Emit warning if a hardcoded link is replaceable; by an extlink, suggesting a replacement.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9961"">#9961</a>: html: Support nested <!-- raw HTML omitted --> HTML elements in other HTML builders</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10013"">#10013</a>: html: Allow to change the loading method of JS via <code>loading_method</code>; parameter for :meth:<code>Sphinx.add_js_file()</code></li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9551"">#9551</a>: html search: &quot;Hide Search Matches&quot; link removes &quot;highlight&quot; parameter; from URL</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9815"">#9815</a>: html theme: Wrap sidebar components in div t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11522:1922,load,load,1922,https://hail.is,https://github.com/hail-is/hail/pull/11522,2,['load'],['load']
Performance,"gle_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4680,cache,cached,4680,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"h is essentially a simplified, staged version of `import_vcf`. I benchmarked the change with:; ```; In [2]: %%time ; ...: import hail as hl ; ...: m = hl.import_matrix_table('/tmp/foo.tsv.gz', ; ...: row_fields={'f0': hl.tstr}, ; ...: no_header=True, ; ...: sep=' ', ; ...: min_partitions=16) ; ...: m = m.key_rows_by(locus=hl.parse_locus(m.f0)) ; ...: m._force_count_rows() ; ```. `/tmp/foo.tsv.gz` is a gzipped (not blocked) 1GB of 1000 rows each containing one row column and 500k sample columns. The entries are the integers from 0 to 499,999. The first column is the first run (when the JIT is warmed) and the second column is the mean of two subsequent runs. All times in seconds. Everything is necessarily executed on one core. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Aside: using the staged stuff is hard, especially when using multiple methods. A couple thoughts:; - Because SRVB generates a fresh SRVB for arrays and structs, you must thread the srvb through your code gen rather than using a single field, this is annoying and error prone; - `init` is not a first class thing in `FunctionBuilder` and I've arguably made the whole situation more ugly by exposing `addInitInstructions`. Without the ability to place code in the constructor, it is hard to coordinate work between multi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6987:1443,perform,performance,1443,https://hail.is,https://github.com/hail-is/hail/pull/6987,1,['perform'],['performance']
Performance,"h non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:19); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:19); 	at is.hail.utils.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:5096,Cache,CacheDir,5096,https://hail.is,https://github.com/hail-is/hail/issues/14513,4,['Cache'],['CacheDir']
Performance,"h non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 20 times, most recent failure: L",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:2478,Cache,CacheDir,2478,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,['Cache'],['CacheDir']
Performance,h=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region_test.cpp -MG -M -MF build/Region_test.d -MT build/Region_test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; g++ -march=,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5659:1917,cache,cache-tests,1917,https://hail.is,https://github.com/hail-is/hail/issues/5659,1,['cache'],['cache-tests']
Performance,hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.Reflection,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5391,optimiz,optimize,5391,https://hail.is,https://github.com/hail-is/hail/issues/6458,2,"['Optimiz', 'optimiz']","['Optimize', 'optimize']"
Performance,"hail.backend.service.ServiceBackendSocketAPI2.$anonfun$parseInputToCommandThunk$4(ServiceBackend.scala:664); at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$parseInputToCommandThunk$3(ServiceBackend.scala:650); at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:822); at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:447); at is.hail.backend.service.Main$.main(Main.scala:15); at is.hail.backend.service.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```; </details>. The code is simple and clearly is running against a path that does not already exist:; ```; if not hl.hadoop_exists(get_aou_util_path('mt_sample_qc')):; print('Run sample qc MT.....'); mt = hl.read_matrix_table(ACAF_MT_PATH); mt = mt.filter_rows(mt.locus.in_autosome()); # mt = mt.filter_rows(mt.locus.contig == 'chr1'); ht = hl.sample_qc(mt, name='mt_sample_qc'); ht.wri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:6488,concurren,concurrent,6488,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['concurren'],['concurrent']
Performance,hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_matrix_ir$2(SparkBackend.scala:689); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:69); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:69); 	at is.hail.utils.package$.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:6485,Load,LoadPlink,6485,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:171); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.service.Worker$.main(Worker.scala:169); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.lang.NullPointerException: null; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionPutTask.call(JsonResumableSessionPutTask.java:201); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.lambda$put$0(JsonResumableSession.java:81); 	at is.hail.relocated.com.google.cloud.storage.Retrying.lambda$run$0(Retrying.java:102); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:99); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.put(JsonResumableSession.java:68); 	at is.hail.relocated.com.google.cloud.storage.ApiaryUnbufferedWritableByteChannel.internal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13937:5676,concurren,concurrent,5676,https://hail.is,https://github.com/hail-is/hail/issues/13937,1,['concurren'],['concurrent']
Performance,"hail/hail/python/hail/table.py"", line 1292, in _ascii_str; rows, has_more, dtype = self.data(); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1276, in data; rows, has_more = t._take_n(self.n); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1423, in _take_n; rows = self.take(n + 1); File ""<decorator-gen-1095>"", line 2, in take; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 2087, in take; return self.head(n).collect(_localize); File ""<decorator-gen-1089>"", line 2, in collect; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1886, in collect; return Env.backend().execute(e._ir); File ""/Users/konradk/hail/hail/python/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/Users/konradk/programs/spark-2.4.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/hail/hail/python/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: NoSuchElementException: key not found: 1; [...]; java.util.NoSuchElementException: key not found: 1; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.MapLike$class.apply(MapLike.scala:141); at scala.collection.AbstractMap.apply(Map.scala:59); at is.hail.types.encoded.EBaseStruct.fieldType(EBaseStruct.scala:34); at is.hail.types.encoded.EBaseStruct$$anonfun$8.apply(EBaseStruct.scala:84); at is.hail.types.encoded.EBaseStruct$$anonfun$8.apply(EBaseStruct.scala:83); at scala.collection.Tr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9016:4205,load,loads,4205,https://hail.is,https://github.com/hail-is/hail/issues/9016,1,['load'],['loads']
Performance,"hanged. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/102534"">kubernetes/kubernetes#102534</a>, <a href=""https://github.com/wangyysde""><code>@wangyysde</code></a>) [SIG API Machinery, Apps, Auth, Autoscaling and Testing]</li>; <li>Ephemeral containers graduated to beta and are now available by default. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105405"">kubernetes/kubernetes#105405</a>, <a href=""https://github.com/verb""><code>@verb</code></a>)</li>; <li>Fix kube-proxy regression on UDP services because the logic to detect stale connections was not considering if the endpoint was ready. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106163"">kubernetes/kubernetes#106163</a>, <a href=""https://github.com/aojea""><code>@aojea</code></a>) [SIG API Machinery, Apps, Architecture, Auth, Autoscaling, CLI, Cloud Provider, Contributor Experience, Instrumentation, Network, Node, Release, Scalability, Scheduling, Storage, Testing and Windows]</li>; <li>If a conflict occurs when creating an object with <code>generateName</code>, the server now returns an &quot;AlreadyExists&quot; error with a retry option. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104699"">kubernetes/kubernetes#104699</a>, <a href=""https://github.com/vincepri""><code>@vincepri</code></a>)</li>; <li>Implement support for recovering from volume expansion failures (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106154"">kubernetes/kubernetes#106154</a>, <a href=""https://github.com/gnufied""><code>@gnufied</code></a>) [SIG API Machinery, Apps and Storage]</li>; <li>In kubelet, log verbosity and flush frequency can also be configured via the configuration file and not just via command line flags. In other commands (kube-apiserver, kube-controller-manager), the flags are listed in the &quot;Logs flags&quot; group and not under &quot;Global&quot; or &quot;Misc&quot;. The",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:4143,Scalab,Scalability,4143,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['Scalab'],['Scalability']
Performance,"hangelog</strong>: <a href=""https://github.com/kjd/idna/compare/v3.6...v3.7"">https://github.com/kjd/idna/compare/v3.6...v3.7</a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/kjd/idna/blob/master/HISTORY.rst"">idna's changelog</a>.</em></p>; <blockquote>; <p>3.7 (2024-04-11); ++++++++++++++++</p>; <ul>; <li>Fix issue where specially crafted inputs to encode() could; take exceptionally long amount of time to process. [CVE-2024-3651]</li>; </ul>; <p>Thanks to Guido Vranken for reporting the issue.</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/kjd/idna/commit/1d365e17e10d72d0b7876316fc7b9ca0eebdd38d""><code>1d365e1</code></a> Release v3.7</li>; <li><a href=""https://github.com/kjd/idna/commit/c1b3154939907fab67c5754346afaebe165ce8e6""><code>c1b3154</code></a> Merge pull request <a href=""https://redirect.github.com/kjd/idna/issues/172"">#172</a> from kjd/optimize-contextj</li>; <li><a href=""https://github.com/kjd/idna/commit/0394ec76ff022813e770ba1fd89658790ea35623""><code>0394ec7</code></a> Merge branch 'master' into optimize-contextj</li>; <li><a href=""https://github.com/kjd/idna/commit/cd58a23173d2b0a40b95ee680baf3e59e8d33966""><code>cd58a23</code></a> Merge pull request <a href=""https://redirect.github.com/kjd/idna/issues/152"">#152</a> from elliotwutingfeng/dev</li>; <li><a href=""https://github.com/kjd/idna/commit/5beb28b9dd77912c0dd656d8b0fdba3eb80222e7""><code>5beb28b</code></a> More efficient resolution of joiner contexts</li>; <li><a href=""https://github.com/kjd/idna/commit/1b121483ed04d9576a1291758f537e1318cddc8b""><code>1b12148</code></a> Update ossf/scorecard-action to v2.3.1</li>; <li><a href=""https://github.com/kjd/idna/commit/d516b874c3388047934938a500c7488d52c4e067""><code>d516b87</code></a> Update Github actions/checkout to v4</li>; <li><a href=""https://github.com/kjd/idna/commit/c095c75943413c75ebf8ac74179757031b7f80b7""><code>c0",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14464:1476,optimiz,optimize-contextj,1476,https://hail.is,https://github.com/hail-is/hail/pull/14464,7,['optimiz'],['optimize-contextj']
Performance,hannelHandlerContext.fireChannelRead(AbstractCt io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at io.nettyentLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.NioEventLoot io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thread.javawork.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.nettyRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:37860,concurren,concurrent,37860,https://hail.is,https://github.com/hail-is/hail/issues/8106,2,['concurren'],['concurrent']
Performance,"hat the dataframe created by hail maintains reference to hail objects and pandas is attempting to recreate these objects when unpickling. I suspect this is not intentional. ```python; # Hail environment; vat_simplified_file = os.path.join(bucket, 'vat.ht'); gwas = hl.read_table(gwas_results_file_no_sex_chr); vat = hl.read_table(vat_simplified_file); gwas = gwas.filter(gwas.p_value <= 1e-4); combined = gwas.join(vat, how='left'); combined_pandas = combined.to_pandas(). gwas_pandas_file = os.path.join(bucket, 'gwas_results.pkl'); combined_pandas.to_pickle(gwas_pandas_file); ```. ```python; # Non hail environment without pyspark; combined_pandas = pd.read_pickle(gwas_pandas_file). ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); /opt/conda/lib/python3.7/site-packages/pandas/io/pickle.py in read_pickle(filepath_or_buffer, compression, storage_options); 216 # expected ""IO[bytes]""; --> 217 return pickle.load(handles.handle) # type: ignore[arg-type]; 218 except excs_to_catch:. /opt/conda/lib/python3.7/site-packages/hail/__init__.py in <module>; 32 # E402 module level import not at top of file; ---> 33 from .table import Table, GroupedTable, asc, desc # noqa: E402; 34 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402. /opt/conda/lib/python3.7/site-packages/hail/table.py in <module>; 4 import numpy as np; ----> 5 import pyspark; 6 from typing import Optional, Dict, Callable, Sequence. ModuleNotFoundError: No module named 'pyspark'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); /tmp/ipykernel_233/4275665471.py in <module>; ----> 1 combined_pandas = pd.read_pickle(gwas_pandas_file). /opt/conda/lib/python3.7/site-packages/pandas/io/pickle.py in read_pickle(filepath_or_buffer, compression, storage_options); 220 # ""No module named 'pandas.core.sparse.series'""; 221 # ""Can't get attribute '__nat_unpickle'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14004:1652,load,load,1652,https://hail.is,https://github.com/hail-is/hail/issues/14004,1,['load'],['load']
Performance,"he mutation is sufficiently well localized, and this is a common compiler pattern. This touches a lot of lines, but the high level changes are:; * Remove the `refMap` from the `IRParserEnvironment`, and remove all code that modifies the typing environment from the parser. Nodes like `Ref` that need a type from the environment get types set to `null`, to be filled in after parsing.; * Add `annotateTypes` pass to fill in type annotations from the environment. This is currently written in the parser, and always called after parsing. This means for the moment we can only parse type correct IR. But in the future we could move this to a separate stage of the compilation pipeline.; * Move typechecking logic on relational nodes from the constructors to a `typecheck` method, which is called from the `TypeCheck` pass.; * Make the `typ` field on IR nodes consistently lazy (or a def when the type is a child's type without modification). Before we sometimes did this for performance, but it's now required to avoid querying children's types during construction.; * Make types mutable on `AggSignature` and `ComparisonOp`, so they can be filled in after parsing.; * Ensure that the structure in `Binds.scala` satisfies the following invariant: to construct the typing environment of child `i`, we only need the types of children with index less than `i`. This was almost always satisfied already, and allows us to use the generic binds infrastucture in the pass to annotate types (where when visiting child `i`, we can only query types of already visited children).; * Change the text representation of `TailLoop`/`Recur` to move the explicit type annotation from `Recur` to `TailLoop`. This is necessary to comply with the previous point. It's also consistent with `Ref`, where types of references are inferred from the environment.; * Add explicit types in the text representation of `TableFilterIntervals` and `MatrixFilterIntervals`, where the types were needed during parsing and we can no longe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13990:3003,perform,performance,3003,https://hail.is,https://github.com/hail-is/hail/pull/13990,1,['perform'],['performance']
Performance,he.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(C,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19260,Load,LoadVCF,19260,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"he.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) t org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.sparkapache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.sparkitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDDputeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MaD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpointla:90) at org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:4cala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$ailureException: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/t sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.javannel(UnixFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:40rage.BlockManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.rator$$anon$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.sparkler.processFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHetty.channel.AbstractChannelHandlerConte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:7294,concurren,concurrent,7294,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['concurren'],['concurrent']
Performance,"he.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. ### Error No. 2; ```python; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 343 method = get_real_method(obj, self.print_method); 344 if method is not None:; --> 345 return method(); 346 return None; 347 else:. /usr/local/lib/python3.6/site-packages/hail/matrixtable.py in _repr_html_(self); 2524 ; 2525 def _repr_html_(self):; -> 2526 s = self.table_show._repr_html_(); 2527 if self.displayed_n_cols != self.actual_n_cols:; 2528 s += '<p style=""background: #fdd; padding: 0.4em;"">'. /usr/local/lib/python3.6/site-packages/hail/table.py in _repr_html_(self); 1256 ; 1257 def _repr_html_(self):; -> 1258 return self._html_str(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:26030,concurren,concurrent,26030,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['concurren'],['concurrent']
Performance,he.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$.repsepUntil(Parser.scala:289); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.scala:1443); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.scala:1443),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:19689,concurren,concurrent,19689,https://hail.is,https://github.com/hail-is/hail/issues/7044,2,['concurren'],['concurrent']
Performance,he.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$.repsepUntil(Parser.scala:289); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.scala:1443); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:7966,concurren,concurrent,7966,https://hail.is,https://github.com/hail-is/hail/issues/7044,2,['concurren'],['concurrent']
Performance,"he.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDR",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13085,Load,LoadVCF,13085,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,"hed Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, dill, Deprecated, bokeh, backcall, asyncinit, aiohttp-session, ipython, hail; Successfully installed Deprecated-1.2.12 Jinja2-2.11.3 MarkupSafe-1.1.1 PyJWT-2.0.1 PyYAML-5.4.1 aiohttp-3.7.4 aiohttp-session-2.7.0 async-timeout-3.0.1 asyncinit-0.2.4 attrs-20.3.0 backcall-0.2.0 bokeh-1.4.0 cachetools-4.2.1 certifi-2020.12.5 chardet-3.0.4 decorator-4.4.2 dill-0.3.3 fsspec-0.8.7 gcsfs-0.7.2 google-api-core-1.26.1 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-cloud-core-1.6.0 google-cloud-storage-1.25.0 google-resumable-media-0.5.1 googleapis-common-protos-1.53.0 hail-0.2.64 humanize-1.0.0 hur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:7560,cache,cachetools,7560,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cachetools']
Performance,"hed tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting mul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2543,cache,cached,2543,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"hed task 0.0 in stage 4.0 (TID 4) in 7 ms on localhost (executor driver) (1/1); 2018-10-09 14:46:42 TaskSchedulerImpl: INFO: Removed TaskSet 4.0, whose tasks have all completed, from pool ; 2018-10-09 14:46:42 DAGScheduler: INFO: ResultStage 4 (collect at utils.scala:197) finished in 0.007 s; 2018-10-09 14:46:42 DAGScheduler: INFO: Job 2 finished: collect at utils.scala:197, took 0.053572 s; 2018-10-09 14:46:42 CodeGenerator: INFO: Code generated in 5.955541 ms; 2018-10-09 14:46:42 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:42 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table7e606a8b83f4` AS `zzz1`; WHERE (0 = 1); 2018-10-09 14:46:42 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:42 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table7e606a8b83f4`; 2018-10-09 14:46:43 root: INFO: optimize: before:; (TableCount; (TableKeyBy () False; (TableLiteral))); 2018-10-09 14:46:43 root: INFO: optimize: after:; (TableCount; (TableLiteral)); 2018-10-09 14:46:43 SparkContext: INFO: Starting job: fold at RVD.scala:361; 2018-10-09 14:46:43 DAGScheduler: INFO: Got job 3 (fold at RVD.scala:361) with 1 output partitions; 2018-10-09 14:46:43 DAGScheduler: INFO: Final stage: ResultStage 5 (fold at RVD.scala:361); 2018-10-09 14:46:43 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 14:46:43 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 14:46:43 DAGScheduler: INFO: Submitting ResultStage 5 (MapPartitionsRDD[28] at mapPartitions at ContextRDD.scala:137), which has no missing parents; 2018-10-09 14:46:43 MemoryStore: INFO: Block broadcast_5 stored as values in memory (estimated size 19.8 KB, free 366.2 MB); 2018-10-09 14:46:43 MemoryStore: INFO: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.1 KB, free 366.2 MB); 2018-10-09 14:46:43 BlockManagerInfo: INFO: Added broadcast_5_piece0 in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:46355,optimiz,optimize,46355,https://hail.is,https://github.com/hail-is/hail/issues/4513,2,['optimiz'],['optimize']
Performance,"hed, and the cached result is returned for n + 1 calls. References; 1. https://reactjs.org/docs/react-api.html#reactmemo; 2. https://scotch.io/tutorials/react-166-reactmemo-for-functional-components-rendering-control. ### Typescript; 2. https://reactjs.org/docs/react-api.html#reactmemo. #### And React Component prop definitions; https://levelup.gitconnected.com/ultimate-react-component-patterns-with-typescript-2-8-82990c516935. ### NextJS; https://nextjs.org/docs/; Next has 4 deviations from normal react:; 1) _app.js: Can be omitted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />; <Component {...pageProps} />; <Footer />; </Container>; ```. 2) _document.js: Optional. Rendered only on the server, exactly one time. Wraps _app. Good place to define external resource you want to load, such as some external stylesheet, font, whatever. . 3) `getInitialProps`: a lifecycle method that is only available to components in the `pages/` folder. `getInitialProps ` runs once during server-side rendering, and again if you navigate to the page that defines it. Only components in pages can specify this property. This is because it is effectively a function triggered during routing and:; * `getInitialProps` is of course only available if you define a stateful component. See [functional components (just JSX wrapped in a function, rather than a class)](https://reactjs.org/docs/components-and-props.html). 4) NextJS includes a light, fast router. Routes are matched based on the names of files in `pages/`, with index.js mapping to `/`. For instance, to navigate to `domain.com/scorecard/users`, you'd make the folder structure:. pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal react components, except they expose `getInitialProps`, des",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:11633,load,load,11633,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['load'],['load']
Performance,"hen injecting esbuild helpers (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8110"">#8110</a>) (<a href=""https://github.com/vitejs/vite/commit/e5556ab"">e5556ab</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8110"">#8110</a></li>; </ul>; <h2><!-- raw HTML omitted -->2.9.8 (2022-05-04)<!-- raw HTML omitted --></h2>; <ul>; <li>fix: inline js and css paths for virtual html (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7993"">#7993</a>) (<a href=""https://github.com/vitejs/vite/commit/d49e3fb"">d49e3fb</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/7993"">#7993</a></li>; <li>fix: only handle merge ssr.noExternal (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8003"">#8003</a>) (<a href=""https://github.com/vitejs/vite/commit/642d65b"">642d65b</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8003"">#8003</a></li>; <li>fix: optimized processing folder renaming in win (fix <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7939"">#7939</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8019"">#8019</a>) (<a href=""https://github.com/vitejs/vite/commit/e5fe1c6"">e5fe1c6</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/7939"">#7939</a> <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8019"">#8019</a></li>; <li>fix(css): do not clean id when passing to postcss (fix <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7822"">#7822</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7827"">#7827</a>) (<a href=""https://github.com/vitejs/vite/commit/72f17f8"">72f17f8</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/7822"">#7822</a> <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/7827"">#7827</a></li>; <li>fix(css): var in image-set (<a href=""https:/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12142:9140,optimiz,optimized,9140,https://hail.is,https://github.com/hail-is/hail/pull/12142,2,['optimiz'],['optimized']
Performance,"hinx/0.8.2/</a>; __ <a href=""https://github.com/spatialaudio/nbsphinx/compare/0.8.1...0.8.2"">https://github.com/spatialaudio/nbsphinx/compare/0.8.1...0.8.2</a></p>; <p>Version 0.8.1 -- 2021-01-18 -- PyPI__ -- diff__</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/beb8fd6f6c5af0af8679b25454e5fa190e8c6f46""><code>beb8fd6</code></a> Release 0.8.8</li>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/efd08d8f777dcf7f435625b4a5726e240b5e2937""><code>efd08d8</code></a> DOC: mention prerequisites in CONTRIBUTING</li>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/948ec64ff8e9854e050b9ce603f70a40b0c34169""><code>948ec64</code></a> DOC: enable the sphinx_codeautolink extension (+ intersphinx)</li>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/01cc76e2d56e563841e339d96a37d811d2a35211""><code>01cc76e</code></a> Insert autolink-concat directive into notebooks if sphinx_codeautolink is loaded</li>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/9a9ee2d0a6f593703771152827428fae0f791ec5""><code>9a9ee2d</code></a> support text builder</li>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/403060082b92ccbe2adcf15dbd4fdaef65707304""><code>4030600</code></a> Replace leading/trailing empty code lines with &lt;br/&gt;</li>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/44829f13d18e676917fc6e7b7ae62d4bac625cd7""><code>44829f1</code></a> Escape &lt;/script&gt; tags in JSON data</li>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/b34f3e41e9fbf821950ad9e7a0c2a1f2689aae1e""><code>b34f3e4</code></a> CSS: update for hiding copy button in prompts</li>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/8bd255757ade0d072e5e13c9da706e002d6fa050""><code>8bd2557</code></a> Change internal representation of _BROKEN_THUMBNAIL</li>; <li><a href=""https://github.com/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11477:5887,load,loaded,5887,https://hail.is,https://github.com/hail-is/hail/pull/11477,1,['load'],['loaded']
Performance,"hon3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8)  ~/sandbox/hail [master|8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1285,cache,cached,1285,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,hreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9818,Load,LoadVCF,9818,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,hreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:4351,Load,LoadVCF,4351,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,"href=""https://github.com/aio-libs/janus/releases"">janus's releases</a>.</em></p>; <blockquote>; <h2>janus 1.0.0 release</h2>; <ul>; <li>Dropped Python 3.6 support</li>; <li>Janus is marked as stable, no API changes was made for years</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/janus/blob/master/CHANGES.rst"">janus's changelog</a>.</em></p>; <blockquote>; <h2>1.0.0 (2021-12-17)</h2>; <ul>; <li>Drop Python 3.6 support</li>; </ul>; <h2>0.7.0 (2021-11-24)</h2>; <ul>; <li>Add SyncQueue and AsyncQueue Protocols to provide type hints for sync and async queues <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/374"">#374</a></li>; </ul>; <h2>0.6.2 (2021-10-24)</h2>; <ul>; <li>Fix Python 3.10 compatibility <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/358"">#358</a></li>; </ul>; <h2>0.6.1 (2020-10-26)</h2>; <ul>; <li>; <p>Raise RuntimeError on queue.join() after queue closing. <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/295"">#295</a></p>; </li>; <li>; <p>Replace <code>timeout</code> type from <code>Optional[int]</code> to <code>Optional[float]</code> <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/267"">#267</a></p>; </li>; </ul>; <h2>0.6.0 (2020-10-10)</h2>; <ul>; <li>; <p>Drop Python 3.5, the minimal supported version is Python 3.6</p>; </li>; <li>; <p>Support Python 3.9</p>; </li>; <li>; <p>Refomat with <code>black</code></p>; </li>; </ul>; <h2>0.5.0 (2020-04-23)</h2>; <ul>; <li>Remove explicit loop arguments and forbid creating queues outside event loops <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/246"">#246</a></li>; </ul>; <h2>0.4.0 (2018-07-28)</h2>; <ul>; <li>; <p>Add <code>py.typed</code> macro <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/89"">#89</a></p>; </li>; <li>; <p>Drop python 3.4 support and fix minimal version python3.5.3 <a href=""h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11466:1170,queue,queue,1170,https://hail.is,https://github.com/hail-is/hail/pull/11466,1,['queue'],['queue']
Performance,"http://34.207.246.132/ for the demo. The copy in the ""About"" section should have a once-over @mkveerapen in particular, and @danking, @cseed, @tpoterba. Future work will provide a multi-column, easier to read version of About, and a linking section with a beautiful code-snippet-containing introduction to Hail (demonstrating file import). I leave these as future tasks, because enough changes are present in this PR. Summary of changes:; * Refined homepage styles, ensured navbar matches to pixel between docs and hail.is root (surprisingly difficult); * Improved mobile styles, especially mobile nav menu (much smoother animation, larger, easier to click on links); * Optimized icon sizes (50KB -> 3.3KB); * Removed all use of bootstrap on hail.is/*.html pages (bootstrap remains on docs, future pr).; * Removed index.md contents. The markdown format is pretty limited. To have a richly-marked up site with consistent styling, the syntax it provides is not enough. The solution is either to add html to index.md, or just write html in a the index.xslt file. I chose the latter, because it's simpler. Future reorganization should simplify this and docs further, though I think I still recommend NextJS and the build system that provides.; * Added threeR115.min.js. This is regrettably large, but doesn't impact page rendering performance in any meaningful way, because it is loaded after all html content (and is cached after the first visit). Future work can go to webgl directly, potentially. There is also ongoing work by the ThreeJS maintainers to allow tree-shaking and smaller builds.; * Added heavily modified fork of VantaJS. Because we are not using something like NextJS, there is no package manager to rely on, so I just checked the file in manually (I have this in a separate repo, we can use that if preferred). License is in line with a note about modifications. Vanta performs very, very poorly (order of 40% CPU usage, old/slow ways of observing whether animated element is in view, u",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8634:670,Optimiz,Optimized,670,https://hail.is,https://github.com/hail-is/hail/pull/8634,1,['Optimiz'],['Optimized']
Performance,"https://github-redirect.dependabot.com/psf/requests/issues/5851"">#5851</a>)</p>; </li>; <li>; <p>Fixed handling for <code>AttributeError</code> when calculating length of files obtained; by <code>Tarfile.extractfile()</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5239"">#5239</a>)</p>; </li>; <li>; <p>Fixed urllib3 exception leak, wrapping <code>urllib3.exceptions.InvalidHeader</code> with; <code>requests.exceptions.InvalidHeader</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5914"">#5914</a>)</p>; </li>; <li>; <p>Fixed bug where two Host headers were sent for chunked requests. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5391"">#5391</a>)</p>; </li>; <li>; <p>Fixed regression in Requests 2.26.0 where <code>Proxy-Authorization</code> was; incorrectly stripped from all requests sent with <code>Session.send</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5924"">#5924</a>)</p>; </li>; <li>; <p>Fixed performance regression in 2.26.0 for hosts with a large number of; proxies available in the environment. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5924"">#5924</a>)</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/psf/requests/blob/main/HISTORY.md"">requests's changelog</a>.</em></p>; <blockquote>; <h2>2.27.1 (2022-01-05)</h2>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Fixed parsing issue that resulted in the <code>auth</code> component being; dropped from proxy URLs. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/6028"">#6028</a>)</li>; </ul>; <h2>2.27.0 (2022-01-03)</h2>; <p><strong>Improvements</strong></p>; <ul>; <li>; <p>Officially added support for Python 3.10. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5928"">#5928</a>)</p>; </li>; <li>; <p>Added a <cod",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11528:3123,perform,performance,3123,https://hail.is,https://github.com/hail-is/hail/pull/11528,2,['perform'],['performance']
Performance,"https://github-redirect.dependabot.com/psf/requests/issues/5851"">#5851</a>)</p>; </li>; <li>; <p>Fixed handling for <code>AttributeError</code> when calculating length of files obtained; by <code>Tarfile.extractfile()</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5239"">#5239</a>)</p>; </li>; <li>; <p>Fixed urllib3 exception leak, wrapping <code>urllib3.exceptions.InvalidHeader</code> with; <code>requests.exceptions.InvalidHeader</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5914"">#5914</a>)</p>; </li>; <li>; <p>Fixed bug where two Host headers were sent for chunked requests. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5391"">#5391</a>)</p>; </li>; <li>; <p>Fixed regression in Requests 2.26.0 where <code>Proxy-Authorization</code> was; incorrectly stripped from all requests sent with <code>Session.send</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5924"">#5924</a>)</p>; </li>; <li>; <p>Fixed performance regression in 2.26.0 for hosts with a large number of; proxies available in the environment. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5924"">#5924</a>)</p>; </li>; <li>; <p>Fixed idna exception leak, wrapping <code>UnicodeError</code> with; <code>requests.exceptions.InvalidURL</code> for URLs with a leading dot (.) in the; domain. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5414"">#5414</a>)</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/psf/requests/commit/31a89d9c8463c3394ca00f408f4b86d814421a09""><code>31a89d9</code></a> v2.27.1</li>; <li><a href=""https://github.com/psf/requests/commit/8fa9724398c4f44090997ff430a1dd3e935a9057""><code>8fa9724</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/psf/requests/issues/6028"">#6028</a> from nateprewitt/prox_aut",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11528:6234,perform,performance,6234,https://hail.is,https://github.com/hail-is/hail/pull/11528,2,['perform'],['performance']
Performance,"https://redirect.github.com/python-pillow/Pillow/issues/7888"">#7888</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Support FITS images with GZIP_1 compression <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7894"">#7894</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Use I;16 mode for 9-bit JPEG 2000 images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7900"">#7900</a> [<a href=""https://github.com/scaramallion""><code>@scaramallion</code></a>]</li>; <li>Raise ValueError if kmeans is negative <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7891"">#7891</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Remove TIFF tag OSUBFILETYPE when saving using libtiff <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7893"">#7893</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Raise ValueError for negative values when loading P1-P3 PPM images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7882"">#7882</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Added reading of JPEG2000 palettes <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7870"">#7870</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Added alpha_quality argument when saving WebP images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7872"">#7872</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Fixed joined corners for ImageDraw rounded_rectangle() non-integer dimensions <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7881"">#7881</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Removed Python and NumPy pinning on Cygwin <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7880"">#7880</a> [<a href=""https://",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:3648,load,loading,3648,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['load'],['loading']
Performance,"hui-sandbox/ICA-AGD/plink1/chr12.fam...; / [0 files][ 0.0 B/910.3 KiB] / [1 files][910.3 KiB/910.3 KiB] Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.bim...; / [1 files][910.3 KiB/369.7 MiB] - - [1 files][ 51.9 MiB/369.7 MiB] \ | | [1 files][107.6 MiB/369.7 MiB] / - - [1 files][162.3 MiB/369.7 MiB] \ \ [1 files][213.9 MiB/369.7 MiB] | / / [1 files][286.6 MiB/369.7 MiB] - \ \ [1 files][342.1 MiB/369.7 MiB] |; Operation completed over 2 objects/369.7 MiB.; | [2 files][369.7 MiB/369.7 MiB] 2024/01/17 20:59:27 Localization script execution complete.; 2024/01/17 20:59:38 Done localization.; 2024/01/17 20:59:39 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint=/bin/bash hailgenetics/hail@sha256:3f22576793ce3161893aed2bd40949b1fc822d2b7e6517dc0ac993b62badaff8 /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.81879b1c; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.81879b1c; 24/01/17 20:59:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.3.2; SparkUI available at http://523bc6a27b69:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.127-bb535cd096c5; LOGGING: writing to /cromwell_root/hail-20240117-2059-0.2.127-bb535cd096c5.log; 2024-01-17 21:01:32.019 Hail: INFO: Found 34523 samples in fam file.; 2024-01-17 21:01:32.020 Hail: INFO: Found 18377527 variants in bim file.; 2024-01-17 21:02:45.920 Hail: INFO: Found 34523 samples in fam file.; 2024-01-17 21:02:45.920 Hail: INFO: Found 18377527 variants in bim file.; Traceback (most recent call last):; File ""<stdin>"", line 38, in <module>; File ""<decorator-gen-1366>"", line 2, in write; File ""/usr/local/lib/python3.10/dist-packages/hail/typecheck/check.py"", line 584, in wr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:11028,load,load,11028,https://hail.is,https://github.com/hail-is/hail/issues/14168,1,['load'],['load']
Performance,"i>; <li>; <p>Replace <code>timeout</code> type from <code>Optional[int]</code> to <code>Optional[float]</code> <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/267"">#267</a></p>; </li>; </ul>; <h2>0.6.0 (2020-10-10)</h2>; <ul>; <li>; <p>Drop Python 3.5, the minimal supported version is Python 3.6</p>; </li>; <li>; <p>Support Python 3.9</p>; </li>; <li>; <p>Refomat with <code>black</code></p>; </li>; </ul>; <h2>0.5.0 (2020-04-23)</h2>; <ul>; <li>Remove explicit loop arguments and forbid creating queues outside event loops <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/246"">#246</a></li>; </ul>; <h2>0.4.0 (2018-07-28)</h2>; <ul>; <li>; <p>Add <code>py.typed</code> macro <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/89"">#89</a></p>; </li>; <li>; <p>Drop python 3.4 support and fix minimal version python3.5.3 <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/88"">#88</a></p>; </li>; <li>; <p>Add property with that indicates if queue is closed <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/86"">#86</a></p>; </li>; </ul>; <h2>0.3.2 (2018-07-06)</h2>; <ul>; <li>Fixed python 3.7 support <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/97"">#97</a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/janus/commit/0783f9b7a9bb7e1c095e93ebb4aad4f1e219f512""><code>0783f9b</code></a> Fix coverage upload</li>; <li><a href=""https://github.com/aio-libs/janus/commit/41c49bafb1b192d2ee25b7394cead2386e452dc2""><code>41c49ba</code></a> Make deployment only if checks are green</li>; <li><a href=""https://github.com/aio-libs/janus/commit/ec94b35b2ae095dcb97827f1369c0cd31b7e8e5e""><code>ec94b35</code></a> Fix CI again</li>; <li><a href=""https://github.com/aio-libs/janus/commit/2303208c2f972e38445e7ecec54fda0f3203f566""><code>2303208</code></a> Fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11466:2300,queue,queue,2300,https://hail.is,https://github.com/hail-is/hail/pull/11466,1,['queue'],['queue']
Performance,"ib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 31, in deco; raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; hail.utils.java.FatalError: ClassFormatError: Too many arguments in method signature in class file __C2866stream. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 8.0 failed 20 times, most recent failure: Lost task 3.19 in stage 8.0 (TID 54368) (leo-test-w-8.australia-southeast1-a.c.ourdna-browser.internal executor 14): java.lang.ClassFormatError: Too many arguments in method signature in class file __C2866stream; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:635); 	at is.hail.asm4s.HailClassLoader.liftedTree1$1(HailClassLoader.scala:10); 	at is.hail.asm4s.HailClassLoader.loadOrDefineClass(HailClassLoader.scala:6); 	at is.hail.asm4s.ClassesBytes.$anonfun$load$1(ClassBuilder.scala:64); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$ano",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:3797,load,load,3797,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['load'],['load']
Performance,"ich we expose on the public internet. When you `curl https://hail.is` this is what happens:. - Your packet travels across the internet until it reaches the Google TCP; LoadBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v localhost:32029; * Trying 127.0.0.1...; * TCP_NODELAY set; * Connected to localhost (127.0.0.1) port 32029 (#0); > GET / HTTP/1.1; > Host: localhost:32029; > User-Agent: curl/7.64.1; > Accept: */*; >; < HTTP/1.1 200 OK; < Content-Type: application/json; < Date: Wed, 05 Feb 2020 20:59:27 GMT; < Content-Length: 88; <; {; 	""service"": {; 		""namespace"": ""default"",; 		""name"": ""gateway""; 	},; 	""localEndpoints"": 1; }; ```; ```; }dking@gke-vdc-non-preemptible-pool-5-80798769-kp8n ~ $ curl -v localhost:32029; * Trying 127.0.0.1...; * TCP_NODELAY set; * Connected to ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8045:1546,Load,LoadBalancer,1546,https://hail.is,https://github.com/hail-is/hail/pull/8045,1,['Load'],['LoadBalancer']
Performance,"ield on spills. The Z at the end of the next line indicates this field is a Boolean, not a byte. 31017 (PutFieldX PUTFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 31018 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;); 31019 25774 (InsnX I2B; 31020 25775 (InsnX IOR; 31021 25776 (InsnX IOR; 31022 25777 (InsnX IOR; 31023 25778 (InsnX IOR; 31024 25779 (LdcX 0 I); 31025 25780 (InsnX ISHL; 31026 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2346null Z; 31027 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31028 25782 (LdcX 0 I))); 31029 25783 (InsnX ISHL; 31030 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2348null Z; 31031 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31032 25785 (LdcX 1 I))); 31033 25786 (InsnX ISHL; 31034 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2350null Z; 31035 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31036 25788 (LdcX 2 I))); 31037 25789 (InsnX ISHL; 31038 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2352null Z; 31039 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31040 25791 (LdcX 3 I))))); 31041 (ReturnX). # Elsewhere, this split method is called, then the resulting field is loaded and written to the output buffer. 11325 (MethodStmtX INVOKEVIRTUAL __C1527collect_distributed_array.__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616_region0_0 (L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)V; 11326 (LoadX arg:0 L__C1527collect_distributed_array;); 11327 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SIns",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:4890,Load,LoadX,4890,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['Load'],['LoadX']
Performance,"ient.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Basically, the ; ```; hl.utils.get_1kg('data/'); ```; ![image](https://user-images.githubusercontent.com/10011161/48459558-9f645c80-e798-11e8-94db-0faa2e44e985.png). directly pr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775:2760,Load,LoadVCF,2760,https://hail.is,https://github.com/hail-is/hail/issues/4775,1,['Load'],['LoadVCF']
Performance,"ients.20uploading.22.3F/near/353337596) has more context as well. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2150, in run; await self.jvm.execute(local_jar_location, self.scratch, self.log_file, self.jar_url, self.argv); File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2629, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:224); at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:186); at is.hail.JVMEntryway.main(JVMEntryway.java:156); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at is.hail.JVMEntryway$1.run(JVMEntryway.java:107); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); ... 7 more; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:1334,concurren,concurrent,1334,https://hail.is,https://github.com/hail-is/hail/issues/12950,1,['concurren'],['concurrent']
Performance,"ild/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(Ro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:2532,Load,LoadVCF,2532,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,ileAndEvaluate.scala:47); at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:450); at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:486); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); at is.hail.utils.package$.using(package.scala:635); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); at is.hail.utils.package$.using(package.scala:635); at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:339); at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:483); at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); at is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:482); at jdk.internal.reflect.GeneratedMethodAccessor60.invoke(Unknown Source); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.base/java.lang.reflect.Method.invoke(Method.java:566); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.105-acd89e80c345; Error summary: ClassCastException: class org.apache.spark.sql.catalyst.expressions.GenericRow cannot be cast to class is.hail.variant.Locus (org.apache.spark.sql.catalyst.expressions.GenericRow is in unnamed module of loader 'app'; is.hail.variant.Locus is in unnamed module of loader org.apache.spark.util.MutableURLClassLoader @62435e70); ```. ### Version. 0.2.105. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046:5729,load,loader,5729,https://hail.is,https://github.com/hail-is/hail/issues/13046,2,['load'],['loader']
Performance,"ill look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will connect to notebook socket server, notebook will issue periodic updates. Same thing, just . Need help/ok to update gateway to test this in production environment. Preferably, as I mentioned to Dan, we would have a staging gateway, which *.dev.hail.is points to, and which is used for more than automated / ci testing, allowing for human interaction, which by some likelihood catches classes of bugs that unit/integration tests do not, and allows us to explore production context performance characteristics (for instance k8 internal network routing latency). cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215:2778,perform,performance,2778,https://hail.is,https://github.com/hail-is/hail/pull/5215,2,"['latency', 'perform']","['latency', 'performance']"
Performance,"illow/issues/7706"">#7706</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Use subprocess with CREATE_NO_WINDOW flag in ImageShow WindowsViewer <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7791"">#7791</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>When saving GIF frame that restores to background color, do not fill identical pixels <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7788"">#7788</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Fixed reading PNG iCCP compression method <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7823"">#7823</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Allow writing IFDRational to UNDEFINED tag <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7840"">#7840</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Fix logged tag name when loading Exif data <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7842"">#7842</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Use maximum frame size in IHDR chunk when saving APNG images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7821"">#7821</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Prevent opening P TGA images without a palette <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7797"">#7797</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Use palette when loading ICO images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7798"">#7798</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Use consistent arguments for load_read and load_seek <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7713"">#7713</a> [<a href=""https://github.com/radarhere""><c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:6222,load,loading,6222,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['load'],['loading']
Performance,"image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sure how much mileage we can get out of layer squashing. You can take a look at a service-base build [here](https://gist.github.com/danking/830af0688970c176ff25dbfeb4b222e7). Note the stdout comes first and then I `cat` the ""trace"" file which is a very weirdly formatted series of JSON objects that give more detailed information. You can clean it up a bit with `jq -c '(.Logs + .Statuses + .Vertexes) | add | {Timestamp, ID, Name}'`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10502:1692,cache,cached-as-possible,1692,https://hail.is,https://github.com/hail-is/hail/pull/10502,1,['cache'],['cached-as-possible']
Performance,"imension is zero in decompression bomb check <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7235"">#7235</a>; [radarhere]</p>; </li>; <li>; <p>Use --config-settings instead of deprecated --global-option <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7171"">#7171</a>; [radarhere]</p>; </li>; <li>; <p>Better C integer definitions <a href=""https://redirect.github.com/python-pillow/Pillow/issues/6645"">#6645</a>; [Yay295, hugovk]</p>; </li>; <li>; <p>Fixed finding dependencies on Cygwin <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7175"">#7175</a>; [radarhere]</p>; </li>; <li>; <p>Changed grabclipboard() to use PNG instead of JPG compression on macOS <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7219"">#7219</a>; [abey79, radarhere]</p>; </li>; <li>; <p>Added in_place argument to ImageOps.exif_transpose() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7092"">#7092</a>; [radarhere]</p>; </li>; <li>; <p>Fixed calling putpalette() on L and LA images before load() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7187"">#7187</a>; [radarhere]</p>; </li>; <li>; <p>Fixed saving TIFF multiframe images with LONG8 tag types <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7078"">#7078</a>; [radarhere]</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python-pillow/Pillow/commit/6e28ed1f36d0eb74053af54e1eddc9c29db698cd""><code>6e28ed1</code></a> 10.0.0 version bump</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/c827f3b30f50bf04fd65daeeba6bbfd56fc7b50e""><code>c827f3b</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7246"">#7246</a> from radarhere/deallocate</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/39a3b1d83edcf826c3864e26bedff5b4e4dd331b""><code>39a3b1d</code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:12054,load,load,12054,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['load'],['load']
Performance,import_bgen optional variants ir is not optimized,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5728:40,optimiz,optimized,40,https://hail.is,https://github.com/hail-is/hail/issues/5728,1,['optimiz'],['optimized']
Performance,import_table fails to load with exponential notation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4238:22,load,load,22,https://hail.is,https://github.com/hail-is/hail/issues/4238,1,['load'],['load']
Performance,import_vcf no longer scans variants. @tpoterba I imagine you want to fight me on this. I want to do binary search on chromosome boundaries when loading VCF files in which case this is quite expensive.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2183:144,load,loading,144,https://hail.is,https://github.com/hail-is/hail/pull/2183,1,['load'],['loading']
Performance,improve aggregator performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1223:19,perform,performance,19,https://hail.is,https://github.com/hail-is/hail/pull/1223,1,['perform'],['performance']
Performance,improve annotatevariants tsv performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/797:29,perform,performance,29,https://hail.is,https://github.com/hail-is/hail/issues/797,1,['perform'],['performance']
Performance,improved performance of VariantSubGen.gen,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2152:9,perform,performance,9,https://hail.is,https://github.com/hail-is/hail/pull/2152,1,['perform'],['performance']
Performance,improves optimization and opportunities for deforestation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4499:9,optimiz,optimization,9,https://hail.is,https://github.com/hail-is/hail/pull/4499,1,['optimiz'],['optimization']
Performance,"in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Basically, the ; ```; hl.utils.get_1kg('data/'); ```; ![image](https://user-images.githubusercontent.com/10011161/48459558",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775:2705,Load,LoadVCF,2705,https://hail.is,https://github.com/hail-is/hail/issues/4775,1,['Load'],['LoadVCF']
Performance,"in order to work with our dynamically-generated Kubernetes test namespaces. Currently, we configure NGINX by creating server blocks that dynamically resolve and dispatch requests based on matching regular expressions on the host and path headers. This is in large part due that at gateway deploy time we do not statically know all of the namespaces and namespace-service combinations that will exist in the cluster in the future. This is true for `default`, but not test namespaces, and NGINX will refuse to start with statically-configured clusters that it cannot reach. Making the server blocks make the routing decisions dynamically circumvents this limitation. However, this prevents usage of NGINX [upstream](http://nginx.org/en/docs/http/ngx_http_upstream_module.html) blocks that provide connection pooling, at least in the community edition, and as a result the gateways will create and terminate a TCP connection per http request. This likely causes minor delays on the front-end through gateway, but this hampers performance greatly in job scheduling. The batch driver is forced to establish a new TCP connection and do an SSL handshake with the internal-gateway multiple times per job, which is expensive and slow. We currently have to dedicate a 2-core NGINX sidecar for the batch-driver just to terminate TLS with internal-gateway and free up cycles in the batch-driver python process. By using proper persistent connections, we can reduce the TLS overhead to single-digit percents of a core. This leads to the first goal of this transition: configure our load balancers to know the full cluster configuration at any point in time so they can properly maintain connection pools with upstream services. However, this is not the only problem. Each ""upstream"" Service in Kubernetes may consist of multiple underlying pods but Kubernetes Services as we use them don't provide proper load-balancing when mixed with persistent connections. When we declare a Service for say, batch in default, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:1707,perform,performance,1707,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['perform'],['performance']
Performance,"in the service, log everything to stdout; - [x] (trivial) d8104a1dc4 [query-service] do not catch CancelledError; - [x] (trivial) efcb345185 [query-service] slightly more useful error message when socket dies; - [ ] (@tpoterba) f79c4023cf [shuffler] if we have an ExecuteContext, use it; - [x] (@daniel-goldstein,fyi: @tpoterba) 259f70dd25 [query-service] JSON Logging; - [ ] (@catoverdrive) f5c3ffcbd1 [query-service] pervasively retry all idempotent operations; - [ ] (@tpoterba) 507db4b468 [hail] fix using; - [x] (@jigold) c32a253bb9 [query] when testing, ensure our thread has an event loop; - [ ] (@tpoterba) 110469c2da [query][lir] avoid dumping massive classes onto stderr; - [ ] (@tpoterba) e4aa1c15fe [query] do not print misleading log in RegionPool.finalizer; - [x] (trivial) 33eab9a80e [query-service] better logging information; - [ ] (@catoverdrive) e358e8feeb [query-service] remove race conditions in user management; - [ ] (@tpoterba) b60cb2bae5 [lir] make LIR genName thread-safe; - [ ] (@catoverdrive) 2d82e5faf5 [query-service] send a token for job identifiability; - [x] (@daniel-goldstein) fd78caedcb [query-service] reduce image size by ~2GB; - [ ] (@catoverdrive) 00d1840421 [query-service] retry CLOSE, CLOSED (i.e. connection dropped); - [ ] (@catoverdrive) c985d3e3de [query-service] remove old test code; - [ ] (@catoverdrive) 0a5dc8c651 [query-service] all operations are idempotent; - [ ] (@cseed) 6d02d173fa [make] fix config.mk; - [x] (@daniel-goldstein) d21df54e63 [devbin] teach devbin/functions.py about multiple containers; - [x] (@jigold) 38878f7874 [batch] remove batch_worker_image false dependency on service_base_image; - [x] (@daniel-goldstein) f03defab3d [java-services] avoid NPEs in isTransientError; - [x] (@jigold) e535bdc00d [dependencies] upgrade gcsfs to 0.7.2 to fix GoogleFS rmtree issue; - [x] (@cseed) 743b5ba62f [query-service] enable auto-scaling for PR and dev deploy; - [ ] (@cseed) 6a52d45f6f [query-service] retry EndOfStream errors from j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10100:3045,race condition,race conditions,3045,https://hail.is,https://github.com/hail-is/hail/pull/10100,1,['race condition'],['race conditions']
Performance,"in this release are 3.8-3.10, Python 3.7; has been dropped. Note that 32 bit wheels are only provided for Python; 3.8 and 3.9 on Windows, all other wheels are 64 bits on account of; Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.; All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix; the occasional problems encountered by folks using truly huge arrays.</p>; <h2>Expired deprecations</h2>; <h3>Deprecated numeric style dtype strings have been removed</h3>; <p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,; and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>; <p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19539"">gh-19539</a>)</p>; <h3>Expired deprecations for <code>loads</code>, <code>ndfromtxt</code>, and <code>mafromtxt</code> in npyio</h3>; <p><code>numpy.loads</code> was deprecated in v1.15, with the recommendation that; users use <code>pickle.loads</code> instead. <code>ndfromtxt</code> and <code>mafromtxt</code> were both; deprecated in v1.17 - users should use <code>numpy.genfromtxt</code> instead with; the appropriate value for the <code>usemask</code> parameter.</p>; <p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19615"">gh-19615</a>)</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/numpy/numpy/commit/4adc87dff15a247e417d50f10cc4def8e1c17a03""><code>4adc87d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20685"">#20685</a> from charris/prepare-for-1.22.0-release</li>; <li><a href=""https://github.com/numpy/numpy/commit/fd66547557f57c430d41be2fc0764f74a62e8ccf""><code>fd66547</code></a> REL: Prepare for the NumPy 1.22.0 release.</li>; <li><a href=""https://github.com/numpy/nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11939:2435,load,loads,2435,https://hail.is,https://github.com/hail-is/hail/pull/11939,2,['load'],['loads']
Performance,infrastructure for query optimizer,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1778:25,optimiz,optimizer,25,https://hail.is,https://github.com/hail-is/hail/pull/1778,1,['optimiz'],['optimizer']
Performance,"ing CPU time:; <img width=""1889"" alt=""Screen Shot 2022-03-21 at 5 10 13 PM"" src=""https://user-images.githubusercontent.com/24440116/159364769-6fd60840-5745-40ab-802e-68b8d4f32078.png"">; <img width=""1885"" alt=""Screen Shot 2022-03-21 at 5 12 33 PM"" src=""https://user-images.githubusercontent.com/24440116/159364787-ca7ec307-877d-479c-9c19-8746b5e82eab.png"">. Looking at Wall time, the before profile is nearly identical because at the current rate limit the driver uses 100% of its CPU shares under this benchmark. On this branch, CPU utilization drops to 40-60%, giving the following wall time profile:; <img width=""1879"" alt=""Screen Shot 2022-03-21 at 5 30 39 PM"" src=""https://user-images.githubusercontent.com/24440116/159367182-0830d6ff-3b6f-4fa7-8004-0fc43283ec4a.png"">. So we can be confident that driver CPU is no longer a bottleneck even in the increased rate limit. ## So what's the bottleneck now?; Since the higher rate limit still leaves the driver plenty of CPU room (I've seen it peak at 60% of a vCPU), why not crank it higher? Well, we're increasing concurrency so latent deadlocks start to be a bigger issue again. We start to see tens of deadlocks per second in the proposed rate limit and hundreds at higher rate limits. As a result, we're spending more cycles repeating queries instead of actually scheduling faster. Next steps should focus on eliminating deadlocks before we can continue to max out CPU use. ## Miscellaneous; We've lumped in a few other monitoring changes that were helpful in this process and tried to leave the commits tidy. The one potentially rude change is enforcing a minimum wait time of half a second for `run_if_changed` loops. This dramatically reduced the number of scheduling loop invocations we were executing, greatly reducing the number of `compute_fair_share` queries, while maintaining the same scheduling ability. This feels like a fine requirement, but I'm unfamiliar with other use cases and can introduce a less invasive change if desired. cc @",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11638:1442,bottleneck,bottleneck,1442,https://hail.is,https://github.com/hail-is/hail/pull/11638,2,"['bottleneck', 'concurren']","['bottleneck', 'concurrency']"
Performance,ing.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_matrix_ir$2(SparkBackend.scala:689); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:69); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:69); 	at is.hail.utils.package$.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:2628,Load,LoadPlink,2628,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,"ingle field's missingness was flipped was a red herring -- all higher bits are flipped to 0 (defined)! Here's a look at the LIR looks like, though it was ultimately the JVM class file printout that tipped me off to the problem:. ```code. # I2B is stored as a class field on spills. The Z at the end of the next line indicates this field is a Boolean, not a byte. 31017 (PutFieldX PUTFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 31018 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;); 31019 25774 (InsnX I2B; 31020 25775 (InsnX IOR; 31021 25776 (InsnX IOR; 31022 25777 (InsnX IOR; 31023 25778 (InsnX IOR; 31024 25779 (LdcX 0 I); 31025 25780 (InsnX ISHL; 31026 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2346null Z; 31027 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31028 25782 (LdcX 0 I))); 31029 25783 (InsnX ISHL; 31030 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2348null Z; 31031 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31032 25785 (LdcX 1 I))); 31033 25786 (InsnX ISHL; 31034 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2350null Z; 31035 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31036 25788 (LdcX 2 I))); 31037 25789 (InsnX ISHL; 31038 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2352null Z; 31039 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31040 25791 (LdcX 3 I))))); 31041 (ReturnX). # Elsewhere, this split method is called, then the resulting field is loaded and written to the output buffer. 11325 (MethodStmtX INVOKEVIRTUAL __C1527collect_distributed_array",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:4624,Load,LoadX,4624,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['Load'],['LoadX']
Performance,invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(Writ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822:10819,concurren,concurrent,10819,https://hail.is,https://github.com/hail-is/hail/issues/1822,1,['concurren'],['concurrent']
Performance,"io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:5189,concurren,concurrent,5189,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['concurren'],['concurrent']
Performance,"io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:12285,concurren,concurrent,12285,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['concurren'],['concurrent']
Performance,"io.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf spark.executor.memory=40G \; --conf spark.driver.extraClassPath=\""$HAIL_HOME",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:18583,load,load,18583,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['load'],['load']
Performance,"ion.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```. 2. gs://hail-common/gencode_and_production_intervals.merged.hg19.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Traversable",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683:2274,load,loading,2274,https://hail.is,https://github.com/hail-is/hail/issues/1683,1,['load'],['loading']
Performance,"ion: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:18522,Load,Loading,18522,https://hail.is,https://github.com/hail-is/hail/issues/9293,2,"['Load', 'load']","['Loading', 'load']"
Performance,ion: sun.reflect.generics.reflectiveObjects.NotImplementedException; Serialization trace:; m (is.hail.annotations.aggregators.KeyedRegionValueAggregator); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: sun.reflect.generics.reflectiveObjects.NotImplementedException; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 10 more; Driver stacktrace:; 	at org.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:1476,concurren,concurrent,1476,https://hail.is,https://github.com/hail-is/hail/issues/4215,1,['concurren'],['concurrent']
Performance,ion: sun.reflect.generics.reflectiveObjects.NotImplementedException; Serialization trace:; m (is.hail.annotations.aggregators.KeyedRegionValueAggregator); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)sun.reflect.generics.reflectiveObjects.NotImplementedException: null; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSe,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:6317,concurren,concurrent,6317,https://hail.is,https://github.com/hail-is/hail/issues/4215,1,['concurren'],['concurrent']
Performance,ionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3040:6537,concurren,concurrent,6537,https://hail.is,https://github.com/hail-is/hail/issues/3040,1,['concurren'],['concurrent']
Performance,"ions and Removals</h2>; <ul>; <li><code>[#468](https://github.com/pytest-dev/pytest-xdist/issues/468) &lt;https://github.com/pytest-dev/pytest-xdist/issues/468&gt;</code>_: The <code>--boxed</code> command line argument is deprecated. Install pytest-forked and use <code>--forked</code> instead. pytest-xdist 3.0.0 will remove the <code>--boxed</code> argument and pytest-forked dependency.</li>; </ul>; <h2>Features</h2>; <ul>; <li>; <p><code>[#722](https://github.com/pytest-dev/pytest-xdist/issues/722) &lt;https://github.com/pytest-dev/pytest-xdist/issues/722&gt;</code>_: Full compatibility with pytest 7 - no deprecation warnings or use of legacy features.</p>; </li>; <li>; <p><code>[#733](https://github.com/pytest-dev/pytest-xdist/issues/733) &lt;https://github.com/pytest-dev/pytest-xdist/issues/733&gt;</code>_: New <code>--dist=loadgroup</code> option, which ensures all tests marked with <code>@pytest.mark.xdist_group</code> run in the same session/worker. Other tests run distributed as in <code>--dist=load</code>.</p>; </li>; </ul>; <h2>Trivial Changes</h2>; <ul>; <li>; <p><code>[#708](https://github.com/pytest-dev/pytest-xdist/issues/708) &lt;https://github.com/pytest-dev/pytest-xdist/issues/708&gt;</code>_: Use <code>@pytest.hookspec</code> decorator to declare hook options in <code>newhooks.py</code> to avoid warnings in <code>pytest 7.0</code>.</p>; </li>; <li>; <p><code>[#719](https://github.com/pytest-dev/pytest-xdist/issues/719) &lt;https://github.com/pytest-dev/pytest-xdist/issues/719&gt;</code>_: Use up-to-date <code>setup.cfg</code>/<code>pyproject.toml</code> packaging setup.</p>; </li>; <li>; <p><code>[#720](https://github.com/pytest-dev/pytest-xdist/issues/720) &lt;https://github.com/pytest-dev/pytest-xdist/issues/720&gt;</code>_: Require pytest&gt;=6.2.0.</p>; </li>; <li>; <p><code>[#721](https://github.com/pytest-dev/pytest-xdist/issues/721) &lt;https://github.com/pytest-dev/pytest-xdist/issues/721&gt;</code>_: Started using type annotations and mypy",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11491:1353,load,load,1353,https://hail.is,https://github.com/hail-is/hail/pull/11491,2,['load'],['load']
Performance,"ions: {regions}.'); 111 path = [dataset['url'][cloud][region]; 112 for dataset in datasets[name]['versions']; 113 if all([dataset['version'] == version,; 114 dataset['reference_genome'] == reference_genome])]; --> 115 assert len(path) == 1; 116 path = path[0]; 117 if path.startswith('s3://'):. AssertionError: ; ```. I'm a new Hail user and don't have the full context here, but it seems like there are at least three problems:. 1. An assert failed in production code, which indicates either the presence of a bug or an incorrect use of assert (e.g. using assert to check for value errors).; 2. The assert has no corresponding error message, so the user learns that something has gone wrong but can't easily tell what.; 3. The assert is bare. Bare asserts can get optimized out of code in ways that are difficult to foresee in advance, and are generally deprecated in favor of the `if error_condition: raise AssertionError(...)` pattern (see: https://discuss.python.org/t/stop-ignoring-asserts-when-running-in-optimized-mode/13132). **The Big Picture**. The bare assert pattern is used over 3k times in Hail. To be fair, many of these usages occur in test directories, where they're fine. But they also occur in application code, and often in the dangerous form `assert(expr1, expr2)` which will never fail (because a tuple with two falsy elements is truthy in python). These asserts are never actually getting checked. . Fixing all of them would be a heavy lift. One compromise solution might be to add a bare assert rule to the linter (e.g. https://pypi.org/project/flake8-assert-msg/). This would prevent the introduction of further bare asserts to the codebase, and encourage authors to clean up existing bare asserts on files they touch. The `assert` keyword is an unfortunate language wart that makes it very easy for developers to write error-checking code that is itself incorrect. I'd encourage considering the alternate pattern `if error_condition: raise AssertionError(...)` and gradually",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12952:1685,optimiz,optimized-mode,1685,https://hail.is,https://github.com/hail-is/hail/issues/12952,1,['optimiz'],['optimized-mode']
Performance,"ip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8)  ~/sandbox/hail [master|8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Coll",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1857,cache,cached,1857,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.loweri,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5646,Optimiz,Optimize,5646,https://hail.is,https://github.com/hail-is/hail/issues/9128,2,['Optimiz'],['Optimize']
Performance,ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.O,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5896,Optimiz,Optimize,5896,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.ex,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5545,Optimiz,Optimize,5545,https://hail.is,https://github.com/hail-is/hail/issues/9128,2,['Optimiz'],['Optimize']
Performance,"irements.txt --output-file=/tmp/tmp.aWUFJ1BMnP; ../check_pip_requirements.sh: line 13: pip-compile: command not found; ```. While I do have pip-compile installed. ```sh ; pip-compile --help; Usage: pip-compile [OPTIONS] [SRC_FILES]... Compiles requirements.txt from requirements.in, pyproject.toml, setup.cfg,; or setup.py specs. Options:; ```. Note that `make clean` did not solve the issue. see logs attached. ### Version. 0.2.120. ### Relevant log output. ```shell; BUILD SUCCESSFUL in 2m 46s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; /usr/lib64/python3.8/distutils/dist.py:274: UserWarning: Unknown distribution option: 'long_description_content_type'; warnings.warn(msg); installing to build/bdist.linux-x86_64/wheel; creating build/bdist.linux-x86_64/wheel/hail-0.2.120.dist-info/WHEEL; creating 'dist/hail-0.2.120-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it; adding 'hail/__init__.py'; adding 'hail/builtin_references.py'; adding 'hail/conftest.py'; adding 'hail/context.py'; adding 'hail/hail_logging.py'; adding 'hail/hail_pip_version'; adding 'hail/hail_revision'; adding 'hail/hail_version'; adding 'hail/matrixtable.py'; adding 'hail/table.py'; adding 'hail/backend/__init__.py'; adding 'hail/backend/backend.py'; adding 'hail/backend/hail-all-spark.jar'; adding 'hail/backend/local_backend.py'; adding 'hail/backend/py4j_backend.py'; adding 'hail/backend/service_backend.py'; adding 'hail/backend/spark_backend",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13445:1413,cache,cache,1413,https://hail.is,https://github.com/hail-is/hail/issues/13445,1,['cache'],['cache']
Performance,"is that the driver currently must wait for all the requests to workers in an iteration to complete before it starts the next iteration of the scheduler. This leaves the scheduler vulnerable to problematic workers or workers that happen to be preempted during the scheduling process. So, the driver sets a [2 second timeout](https://github.com/hail-is/hail/blob/b27737f67bf9e69f1abed2fec07fc7c921790ef8/batch/batch/driver/job.py#L585) on the call to `/api/v1alpha/batches/jobs/create`. Additionally, this general design means that in the event of a request timeout or transient error, Batch cannot guarantee that there is always at most one concurrent running attempt for a given job. This ends up being a fine (and intentional) concession in practice because the idempotent design of preemptible jobs tends to cover this scenario, but it is regardless wasted compute and cost to users. Nevertheless, we strive to minimize cases where we might halt the scheduling loop or double-schedule work, and one way to do that in the current design is to minimize the variance in latency of `/api/v1alpha/batches/jobs/create`. The largest source of this latency is the request to blob storage. While GCS and ABS are relatively fast and highly available, Batch in Azure Terra requires first obtaining SAS tokens from the Terra control plane, which can introduce much higher and more variable latency. There have also been occurrences in the past of corrupted or deleted specs, which introduce unexpected failure modes that should error the job but instead disrupt the scheduling loop. Many of these problems would be mitigated by moving the read from object storage outside of the `/api/v1alpha/batches/jobs/create` endpoint. The endpoint should push this read into the asynchronous task that ultimately runs the job and therefore return its acknowledgement to the driver faster. If the worker encounters errors later on while reading the spec, those should result in `error`ing the job instead of raising a 500 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14456:1927,latency,latency,1927,https://hail.is,https://github.com/hail-is/hail/issues/14456,1,['latency'],['latency']
Performance,"is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.130-bea04d9c79b5; Error summary: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:19902,cache,cache,19902,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['cache'],['cache']
Performance,"is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:822); at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:447); at is.hail.backend.service.Main$.main(Main.scala:15); at is.hail.backend.service.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```; </details>. The code is simple and clearly is running against a path that does not already exist:; ```; if not hl.hadoop_exists(get_aou_util_path('mt_sample_qc')):; print('Run sample qc MT.....'); mt = hl.read_matrix_table(ACAF_MT_PATH); mt = mt.filter_rows(mt.locus.in_autosome()); # mt = mt.filter_rows(mt.locus.contig == 'chr1'); ht = hl.sample_qc(mt, name='mt_sample_qc'); ht.write(get_aou_util_path('mt_sample_qc'), overwrite=args.overwrite); ```. Job log: https://batch.hail.is/batches/8058522/jobs/171029. <details>; <summary>The last TableIR logged</summary>. ```; 2023-10-13 02:14:44.213 : INFO: after optimize: darrayLowerer, after LowerAndExecuteShuffles: IR size 232: . !ht = TableRead [Table{global:Struct{},key:[locus,alleles],",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:6846,concurren,concurrent,6846,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['concurren'],['concurrent']
Performance,is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.11-cf54f08305d1; Error summary: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:15295,concurren,concurrent,15295,https://hail.is,https://github.com/hail-is/hail/issues/5718,2,['concurren'],['concurrent']
Performance,iteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5505,Optimiz,Optimize,5505,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,"ithub-redirect.dependabot.com/googleapis/java-storage/issues/1806"">#1806</a>) (<a href=""https://github.com/googleapis/java-storage/commit/0f24a11c5289a4c07f27d8a3c29fab34520b036f"">0f24a11</a>)</li>; <li>Implement GrpcStorageImpl#deleteDefaultAcl (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1807"">#1807</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c78327717a7936492161ddcc64c86374db72c48c"">c783277</a>)</li>; <li>Implement GrpcStorageImpl#getDefaultAcl (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1802"">#1802</a>) (<a href=""https://github.com/googleapis/java-storage/commit/b9b7c49fcfcab285da156b34b186a007150e876f"">b9b7c49</a>)</li>; <li>Implement GrpcStorageImpl#listDefaultAcl (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1805"">#1805</a>) (<a href=""https://github.com/googleapis/java-storage/commit/03c2e6660721b4a8bfc09b241ef44f3e4e08865b"">03c2e66</a>)</li>; <li>Improve throughput of http based storage#reader between 100 MiB/s and 200 MiB/s (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1799"">#1799</a>) (<a href=""https://github.com/googleapis/java-storage/commit/94cd2887f22f6d1bb82f9929b388c27c63353d77"">94cd288</a>)</li>; <li>Update GrpcBlobReadChannel to allow seek/limit after read (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1834"">#1834</a>) (<a href=""https://github.com/googleapis/java-storage/commit/45dc983a4af8e7feb937263ce611bd34eda37e03"">45dc983</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>Add missing preconditions and update samples (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1753"">#1753</a>) (<a href=""https://github.com/googleapis/java-storage/commit/96beca2465158fb4633d58fe09a9776a4b171811"">96beca2</a>)</li>; <li><strong>grpc:</strong> Fix bucket logging conversion to allow clearing (<a href=""https://github-redirect.dependabot.com/googl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12598:2563,throughput,throughput,2563,https://hail.is,https://github.com/hail-is/hail/pull/12598,2,['throughput'],['throughput']
Performance,"ity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Open Redirect <br/>[SNYK-PYTHON-NOTEBOOK-1041707](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-1041707) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | Information Exposure <br/>[SNYK-PYTHON-NOTEBOOK-2441824](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2441824) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Access Restriction Bypass <br/>[SNYK-PYTHON-NOTEBOOK-2928995](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2928995) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | Race Condition <br/>[SNYK-PYTHON-PROMPTTOOLKIT-6141120](https://snyk.io/vuln/SNYK-PYTHON-PROMPTTOOLKIT-6141120) | `prompt-toolkit:` <br> `1.0.18 -> 3.0.13` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-1086606](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1086606) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | Denial of Service (DoS) <br/>[SNYK-PYTHON-PYGMENTS-1088505](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1088505) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-5750273](https://snyk.io/vuln/SNYK-PYTHO",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14257:4329,Race Condition,Race Condition,4329,https://hail.is,https://github.com/hail-is/hail/pull/14257,2,['Race Condition'],['Race Condition']
Performance,"ize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- LoweringTransformation : 77.664ms, total 485.244ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 1.780ms, total 487.281ms, tagged coverage 0.0; ...; ...; ...; 2019-11-06 18:44:11 root: INFO: Timer: aggregate:; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'Verify' (8): 8.109ms; 2019-11-06 18:44:11 root: INFO: Time taken fo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:2594,Optimiz,Optimize,2594,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,"java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 5.0 failed 20 times, most recent failure: Lost task 22.19 in stage 5.0 (TID 133, seqr-pipeline-cluster-grch38-w-1.c.seqr-project.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822:4399,concurren,concurrent,4399,https://hail.is,https://github.com/hail-is/hail/issues/1822,1,['concurren'],['concurrent']
Performance,java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: is.hail.asm4s.AsmFunction2; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:13982,load,loadOrDefineClass,13982,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['load'],['loadOrDefineClass']
Performance,"java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1515); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:610); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:599); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:280); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:278); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:25); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$15(ServiceBackend.scala:225); 	at scala.util.Try$.apply(Try.scala:213); 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:995); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); ```. The driver will have log output like this:; ```; 2023-09-22 19:11:13.051 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/8042383 response 200; 2023-09-22 19:11:13.052 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=: reading results; 2023-09-22 19:11:13.125 ServiceBackend$: INFO: all results read. 0.072746861 s. 0.0 result/s. 0.0 MiB/s.; 2023-09-22 19:11:13.125 : INFO: [collectDArray|table_native_writer]: executed 5 tasks in 1.822s; 2023-09-22 19:11:13.126 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:13.126 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:2197,concurren,concurrent,2197,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1515); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:610); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:599); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:280); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:278); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:25); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$15(ServiceBackend.scala:225); 	at scala.util.Try$.apply(Try.scala:213); 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:995); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); ```. but the worker looks like this:; ```; 2023-09-22 19:11:12.125 JVMEntryway: INFO: is.hail.JVMEntryway received arguments:; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 0: /hail-jars/gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 1: is.hail.backend.service.Main; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 2: /batch/fe537a243a3046d29d76861ffee94b92; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 3: /batch/fe537a243a3046d29d76861ffee94b92/log; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 4: gs://hail-query-ger0g/jars/b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 5: worker; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 6: gs://1-day/,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:6195,concurren,concurrent,6195,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"jects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . ~/projects/hail/python/hail/table.py in _process_joins(self, *exprs); 1463 def broadcast_f(left, data, jt):; 1464 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1465 return process_joins(self, exprs, broadcast_f); 1466 ; 1467 def cache(self):. ~/projects/hail/python/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 365 all_uids.extend(list(t)); 366 data = hail.Struct(**{b.uid: b.value for b in broadcasts}); --> 367 data_json = t._to_json(data); 368 left = broadcast_f(left, data_json, t._jtype); 369 . ~/projects/hail/python/hail/expr/types.py in _to_json(self, x); 176 def _to_json(self, x):; 177 converted = self._convert_to_json_na(x); --> 178 return json.dumps(converted); 179 ; 180 def _convert_to_json_na(self, x):. ~/anaconda2/envs/hail/lib/python3.6/json/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw); 229 cls is None and indent is None and separators is None and; 230 default is None and not sort_keys and not kw):; --> 231 return _default_encoder.encode(obj); 232 if cls is None:; 233 cls = JSONEncoder. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in encode(self, o); 1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3708:3229,cache,cache,3229,https://hail.is,https://github.com/hail-is/hail/issues/3708,1,['cache'],['cache']
Performance,"json (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2150"">#2150</a>) (<a href=""https://github.com/googleapis/java-storage/commit/330e795040592e5df22d44fb5216ad7cf2448e81"">330e795</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.14.0 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2151"">#2151</a>) (<a href=""https://github.com/googleapis/java-storage/commit/eba8b6a235919a27d1f6dadf770140c7d143aa1a"">eba8b6a</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.24.0...v2.25.0"">2.25.0</a> (2023-07-24)</h2>; <h3>Features</h3>; <ul>; <li>BlobWriteChannelV2 - same throughput less GC (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2110"">#2110</a>) (<a href=""https://github.com/googleapis/java-storage/commit/1b52a1053130620011515060787bada10c324c0b"">1b52a10</a>)</li>; <li>Update Storage.createFrom(BlobInfo, Path) to have 150% higher throughput (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2059"">#2059</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4c2f44e28a1ff19ffb2a02e3cefc062a1dd98fdc"">4c2f44e</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>Update BlobWriteChannelV2 to properly carry forward offset after incremental flush (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2125"">#2125</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c099a2f4f8ea9afa6953270876653916b021fd9f"">c099a2f</a>)</li>; <li>Update GrpcStorageImpl.createFrom(BlobInfo, Path) to use RewindableContent (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2112"">#2112</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c80505129baa831e492a5514e937875407211595"">c805051</a>)</li>; </ul>; <h3>Documentation</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13605:8129,throughput,throughput,8129,https://hail.is,https://github.com/hail-is/hail/pull/13605,1,['throughput'],['throughput']
Performance,"k ></Link>`; ex:; ```jsx; <Link href='/path/to/page'><a>Page Name</a></Link>; ```. This simply adds the client-side routing logic, and passes the href to <a href=. . ### Prefetching; One of the neat things about Next is how easy it makes prefetching pages. This allows perceived page loading times on the order of 5ms, even when the page requires very complex state (say a GraphQL or series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:14496,cache,cache,14496,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['cache'],['cache']
Performance,"k.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) t org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.sparkapache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.sparkitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDDputeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MaD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpointla:90) at org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:4cala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$ailureException: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/t sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.javannel(UnixFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:40rage.BlockManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.rator$$anon$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.sparkler.processFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.Ab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:7377,concurren,concurrent,7377,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['concurren'],['concurrent']
Performance,k.serialization.ScalaValueWriter.write(ScalaValueWriter.scala:46); 	at org.elasticsearch.hadoop.serialization.builder.ContentBuilder.value(ContentBuilder.java:53); 	at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.doWriteObject(TemplatedBulk.java:71); 	at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.write(TemplatedBulk.java:58); 	at org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:168); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:67); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:107); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:107); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4250:5414,concurren,concurrent,5414,https://hail.is,https://github.com/hail-is/hail/issues/4250,1,['concurren'],['concurrent']
Performance,"k.serialization.ScalaValueWriter.write(ScalaValueWriter.scala:46); 	at org.elasticsearch.hadoop.serialization.builder.ContentBuilder.value(ContentBuilder.java:53); 	at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.doWriteObject(TemplatedBulk.java:71); 	at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.write(TemplatedBulk.java:58); 	at org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:168); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:67); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:107); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:107); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-408f188; Error summary: EsHadoopIllegalArgumentException: Spark SQL types are not handled through basic RDD saveToEs() calls; typically this is a mistake(as the SQL schema will be ignored). Use 'org.elasticsearch.spark.sql' package instead; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [ffc9fb0b99f64080b674ab7a07962df9] entered state [ERROR] while waiting for [DONE].; ```. Ideally it would get exported as nested objects: https://www.elastic.co/guide/en/elasticsearch/reference/current/nested.html#_using_literal_nested_literal_fields_for_arrays_of_objects. with elasticsearch mapping:; ```; u'vep': {'type': 'nested', 'properties': {u'category': {'type': 'keyword'}, u'major_consequence': {'type': 'keyword'}, u'gene_id': {'type': 'keyword'}, u'major_consequence_rank': {'type': 'integer'}, u'gene_symbol': {'type': 'keyword'}, u'transcript_id': {'type': 'keyword'}, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4250:10968,concurren,concurrent,10968,https://hail.is,https://github.com/hail-is/hail/issues/4250,1,['concurren'],['concurrent']
Performance,k.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19196,Load,LoadVCF,19196,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,kage$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3040:6622,concurren,concurrent,6622,https://hail.is,https://github.com/hail-is/hail/issues/3040,1,['concurren'],['concurrent']
Performance,ke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:7885,Load,LoadVCF,7885,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,ke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:750). java.lang.ClassFormatError: Too many arguments in method signature in class file __C2866stream; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:635); 	at is.hail.asm4s.HailClassLoader.liftedTree1$1(HailClassLoader.scala:10); 	at is.hail.asm4s.HailClassLoader.loadOrDefineClass(HailClassLoader.scala:6); 	at is.hail.asm4s.ClassesBytes.$anonfun$load$1(ClassBuilder.scala:64); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:11789,load,loadOrDefineClass,11789,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['load'],['loadOrDefineClass']
Performance,ke.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8338:11869,Optimiz,Optimize,11869,https://hail.is,https://github.com/hail-is/hail/issues/8338,1,['Optimiz'],['Optimize']
Performance,"kendUtils.scala:52); app//is.hail.backend.BackendUtils$Lambda$783/0x000000080080c040.apply(Unknown Source); app//is.hail.utils.package$.using(package.scala:635); app//is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); app//is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:51); app//is.hail.backend.BackendUtils$Lambda$757/0x00000008007bcc40.apply(Unknown Source); app//is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:751); app//org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); app//org.apache.spark.rdd.RDD.iterator(RDD.scala:329); app//org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); app//org.apache.spark.scheduler.Task.run(Task.scala:136); app//org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); app//org.apache.spark.executor.Executor$TaskRunner$Lambda$608/0x0000000800652c40.apply(Unknown Source); app//org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); app//org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); java.base@11.0.17/java.lang.Thread.run(Thread.java:829); ```. A few things:; 1. Verify that this case statement is evaluated intelligently. In particular, we really want to evaluate each predicate once, and only if necessary.; 2. We *should not allocate* just to evaluate these reference genome predicates, but that is [exactly what we do](https://github.com/hail-is/hail/blob/main/hail/src/main/scala/is/hail/expr/ir/functions/LocusFunctions.scala#L67-L72). It seems like the right fix is for the ReferenceGenome's intervals to be shipped as literals so that we can perform `inXPar` or `isAutosomal` checks without allocating contig strings or locus objects. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13862:3582,concurren,concurrent,3582,https://hail.is,https://github.com/hail-is/hail/issues/13862,3,"['concurren', 'perform']","['concurrent', 'perform']"
Performance,kingOutputStream.scala:23); 	at is.hail.io.index.IndexWriterUtils.close(IndexWriter.scala:225); 	at __C1756collect_distributed_array_table_native_writer.apply_region99_120(Unknown Source); 	at __C1756collect_distributed_array_table_native_writer.apply_region5_223(Unknown Source); 	at __C1756collect_distributed_array_table_native_writer.apply(Unknown Source); 	at __C1756collect_distributed_array_table_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$16(BackendUtils.scala:91); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90); 	at is.hail.backend.service.Worker$.$anonfun$main$9(Worker.scala:172); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:171); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.service.Worker$.main(Worker.scala:169); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.125-6e6f46797aed; Error summary: NullPointerException: null; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13937:9616,concurren,concurrent,9616,https://hail.is,https://github.com/hail-is/hail/issues/13937,6,['concurren'],['concurrent']
Performance,"ks / 384.0K chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.656 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""domain"": ""global"",; ""reason"": """,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:10088,concurren,concurrent,10088,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"ks requested=51, cache hits=0; 2023-09-24 01:58:51.513 : INFO: RegionPool: FREE: 4.3M allocated (2.2M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:51.515 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/aou_tmp/o?name=tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89&uploadType=resumable&upload_id=ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guploader-uploadid: ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	| ; 	at is.hail.relocated.com.google.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721:4560,concurren,concurrent,4560,https://hail.is,https://github.com/hail-is/hail/issues/13721,1,['concurren'],['concurrent']
Performance,"l upgrades. We also encourage upgrading to MarkupSafe 2.1.1, the latest version at this time.</p>; <ul>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-0"">https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-0</a></li>; <li>Milestone: <a href=""https://github.com/pallets/jinja/milestone/8?closed=1"">https://github.com/pallets/jinja/milestone/8?closed=1</a></li>; <li>MarkupSafe changes: <a href=""https://markupsafe.palletsprojects.com/en/2.1.x/changes/#version-2-1-1"">https://markupsafe.palletsprojects.com/en/2.1.x/changes/#version-2-1-1</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/blob/main/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 3.1.2</h2>; <p>Released 2022-04-28</p>; <ul>; <li>Add parameters to <code>Environment.overlay</code> to match <code>__init__</code>.; :issue:<code>1645</code></li>; <li>Handle race condition in <code>FileSystemBytecodeCache</code>. :issue:<code>1654</code></li>; </ul>; <h2>Version 3.1.1</h2>; <p>Released 2022-03-25</p>; <ul>; <li>The template filename on Windows uses the primary path separator.; :issue:<code>1637</code></li>; </ul>; <h2>Version 3.1.0</h2>; <p>Released 2022-03-24</p>; <ul>; <li>; <p>Drop support for Python 3.6. :pr:<code>1534</code></p>; </li>; <li>; <p>Remove previously deprecated code. :pr:<code>1544</code></p>; <ul>; <li><code>WithExtension</code> and <code>AutoEscapeExtension</code> are built-in now.</li>; <li><code>contextfilter</code> and <code>contextfunction</code> are replaced by; <code>pass_context</code>. <code>evalcontextfilter</code> and; <code>evalcontextfunction</code> are replaced by <code>pass_eval_context</code>.; <code>environmentfilter</code> and <code>environmentfunction</code> are replaced; by <code>pass_environment</code>.</li>; <li><code>Markup</code> and <code>escape</code> should be imported from MarkupSafe.</li>; <li>Comp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12173:2438,race condition,race condition,2438,https://hail.is,https://github.com/hail-is/hail/pull/12173,1,['race condition'],['race condition']
Performance,"l.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:6271,Load,LoadVCF,6271,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,l.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:6199,Optimiz,Optimize,6199,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,l.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:6124,Load,LoadPlink,6124,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,"l; sp.check_output(args, stderr=sp.STDOUT, **kwargs); File ""/opt/conda/default/lib/python3.11/subprocess.py"", line 466, in check_output; return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/opt/conda/default/lib/python3.11/subprocess.py"", line 571, in run; raise CalledProcessError(retcode, process.args,; subprocess.CalledProcessError: Command '('pip', 'install', 'setuptools', 'mkl<2020', 'lxml<5', 'https://github.com/hail-is/jgscm/archive/v0.1.13+hail.zip', 'ipykernel==6.22.0', 'ipywidgets==8.0.6', 'jupyter-console==6.6.3', 'nbconvert==7.3.1', 'notebook==6.5.6', 'qtconsole==5.4.2', 'aiodns==2.0.0', 'aiohttp==3.9.5', 'aiosignal==1.3.1', 'async-timeout==4.0.3', 'attrs==23.2.0', 'avro==1.11.3', 'azure-common==1.1.28', 'azure-core==1.30.2', 'azure-identity==1.17.1', 'azure-mgmt-core==1.4.0', 'azure-mgmt-storage==20.1.0', 'azure-storage-blob==12.20.0', 'bokeh==3.3.4', 'boto3==1.34.138', 'botocore==1.34.138', 'cachetools==5.3.3', 'certifi==2024.6.2', 'cffi==1.16.0', 'charset-normalizer==3.3.2', 'click==8.1.7', 'commonmark==0.9.1', 'contourpy==1.2.1', 'cryptography==42.0.8', 'decorator==4.4.2', 'deprecated==1.2.14', 'dill==0.3.8', 'frozenlist==1.4.1', 'google-auth==2.31.0', 'google-auth-oauthlib==0.8.0', 'humanize==1.1.0', 'idna==3.7', 'isodate==0.6.1', 'janus==1.0.0', 'jinja2==3.1.4', 'jmespath==1.0.1', 'jproperties==2.1.1', 'markupsafe==2.1.5', 'msal==1.29.0', 'msal-extensions==1.2.0', 'msrest==0.7.1', 'multidict==6.0.5', 'nest-asyncio==1.6.0', 'numpy==1.26.4', 'oauthlib==3.2.2', 'orjson==3.10.6', 'packaging==24.1', 'pandas==2.2.2', 'parsimonious==0.10.0', 'pillow==10.4.0', 'plotly==5.22.0', 'portalocker==2.10.0', 'protobuf==3.20.2', 'py4j==0.10.9.7', 'pyasn1==0.6.0', 'pyasn1-modules==0.4.0', 'pycares==4.4.0', 'pycparser==2.22', 'pygments==2.18.0', 'pyjwt==2.8.0', 'python-dateutil==2.9.0.post0', 'python-json-logger==2.0.7', 'pytz==2024.1', 'pyyaml==6.0.1', 'regex==2024.5.15', 'requests==2.32.3'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14652:5944,cache,cachetools,5944,https://hail.is,https://github.com/hail-is/hail/issues/14652,1,['cache'],['cachetools']
Performance,la:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_matrix_ir$2(SparkBackend.scala:689); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:69); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:69); 	at is.hail.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:6465,Load,LoadPlink,6465,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,la:270); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:268); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:304); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:300); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1743); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1741); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at is.hail.annotations.UnsafeIndexedSeq.foreach(UnsafeRow.scala:51); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1741); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1734); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1734); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1728); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3276:2518,concurren,concurrent,2518,https://hail.is,https://github.com/hail-is/hail/issues/3276,2,['concurren'],['concurrent']
Performance,"la:630); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:848); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:817); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open databa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:15311,cache,cache,15311,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['cache'],['cache']
Performance,la:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:108); at scala.collection.TraversableLike$class.map,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:3302,concurren,concurrent,3302,https://hail.is,https://github.com/hail-is/hail/issues/4780,1,['concurren'],['concurrent']
Performance,la:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:12480,load,loadClass,12480,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['load'],['loadClass']
Performance,"lang.Thread.run(Thread.java:745); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 35 in stage 7.0 failed 20 times, most recent failure: Lost task 35.19 in stage 7.0 (TID 6963, gnomad-prod-sw-m8lk.c.broad-mpg-gnomad.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$han",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202:4784,concurren,concurrent,4784,https://hail.is,https://github.com/hail-is/hail/issues/1202,1,['concurren'],['concurrent']
Performance,"lation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8)  ~/sandbox/hail [master|8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting deco",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1783,cache,cached,1783,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"lctl/describe.py:104 in describe ;  ;  101  ''' ;  102  Describe the MatrixTable or Table at path FILE. ;  103  ''' ;   104  asyncio.get_event_loop().run_until_complete(async_describe(file, requester_pays_proj ;  105 ;  106 ;  107 async def async_describe( ;  ;  /opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.1 ;  0/asyncio/base_events.py:649 in run_until_complete ;  ;  646   if not future.done(): ;  647    raise RuntimeError('Event loop stopped before Future completed.') ;  648   ;   649   return future.result() ;  650  ;  651  def stop(self): ;  652   """"""Stop running the event loop. ;  ;  /Users/cvittal/src/hail/hail/python/hailtop/hailctl/describe.py:119 in async_describe ;  ;  116   gcs_kwargs['project'] = requester_pays_project_id ;  117  ;  118  async with aio_contextlib.closing(RouterAsyncFS(gcs_kwargs=gcs_kwargs)) as fs: ;   119   j = orjson.loads(decompress(await fs.read(path.join(file, 'metadata.json.gz')), ;  120   ;  121   # Get the file schema ;  122   file_schema = parse_schema(j[next(k for k in j.keys() if k.endswith('type'))]) ;  ;  /Users/cvittal/src/hail/hail/python/hailtop/aiotools/fs/fs.py:281 in read ;  ;  278    pass ;  279  ;  280  async def read(self, url: str) -> bytes: ;   281   async with await self.open(url) as f: ;  282    return await f.read() ;  283  ;  284  async def read_from(self, url: str, start: int) -> bytes: ;  ;  /Users/cvittal/src/hail/hail/python/hailtop/aiotools/router_fs.py:75 in open ;  ;  72   return self._load_fs(uri) ;  73  ;  74  async def open(self, url: str) -> ReadableStream: ;   75   fs = self._get_fs(url) ;  76   return await fs.open(url) ;  77  ;  78  async def _open_from(self, url: str, start: int, *, length: Optional[int] = None) -> ;  ;  /Users/cvittal/src/hail/hail/python/hailtop/aiotools/router_fs.py:72 in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13793:1542,load,loads,1542,https://hail.is,https://github.com/hail-is/hail/issues/13793,1,['load'],['loads']
Performance,lczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json | jq '.' | less; ```. which contained an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: is.hail.backend.service.EndOfInputException; 	at is.hail.backend.service.ServiceBackendSocketAPI2.read(ServiceBackend.scala:497); 	at is.hail.backend.service.ServiceBackendSocketAPI2.readInt(ServiceBackend.scala:510); 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:561); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.scala:462); 	at is.hail.backend.service.ServiceBackendSocketAP,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:4569,concurren,concurrent,4569,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance,"le if it is ready but has not been cancelled. Now we aim to incremental maintain the following information:. Globally:; - runnable jobs and cores. Per user:; - runnable and running jobs and cores,; - running cancelled jobs, and; - ready cancelled jobs. The global runnable cores are needed by the instance pool controller. The per-user stats are needed by the three threads of the scheduler:; - for the fair share allocator and the scheduler,; - to cancel running jobs on workers that have been cancelled (because the batch was cancelled),; - to cancel ready jobs that have been cancelled (either because the batch was cancelled or a parent failed). In order to update these values efficiently when a batch is cancelled, we also track in `batch_cancellable_resources` table, per batch:; - cancellable ready jobs and cores,; - cancellable running jobs and cores.; I added a `cancel_batch` procedure that uses these values to update ready_cores and user_resources when a batch is cancelled. I also reorganized the threads of the scheduler. Each one uses the above structures to compute a fair share for each user of work to do in a give iteration (dividing up 1000 tasks, with a per-user min of 20). Those tasks are then executed with 100-way parallelism. Other changes:; - I added a recompute_incremental procedure for recomputing all the incremental structures,; - I added a batches.state field (open, running or complete) and removed the closed column,; - I updated the batch and jobs indexes to make sure all scheduler queries are probably indexed. This isn't the case right now and we're seeing a lot of load on the database because of it. I'm going to do some more testing and possibly rename some stuff, but it is passing and the incremental structures all line up at the end of the tests. The only issue I see is I loop through all running batches for a user in the scheduler. If a user submits many many small batches, this could be an issue. I plan to address this in a later PR. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7933:1865,load,load,1865,https://hail.is,https://github.com/hail-is/hail/pull/7933,1,['load'],['load']
Performance,"leQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-63d60cc; Error summary: HailException: invalid allele ""<DEL>"". ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413:9512,concurren,concurrent,9512,https://hail.is,https://github.com/hail-is/hail/issues/3413,2,['concurren'],['concurrent']
Performance,"leases"">orjson's releases</a>.</em></p>; <blockquote>; <h2>3.10.0</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.10.0 - 2024-03-27</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14427:1185,optimiz,optimization,1185,https://hail.is,https://github.com/hail-is/hail/pull/14427,1,['optimiz'],['optimization']
Performance,"lect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz: caught java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(Ro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:9628,Load,LoadVCF,9628,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,"lected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8)  ~/sandbox/hail [master|8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cach",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1542,cache,cached,1542,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,lection.AbstractTraversable.map(Traversable.scala:108); at is.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:113); at is.hail.expr.ir.Pretty.header(Pretty.scala:405); at is.hail.expr.ir.Pretty.pretty$1(Pretty.scala:463); at is.hail.expr.ir.Pretty.$anonfun$sexprStyle$4(Pretty.scala:453); at scala.collection.Iterator$$anon$10.next(Iterator.scala:459); at scala.collection.Iterator$ConcatIterator.next(Iterator.scala:230); at is.hail.utils.richUtils.RichIterator$$anon$3.next(RichIterator.scala:67); at is.hail.utils.prettyPrint.Doc$.advance$1(PrettyPrintWriter.scala:68); at is.hail.utils.prettyPrint.Doc$.render(PrettyPrintWriter.scala:139); at is.hail.utils.prettyPrint.Doc.render(PrettyPrintWriter.scala:163); at is.hail.utils.prettyPrint.Doc.render(PrettyPrintWriter.scala:167); at is.hail.expr.ir.Pretty.sexprStyle(Pretty.scala:466); at is.hail.expr.ir.Pretty.apply(Pretty.scala:429); at is.hail.expr.ir.Pretty$.apply(Pretty.scala:22); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:45); at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(Wrapped,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046:2769,Optimiz,Optimize,2769,https://hail.is,https://github.com/hail-is/hail/issues/13046,1,['Optimiz'],['Optimize']
Performance,"les in the batch-driver python process. By using proper persistent connections, we can reduce the TLS overhead to single-digit percents of a core. This leads to the first goal of this transition: configure our load balancers to know the full cluster configuration at any point in time so they can properly maintain connection pools with upstream services. However, this is not the only problem. Each ""upstream"" Service in Kubernetes may consist of multiple underlying pods but Kubernetes Services as we use them don't provide proper load-balancing when mixed with persistent connections. When we declare a Service for say, batch in default, Kubernetes adds a DNS record for `batch.default` that resolves to a single IP pointing at kube-proxy. When a new TCP connection is established with kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are create",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:2999,load,load-balancer,2999,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['load'],['load-balancer']
Performance,less to optimize.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3333:8,optimiz,optimize,8,https://hail.is,https://github.com/hail-is/hail/issues/3333,1,['optimiz'],['optimize']
Performance,lhost): org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'; 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:247); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Unsupported/Unknown Elasticsearch version 6.0.0; 	at org.elasticsearch.hadoop.util.EsMajorVersion.parse(EsMajorVersion.java:79); 	at org.elasticsearch.hadoop.rest.RestClient.remoteEsVersion(RestClient.java:613); 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:240); 	... 10 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(Array,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4138:2579,concurren,concurrent,2579,https://hail.is,https://github.com/hail-is/hail/issues/4138,1,['concurren'],['concurrent']
Performance,"linux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5874,cache,cached,5874,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"listing all batches readble by that user. This change fixes that; regression by making use index hints and `STRAIGHT_JOIN`s. The index hint tells MySQL to never consider the index `batches_deleted` as it; has very low cardinality. In some forms of this query, the planner tries to use; it to its peril. A problem in query 0 with #14629 (see below) was that fewer filters on batches; made the optimiser consider joins in a suboptimal order - it did a table scan ; on `job_groups` first then sorted the results by to `batches.id DESC` instead; of doing an index scan on `batches` in reverse. Using `STRAIGHT_JOIN`s instead of `INNER JOIN` mades the optimiser start from; `batches` and read its index in reverse before considering other tables in ; subsequent joins. From the [documentation](https://dev.mysql.com/doc/refman/8.4/en/join.html):. > STRAIGHT_JOIN is similar to JOIN, except that the left table is always read; before the right table. This can be used for those (few) cases for which the; join optimizer processes the tables in a suboptimal order. This is advantageous for a couple of reasons:; - We want to list newer batches first; - For this query, the `batches` table has more applicables indexes; - We want the variable to order by to be in the primary key of the first; table so we can read the index in reverse. Before and after timings, collected by running the query 5 times, then using; profiles gathered by MySQL.; ```; +-------+---------------------------------------------------*; | query | description | ; +-------+---------------------------------------------------+; | 0 | All batches accessible to user `ci` |; | 1 | All batches accessible to user `ci` owned by `ci` |; +-------+---------------------------------------------------*. +-------+--------+--------------------------------------------------------+------------+------------+; | query | branch | timings | mean | stdev | ; +-------+--------+--------------------------------------------------------+------------+----",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14649:1097,optimiz,optimizer,1097,https://hail.is,https://github.com/hail-is/hail/pull/14649,1,['optimiz'],['optimizer']
Performance,"llecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6449,cache,cached,6449,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"llecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, dill, Deprecated, bokeh, backcall, asyncinit, aiohttp-session, ipython, hail; Successfully installed Deprecated-1.2.12 Jinja",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:7293,cache,cached,7293,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"lled from pip. (Installed Spark 2.2.2 separately and set `SPARK_HOME` accordingly). . ### What you did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixKeyRowsBy.execute(MatrixIR.scala:1317); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapRows.execute(MatrixIR.scala:1352); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:18",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733:1299,load,loadLibrary,1299,https://hail.is,https://github.com/hail-is/hail/issues/4733,1,['load'],['loadLibrary']
Performance,load .gtf and .reg files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9:0,load,load,0,https://hail.is,https://github.com/hail-is/hail/issues/9,1,['load'],['load']
Performance,load BCF files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/35:0,load,load,0,https://hail.is,https://github.com/hail-is/hail/issues/35,1,['load'],['load']
Performance,load FASTA files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6:0,load,load,0,https://hail.is,https://github.com/hail-is/hail/issues/6,1,['load'],['load']
Performance,"loadField, loadElement returns the field/element address and not the corresponding value (requiring a 2nd load operations specific to the underlying type to retrieve the value, such as unsafe.getInt via Memory.loadInt). Makes for slightly strange semantics, as seen in ArrayElementLengthCheckAggregator.scala:. ```scala; def copyFromAddress(src: Code[Long]): Code[Unit] = {; val srcOff = fb.newField[Long]; //loadField is actually an offset/memory address; // and is actually a no-op unless the field is a pointer type (array currently); val initOffset = typ.loadField(srcOff, 0); //same ; val eltOffset = arrayType.loadElement(region, typ.loadField(srcOff, 1), idx) . Code(; srcOff := src,; init(initContainer.copyFrom(initOffset), initLen = false),; typ.isFieldMissing(srcOff, 1).mux(; Code(typ.setFieldMissing(off, 1),; lenRef := -1),; Code(; lenRef := arrayType.loadLength(typ.loadField(srcOff, 1)), #loadLength calls loadInt on the address returned; seq(container.copyFrom(eltOffset))))); }; ```. cc @tpoterba in case you disagree",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7829:0,load,loadField,0,https://hail.is,https://github.com/hail-is/hail/issues/7829,12,['load'],"['load', 'loadElement', 'loadField', 'loadInt', 'loadLength']"
Performance,"loadIRIntermediate load binary and array, so it must be passed the element, but the element offset. It probably shouldn't do that but this was a quicker fix.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3321:0,load,loadIRIntermediate,0,https://hail.is,https://github.com/hail-is/hail/pull/3321,2,['load'],"['load', 'loadIRIntermediate']"
Performance,"loading a file that has 12 partitions. when setting `hl.init(min_block_size=0)` and then `hl.import_table(..., min_partitions=100)`, now getting only 3 partitions.... ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5603:0,load,loading,0,https://hail.is,https://github.com/hail-is/hail/issues/5603,1,['load'],['loading']
Performance,loading a second HailContext() does not produce an error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1137:0,load,loading,0,https://hail.is,https://github.com/hail-is/hail/issues/1137,1,['load'],['loading']
Performance,loading pedigree with samples not in dataset fails,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/94:0,load,loading,0,https://hail.is,https://github.com/hail-is/hail/issues/94,1,['load'],['loading']
Performance,"lp. In [1]: import hail as hl ; hl.import_; In [2]: t = hl.import_table('/tmp/bar') ; ...: t.describe() ; ...: t = t.key_by('sample_id') ; Initializing Spark and Hail with default parameters...; using hail jar at /usr/local/lib/python3.7/site-packages/hail/hail-all-spark.jar; 19/06/13 14:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.1; SparkUI available at http://wm06b-953.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.14-5cb00c115421; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20190613-1408-0.2.14-5cb00c115421.log; 2019-06-13 14:08:15 Hail: INFO: Reading table with no type imputation; Loading column '?sample_id' as type 'str' (type not specified). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'sample_id': str ; ----------------------------------------; Key: []; ----------------------------------------; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-2-6b119cf7ec41> in <module>; 1 t = hl.import_table('/tmp/bar'); 2 t.describe(); ----> 3 t = t.key_by('sample_id'). </usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-958> in key_by(self, *keys, **named_keys). /usr/local/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /u",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6342:1865,Load,Loading,1865,https://hail.is,https://github.com/hail-is/hail/issues/6342,1,['Load'],['Loading']
Performance,"ls.py:398>>]; path = '/tmp/JnQ2m'. async def rm_dir(pool: OnlineBoundedGather2,; contents_tasks: List[asyncio.Task],; path: str):; assert listener is not None; listener(1); if contents_tasks:; await pool.wait(contents_tasks); try:; > await self.rmdir(path). /usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py:378: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; /usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py:352: in rmdir; return await blocking_to_async(self._thread_pool, os.rmdir, path); /usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:162: in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; /usr/lib/python3.9/asyncio/futures.py:284: in __await__; yield self # This tells Task to wait for completion.; /usr/lib/python3.9/asyncio/tasks.py:328: in __wakeup; future.result(); /usr/lib/python3.9/asyncio/futures.py:201: in result; raise self._exception; /usr/lib/python3.9/concurrent/futures/thread.py:58: in run; result = self.fn(*self.args, **self.kwargs); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . > thread_pool, lambda: fun(*args, **kwargs)); E OSError: [Errno 39] Directory not empty: '/tmp/JnQ2m'. /usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:163: OSError. During handling of the above exception, another exception occurred:. self = <hailtop.aiotools.local_fs.LocalAsyncFS object at 0x7f264046f700>; sema = <asyncio.locks.Semaphore object at 0x7f263d7a61c0 [unlocked, value:50]>; url = '/tmp/JnQ2m'; listener = <function LocalAsyncFS.rmtree.<locals>.<lambda> at 0x7f263e041820>. async def rmtree(self,; sema: Optional[asyncio.Semaphore],; url: str,; listener: Optional[Callable[[int], None]] = None) -> None:; path = self._get_path(url); if listener is None:; listener = lambda _: None; if sema is None:; sema = asyncio.Semaphore(50); ; async def rm_file(path: str):; assert listener is not None; listener(1); await self.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:6146,concurren,concurrent,6146,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,"lus/kubernetes_asyncio/pull/218"">#218</a>, <a href=""https://github.com/tomplus""><code>@tomplus</code></a>)</li>; </ul>; <h1>v24.2.1</h1>; <ul>; <li>fixed watch.stream bug of not working with apis with follow kwarg (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/pull/216"">#216</a>, <a href=""https://github.com/mcreng""><code>@mcreng</code></a>)</li>; </ul>; <h1>v24.2.0</h1>; <p>Kubernetes API Version: v1.24.2</p>; <h3>API Change</h3>; <ul>; <li>Add 2 new options for kube-proxy running in winkernel mode. <code>--forward-healthcheck-vip</code>, if specified as true, health check traffic whose destination is service VIP will be forwarded to kube-proxy's healthcheck service. <code>--root-hnsendpoint-name</code> specifies the name of the hns endpoint for the root network namespace. This option enables the pass-through load balancers like Google's GCLB to correctly health check the backend services. Without this change, the health check packets is dropped, and Windows node will be considered to be unhealthy by those load balancers. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/99287"">kubernetes/kubernetes#99287</a>, <a href=""https://github.com/anfernee""><code>@anfernee</code></a>)</li>; <li>Added CEL runtime cost calculation into CustomerResource validation. CustomerResource validation will fail if runtime cost exceeds the budget. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108482"">kubernetes/kubernetes#108482</a>, <a href=""https://github.com/cici37""><code>@cici37</code></a>)</li>; <li>Added a new metric <code>webhook_fail_open_count</code> to monitor webhooks that fail to open. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/107171"">kubernetes/kubernetes#107171</a>, <a href=""https://github.com/ltagliamonte-dd""><code>@ltagliamonte-dd</code></a>)</li>; <li>Adds a new Status subresource in Network Policy objects (<a href=""https://github-redirect.dependabot.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12196:1473,load,load,1473,https://hail.is,https://github.com/hail-is/hail/pull/12196,1,['load'],['load']
Performance,"m Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826:1125,load,load,1125,https://hail.is,https://github.com/hail-is/hail/issues/7826,1,['load'],['load']
Performance,"ma, url, listener); /usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py:409: in rmtree; await rm_dir(pool, contents_tasks_by_dir.get(path, []), path); /usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:463: in __aexit__; raise self._exception; /usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:402: in run_and_cleanup; retval = await f(*args, **kwargs); /usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py:367: in rm_file; await self.remove(path); /usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py:348: in remove; return await blocking_to_async(self._thread_pool, os.remove, path); /usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:162: in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; /usr/lib/python3.9/asyncio/base_events.py:819: in run_in_executor; executor.submit(func, *args), loop=self); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <concurrent.futures.thread.ThreadPoolExecutor object at 0x7f263d862100>; fn = <function blocking_to_async.<locals>.<lambda> at 0x7f263d781040>, args = (); kwargs = {}. def submit(self, fn, /, *args, **kwargs):; > with self._shutdown_lock, _global_shutdown_lock:; E Failed: Timeout >600.0s. /usr/lib/python3.9/concurrent/futures/thread.py:162: Failed; ---------------------------- Captured log teardown -----------------------------; INFO hailtop.utils:utils.py:450 discarding exception; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 378, in rm_dir; await self.rmdir(path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 352, in rmdir; return await blocking_to_async(self._thread_pool, os.rmdir, path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 162, in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; File ""/usr/lib/python3.9/asyncio/futu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:11791,concurren,concurrent,11791,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,make array elements required for INFO and FORMAT signatures in LoadVCF,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2417:63,Load,LoadVCF,63,https://hail.is,https://github.com/hail-is/hail/pull/2417,1,['Load'],['LoadVCF']
Performance,"mance</h3>; <ul>; <li>Speed-up the new backtracking parser about 4X in general (enabled when <code>--target-version</code> is set to 3.10 and higher). (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2728"">#2728</a>)</li>; <li>Black is now compiled with mypyc for an overall 2x speed-up. 64-bit Windows, MacOS, and Linux (not including musl) are supported. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/1009"">#1009</a>, <a href=""https://github-redirect.dependabot.com/psf/black/issues/2431"">#2431</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Do not accept bare carriage return line endings in pyproject.toml (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2408"">#2408</a>)</li>; <li>Add configuration option (<code>python-cell-magics</code>) to format cells with custom magics in Jupyter Notebooks (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2744"">#2744</a>)</li>; <li>Allow setting custom cache directory on all platforms with environment variable <code>BLACK_CACHE_DIR</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2739"">#2739</a>).</li>; <li>Enable Python 3.10+ by default, without any extra need to specify -<code>-target-version=py310</code>. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2758"">#2758</a>)</li>; <li>Make passing <code>SRC</code> or <code>--code</code> mandatory and mutually exclusive (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2804"">#2804</a>)</li>; </ul>; <h3>Output</h3>; <ul>; <li>Improve error message for invalid regular expression (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2678"">#2678</a>)</li>; <li>Improve error message when parsing fails during AST safety check by embedding the underlying SyntaxError (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2693"">#2693</a>)</li>; <li>No longer color diff headers white as it's unreadable in light themed terminals (<a href=""https:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11468:4956,cache,cache,4956,https://hail.is,https://github.com/hail-is/hail/pull/11468,1,['cache'],['cache']
Performance,mand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.71-f3a54b530979; Error summary: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:15426,concurren,concurrent,15426,https://hail.is,https://github.com/hail-is/hail/issues/10682,2,['concurren'],['concurrent']
Performance,"md64 <code>musllinux_1_1</code> wheels to PyPI.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix build requiring <code>python</code> on <code>PATH</code>.</li>; </ul>; <h2>3.6.6</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing <code>datetime.datetime</code> using <code>tzinfo</code> that; are <code>zoneinfo.ZoneInfo</code>.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix invalid indexing in line and column number reporting in; <code>JSONDecodeError</code>.</li>; <li>Fix <code>orjson.OPT_STRICT_INTEGER</code> not raising an error on; values exceeding a 64-bit integer maximum.</li>; </ul>; <h2>3.6.5</h2>; <h3>Fixed</h3>; <ul>; <li>Fix build on macOS aarch64 CPython 3.10.</li>; <li>Fix build issue on 32-bit.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.6.7 - 2022-02-14</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of deserializing almost-empty documents.</li>; <li>Publish arm7l <code>manylinux_2_17</code> wheels to PyPI.</li>; <li>Publish amd4 <code>musllinux_1_1</code> wheels to PyPI.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix build requiring <code>python</code> on <code>PATH</code>.</li>; </ul>; <h2>3.6.6 - 2022-01-21</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing <code>datetime.datetime</code> using <code>tzinfo</code> that; are <code>zoneinfo.ZoneInfo</code>.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix invalid indexing in line and column number reporting in; <code>JSONDecodeError</code>.</li>; <li>Fix <code>orjson.OPT_STRICT_INTEGER</code> not raising an error on; values exceeding a 64-bit integer maximum.</li>; </ul>; <h2>3.6.5 - 2021-12-05</h2>; <h3>Fixed</h3>; <ul>; <li>Fix build on macOS aarch64 CPython 3.10.</li>; <li>Fix build issue on 32-bit.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://git",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11572:1424,perform,performance,1424,https://hail.is,https://github.com/hail-is/hail/pull/11572,1,['perform'],['performance']
Performance,"me.datetime</code> using <code>tzinfo</code> that; are <code>zoneinfo.ZoneInfo</code>.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix invalid indexing in line and column number reporting in; <code>JSONDecodeError</code>.</li>; <li>Fix <code>orjson.OPT_STRICT_INTEGER</code> not raising an error on; values exceeding a 64-bit integer maximum.</li>; </ul>; <h2>3.6.5 - 2021-12-05</h2>; <h3>Fixed</h3>; <ul>; <li>Fix build on macOS aarch64 CPython 3.10.</li>; <li>Fix build issue on 32-bit.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ijl/orjson/commit/aee8a9fed45f84d227cf2cb7102656aa65a4890a""><code>aee8a9f</code></a> 3.6.7</li>; <li><a href=""https://github.com/ijl/orjson/commit/622cd7b1167262ffe458f6a2c15ec239f015d174""><code>622cd7b</code></a> Add special casing for deserializing empty objects, lists and strings</li>; <li><a href=""https://github.com/ijl/orjson/commit/5da14a00fed93dc55a5e01e4eba0e3d77b0a89fc""><code>5da14a0</code></a> Add benchmark for loading empty objects</li>; <li><a href=""https://github.com/ijl/orjson/commit/12b867c7bfbd9c6404b2f2e859c134822af05e73""><code>12b867c</code></a> cargo update, nightly-2022-02-13</li>; <li><a href=""https://github.com/ijl/orjson/commit/ab633b6d0fa064b0c4b248bee8dc1062f0fe9d32""><code>ab633b6</code></a> Build x86_64 musllinux wheels (<a href=""https://github-redirect.dependabot.com/ijl/orjson/issues/242"">#242</a>)</li>; <li><a href=""https://github.com/ijl/orjson/commit/8bf078b27e7479f2cfbea1bac7155d4449ce7e30""><code>8bf078b</code></a> Cross compile wheels for armv7l on GitHub Actions (<a href=""https://github-redirect.dependabot.com/ijl/orjson/issues/241"">#241</a>)</li>; <li><a href=""https://github.com/ijl/orjson/commit/c196f0e55bd51d3693d381ccc06f2fd4b5443d86""><code>c196f0e</code></a> 3.6.6</li>; <li><a href=""https://github.com/ijl/orjson/commit/81890b097f7a479d1c1e697d21467952e0be24a9""><code>81890b0</code></a> Fix 53-bit error on value between isize and usize</li>;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11572:2855,load,loading,2855,https://hail.is,https://github.com/hail-is/hail/pull/11572,1,['load'],['loading']
Performance,"ments, like <code>case Foo(bar=baz as quux)</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2749"">#2749</a>)</li>; <li>Tuple unpacking on <code>return</code> and <code>yield</code> constructs now implies 3.8+ (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2700"">#2700</a>)</li>; <li>Unparenthesized tuples on annotated assignments (e.g <code>values: Tuple[int, ...] = 1, 2, 3</code>) now implies 3.8+ (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2708"">#2708</a>)</li>; <li>Fix handling of standalone <code>match()</code> or <code>case()</code> when there is a trailing newline or a comment inside of the parentheses. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2760"">#2760</a>)</li>; <li><code>from __future__ import annotations</code> statement now implies Python 3.7+ (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2690"">#2690</a>)</li>; </ul>; <h3>Performance</h3>; <ul>; <li>Speed-up the new backtracking parser about 4X in general (enabled when <code>--target-version</code> is set to 3.10 and higher). (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2728"">#2728</a>)</li>; <li>Black is now compiled with mypyc for an overall 2x speed-up. 64-bit Windows, MacOS, and Linux (not including musl) are supported. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/1009"">#1009</a>, <a href=""https://github-redirect.dependabot.com/psf/black/issues/2431"">#2431</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Do not accept bare carriage return line endings in pyproject.toml (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2408"">#2408</a>)</li>; <li>Add configuration option (<code>python-cell-magics</code>) to format cells with custom magics in Jupyter Notebooks (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2744"">#2744</a>)</li>; <li>Allow setting custom cache directory on all platforms with environment variable <cod",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11468:3976,Perform,Performance,3976,https://hail.is,https://github.com/hail-is/hail/pull/11468,1,['Perform'],['Performance']
Performance,"mer$.time(ExecutionTimer.scala:52); at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$parseInputToCommandThunk$3(ServiceBackend.scala:650); at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:822); at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:447); at is.hail.backend.service.Main$.main(Main.scala:15); at is.hail.backend.service.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```; </details>. The code is simple and clearly is running against a path that does not already exist:; ```; if not hl.hadoop_exists(get_aou_util_path('mt_sample_qc')):; print('Run sample qc MT.....'); mt = hl.read_matrix_table(ACAF_MT_PATH); mt = mt.filter_rows(mt.locus.in_autosome()); # mt = mt.filter_rows(mt.locus.contig == 'chr1'); ht = hl.sample_qc(mt, name='mt_sample_qc'); ht.write(get_aou_util_path('mt_sample_qc'), overwrite=args.overwrite); ```. Job log: https://batch.hail.is/batches/8058522/jobs/171029. <detail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:6625,concurren,concurrent,6625,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['concurren'],['concurrent']
Performance,minor decode optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3415:13,optimiz,optimization,13,https://hail.is,https://github.com/hail-is/hail/pull/3415,1,['optimiz'],['optimization']
Performance,minor import_bgen optimizations,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3529:18,optimiz,optimizations,18,https://hail.is,https://github.com/hail-is/hail/pull/3529,1,['optimiz'],['optimizations']
Performance,"missed during testing: there is a brief interval where z-height of placeholder navbar is smaller than the sphinx side nav element, causing a flash from blue to white after load.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8546:172,load,load,172,https://hail.is,https://github.com/hail-is/hail/pull/8546,1,['load'],['load']
Performance,"mitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[28] at mapPartitions at ContextRDD.scala:137) (first 15 tasks are for partitions Vector(0)); 2018-10-09 15:04:38 TaskSchedulerImpl: INFO: Adding task set 5.0 with 1 tasks; 2018-10-09 15:04:38 TaskSetManager: INFO: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4777 bytes); 2018-10-09 15:04:38 Executor: INFO: Running task 0.0 in stage 5.0 (TID 5); 2018-10-09 15:04:38 BlockManager: INFO: Found block rdd_9_0 locally; 2018-10-09 15:04:38 CodeGenerator: INFO: Code generated in 14.135243 ms; 2018-10-09 15:04:38 CodeGenerator: INFO: Code generated in 8.306294 ms; 2018-10-09 15:04:38 Executor: INFO: Finished task 0.0 in stage 5.0 (TID 5). 1119 bytes result sent to driver; ```; </details>. <details>; <summary>Broken hail.log</summary>. ```; 2018-10-09 14:46:38 Hail: INFO: SparkUI: http://10.32.119.167:4040; 2018-10-09 14:46:38 Hail: INFO: Running Hail version devel-e7552fd55a9d; 2018-10-09 14:46:38 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 14:46:38 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 14:46:38 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@28f0ac7{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@49a30f89{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4495af6e{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6baf9f3b{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:31112,load,loading,31112,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['load'],['loading']
Performance,"mize -- FoldConstants : 5.811ms, total 29.474ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:1280,Optimiz,Optimize,1280,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,"mize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:1715,Optimiz,Optimize,1715,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,"mize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEv",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:2014,Optimiz,Optimize,2014,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,move load/store methods onto Region object,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6644:5,load,load,5,https://hail.is,https://github.com/hail-is/hail/pull/6644,1,['load'],['load']
Performance,"moved bootstrap dependencies. Did we ever actually use these?; - Removed ""clipboard.js"" dependency. Also not clear from where this came.; - Removed use of the `subtitle` tag, which isn't actually an HTML tag?. Future work:. - Simplify our CSS. It's not possible to logically reason about our CSS. And it; interacts in bad ways with the latent RTD themes. I want a unified Hail visual; theme.; - Clean up the search-related JavaScript in nav-bottom.html and; search.html. These both seem too complicated to just make search work. ---. The thrust of this PR is to restructure Hail's website and documentation to; entirely rely on Jinja2 templates. Previously, we used a mix of Jinja2, XSLT,; and in-browser JavaScript DOM manipulation to piece together a web page. Now, all of Hail's non-service website derives from; `site/templates/base.html`. It is a Jinja2 template with four blocks: title,; meta_description, head, and content. It ensures that:; - Hail's CSS is loaded,; - the Hail icon is set,; - the fonts are loaded,; - the source code highlighter is loaded (prism.js, only used outside the docs); - the nav bar is present and configured.; The nav bar is somewhat complicated. There are two pieces. `nav-top.html`; contains the nav bar HTML elements. `nav-bottom.html` contains the JavaScript; code that hooks the search bar up to Algolia and sets the active page in the; navigation. I believe JavaScript which modifies the HTML DOM is traditionally; placed at the bottom of the `body` tag so that it is executed *after* the HTML; DOM is mostly rendered. That's why the navigation/search bar is split across two; files. I also load the `prism.js` source code highlighter at the end of the; body. All of the non-docs pages are defined by html files in `pages`. Each one of; these is a Jinja2 template which derives from the base template. `make render`; converts every template in `pages` into a real HTML file in `www`. Check out; 404.html for a simple example. Once I had the site in working or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9597:1783,load,loaded,1783,https://hail.is,https://github.com/hail-is/hail/pull/9597,3,['load'],['loaded']
Performance,"mpatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h2>3.9.11 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14357:1753,load,loads,1753,https://hail.is,https://github.com/hail-is/hail/pull/14357,3,['load'],['loads']
Performance,"mplates. Previously, we used a mix of Jinja2, XSLT,; and in-browser JavaScript DOM manipulation to piece together a web page. Now, all of Hail's non-service website derives from; `site/templates/base.html`. It is a Jinja2 template with four blocks: title,; meta_description, head, and content. It ensures that:; - Hail's CSS is loaded,; - the Hail icon is set,; - the fonts are loaded,; - the source code highlighter is loaded (prism.js, only used outside the docs); - the nav bar is present and configured.; The nav bar is somewhat complicated. There are two pieces. `nav-top.html`; contains the nav bar HTML elements. `nav-bottom.html` contains the JavaScript; code that hooks the search bar up to Algolia and sets the active page in the; navigation. I believe JavaScript which modifies the HTML DOM is traditionally; placed at the bottom of the `body` tag so that it is executed *after* the HTML; DOM is mostly rendered. That's why the navigation/search bar is split across two; files. I also load the `prism.js` source code highlighter at the end of the; body. All of the non-docs pages are defined by html files in `pages`. Each one of; these is a Jinja2 template which derives from the base template. `make render`; converts every template in `pages` into a real HTML file in `www`. Check out; 404.html for a simple example. Once I had the site in working order, I turned my eyes to the docs. I converted; `docs/_templates/layout.html`, the base template for our docs, into a template; which derives from `site/templates/base.html`. That ensures everyone is using; the same CSS, the same navigation/search bar, same icon set, etc. Convincing; Sphinx to work like this was actually really easy because Sphinx already uses; Jinja2 templates! I just added site's templates folder to the Sphinx; `templates_path`. I eliminated a few conditionals that are only relevant if your docs are also; rendered on RTD's server, which ours are not. Finally, in order to experiment quickly with this, I changed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9597:2451,load,load,2451,https://hail.is,https://github.com/hail-is/hail/pull/9597,1,['load'],['load']
Performance,"mpty, return an empty list.</li>; </ul>; <h1>v4.11.0</h1>; <ul>; <li>bpo-46246: Added <code>__slots__</code> to <code>EntryPoints</code>.</li>; </ul>; <h1>v4.10.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/365"">#365</a> and bpo-46546: Avoid leaking <code>method_name</code> in; <code>DeprecatedList</code>.</li>; </ul>; <h1>v4.10.1</h1>; <h1>v2.1.3</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/361"">#361</a>: Avoid potential REDoS in <code>EntryPoint.pattern</code>.</li>; </ul>; <h1>v4.10.0</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/354"">#354</a>: Removed <code>Distribution._local</code> factory. This; functionality was created as a demonstration of the; possible implementation. Now, the; <code>pep517 &lt;https://pypi.org/project/pep517&gt;</code>_ package; provides this functionality directly through; <code>pep517.meta.load &lt;https://github.com/pypa/pep517/blob/a942316305395f8f757f210e2b16f738af73f8b8/pep517/meta.py#L63-L73&gt;</code>_.</li>; </ul>; <h1>v4.9.0</h1>; <ul>; <li>Require Python 3.7 or later.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python/importlib_metadata/commit/99a2ec4489da45407d8224be2804ff323a164ac0""><code>99a2ec4</code></a> Update changelog.</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/dbe114cbdc49ff42026974e48ca7178a091e7530""><code>dbe114c</code></a> Add docstring with tests for EntryPoint.matches. Ref <a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/373"">#373</a>.</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/ee566d048c0061b4f846f100ebfd93eefbcbf608""><code>ee566d0</code></a> Remove cast of path items to strings. Ref <a href=""https://github-redirect.dependabot.com/python/importlib_met",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11596:1952,load,load,1952,https://hail.is,https://github.com/hail-is/hail/pull/11596,1,['load'],['load']
Performance,"mpty, return an empty list.</li>; </ul>; <h1>v4.11.0</h1>; <ul>; <li>bpo-46246: Added <code>__slots__</code> to <code>EntryPoints</code>.</li>; </ul>; <h1>v4.10.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/365"">#365</a> and bpo-46546: Avoid leaking <code>method_name</code> in; <code>DeprecatedList</code>.</li>; </ul>; <h1>v4.10.1</h1>; <h1>v2.1.3</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/361"">#361</a>: Avoid potential REDoS in <code>EntryPoint.pattern</code>.</li>; </ul>; <h1>v4.10.0</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/354"">#354</a>: Removed <code>Distribution._local</code> factory. This; functionality was created as a demonstration of the; possible implementation. Now, the; <code>pep517 &lt;https://pypi.org/project/pep517&gt;</code>_ package; provides this functionality directly through; <code>pep517.meta.load &lt;https://github.com/pypa/pep517/blob/a942316305395f8f757f210e2b16f738af73f8b8/pep517/meta.py#L63-L73&gt;</code>_.</li>; </ul>; <h1>v4.9.0</h1>; <ul>; <li>Require Python 3.7 or later.</li>; </ul>; <h1>v4.8.3</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/357"">#357</a>: Fixed requirement generation from egg-info when a</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python/importlib_metadata/commit/14cce75299645467adcd17352cb07caada32c444""><code>14cce75</code></a> Prefer re.findall, which returns materialized results. Fixes <a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/369"">#369</a>.</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/b4661fd8988b4101d4042e4cc4a8ed74423ec410""><code>b4661fd</code></a> Add test capturing missed expectation on extras. Ref <a href=""https://gi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11525:1763,load,load,1763,https://hail.is,https://github.com/hail-is/hail/pull/11525,1,['load'],['load']
Performance,mt.filter_entries(...).entries() and mt.entries().filter(...) should have equivalent performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6905:85,perform,performance,85,https://hail.is,https://github.com/hail-is/hail/issues/6905,1,['perform'],['performance']
Performance,"multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130:2185,load,loading,2185,https://hail.is,https://github.com/hail-is/hail/pull/5130,1,['load'],['loading']
Performance,mysterious latency in service after change to networking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8047:11,latency,latency,11,https://hail.is,https://github.com/hail-is/hail/issues/8047,1,['latency'],['latency']
Performance,"n failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); ```. For either of these database updates to succeed, the thread of control must have; thought the `_state` was `Cancelled` or we moved through some intermediate; state. We continue under the assumption that we went directly to `Running`. Who calls `_create_pod`?. - `start_pod`, but it checks that the state is in `Ready`; - `mark_complete`, but that's only if there's a ""next task"", this job has only; one task. That leaves `create_if_ready` and `mark_unscheduled`. `create_if_ready` is only; called by methods that are triggered when a parent with children finishes. We; have no parent-child relationships here. By process of elimination, `mark_unscheduled` must be the culprit. But how?; `mark_unscheduled` is called when a pod is evicted or by the k8s update loop if; there exists no pod. In those cases a message a special log is printed. That log; appears later (because the pod is missing) but it does not appear during the; initial sequence of events. Let's set that aside and focus on the other path by; which `mark_unscheduled` is called: `mark_complete` when the pod log cannot be; retrieved. Proposed sequence of events:. - pod is created; - k8s sends an event that the pod is terminated (but without timing information); - we load a Job object from the db. the job is Pending, the batch is cancelled; - we enter `update_job_with_pod` then `mark_complete`; - we fail to retrieve the logs; - we try to mark unscheduled, but the batch is cancelled so we do nothing.; ...; - we realize a Pending pod was never created, we try to mark_unscheduled, but; the batch is cancelled, so we do nothing; ...; - we realize a Pending pod was never created, we try ... ad infinitum",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6617:19529,load,load,19529,https://hail.is,https://github.com/hail-is/hail/issues/6617,1,['load'],['load']
Performance,"n$apply$2$$anonfun$apply$3.apply(RowStore.scala:767); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:766); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:766); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:763); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:763); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-abac611; Error summary: NumberFormatException: For input string: ""-66.2667,0,-25.4754""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:16011,concurren,concurrent,16011,https://hail.is,https://github.com/hail-is/hail/issues/3361,2,['concurren'],['concurrent']
Performance,"n(Main.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 4, localhost): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1260:7142,concurren,concurrent,7142,https://hail.is,https://github.com/hail-is/hail/issues/1260,1,['concurren'],['concurrent']
Performance,"n, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][""loadings""],; pca_af=pcloadings[sibs.row_key][""pca_af""]; ). mt = mt.filter_rows(hl.is_defined(mt.pca_loadings) & hl.is_defined(mt.pca_af) &; (mt.pca_af > 0) & (mt.pca_af < 1)). gt_norm = (mt.GT.n_alt_alleles() - 2 * mt.pca_af) / hl.sqrt(n_variants * 2 * mt.pca_af * (1 - mt.pca_af)). mt = mt.annotate_cols(scores=hl.agg.array_sum(mt.pca_loadings * gt_norm)). related_scores = mt.cols().select('scores'); ```. ### What went wrong (all error messages here, including the full java stack trace):; No error messages, but my pipeline craps out at the following lines:; ```; pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(); ```. `.count()` works instantaneously up until that `pcloadings.annotate()` line. After that line, it gets stuck at 0 of 1 tasks for ~20 minutes before I give up and cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3953:3941,load,loadings,3941,https://hail.is,https://github.com/hail-is/hail/issues/3953,1,['load'],['loadings']
Performance,"n-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3297,cache,cached,3297,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"n.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-544bf8f; Error summary: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:15941,concurren,concurrent,15941,https://hail.is,https://github.com/hail-is/hail/issues/3379,2,['concurren'],['concurrent']
Performance,"n.scala) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 		at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 		at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; 	Caused by: com.google.api.client.http.HttpResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""domain"": ""global"",; ""reason"": ""forbidden""; }; ]; }; }. 		at com.google.api.client.http.HttpResponseException$Builder.build(HttpResponseExcep",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:25529,concurren,concurrent,25529,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"nPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:12.481 GoogleStorageFS$: INFO: createNoCompression: gs://neale-bge/foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index; 2023-09-22 19:11:12.486 : INFO: RegionPool: REPORT_THRESHOLD: 257.0K allocated (129.0K blocks / 128.0K chunks), regions.size = 3, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.486 : INFO: RegionPool: REPORT_THRESHOLD: 577.0K allocated (193.0K blocks / 384.0K chunks), regions.size = 4, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.486 GoogleStorageFS$: INFO: createNoCompression: gs://neale-bge/foo.ht/rows/parts/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79; 2023-09-22 19:11:12.625 GoogleStorageFS$: INFO: close: gs://neale-bge/foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index; 2023-09-22 19:11:12.656 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=656384, peakBytesReadable=641.00 KiB, chunks requested=4, cache hits=2; 2023-09-22 19:11:12.656 : INFO: RegionPool: FREE: 641.0K allocated (257.0K blocks / 384.0K chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.656 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at ja",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:9000,cache,cache,9000,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['cache'],['cache']
Performance,nally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9677,Load,LoadVCF,9677,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,native_writer.apply(Unknown Source); 	at __C1310collect_distributed_array_table_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSet,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:7947,concurren,concurrent,7947,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['concurren'],['concurrent']
Performance,"native_writer.apply(Unknown Source); 	at __C1310collect_distributed_array_table_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.130-bea04d9c79b5; Error summary: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splic",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:19537,concurren,concurrent,19537,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['concurren'],['concurrent']
Performance,"nc, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/methods/impex.py in read_matrix_table(path, _intervals, _filter_intervals, _drop_cols, _drop_rows, _n_partitions); 2009 :class:`.MatrixTable`; 2010 """"""; -> 2011 for rg_config in Env.backend().load_references_from_dataset(path):; 2012 hl.ReferenceGenome._from_config(rg_config); 2013 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/spark_backend.py in load_references_from_dataset(self, path); 321 ; 322 def load_references_from_dataset(self, path):; --> 323 return json.loads(Env.hail().variant.ReferenceGenome.fromHailDataset(self.fs._jfs, path)); 324 ; 325 def from_fasta_file(self, name, fasta_file, index_file, x_contigs, y_contigs, mt_contigs, par):. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/py4j/java_gateway.py in __call__(self, *args); 1302 ; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: UnsupportedFileSystemException: No FileSystem for scheme ""gs"". Java stack trace:; org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10530:2362,load,loads,2362,https://hail.is,https://github.com/hail-is/hail/issues/10530,1,['load'],['loads']
Performance,ncurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:2556,Load,LoadVCF,2556,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,"nd': ['/bin/bash',; '-c',; 'set -e; mkdir -p /io/pipeline/pipeline-f0c3c92aa1c4/__TASK__0/; true'],; 'image': 'gcr.io/hail-vdc/benchmark_tpoterba:latest',; 'job_id': 1,; 'mount_docker_socket': False,; 'resources': {'cpu': '1', 'memory': '7G'},; 'pvc_size': '100G',; 'secrets': [{'namespace': 'batch-pods',; 'name': 'dking-gsa-key',; 'mount_path': '/gsa-key',; 'mount_in_copy': True}],; 'env': []},; 'attributes': {'task_uid': '__TASK__0', 'name': 'replicate_0'},; 'status': {'worker': 'batch-worker-default-5t5e9',; 'batch_id': 767,; 'job_id': 1,; 'attempt_id': 'be692b',; 'user': 'dking',; 'state': 'succeeded',; 'container_statuses': {'main': {'name': 'main',; 'state': 'succeeded',; 'timing': {'pulling': {'start_time': 1576710190946,; 'finish_time': 1576710248882,; 'duration': 57936},; 'creating': {'start_time': 1576710248882,; 'finish_time': 1576710248963,; 'duration': 81},; 'runtime': {'start_time': 1576710248963,; 'finish_time': 1576710250461,; 'duration': 1498},; 'starting': {'start_time': 1576710248963,; 'finish_time': 1576710249898,; 'duration': 935},; 'running': {'start_time': 1576710249898,; 'finish_time': 1576710250461,; 'duration': 563},; 'uploading_log': {'start_time': 1576710250464,; 'finish_time': 1576710250742,; 'duration': 278},; 'deleting': {'start_time': 1576710250743,; 'finish_time': 1576710250776,; 'duration': 33}},; 'container_status': {'state': 'exited',; 'started_at': '2019-12-18T23:04:09.890460985Z',; 'finished_at': '2019-12-18T23:04:10.111873413Z',; 'out_of_memory': False,; 'exit_code': 0}}},; 'start_time': 1576710248963,; 'end_time': 1576710250461},; 'msec_mcpu': 2796766,; 'cost': '$0.0000'}; ```. I'd like to be able to load this list as a struct with one command. One would think this arcane magic would do it:; ```; In [16]: t = hl.Table.parallelize([hl.struct(**x) for x in jobs]) ; ```; but of course, nested fields. Probably some partial specification of the type will be necessary, but I would like to avoid specifying the whole thing if possible.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7778:1780,load,load,1780,https://hail.is,https://github.com/hail-is/hail/issues/7778,1,['load'],['load']
Performance,"nd.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.115-71fc978b5c22; Error summary: SocketException: Connection reset. -------------------. Some more content from the failing worker job:. ...; 2023-05-04 01:04:35.959 : INFO: executing D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e61c-4fa9-9419-2278528f3c86; 2023-05-04 01:04:37.559 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=132096, peakBytesReadable=129.00 KiB, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.560 : INF",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:21733,concurren,concurrent,21733,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['concurren'],['concurrent']
Performance,nd.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:464); 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:237); 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190); 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109); 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1400); 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1368); 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73); 	at sun.security.ssl.SSLSocketImp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12982:14019,concurren,concurrent,14019,https://hail.is,https://github.com/hail-is/hail/issues/12982,3,['concurren'],['concurrent']
Performance,"nd_hail_fs.py::test_hadoop_methods_3[local] PASSED; +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_1 (139802083059456) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_0 (139802091452160) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-0_0 (139802205742848) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~~~~~~~~ Stack of asyncio_0 (139802248206080) ~~~~~~~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = wo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:1490,concurren,concurrent,1490,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,"ndencies &amp; tests</li>; <li>fix site deployment</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.3 stable</h2>; <ul>; <li>fix minor typo (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>minor example fix (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>misc tidying &amp; refactoring</li>; <li>misc build/dev framework updates; <ul>; <li>update dependencies</li>; <li>update linters</li>; <li>update docs deployment branches</li>; </ul>; </li>; <li>misc test/ci updates; <ul>; <li>test forks</li>; <li>tidy OS &amp; Python version tests</li>; <li>bump primary python version 3.7 =&gt; 3.8</li>; <li>beta py3.10 testing</li>; <li>fix py2.7 tests</li>; <li>better timeout handling</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.2 stable</h2>; <ul>; <li>fix notebook memory leak (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1216"">#1216</a>)</li>; <li>fix <code>contrib.concurrent</code> with generators (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1233"">#1233</a> &lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1231"">#1231</a>)</li>; </ul>; <h2>tqdm v4.62.1 stable</h2>; <ul>; <li><code>contrib.logging</code>: inherit existing handler output stream (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1191"">#1191</a>)</li>; <li>fix <code>PermissionError</code> by using <code>weakref</code> in <code>DisableOnWriteError</code> (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1207"">#1207</a>)</li>; <li>fix <code>contrib.telegram</code> creation rate limit handling (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1223"">#1223</a>, <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1221"">#1221</a> &lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1220"">#1220</a>, <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1076"">#1076</a>)</li>; <li>tests: fix py27 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11587:2097,concurren,concurrent,2097,https://hail.is,https://github.com/hail-is/hail/pull/11587,1,['concurren'],['concurrent']
Performance,"ndlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 YarnScheduler: ERROR: Lost executor 1 on bw2-sw-dp3j.c.seqr-project.internal: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1574.1 in stage 8.0 (TID 21699, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1559.1 in stage 8.0 (TID 21702, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1561.1 in stage 8.0 (TID 21701, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exite",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:3670,concurren,concurrent,3670,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['concurren'],['concurrent']
Performance,"ne: 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20) 	at is.hail.utils.package$.fatal(package.scala:26) 	at is.hail.utils.Context.wrapException(Context.scala:19) 	at is.hail.utils.WithContext.foreach(Context.scala:51) 	at is.hail.utils.TextTableReader$$anonfun$5$$anonfun$apply$2.apply(TextTableReader.scala:126) 	at is.hail.utils.TextTableReader$$anonfun$5$$anonfun$apply$2.apply(TextTableReader.scala:126) 	at scala.collection.Iterator$class.foreach(Iterator.scala:893) 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) 	at is.hail.utils.TextTableReader$$anonfun$5.apply(TextTableReader.scala:126) 	at is.hail.utils.TextTableReader$$anonfun$5.apply(TextTableReader.scala:122) 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797) 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:108) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) Caused by: is.hail.utils.HailException: expected 13 fields, but found 1 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9) 	at is.hail.utils.package$.fatal(package.scala:26) 	at is.hail.utils.TextTableReader$$anonfun$5$$anonfun$apply$2$$anonfun$apply$3.apply(TextTableReader.scala:129) 	at is.hail.utils.TextTableReader$$anonfun$5$$anonfun$apply$2$$anonfun$apply$3.apply(TextTableReader.scala:126) 	at is.hail.utils.WithContext.foreach(Context.scala:49) 	... 17 more; --. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4100:1783,concurren,concurrent,1783,https://hail.is,https://github.com/hail-is/hail/issues/4100,2,['concurren'],['concurrent']
Performance,nfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2803:3055,concurren,concurrent,3055,https://hail.is,https://github.com/hail-is/hail/issues/2803,2,['concurren'],['concurrent']
Performance,"ng newest version of hail:; ```; tgp = hl.import_vcf('gs://genomics-public-data/1000-genomes-phase-3/vcf-20150220/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf'); tgp.describe(); tgp.rows().show(); ```; Getting:; ```; hail.utils.java.FatalError: NoSuchElementException: key not found: GT. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 104, pca-w-1.c.daly-ibd.internal, executor 2): is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.for",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:1013,Load,LoadVCF,1013,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,"ng server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this better; 3. Performance. NodeJS is faster than Flask, React is ~fastest JS view layer. Next makes it really easy to split app into page bundles, and (on localhost) achieves DOMContentLoaded of ~70-100ms, and faster interactivity: first loaded page (the page of the current route) is ~6-10ms.; * [Techempower]: https://www.techempower.com/benchmarks/; * [Node vs , ](https://medium.com/@mihaigeorge.c/web-rest-api-benchmark-on-a-real-life-application-ebb743a5d7a3). * React vs other client side micro bench (pay attention to ""Non-keyed""): https://krausest.github.io/js-framework-benchmark/current.html; 4. Structure, aforementioned; 5. Path to relatively performant desktop and mobile applications, via [Electron](https://getstream.io/blog/takeaways-on-building-a-react-based-app-with-electron/). [Visual Studio Code](https://github.com/Microsoft/vscode) and [Slack](https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview-2759690d9c7b) are good examples. Facebook Messenger written in React Native, which we have an even more straightforward path to.; 6. Low cognitive cost (relative to Angular, others. React is just a view layer, and has a tiny API. I've deve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:3567,load,loaded,3567,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['load'],['loaded']
Performance,"ng/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:3091,Load,LoadMatrix,3091,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"ngMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```. 2. gs://hail-common/gencode_and_production_intervals.merged.hg19.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collecti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683:2629,load,loading,2629,https://hail.is,https://github.com/hail-is/hail/issues/1683,1,['load'],['loading']
Performance,"nginx can use more than one core and at max load + PRs we're maxing out internal-gateway cycles, which add latency, timeouts, retries, and generally degrade the experience in one namespace based on activity in others. Allowing nginx to use more cores (in this case this is up to half our node size) got our system back into its intended state with graceful throttling. I'll admit, other than being half a node size, 4 is a bit arbitrary here. I think our k8s nodes are annoyingly underutilized enough that we shouldn't see issues in practice with letting internal-gateway use cores that are very likely idle.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11876:44,load,load,44,https://hail.is,https://github.com/hail-is/hail/pull/11876,2,"['latency', 'load']","['latency', 'load']"
Performance,"nished task 0.0 in stage 4.0 (TID 4) in 7 ms on localhost (executor driver) (1/1); 2018-10-09 15:04:37 TaskSchedulerImpl: INFO: Removed TaskSet 4.0, whose tasks have all completed, from pool ; 2018-10-09 15:04:37 DAGScheduler: INFO: ResultStage 4 (collect at utils.scala:197) finished in 0.008 s; 2018-10-09 15:04:37 DAGScheduler: INFO: Job 2 finished: collect at utils.scala:197, took 0.051042 s; 2018-10-09 15:04:37 CodeGenerator: INFO: Code generated in 5.011153 ms; 2018-10-09 15:04:37 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:37 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table8508c46074` AS `zzz1`; WHERE (0 = 1); 2018-10-09 15:04:37 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:37 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table8508c46074`; 2018-10-09 15:04:38 root: INFO: optimize: before:; (TableCount; (TableKeyBy () False; (TableLiteral))); 2018-10-09 15:04:38 root: INFO: optimize: after:; (TableCount; (TableLiteral)); 2018-10-09 15:04:38 SparkContext: INFO: Starting job: fold at RVD.scala:361; 2018-10-09 15:04:38 DAGScheduler: INFO: Got job 3 (fold at RVD.scala:361) with 1 output partitions; 2018-10-09 15:04:38 DAGScheduler: INFO: Final stage: ResultStage 5 (fold at RVD.scala:361); 2018-10-09 15:04:38 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 15:04:38 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 15:04:38 DAGScheduler: INFO: Submitting ResultStage 5 (MapPartitionsRDD[28] at mapPartitions at ContextRDD.scala:137), which has no missing parents; 2018-10-09 15:04:38 MemoryStore: INFO: Block broadcast_5 stored as values in memory (estimated size 19.8 KB, free 366.2 MB); 2018-10-09 15:04:38 MemoryStore: INFO: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.1 KB, free 366.2 MB); 2018-10-09 15:04:38 BlockManagerInfo: INFO: Added broadcast_5_piece0 in m",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:28828,optimiz,optimize,28828,https://hail.is,https://github.com/hail-is/hail/issues/4513,2,['optimiz'],['optimize']
Performance,"nly used functions, improvements to F2PY, and better documentation.</p>; <p>The Python versions supported in this release are 3.8-3.10, Python 3.7; has been dropped. Note that 32 bit wheels are only provided for Python; 3.8 and 3.9 on Windows, all other wheels are 64 bits on account of; Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.; All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix; the occasional problems encountered by folks using truly huge arrays.</p>; <h2>Expired deprecations</h2>; <h3>Deprecated numeric style dtype strings have been removed</h3>; <p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,; and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>; <p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19539"">gh-19539</a>)</p>; <h3>Expired deprecations for <code>loads</code>, <code>ndfromtxt</code>, and <code>mafromtxt</code> in npyio</h3>; <p><code>numpy.loads</code> was deprecated in v1.15, with the recommendation that; users use <code>pickle.loads</code> instead. <code>ndfromtxt</code> and <code>mafromtxt</code> were both; deprecated in v1.17 - users should use <code>numpy.genfromtxt</code> instead with; the appropriate value for the <code>usemask</code> parameter.</p>; <p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19615"">gh-19615</a>)</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/numpy/numpy/commit/4adc87dff15a247e417d50f10cc4def8e1c17a03""><code>4adc87d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20685"">#20685</a> from charris/prepare-for-1.22.0-release</li>; <li><a href=""https://github.com/numpy/numpy/commit/fd66547557f57c430d41be2fc0764f74a62e8ccf""><code>fd6",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11939:2340,load,loads,2340,https://hail.is,https://github.com/hail-is/hail/pull/11939,2,['load'],['loads']
Performance,nonfun$apply$11.apply(TableIR.scala:627); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:626); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1264); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1258); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$JoinIterator.next(Iterator.scala:232); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1138); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1371); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5174:2681,concurren,concurrent,2681,https://hail.is,https://github.com/hail-is/hail/issues/5174,2,['concurren'],['concurrent']
Performance,nonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1014); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.RVD$$anonfun$4$$anon$1.hasNext(RVD.scala:226); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1014); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:471); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-f2b0dca9f506; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4114:8971,concurren,concurrent,8971,https://hail.is,https://github.com/hail-is/hail/issues/4114,2,['concurren'],['concurrent']
Performance,nonfun$main$4(ServiceBackend.scala:460); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.115-71fc978b5c22; Error summary: SocketException: Connection reset. -------------------. Some more content from the failing worker job:. ...; 2023-05-04 01:04:35.959 : INFO: executing D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:21517,concurren,concurrent,21517,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['concurren'],['concurrent']
Performance,nonfun$main$4(ServiceBackend.scala:460); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:464); 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:237); 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190); 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109); 	at sun.security.ssl.SSLSocketImpl.dec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12982:13803,concurren,concurrent,13803,https://hail.is,https://github.com/hail-is/hail/issues/12982,3,['concurren'],['concurrent']
Performance,"nsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 YarnScheduler: ERROR: Lost executor 1 on bw2-sw-dp3j.c.seqr-project.internal: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1574.1 in stage 8.0 (TID 21699, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1559.1 in stage 8.0 (TID 21702, bw2-sw-dp3j.c.seqr-project.internal, executor 1): E",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:3405,concurren,concurrent,3405,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['concurren'],['concurrent']
Performance,"nscript is below. (pyve is an alias to create a python virtual env and activate it). ---. ```; snafu$ pyve; + python3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8)  ~/sandbox/hail [master|8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1178,cache,cached,1178,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"nsequence_terms); worst_csq = hl.literal(CSQ_ORDER).find(lambda x: all_csq_terms.contains(x)); return hl.struct(worst_csq=worst_csq, protein_coding=protein_coding, lof=lof, no_lof_flags=no_lof_flags). protein_coding = ht.vep.transcript_consequences.filter(lambda x: x.biotype == 'protein_coding'); return ht.annotate(**hl.case(missing_false=True); .when(hl.len(protein_coding) > 0, get_worst_csq(protein_coding, True)); .when(hl.len(ht.vep.transcript_consequences) > 0, get_worst_csq(ht.vep.transcript_consequences, False)); .when(hl.len(ht.vep.regulatory_feature_consequences) > 0, get_worst_csq(ht.vep.regulatory_feature_consequences, False)); .when(hl.len(ht.vep.motif_feature_consequences) > 0, get_worst_csq(ht.vep.motif_feature_consequences, False)); .default(get_worst_csq(ht.vep.intergenic_consequences, False))); ```; When the `csq_list = hl.cond(hl.is_defined(lof), csq_list.filter(lambda x: x.lof == lof), csq_list)` line triggers, this seems to fail to `find` the consequences entirely:; ```; all_csq_terms = csq_list.flatmap(lambda x: x.consequence_terms); all_csq_terms.show(); 2019-03-09 17:48:20 Hail: INFO: interval filter loaded 1 of 9997 partitions; +---------------+------------+-------------------------------+; | locus | alleles | <expr> |; +---------------+------------+-------------------------------+; | locus<GRCh37> | array<str> | array<str> |; +---------------+------------+-------------------------------+; | 1:55509603 | [""C"",""T""] | [""stop_gained"",""stop_gained""] |; +---------------+------------+-------------------------------+; worst_csq = hl.literal(CSQ_ORDER).find(lambda x: all_csq_terms.contains(x)); worst_csq.show(); 2019-03-09 17:48:32 Hail: INFO: interval filter loaded 1 of 9997 partitions; +---------------+------------+--------+; | locus | alleles | <expr> |; +---------------+------------+--------+; | locus<GRCh37> | array<str> | str |; +---------------+------------+--------+; | 1:55509603 | [""C"",""T""] | NA |; +---------------+------------+--------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5575:1751,load,loaded,1751,https://hail.is,https://github.com/hail-is/hail/issues/5575,2,['load'],['loaded']
Performance,nstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:415); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/120:4152,concurren,concurrent,4152,https://hail.is,https://github.com/hail-is/hail/issues/120,1,['concurren'],['concurrent']
Performance,"nt (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting them from its callers. Tim Poterba: hmmm, you're right. Passing the region on load is not sufficient -- that region needs to be the owning region for the original data. Alex Kotlar: Would we want to associate an instance of a PType with a single region?. Patrick Schultz: I think we have most of the infrastructure needed to have a hail type hol",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826:1507,load,load,1507,https://hail.is,https://github.com/hail-is/hail/issues/7826,1,['load'],['load']
Performance,nt-name haildevtest --container test --name batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json | jq '.' | less; ```. which contained an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: is.hail.backend.service.EndOfInputException; 	at is.hail.backend.service.ServiceBackendSocketAPI2.read(ServiceBackend.scala:497); 	at is.hail.backend.service.ServiceBackendSocketAPI2.readInt(ServiceBackend.scala:510); 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:561); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:4507,concurren,concurrent,4507,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance,"ntained in `body` (including in nested `Group`s) by their `ifFlat` alternative (almost always either "" "" or """"), or, if that would cause the line to exceed `width`; * print `body` normally, as described in the previous paragraph, allowing nested `Group`s to print either flat or normally. This pretty-printer DSL has become fairly standard, with some common enhancements that I don't think we need. It was first described in [A prettier printer](https://homepages.inf.ed.ac.uk/wadler/papers/prettier/prettier.pdf) by Wadler (though my implementation is completely different). This achieves stack safety by `Concat` taking an `Iterable`, so each contained `Doc` can be produced on demand. `render` pulls from these iterators, keeping in memory only things that might print to the current line, but where the format hasn't been decided yet. As soon as the formatting of a group is decided, as much of its body as possible is written to the `java.io.Writer`. A very quick and dirty performance comparison had the new pretty printer about 20% slower. That's paying for both the stack safety and the added smarts. And I think there's still room for optimization if it becomes necessary. Here is a snippet of the IR generated by `test_ld_score_regression`, first on master, then this PR:; ```; (InsertFields; (SelectFields (SNP A1 A2 N Z); (Ref row)); None; (chi_squared; (Apply pow () Float64; (GetField Z; (Ref row)); (ApplyIR toFloat64 () Float64; (I32 2)))); (n; (GetField N; (Ref row))); (ld_score; (GetField L2; (GetField __uid_3; (Ref row)))); (locus; (Apply Locus () Locus(GRCh37); (GetField CHR; (GetField __uid_4; (Ref row))); (GetField BP; (GetField __uid_5; (Ref row))))); (alleles; (MakeArray Array[String]; (GetField A2; (Ref row)); (GetField A1; (Ref row)))); (phenotype; (Str ""50_irnt""))))); (InsertFields; (SelectFields (locus alleles chi_squared n ld_score phenotype); (SelectFields (SNP A1 A2 N Z chi_squared n ld_score locus alleles phenotype); (Ref row))); None)); ```; ```; (InsertFiel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9652:3556,perform,performance,3556,https://hail.is,https://github.com/hail-is/hail/pull/9652,1,['perform'],['performance']
Performance,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:5903,load,load,5903,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['load'],['load']
Performance,ntext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.schedu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5783,Load,LoadMatrix,5783,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"ntig_row_list = pca_rows.collect(); print('finished collecting'); contig_reformed = [(x['contig'], x['file_row_idx']) for x in contig_row_list]; print('reformed'); from collections import defaultdict; contig_row_dict = defaultdict(list); for k, v in contig_reformed:; contig_row_dict[k].append(v); print('dictionary created'). with hl.hadoop_open(contig_row_dict_location, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][""loadings""],; pca_af=pcloadings[sibs.row_key][""pca_af""]; ). mt = mt.filter_rows(hl.is_defined(mt.pca_loadings) & hl.is_defined(mt.pca_af) &; (mt.pca_af > 0) & (mt.pca_af < 1)). gt_norm = (mt.GT.n_alt_alleles() - 2 * mt.pca_af) / hl.sqrt(n_variants * 2 * mt.pca_af * (1 - mt.pca_af)). mt = mt.annotate_cols(scores=hl.agg.array_sum(mt.pca_loadings * gt_norm)). related_scores = mt.cols().select('scores'); ```. ### What went wrong (all error messages here, including the full java stack t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3953:3408,load,load,3408,https://hail.is,https://github.com/hail-is/hail/issues/3953,1,['load'],['load']
Performance,null cachedAltAlleles when updating region,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2485:5,cache,cachedAltAlleles,5,https://hail.is,https://github.com/hail-is/hail/pull/2485,1,['cache'],['cachedAltAlleles']
Performance,nvokeChannelRead(AbstractChannelHandlerCetty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.innelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at lerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(Abstrac0) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at ocessSelectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at ava:459) at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent. Java stack trace:; org.apache.spark.SparkException: Job aborted.; at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunct,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:11038,concurren,concurrent,11038,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['concurren'],['concurrent']
Performance,nvokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHaetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerCactChannelHandlerContext.java:340) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at nnelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(A1359) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.Abstracead(DefaultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at ed(NioEventLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.Nio at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thrpark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.innelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at eChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerCetty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.innelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at lerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(Abstrac0) at io.netty.channel.DefaultChannelPipel,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:9541,concurren,concurrent,9541,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['concurren'],['concurrent']
Performance,nvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at org.apache.hadoop.fs.Path.initialize(Path.java:205); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.net.URI.checkPath(URI.java:1823); 	at java.net.URI.<init>(URI.java:745); 	at org.apache.hadoop.fs.Path.initialize(Path.java:202); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSyst,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:5208,concurren,concurrentGlobInternal,5208,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrentGlobInternal']
Performance,"ny.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, go",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6618,cache,cached,6618,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"o report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-ccaf3640241f. ### What you did:. ```; ht = hl.import_table('gs://gnomad/annotations/hail-0.2/ht/genomes/score_rankings/gnomad.sites.RF.newStats24.txt.bgz', types={'chrom': hl.tstr}, impute=True, min_partitions=100).cache(); ht.export('gs://gnomad-tmp/genomes_rf.txt.bgz', parallel=True); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; Py4JError Traceback (most recent call last); <ipython-input-17-671d2e9c22c8> in <module>(); 1 #ht = hl.import_table('gs://gnomad/annotations/hail-0.2/ht/genomes/score_rankings/gnomad.sites.RF.newStats24.txt.bgz', types={'chrom': hl.tstr}, impute=True, min_partitions=100).cache(); ----> 2 ht.export('gs://gnomad-tmp/genomes_rf.txt.bgz', parallel=True). /home/hail/hail.zip/hail/table.py in export(self, output, types_file, header, parallel); 994 """"""; 995 ; --> 996 self._jt.export(output, types_file, header, Env.hail().utils.ExportType.getExportType(parallel)); 997 ; 998 def group_by(self, *exprs, **named_exprs) -> 'GroupedTable':. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 186 import pyspark; 187 try:; --> 188 return f(*args, **kwargs); 189 except py4j.protocol.Py4JJavaError as e:; 190 s = e.java_exception.toString(). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 321 raise Py4JE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4033:991,cache,cache,991,https://hail.is,https://github.com/hail-is/hail/issues/4033,1,['cache'],['cache']
Performance,"o short for Google Cloud Storage. I am not sure why but we; timeout substantially more frequently. I have observed this myself on my laptop. Just this; morning I saw it happen to Daniel. 2. When using an `aiohttp.AsyncIterablePayload`, it is *critical* to always check if the coroutine; which actually writes to GCS (which is stashed in the variable `request_task`) is still; alive. In the current `main`, we do not do this which causes hangs (in particular the timeout; exceptions are never thrown ergo we never retry). To understand the second problem, you must first recall how writing works in aiogoogle. There are; two Tasks and an `asyncio.Queue`. The terms ""writer"" and ""reader"" are somewhat confusing, so let's; use left and right. The left Task has the owning reference to both the source ""file"" and the; destination ""file"". In particular, it is the *left* Task which closes both ""files"". Moreover, the; left Task reads chunks from the source file and places those chunks on the `asyncio.Queue`. The; right Task takes chunks off the queue and writes those chunks to the destination file. This situation can go awry in two ways. First, if the right Task encounters any kind of failure, it will stop taking chunks off of the; queue. When the queue (which has a size limit of one) is full, the left Task will hang. The system; is stuck. The left Task will wait forever for the right Task to empty the queue. The second scenario is exactly the same except that the left Task is trying to add the ""stop""; message to the queue rather than a chunk. In either case, it is critical that the left Task waits simultaneously on the queue operation *and*; on the right Task completing. If the right Task has died, no further writes can occur and the left; Task must raise an exception. In the first scenario, we do not observe the right Task's exception; because that will be done when we close the `InsertObjectStream` (which represents the destination; ""file""). ---. I also added several types, assertio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11830:1160,Queue,Queue,1160,https://hail.is,https://github.com/hail-is/hail/pull/11830,1,['Queue'],['Queue']
Performance,"o single-digit percents of a core. This leads to the first goal of this transition: configure our load balancers to know the full cluster configuration at any point in time so they can properly maintain connection pools with upstream services. However, this is not the only problem. Each ""upstream"" Service in Kubernetes may consist of multiple underlying pods but Kubernetes Services as we use them don't provide proper load-balancing when mixed with persistent connections. When we declare a Service for say, batch in default, Kubernetes adds a DNS record for `batch.default` that resolves to a single IP pointing at kube-proxy. When a new TCP connection is established with kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as fo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:3102,load,load-balancer,3102,https://hail.is,https://github.com/hail-is/hail/pull/12095,2,['load'],"['load-balance', 'load-balancer']"
Performance,"oadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31040 25791 (LdcX 3 I))))); 31041 (ReturnX). # Elsewhere, this split method is called, then the resulting field is loaded and written to the output buffer. 11325 (MethodStmtX INVOKEVIRTUAL __C1527collect_distributed_array.__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616_region0_0 (L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)V; 11326 (LoadX arg:0 L__C1527collect_distributed_array;); 11327 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11328 25772 (MethodStmtX INVOKEINTERFACE is/hail/io/OutputBuffer.writeByte (B)Vinterface; 11329 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2354null Lis/hail/io/OutputBuffer;; 11330 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11331 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 11332 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;))); ```. # Pervasiveness. There is absolutely nothing about this bug that is whole-stage-codegen-specific, but I suspect the much larger single IRs compiled in whole stage code generation made it exponentially more likely for this corner case to occur. I imagine it would be possible to construct a failing pipeline with whole stage code generation turned off. # Testing. This is super hard to reproduce using small/public examples, and any unit tests to capture this *crazy edge case* are pretty much meaningless. John suggested we programmatically check the TypeInfo inference against some JVM reference, and I agree that's the best bet, but don't want to block this critical fix on that project. I fixed BALOAD for the same reason, but it doesn't appear that has caused trouble yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:6538,Load,LoadX,6538,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['Load'],['LoadX']
Performance,"oadcast at SparkBackend.scala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.ut",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:1926,Load,LoadPlink,1926,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,"ode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add ter",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:1417,Load,LoadSelfFirstURLClassLoader,1417,https://hail.is,https://github.com/hail-is/hail/pull/10279,1,['Load'],['LoadSelfFirstURLClassLoader']
Performance,"ode>~cryptography.x509.CertificateBuilder</code>, other X.509 builders, and; PKCS7 has been removed.</li>; <li><strong>BACKWARDS INCOMPATIBLE:</strong> Dropped support for macOS 10.10 and 10.11, macOS; users must upgrade to 10.12 or newer.</li>; <li><strong>ANNOUNCEMENT:</strong> The next version of <code>cryptography</code> (40.0) will change; the way we link OpenSSL. This will only impact users who build; <code>cryptography</code> from source (i.e., not from a <code>wheel</code>), and specify their; own version of OpenSSL. For those users, the <code>CFLAGS</code>, <code>LDFLAGS</code>,; <code>INCLUDE</code>, <code>LIB</code>, and <code>CRYPTOGRAPHY_SUPPRESS_LINK_FLAGS</code> environment; variables will no longer be respected. Instead, users will need to; configure their builds <code>as documented here</code>_.</li>; <li>Added support for; :ref:<code>disabling the legacy provider in OpenSSL 3.0.x&lt;legacy-provider&gt;</code>.</li>; <li>Added support for disabling RSA key validation checks when loading RSA; keys via; :func:<code>~cryptography.hazmat.primitives.serialization.load_pem_private_key</code>,; :func:<code>~cryptography.hazmat.primitives.serialization.load_der_private_key</code>,; and; :meth:<code>~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateNumbers.private_key</code>.; This speeds up key loading but is :term:<code>unsafe</code> if you are loading potentially; attacker supplied keys.</li>; <li>Significantly improved performance for; :class:<code>~cryptography.hazmat.primitives.ciphers.aead.ChaCha20Poly1305</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyca/cryptography/commit/d6951dca25de45abd52da51b608055371fbcde4e""><code>d6951dc</code></a> changelog + security fix backport (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8231"">#8231</a>)</li>; <li><a href=""https://github.com/pyca/cryptography",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12668:2666,load,loading,2666,https://hail.is,https://github.com/hail-is/hail/pull/12668,4,['load'],['loading']
Performance,"of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h2>3.9.11 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ijl/orjson/commit/a348f59f0b55d92a1364523560f52f5b3cf9c12a""><code>a348f59</code></a> 3.9.15</li>; <li><a href=""https://github.com/ijl/orjson/commit/b0e4d2c06ce06c6e63981bf0276e4b7c74e5845e""><code>b0e4d2c</code></a> yyjson 0eca326, recursion limit</li>; <li><a href=""https://github.com/ijl/orjson/commit/5067eadc84cf516e4eb33bcb09ad756bb59dc42e""><code>5067ead</code></a> impl_escape_unchecked() byte exact read</li>; <li><a href=""https://github.com/ijl/orjson/commit/e04ea735b087742b6cee738aa295d8b835c3a195""><code>e04ea73</code></a> cargo update, build misc</li>; <li><a href=""https://github.com/ijl/orjson/commit/ba8c701292e4720b4e10210b266be5666d098fb6""><code>ba8c701</code></a> 3.9.14</li>; <li><",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14357:2745,perform,performance,2745,https://hail.is,https://github.com/hail-is/hail/pull/14357,3,['perform'],['performance']
Performance,"og output. ```shell; File ~/Library/Python/3.9/lib/python/site-packages/hail/table.py:2814, in Table.collect(self, _localize, _timed); 2812 e = construct_expr(rows_ir, hl.tarray(t.row.dtype)); 2813 if _localize:;  2814 return Env.backend().execute(e._ir, timed=_timed); 2815 else:; 2816 return e. File ~/Library/Python/3.9/lib/python/site-packages/hail/backend/backend.py:188, in Backend.execute(self, ir, timed); 186 payload = ExecutePayload(self._render_ir(ir), {name:StreamBufferSpec}, timed); 187 try:;  188 result, timings = self._rpc(ActionTag.EXECUTE, payload); 189 except FatalError as e:; 190 raise e.maybe_user_error(ir) from None. File ~/Library/Python/3.9/lib/python/site-packages/hail/backend/py4j_backend.py:218, in Py4JBackend._rpc(self, action, payload); 216 path = action_routes[action]; 217 port = self._backend_server_port;  218 resp = self._requests_session.post(fhttp://localhost:{port}{path}', data=data); 219 if resp.status_code >= 400:; 220 error_json = orjson.loads(resp.content). File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:637, in Session.post(self, url, data, json, **kwargs); 626 def post(self, url, data=None, json=None, **kwargs):; 627 r""""Sends a POST request. Returns :class:Response object.; 628; 629 :param url: URL for the new :class:Request object.; (); 634 :rtype: requests.Response; 635 ;  637 return self.request(POST, url, data=data, json=json, **kwargs). File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:589, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json); 584 send_kwargs = {; 585 timeout: timeout,; 586 allow_redirects: allow_redirects,; 587 }; 588 send_kwargs.update(settings);  589 resp = self.send(prep, **send_kwargs); 591 return resp. File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:703, in Session.send(self, request, **kwargs); 700 start = preferr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14557:1527,load,loads,1527,https://hail.is,https://github.com/hail-is/hail/issues/14557,1,['load'],['loads']
Performance,ogle.cloud.storage.ApiaryUnbufferedReadableByteChannel.read(ApiaryUnbufferedReadableByteChannel.java:104); 	at is.hail.relocated.com.google.cloud.storage.UnbufferedReadableByteChannelSession$UnbufferedReadableByteChannel.read(UnbufferedReadableByteChannelSession.java:36); 	at is.hail.relocated.com.google.cloud.storage.DefaultBufferedReadableByteChannel.read(DefaultBufferedReadableByteChannel.java:106); 	at is.hail.relocated.com.google.cloud.storage.StorageByteChannels$SynchronizedBufferedReadableByteChannel.read(StorageByteChannels.java:84); 	at is.hail.relocated.com.google.cloud.storage.BaseStorageReadChannel.read(BaseStorageReadChannel.java:91); 	at is.hail.io.fs.GoogleStorageFS$$anon$1.readHandlingRequesterPays(GoogleStorageFS.scala:205); 	at is.hail.io.fs.GoogleStorageFS$$anon$1.fill(GoogleStorageFS.scala:242); 	at is.hail.io.fs.FSSeekableInputStream.read(FS.scala:164); 	at java.io.DataInputStream.read(DataInputStream.java:100); 	at is.hail.expr.ir.GenericLines$$anon$2.loadBuffer(GenericLines.scala:84); 	at is.hail.expr.ir.GenericLines$$anon$2.readLine(GenericLines.scala:194); 	at is.hail.expr.ir.GenericLines$$anon$2.hasNext(GenericLines.scala:214); 	at __C18collect_distributed_array_shuffle_initial_write.apply_region1_42(Unknown Source); 	at __C18collect_distributed_array_shuffle_initial_write.apply(Unknown Source); 	at __C18collect_distributed_array_shuffle_initial_write.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$2(BackendUtils.scala:38); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$1(BackendUtils.scala:37); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:36); 	at __C5Compiled.__m7split_Let(Emit.scala); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:74); 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.ja,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:5982,load,loadBuffer,5982,https://hail.is,https://github.com/hail-is/hail/issues/12983,4,['load'],['loadBuffer']
Performance,"ography/issues/10442"">#10442</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/7a4d012991061974da5d9cb7614de65eac94f49b""><code>7a4d012</code></a> Fixes <a href=""https://redirect.github.com/pyca/cryptography/issues/10422"">#10422</a> -- don't crash when a PKCS#12 key and cert don't match (<a href=""https://redirect.github.com/pyca/cryptography/issues/10423"">#10423</a>) ...</li>; <li><a href=""https://github.com/pyca/cryptography/commit/df314bb182bdfd661333969a94325e4680d785f6""><code>df314bb</code></a> backport actions m1 switch to 42.0.x (<a href=""https://redirect.github.com/pyca/cryptography/issues/10415"">#10415</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/c49a7a5271178c6e8ef36fa1c499f62c63ec19b9""><code>c49a7a5</code></a> changelog and version bump for 42.0.3 (<a href=""https://redirect.github.com/pyca/cryptography/issues/10396"">#10396</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/396bcf64c5be826ec00e7d7f45838c858c049cbc""><code>396bcf6</code></a> fix provider loading take two (<a href=""https://redirect.github.com/pyca/cryptography/issues/10390"">#10390</a>) (<a href=""https://redirect.github.com/pyca/cryptography/issues/10395"">#10395</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/0e0e46f5f73f477b8ee9682738c42129d5d60177""><code>0e0e46f</code></a> backport: initialize openssl's legacy provider in rust (<a href=""https://redirect.github.com/pyca/cryptography/issues/10323"">#10323</a>) (<a href=""https://redirect.github.com/pyca/cryptography/issues/10333"">#10333</a>)</li>; <li>See full diff in <a href=""https://github.com/pyca/cryptography/compare/42.0.2...42.0.4"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=cryptography&package-manager=pip&previous-version=42.0.2&new-version=42.0.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14332:2537,load,loading,2537,https://hail.is,https://github.com/hail-is/hail/pull/14332,3,['load'],['loading']
Performance,oke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at java.util.concurrent.AbstractExecutorService.doInvokeAny(AbstractExecutorService.java:193); 	at java.util.concurrent.AbstractExecutorService.invokeAny(AbstractExecutorService.java:215); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1282); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCom,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:3458,concurren,concurrentGlobInternal,3458,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrentGlobInternal']
Performance,oke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Hail version: devel-9a5678f; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3516:7609,concurren,concurrent,7609,https://hail.is,https://github.com/hail-is/hail/issues/3516,2,['concurren'],['concurrent']
Performance,oke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at java.util.concurrent.AbstractExecutorService.doInvokeAny(AbstractExecutorService.java:193); 	at java.util.concurrent.AbstractExecutorService.invokeAny(AbstractExecutorService.java:215); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1282); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(Reflectio,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:3316,concurren,concurrent,3316,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,ollection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8545:9054,concurren,concurrent,9054,https://hail.is,https://github.com/hail-is/hail/issues/8545,1,['concurren'],['concurrent']
Performance,ollection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:81881,concurren,concurrent,81881,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['concurren'],['concurrent']
Performance,"ollowing error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(Thread.java:745)`. `Hail version: 0.1-4238176`; `Error summary: HailException: arguments refer to no files`. It's probably something quick, but I can't seem to figure it out?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2070:1177,Load,LoadVCF,1177,https://hail.is,https://github.com/hail-is/hail/issues/2070,2,['Load'],['LoadVCF']
Performance,"om Step 1 in groups of 100 attempts; 4. Randomize the offsets and have a burn in period of 5000 to avoid the birthday problem where we populate the `aggregated_*_resources_by_date` tables.; 5. In 10-way parallelism (maxes out a 4 core database), randomly populate the tables for each chunk.; 6. From the last offset (original first running batch id), we sequentially process attempts in groups of 100. We take note of where we are at with tracking any updates to the attempts table (`attempts_time_msecs_diff`), populate the `aggregated_*_resources_by_date` tables, and then do a final catchup step where we apply any updates from `attempts_time_msecs_diff` for any attempts that we have already processed.; 7. Once we have reached the ""end"" of the attempts table, we lock all tables of interest especially the `attempts` table, and do one last final processing step before we add the new triggers that will auto-populate the `aggregated_*_resources_by_date` tables.; 8. Then we perform an audit and make sure things look correct. (I might need to change or eliminate the billing_project audit query because there are 5 batches with ~20 jobs that aren't perfectly tracked when we did the original switch over to the new billing tables).; 9. If there are any failures, we revert the triggers back to the original state. Also to note, is the new table for billing_projects is keyed by (billing_project, user) which will make queries much faster so they don't have to scan the batches aggregated resources table. I ran the migration successfully on a full test database and the audit was clean for jobs and batches except for the 5 batches that were running right when we started populating the original aggregated billing tables. . I'd like to gather all feedback and then will run the migration one more time to do a final test. Note, that most of the queries in the migration are not tested. The key thing to double check is the triggers will continue to insert data into the old tables and we get th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11996:1833,perform,perform,1833,https://hail.is,https://github.com/hail-is/hail/pull/11996,1,['perform'],['perform']
Performance,ommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advan,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5822,Load,LoadPlink,5822,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,"ommitter; spark.hadoop.mapred.output.committer.class org.apache.hadoop.mapred.DirectFileOutputCommitter; spark.hadoop.mapreduce.use.directfileoutputcommitter true; spark.hadoop.spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; ```. Code and stack trace:; ```; ================================================================================================== FAILURES ===================================================================================================; __________________________________________________________________________________________ TestHAIL.test_export_vcf ___________________________________________________________________________________________. self = <test_hail.TestHAIL testMethod=test_export_vcf>. def test_export_vcf(self):; # define files; bgen_file = os.path.join(self.testdir, 'example.10bits.bgen'); sample_file = os.path.join(self.testdir, 'example.sample'); # make index; self.hc.index_bgen(bgen_file); # load to vds; bgen_vds = self.hc.import_bgen(bgen_file, sample_file=sample_file); # export vcf; out_path = 'file://' + os.path.join(self.tmpdir, 'test_vcf_export.vcf.bgz'); > bgen_vds.export_vcf(out_path, export_pp=False, parallel=False). tests/hail/test_hail.py:55:; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; <decorator-gen-398>:2: in export_vcf; ???; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. func = <function export_vcf at 0x7fa13c4d9938>, args = (<hail.dataset.VariantDataset object at 0x7fa13c3c9390>, 'file:///scratch/test_vcf_export.vcf.bgz', None, False, False), kwargs = {}; e = Py4JJavaError(u'An error occurred while calling o160.exportVCF.\n', JavaObject id",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:1693,load,load,1693,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['load'],['load']
Performance,"on$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/b09ec92a-49f4-4d16-ad6d-efc5a5805e92/05_variant_qc.py"", line 201, in <module>; cumcounts = {'step0': rt.aggregate(hl.agg.sum(hl.cond(rt.qccum.step0, 1, 0))),; File ""<decorator-gen-519>"", line 2, in aggregate; File ""/home/hail/hail.zip/hail/utils/java.py"", line 191, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueB",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3063:11148,concurren,concurrent,11148,https://hail.is,https://github.com/hail-is/hail/issues/3063,1,['concurren'],['concurrent']
Performance,on$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3063:4884,concurren,concurrent,4884,https://hail.is,https://github.com/hail-is/hail/issues/3063,2,['concurren'],['concurrent']
Performance,"on.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```. 2. gs://hail-common/gencode_and_production_intervals.merged.hg19.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAcc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683:2767,load,loading,2767,https://hail.is,https://github.com/hail-is/hail/issues/1683,1,['load'],['loading']
Performance,on.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8338:11925,Optimiz,Optimize,11925,https://hail.is,https://github.com/hail-is/hail/issues/8338,1,['Optimiz'],['Optimize']
Performance,"oncurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); ```. The driver will have log output like this:; ```; 2023-09-22 19:11:13.051 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/8042383 response 200; 2023-09-22 19:11:13.052 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=: reading results; 2023-09-22 19:11:13.125 ServiceBackend$: INFO: all results read. 0.072746861 s. 0.0 result/s. 0.0 MiB/s.; 2023-09-22 19:11:13.125 : INFO: [collectDArray|table_native_writer]: executed 5 tasks in 1.822s; 2023-09-22 19:11:13.126 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:13.126 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-09-22 19:11:13.126 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-09-22 19:11:13.127 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: RegionPool: FREE: 128.0K allocated (128.0K blocks / 0 chunks), regions.size = 2, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.138 : ERROR: GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:3194,cache,cache,3194,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['cache'],['cache']
Performance,oncurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19361,Load,LoadVCF,19361,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"one-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3477,cache,cached,3477,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"one-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3569,cache,cached,3569,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"onious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib \; --master-machine-type=n1-highmem-8 \; --master-boot-disk-size=100GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-8 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --labels=creator=weisburd_broadinstitute_org \; --max-idle=12h; Starting cluster 'bw2'...; Waiting on operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08].; Waiting for cluster creation operation...; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; Waiting for cluster creation operation...done.; ERROR: (gcloud.beta.dataproc.clusters.create) Operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08] failed: Initialization action failed. Failed action 'gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py', see output in: gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output.; Traceback (most recent call last):; File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6634:2044,perform,performance,2044,https://hail.is,https://github.com/hail-is/hail/issues/6634,2,['perform'],['performance']
Performance,oolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-1908254; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822:12608,concurren,concurrent,12608,https://hail.is,https://github.com/hail-is/hail/issues/1822,2,['concurren'],['concurrent']
Performance,"ools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8)  ~/sandbox/hail [master|8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1371,cache,cached,1371,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,optimize splitmulti by adding flag to importvcf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/821:0,optimiz,optimize,0,https://hail.is,https://github.com/hail-is/hail/issues/821,1,['optimiz'],['optimize']
Performance,option to add variant loadings to variant annotations in PCA,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/166:22,load,loadings,22,https://hail.is,https://github.com/hail-is/hail/issues/166,1,['load'],['loadings']
Performance,option to load just a bit of a file for testing pipelines,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1719:10,load,load,10,https://hail.is,https://github.com/hail-is/hail/issues/1719,1,['load'],['load']
Performance,options for further improving filter intervals performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6084:47,perform,performance,47,https://hail.is,https://github.com/hail-is/hail/issues/6084,1,['perform'],['performance']
Performance,"or.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. 	at is.hail.utils.ErrorHandling.fatal(E",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:4990,Cache,CacheDir,4990,https://hail.is,https://github.com/hail-is/hail/issues/14513,4,['Cache'],['CacheDir']
Performance,"or.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. Java stack trace:; org.apache.spark.Spa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:2372,Cache,CacheDir,2372,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,['Cache'],['CacheDir']
Performance,or.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:7252,concurren,concurrent,7252,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['concurren'],['concurrent']
Performance,or.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseL,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:1573,concurren,concurrent,1573,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['concurren'],['concurrent']
Performance,"orImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: hybrid.m37m.vcf.bgz: caught htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; offending line: 3	60830534	.	M	C	40	.	.	GT:AD	1/1:0,40; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:6184,Load,LoadVCF,6184,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,"orImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz: caught java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:9582,Load,LoadVCF,9582,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	... 34 more; Caused by: org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: java.lang.ArrayIndexOutOfBoundsException; ```. And the actual informative trace nested in the `hail.log`:; ```; Caused by: java.lang.ArrayIndexOutOfBoundsException: 1; at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.BinaryFun.apply(Fun.scala:108); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:143); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$lookupMethod$1$$anonfun$36.apply(FunctionRegistry.scala:228); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOpti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202:7608,concurren,concurrent,7608,https://hail.is,https://github.com/hail-is/hail/issues/1202,1,['concurren'],['concurrent']
Performance,"ort any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made this release possible.</p>; <h2>Pandas 1.5.1</h2>; <p>This is a patch release in the 1.5.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.1/whatsnew/v1.5.1.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made this release possible.</p>; <h2>Pandas 1.5.0</h2>; <p>This release includes some new features, bug fixes, and performance improvements. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.0/whatsnew/v1.5.0.html"">full whatsnew</a> for a list of all the changes. pandas 1.5.0 supports Python 3.8 and higher.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <p><code>conda install -c conda-forge pandas</code></p>; <p>Or via PyPI:</p>; <p><code>python3 -m pip install --upgrade pandas</code></p>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.5.0rc0</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pandas-dev/pandas/commit/8dab54d6573f7186ff0c3b6364d5e4dd635ff3e7""><code>8dab54d</code></a> RLS: 1.5.2</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12564:1813,perform,performance,1813,https://hail.is,https://github.com/hail-is/hail/pull/12564,1,['perform'],['performance']
Performance,"otocol); * In IntelliJ, go to File->Open, and choose the hail root directory; * When the project is open, go to File->Project Structure; * in the Project pane, set an sdk (8 or 11), and set the language level to 8; * in the Modules pane, delete the existing root module, click the plus sign -> Import Module, choose the `hail/` subdirectory, and choose ""Import module from external model"" and `BSP`; * you should see a progress bar at the bottom as it imports the project; * when it's done, quit and reopen IntelliJ. There should now be a bsp icon (two bars with two arrows between them) on the right, where the gradle elephant used to be. Just like before, sometimes you'll need to click the ""reload"" icon in there if things get wonky.; * if it says ""scalafmt configuration detected"", go ahead and enable the formatter. ## Metals setup. * delete any `.metals` directories; * open the hail repo in VSCode (even if you won't use VSCode, this seems to be the best way to get metals set up initially); * it should ask you to import a Mill build; * when that finishes, at the bottom it should say it's connected to a Bloop build server. In general, I think using Mill as the BSP directly will work best, but I don't have much experience to say for sure. To switch, run `Metals: switch build server` from the command palette. ## Debug and release builds. As before, debug mode adds some (fairly expensive) checking to our native memory system. But now there are a few other differences:; * treat warnings as errors only in release mode, so you can still compile, run tests, etc. during development without fixing all warnings; * enable optimization in scalac only in release mode. The intention is that we use debug mode during development, and release mode ony for published artifacts, or performance profiling. Mill will use debug mode by default. To enable release mode, define `HAIL_RELEASE_MODE` in your environment. Note changing this will invalidate all mill intermediates and rebuild from scratch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147:5452,optimiz,optimization,5452,https://hail.is,https://github.com/hail-is/hail/pull/14147,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"ound. In the world of low-level container runtimes there exists the term ""bundle"", which basically means the pair of a root filesystem and a `config.json` file containing all of the other necessary information to run the container. If you invoke `crun` or `runc` with `--bundle /path/to/bundle`, the runtime assumes the following:. - The configuration file for the container is located at `/path/to/bundle/config.json`; - That `config.json` contains a field [`root.path`](https://github.com/opencontainers/runtime-spec/blob/main/config.md#root) that specifies the location of the root filesystem, most commonly as a path relative to `/path/to/bundle`. `crun` offers a way to explicitly reference the location of `config.json` through its `--config` flag. This seems fairly innocuous, but specifying a custom `--config` path can have some unfortunate unintended consequences because it invalidates the assumption in the specification that the configuration resides at `/path/to/bundle/config.json`. Specifically, it breaks [Hooks](https://github.com/opencontainers/runtime-spec/blob/main/config.md#posix-platform-hooks). When a hook is run, the runtime (crun) feeds it the [container state](https://github.com/opencontainers/runtime-spec/blob/main/runtime.md#state), a JSON of information about the container including the `bundle` path. Any hook that attempts to load the `config.json`, like for example, the `nvidia-container-runtime-hook`, will crash. ### Change. This change stops using the `--config` flag for crun and instead does the following to create a well-formed bundle:. - Instead of the bundle being the merged directory of the container overlay, it is the container's scratch directory; - `root.path` is adjusted inside of `config.json` to now point to the merged directory of the container overlay. I've opted to use an absolute path here because why use a relative path.; - Move `config.json` into the container scratch directory so that it is inside the root of the bundle directory.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13438:1373,load,load,1373,https://hail.is,https://github.com/hail-is/hail/pull/13438,1,['load'],['load']
Performance,ources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region_test.cpp -MG -M -MF build/Region_test.d -MT build/Region_test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../reso,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5659:1974,cache,cache-tests,1974,https://hail.is,https://github.com/hail-is/hail/issues/5659,1,['cache'],['cache-tests']
Performance,"out); File ""/usr/local/lib/python3.7/asyncio/tasks.py"", line 442, in wait_for; return fut.result(); File ""/usr/local/lib/python3.7/asyncio/futures.py"", line 181, in result; raise self._exception; File ""/usr/local/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/usr/local/lib/python3.7/site-packages/aiodocker/images.py"", line 46, in get; return await self.inspect(name); File ""/usr/local/lib/python3.7/site-packages/aiodocker/images.py"", line 36, in inspect; response = await self.docker._query_json(""images/{name}/json"".format(name=name)); File ""/usr/local/lib/python3.7/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.7/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.7/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'no such image: gcr.io/hail-vdc/query:tfkm2kev7zcf: No such image: gcr.io/hail-vdc/query:tfkm2kev7zcf'). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.7/site-packages/batch/worker/worker.py"", line 372, in run; await self.ensure_image_is_pulled(); File ""/usr/local/lib/python3.7/site-packages/batch/worker/worker.py"", line 363, in ensure_image_is_pulled; docker.images.pull, self.image); File ""/usr/local/lib/python3.7/site-packages/batch/worker/worker.py"", line 111, in wrapper; return await asyncio.wait_for(f(*args, **kwargs), timeout); File ""/usr/local/lib/python3.7/asyncio/tasks.py"", line 442, in wait_for; return fut.result(); File ""/usr/local/lib/python3.7/asyncio/futures.py"", line 181, in result; raise self._exception; File ""/usr/local/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/usr/l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9902:2646,load,loads,2646,https://hail.is,https://github.com/hail-is/hail/pull/9902,1,['load'],['loads']
Performance,"p/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89; 2023-09-24 01:58:51.513 : INFO: TaskReport: stage=0, partition=9571, attempt=0, peakBytes=4507648, peakBytesReadable=4.30 MiB, chunks requested=51, cache hits=0; 2023-09-24 01:58:51.513 : INFO: RegionPool: FREE: 4.3M allocated (2.2M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:51.515 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/aou_tmp/o?name=tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89&uploadType=resumable&upload_id=ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721:4314,concurren,concurrent,4314,https://hail.is,https://github.com/hail-is/hail/issues/13721,1,['concurren'],['concurrent']
Performance,pPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3235:5427,concurren,concurrent,5427,https://hail.is,https://github.com/hail-is/hail/issues/3235,1,['concurren'],['concurrent']
Performance,pache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:81,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:2426,Load,LoadVCF,2426,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed. /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:477: FatalError; ------------------------------ Captured log call -------------------------------; INFO batch_client.aioclient:aioclient.py:753 created batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 37,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:23067,concurren,concurrent,23067,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,"package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed.; ```. ### Version. 0.2.115-f6017673dbb6. ### Relevant log output. ```shell; ________________________________ test_spectra_4 ________________________________; [gw2] linux -- Python 3.8.10 /usr/bin/python3. def test_spectra_4():; > spectra_helper(spec4). test/hail/methods/test_pca.py:229: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/hail/methods/test_pca.py:172: in spectra_helper; hail_V = (np.array(scores.scores.collect()) / singulars).T; <decorator-gen-538>:2: in collect; ???; /usr/local/lib/python3.8/dist-packages/hail/typecheck/check.py:584: in wrapper; return __original_func(*args_, **kwargs_); /usr/local/lib/python3.8/dist-packages/hail/expr/expressions/base_expression.py:1132: in collect; return hl.eval(e); <decorato",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:9449,concurren,concurrent,9449,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,"park-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2802,cache,cached,2802,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"park-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ***/hail-all-spark.jar --master yarn-client importvcf /user/hail/sample.vcf splitmulti write -o /user/hail/sample_1.vds exportvcf -o /user/hail/sample_1.vcf. Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/solr/client/solrj/SolrClient : Unsupported major.minor version 52.0; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:800); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:449); at java.net.URLClassLoader.access$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.Spark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825:1066,load,loadClass,1066,https://hail.is,https://github.com/hail-is/hail/issues/825,1,['load'],['loadClass']
Performance,"pe hints for sync and async queues <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/374"">#374</a></li>; </ul>; <h2>0.6.2 (2021-10-24)</h2>; <ul>; <li>Fix Python 3.10 compatibility <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/358"">#358</a></li>; </ul>; <h2>0.6.1 (2020-10-26)</h2>; <ul>; <li>; <p>Raise RuntimeError on queue.join() after queue closing. <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/295"">#295</a></p>; </li>; <li>; <p>Replace <code>timeout</code> type from <code>Optional[int]</code> to <code>Optional[float]</code> <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/267"">#267</a></p>; </li>; </ul>; <h2>0.6.0 (2020-10-10)</h2>; <ul>; <li>; <p>Drop Python 3.5, the minimal supported version is Python 3.6</p>; </li>; <li>; <p>Support Python 3.9</p>; </li>; <li>; <p>Refomat with <code>black</code></p>; </li>; </ul>; <h2>0.5.0 (2020-04-23)</h2>; <ul>; <li>Remove explicit loop arguments and forbid creating queues outside event loops <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/246"">#246</a></li>; </ul>; <h2>0.4.0 (2018-07-28)</h2>; <ul>; <li>; <p>Add <code>py.typed</code> macro <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/89"">#89</a></p>; </li>; <li>; <p>Drop python 3.4 support and fix minimal version python3.5.3 <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/88"">#88</a></p>; </li>; <li>; <p>Add property with that indicates if queue is closed <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/86"">#86</a></p>; </li>; </ul>; <h2>0.3.2 (2018-07-06)</h2>; <ul>; <li>Fixed python 3.7 support <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/97"">#97</a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/janus/commit/0783f9b7a9bb7e1c095e93ebb4aad4f1e2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11466:1798,queue,queues,1798,https://hail.is,https://github.com/hail-is/hail/pull/11466,1,['queue'],['queues']
Performance,"peline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 YarnScheduler: ERROR: Lost executor 1 on bw2-sw-dp3j.c.seqr-project.internal: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1574.1 in stage 8.0 (TID 21699, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1559.1 in stage 8.0 (TID 21702, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1561.1 in stage 8.0 (T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:3573,concurren,concurrent,3573,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['concurren'],['concurrent']
Performance,"pha/batches/8042383 response 200; 2023-09-22 19:11:13.052 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=: reading results; 2023-09-22 19:11:13.125 ServiceBackend$: INFO: all results read. 0.072746861 s. 0.0 result/s. 0.0 MiB/s.; 2023-09-22 19:11:13.125 : INFO: [collectDArray|table_native_writer]: executed 5 tasks in 1.822s; 2023-09-22 19:11:13.126 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:13.126 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-09-22 19:11:13.126 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-09-22 19:11:13.127 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: RegionPool: FREE: 128.0K allocated (128.0K blocks / 0 chunks), regions.size = 2, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.138 : ERROR: GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=/result.0; From is.hail.relocated.com.google.cloud.storage.StorageException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:3587,cache,cache,3587,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['cache'],['cache']
Performance,pl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-ebabd77; Error summary: IllegalArgumentException: Size exceeds Integer.MAX_VALUE. ; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806:6638,concurren,concurrent,6638,https://hail.is,https://github.com/hail-is/hail/issues/1806,2,['concurren'],['concurrent']
Performance,"plit. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- both Boolean and Byte use a single slot, and so the code still works even with the wrong variable type. However, the method splitter then **generated a method split at the same point where the region was split**. This means that the local variable resulting from I2B is spilled to a class field on the spills class. Our incorrectly-Boolean local becomes an incorrectly-Boolean **field**, and this is where things go wrong -- it seems as though Boolean class fields (appropriately) truncate on store and load a single bit. Our value of `3` was stored as a class Boolean, and came out `1`. The fact that a single field's missingness was flipped was a red herring -- all higher bits are flipped to 0 (defined)! Here's a look at the LIR looks like, though it was ultimately the JVM class file printout that tipped me off to the problem:. ```code. # I2B is stored as a class field on spills. The Z at the end of the next line indicates this field is a Boolean, not a byte. 31017 (PutFieldX PUTFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 31018 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;); 31019 25774 (InsnX I2B; 31020 25775 (InsnX IOR; 31021 25776 (InsnX IOR; 31022 25777 (InsnX IOR; 31023 25778 (InsnX IOR; 31024 25779 (LdcX 0 I); 31025 25780 (InsnX ISHL; 31026 (GetFieldX GETFIELD __C2316__m1984ENCODE_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:3633,load,load,3633,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['load'],['load']
Performance,"plitting the authority component.</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.5 (2021-05-26)</h2>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting; the authority component.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/d1616473df94b94f0f5ad19d2a6608cfe93b7cdf""><code>d161647</code></a> Release 1.26.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2d4a3fee6de2fa45eb82169361918f759269b4ec""><code>2d4a3fe</code></a> Improve performance of sub-authority splitting in URL</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2698537d52f8ff1f0bbb1d45cf018b118e91f637""><code>2698537</code></a> Update vendored six to 1.16.0</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Python 3.10 ssl module</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/d725a9b56bb8baf87c9e6eee0e9edf010034b63b""><code>d725a9b</code></a> Add Python 3.10 to GitHub Actions</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/339ad34c677c98fd9ad008de1d8bbeb9dbf34381""><code>339ad34</code></a> Use pytest==6.2.4 on Python 3.10+</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/f271c9c3149e20d7feffb6429b135bbb6c09ddf4""><code>f271c9c</code></a> Apply latest Black formatting</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1884878aac87ef0494b282e940c32c24ee917d52""><code>1884878</code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10544:1638,perform,performance,1638,https://hail.is,https://github.com/hail-is/hail/pull/10544,1,['perform'],['performance']
Performance,plus some router config fixes. Mainly I'm PR'ing this so I can use the cached base image when testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6936:71,cache,cached,71,https://hail.is,https://github.com/hail-is/hail/pull/6936,1,['cache'],['cached']
Performance,ply$27.apply(ContextRDD.scala:359); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-15f58831fe57; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4263:12081,concurren,concurrent,12081,https://hail.is,https://github.com/hail-is/hail/issues/4263,2,['concurren'],['concurrent']
Performance,ply$4.apply(LinearRegression.scala:130); at org.broadinstitute.hail.methods.LinearRegression$$anonfun$apply$4.apply(LinearRegression.scala:129); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$20.next(Iterator.scala:635); at scala.collection.Iterator$$anon$20.next(Iterator.scala:633); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1109); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/336:3005,concurren,concurrent,3005,https://hail.is,https://github.com/hail-is/hail/issues/336,2,['concurren'],['concurrent']
Performance,ply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.Cont,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:13105,Load,LoadMatrix,13105,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,pply(ContextRDD.scala:373); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$30.apply(ContextRDD.scala:373); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:153); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:153); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5371:8285,concurren,concurrent,8285,https://hail.is,https://github.com/hail-is/hail/issues/5371,2,['concurren'],['concurrent']
Performance,pply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.bac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8338:11774,Optimiz,Optimize,11774,https://hail.is,https://github.com/hail-is/hail/issues/8338,1,['Optimiz'],['Optimize']
Performance,pply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189); 	at scala.Option.foreach(Option.scala:407); 	at org.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:8042,concurren,concurrent,8042,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['concurren'],['concurrent']
Performance,"pply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.130-bea04d9c79b5; Error summary: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:19632,concurren,concurrent,19632,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['concurren'],['concurrent']
Performance,"pport for jetty via <a href=""http://kohlschutter.github.io/junixsocket/junixsocket-jetty/"">junixsocket-jetty</a></li>; <li>Fix Selector logic (more bug fixes)</li>; <li>Documentation updates</li>; </ul>; <h2>junixsocket 2.5.0</h2>; <ul>; <li>New supported platforms: AIX 7 Power64, IBM i Power64, Windows ARM64, Windows Server 2019 &amp; 2022</li>; <li>Generic rework to support more than just Unix Domain sockets</li>; <li>Add support for AF_TIPC (on Linux)</li>; <li>Add support for using sockets passed as standard input</li>; <li>Add support for address-specific, non-standard URIs (for example; unix:// and tipc://), as well as socat addresses</li>; <li>Add support for using FileDescriptor for ProcessBuilder Redirects (Java 9+)</li>; <li>Add support for peer credentials (PID) on Windows</li>; <li>Fix Selector logic</li>; <li>Fix cross-compilation on Apple Silicon</li>; <li>Fix a file descriptor leak (regression in 2.4.0)</li>; <li>Improve behavior on partially unsupported platforms and allow loading of Windows 10 native; library on other Windows versions (e.g., Windows Server 2022, Windows 8.1).</li>; <li>Javadoc improvements, Code cleanup</li>; <li>Deprecate AFUNIXSocketCapability in favor of AFSocketCapability</li>; <li>Drop support for Java 7</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/8bfc43a332d5573397b72b778fed2b8c13d1dfc1""><code>8bfc43a</code></a> Fix PMD warning</li>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/53e668df0d279b368d81db8b67576342927ad892""><code>53e668d</code></a> native: Disable DEBUG by default</li>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/f8423eaee2871623113bd0200f510a292aa165d1""><code>f8423ea</code></a> docs: Update GraalVM instructions</li>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/90a31b6309e653d2714dd6a35d43b45bc8e94002""><",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12483:3040,load,loading,3040,https://hail.is,https://github.com/hail-is/hail/pull/12483,1,['load'],['loading']
Performance,"pported --output minimaljre --strip-debug --no-man-pages --no-header-files --compress=2; ```; comes in at under 30MB gzipped, which would increase the PyPI package by about 20% in size, while allowing users to install and run Hail in _any_ supported python environment without having to consider Java versions at all. Alternatively, have you ever considered distributing Hail through conda-forge or bioconda, where you could specify a JRE that should be installed with it and automatically linked?. Is there a better channel than Github Issues for feature requests? I realize this is not a bug report, and if you want to just close it and say ""nope"" that's fine, but I've seen a good number of first-time hail users get a bad impression because of this. . ### Ramble about other nitpicks below. I don't want to spam this repo with issues, but I also noticed while poking around at hail:; 1. It seems to use the default Java GC, which is now G1 in Java 9+. Performance on newer Javas would likely improve with `-XX:+UseParallelGC` in java opts; 2. The Hail jar includes module-info.class from azure storage, this broke my first attempt to use `jdeps` to see what modules it needs. Specifically, `hail-all-spark.jar` says it exports:; ```; backend % jar --describe-module --file=hail-all-spark.jar; releases: 9. com.azure.storage.blob@12.22.0 jar:file:///Users/alex/src/hail/hail/build/deploy/hail/backend/hail-all-spark.jar!/module-info.class; exports com.azure.storage.blob; exports com.azure.storage.blob.models; exports com.azure.storage.blob.options; exports com.azure.storage.blob.sas; exports com.azure.storage.blob.specialized; requires com.azure.storage.common transitive; requires com.azure.storage.internal.avro; requires com.fasterxml.jackson.dataformat.xml; requires java.base mandated; qualified exports com.azure.storage.blob.implementation to com.azure.storage.blob.batch com.azure.storage.blob.cryptography com.azure.storage.file.datalake; qualified exports com.azure.storage.blob.imple",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14433:3539,Perform,Performance,3539,https://hail.is,https://github.com/hail-is/hail/issues/14433,1,['Perform'],['Performance']
Performance,"present. These should have written the byte. 1<<0 | 1<<1 | 0<<2 | 0<<3; ==> b00000011; ==> 3. But instead wrote the byte `b00000001 or 1`, incorrectly leading readers to try to read field B when it was missing (and not written). This is due to the load-bearing and incorrect type of an I2B instruction generated [here](https://github.com/hail-is/hail/blob/8bd9b7b2224b77372a72f02f2b13806267892a35/hail/src/main/scala/is/hail/types/encoded/EBaseStruct.scala#L107). I2B is an instruction that truncates an integer to a byte, and it is used in various places in code generation but primarily encoding missing bits in arrays and structs. . I2B loads a byte to the stack, not a boolean. TypeInfos are mostly non-structural since they rarely influence the bytecode generated. Here is an exception, and that's where the method splitter comes in. Method splitting exists in the Hail compiler because not only does the JVM have limits on how large methods can be, but also the JIT compiler handles small methods much more effectively than large methods (and so splitting a large method into two small ones can make an order of magnitude or more in performance difference). We have three forms of method splitting in the Hail Query compiler. The first is a heuristic and greedy IR-level method splitter that generates new methods every X IR nodes, simply based on node count. However, the size of code generated by each IR can vary widely (`I32` vs `LowerBoundOnOrderedCollection` for instance), and so we have two other kinds of splitting that operate on the LIR level. The first is region splitting, which is used to split large blocks of LIR. In order to insert a split, any variables on the stack are stored in local variables before the split and loaded from those locals after the split. The second is method splitting, which is used to split large single methods. A single-exit group of blocks can be split into a separate method, and we have some machinery for replacing control flow instructions (which",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:1561,perform,performance,1561,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['perform'],['performance']
Performance,print line of file on error in LoadMatrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3849:31,Load,LoadMatrix,31,https://hail.is,https://github.com/hail-is/hail/pull/3849,1,['Load'],['LoadMatrix']
Performance,"ps://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no files; ```; How can I solve it ?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076:2282,Load,LoadVCF,2282,https://hail.is,https://github.com/hail-is/hail/issues/2076,2,['Load'],['LoadVCF']
Performance,"ps://redirect.github.com/googleapis/java-storage/issues/2150"">#2150</a>) (<a href=""https://github.com/googleapis/java-storage/commit/330e795040592e5df22d44fb5216ad7cf2448e81"">330e795</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.14.0 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2151"">#2151</a>) (<a href=""https://github.com/googleapis/java-storage/commit/eba8b6a235919a27d1f6dadf770140c7d143aa1a"">eba8b6a</a>)</li>; </ul>; <h2>v2.25.0</h2>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.24.0...v2.25.0"">2.25.0</a> (2023-07-24)</h2>; <h3>Features</h3>; <ul>; <li>BlobWriteChannelV2 - same throughput less GC (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2110"">#2110</a>) (<a href=""https://github.com/googleapis/java-storage/commit/1b52a1053130620011515060787bada10c324c0b"">1b52a10</a>)</li>; <li>Update Storage.createFrom(BlobInfo, Path) to have 150% higher throughput (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2059"">#2059</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4c2f44e28a1ff19ffb2a02e3cefc062a1dd98fdc"">4c2f44e</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>Update BlobWriteChannelV2 to properly carry forward offset after incremental flush (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2125"">#2125</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c099a2f4f8ea9afa6953270876653916b021fd9f"">c099a2f</a>)</li>; <li>Update GrpcStorageImpl.createFrom(BlobInfo, Path) to use RewindableContent (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2112"">#2112</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c80505129baa831e492a5514e937875407211595"">c805051</a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://gi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13605:3685,throughput,throughput,3685,https://hail.is,https://github.com/hail-is/hail/pull/13605,1,['throughput'],['throughput']
Performance,ptions=-Xss4M|||spark:spark.executor.extraJavaOptions=-Xss4M|||spark:spark.speculation=true|||hdfs:dfs.replication=1|||dataproc:dataproc.logging.stackdriver.enable=false|||dataproc:dataproc.monitoring.stackdriver.enable=false|||spark:spark.driver.memory=36g|||yarn:yarn.nodemanager.resource.memory-mb=29184|||yarn:yarn.scheduler.maximum-allocation-mb=14592|||spark:spark.executor.cores=4|||spark:spark.executor.memory=5837m|||spark:spark.executor.memoryOverhead=8755m|||spark:spark.memory.storageFraction=0.2|||spark:spark.executorEnv.HAIL_WORKER_OFF_HEAP_MEMORY_PER_CORE_MB=3648' \; 9c9; < --metadata=^|||^WHEEL=gs://hail-30-day/hailctl/dataproc/dking-dev/0.2.126-a51eabd65859/hail-0.2.126-py3-none-any.whl|||PKGS=aiodns==2.0.0|aiohttp==3.9.1|aiosignal==1.3.1|async-timeout==4.0.3|attrs==23.1.0|avro==1.11.3|azure-common==1.1.28|azure-core==1.29.5|azure-identity==1.15.0|azure-mgmt-core==1.4.0|azure-mgmt-storage==20.1.0|azure-storage-blob==12.19.0|bokeh==3.3.1|boto3==1.33.1|botocore==1.33.1|cachetools==5.3.2|certifi==2023.11.17|cffi==1.16.0|charset-normalizer==3.3.2|click==8.1.7|commonmark==0.9.1|contourpy==1.2.0|cryptography==41.0.7|decorator==4.4.2|deprecated==1.2.14|dill==0.3.7|frozenlist==1.4.0|google-auth==2.23.4|google-auth-oauthlib==0.8.0|humanize==1.1.0|idna==3.6|isodate==0.6.1|janus==1.0.0|jinja2==3.1.2|jmespath==1.0.1|jproperties==2.1.1|markupsafe==2.1.3|msal==1.25.0|msal-extensions==1.0.0|msrest==0.7.1|multidict==6.0.4|nest-asyncio==1.5.8|numpy==1.26.2|oauthlib==3.2.2|orjson==3.9.10|packaging==23.2|pandas==2.1.3|parsimonious==0.10.0|pillow==10.1.0|plotly==5.18.0|portalocker==2.8.2|protobuf==3.20.2|py4j==0.10.9.5|pyasn1==0.5.1|pyasn1-modules==0.3.0|pycares==4.4.0|pycparser==2.21|pygments==2.17.2|pyjwt[crypto]==2.8.0|python-dateutil==2.8.2|python-json-logger==2.0.7|pytz==2023.3.post1|pyyaml==6.0.1|regex==2023.10.3|requests==2.31.0|requests-oauthlib==1.3.1|rich==12.6.0|rsa==4.9|s3transfer==0.8.0|scipy==1.11.4|six==1.16.0|sortedcontainers==2.4.0|tabulate==0.9.0|tenacity=,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14127:2095,cache,cachetools,2095,https://hail.is,https://github.com/hail-is/hail/pull/14127,1,['cache'],['cachetools']
Performance,"ptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8)  ~/sandbox/hail [master|8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1613,cache,cached,1613,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"py"", line 551, in pull; await docker_call_retry(; File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 840, in retry_transient_errors_with_debug_string; return await f(*args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 460, in timed_out_f; return await asyncio.wait_for(f(*args, **kwargs), timeout); File ""/usr/lib/python3.9/asyncio/tasks.py"", line 479, in wait_for; return fut.result(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 484, in _pull_with_auth_refresh; return await docker.images.pull(image_ref_str, auth=credentials); File ""/usr/local/lib/python3.9/dist-packages/aiodocker/images.py"", line 133, in _handle_list; async with cm as response:; File ""/usr/local/lib/python3.9/dist-packages/aiodocker/utils.py"", line 309, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.9/dist-packages/aiodocker/docker.py"", line 275, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(500, 'Head ""https://us-docker.pkg.dev/v2/1/does-not-exist/manifests/latest"": denied: Permission ""artifactregistry.repositories.downloadArtifacts"" denied on resource ""projects/1/locations/us/repositories/does-not-exist"" (or it may not exist)'). The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 915, in run; await self.create(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 840, in create; await self._run_until_done_or_deleted(self.image.pull); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1012, in _run_until_done_or_deleted; return await run_until_done_or_deleted(self.deleted_event, f, *args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 682, in run_until_done_or_deleted; return step.result(); F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13907:1335,load,loads,1335,https://hail.is,https://github.com/hail-is/hail/issues/13907,1,['load'],['loads']
Performance,"py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, dill, Deprecated, bokeh, backcall, asyncinit, aiohttp-session, ipython, hail; Successfully installed Deprecated-1.2.12 Jinja2-2.11.3 MarkupSafe-1.1.1 PyJWT-2.0.1 PyYAML-5.4.1 aiohttp-3.7.4 aiohttp-session-2.7.0 asyn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:7388,cache,cached,7388,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2722,cache,cached,2722,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/python3.6/site-packages/hail/backend/backend.py in execute(self, ir, timed); 106 ; 107 def execute(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37. ```. ### Traces No. 1: . ```java ; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 16.0 failed 4 tim",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:4215,load,loads,4215,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['load'],['loads']
Performance,"python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/python3.6/site-packages/hail/backend/backend.py in execute(self, ir, timed); 106 ; 107 def execute(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37; ```. ### Traces No.2:; ```java; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:29200,load,loads,29200,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['load'],['loads']
Performance,"quote>; <h1>v24.2.2</h1>; <ul>; <li>fix: config reader handles bool types (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/pull/218"">#218</a>, <a href=""https://github.com/tomplus""><code>@tomplus</code></a>)</li>; </ul>; <h1>v24.2.1</h1>; <ul>; <li>fixed watch.stream bug of not working with apis with follow kwarg (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/pull/216"">#216</a>, <a href=""https://github.com/mcreng""><code>@mcreng</code></a>)</li>; </ul>; <h1>v24.2.0</h1>; <p>Kubernetes API Version: v1.24.2</p>; <h3>API Change</h3>; <ul>; <li>Add 2 new options for kube-proxy running in winkernel mode. <code>--forward-healthcheck-vip</code>, if specified as true, health check traffic whose destination is service VIP will be forwarded to kube-proxy's healthcheck service. <code>--root-hnsendpoint-name</code> specifies the name of the hns endpoint for the root network namespace. This option enables the pass-through load balancers like Google's GCLB to correctly health check the backend services. Without this change, the health check packets is dropped, and Windows node will be considered to be unhealthy by those load balancers. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/99287"">kubernetes/kubernetes#99287</a>, <a href=""https://github.com/anfernee""><code>@anfernee</code></a>)</li>; <li>Added CEL runtime cost calculation into CustomerResource validation. CustomerResource validation will fail if runtime cost exceeds the budget. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108482"">kubernetes/kubernetes#108482</a>, <a href=""https://github.com/cici37""><code>@cici37</code></a>)</li>; <li>Added a new metric <code>webhook_fail_open_count</code> to monitor webhooks that fail to open. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/107171"">kubernetes/kubernetes#107171</a>, <a href=""https://github.com/ltagliamonte-dd""><code>@ltagliamonte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12196:1272,load,load,1272,https://hail.is,https://github.com/hail-is/hail/pull/12196,1,['load'],['load']
Performance,"quote>; <h2>Version 2.11.3</h2>; <p>Released 2021-01-31</p>; <ul>; <li>Improve the speed of the <code>urlize</code> filter by reducing regex; backtracking. Email matching requires a word character at the start; of the domain part, and only word characters in the TLD. :pr:<code>1343</code></li>; </ul>; <h2>Version 2.11.2</h2>; <p>Released 2020-04-13</p>; <ul>; <li>Fix a bug that caused callable objects with <code>__getattr__</code>, like; :class:<code>~unittest.mock.Mock</code> to be treated as a; :func:<code>contextfunction</code>. :issue:<code>1145</code></li>; <li>Update <code>wordcount</code> filter to trigger :class:<code>Undefined</code> methods; by wrapping the input in :func:<code>soft_str</code>. :pr:<code>1160</code></li>; <li>Fix a hang when displaying tracebacks on Python 32-bit.; :issue:<code>1162</code></li>; <li>Showing an undefined error for an object that raises; <code>AttributeError</code> on access doesn't cause a recursion error.; :issue:<code>1177</code></li>; <li>Revert changes to :class:<code>~loaders.PackageLoader</code> from 2.10 which; removed the dependency on setuptools and pkg_resources, and added; limited support for namespace packages. The changes caused issues; when using Pytest. Due to the difficulty in supporting Python 2 and; :pep:<code>451</code> simultaneously, the changes are reverted until 3.0.; :pr:<code>1182</code></li>; <li>Fix line numbers in error messages when newlines are stripped.; :pr:<code>1178</code></li>; <li>The special <code>namespace()</code> assignment object in templates works in; async environments. :issue:<code>1180</code></li>; <li>Fix whitespace being removed before tags in the middle of lines when; <code>lstrip_blocks</code> is enabled. :issue:<code>1138</code></li>; <li>:class:<code>~nativetypes.NativeEnvironment</code> doesn't evaluate; intermediate strings during rendering. This prevents early; evaluation which could change the value of an expression.; :issue:<code>1186</code></li>; </ul>; <h2>Version 2.1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10209:3794,load,loaders,3794,https://hail.is,https://github.com/hail-is/hail/pull/10209,1,['load'],['loaders']
Performance,"r an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offset into arrayAddress, otherwise returns that address. ## <a name=""parray""></a> PCanonicalArray. A growable array that is accessed by a pointer. ### Structure. Starting at `arrayAddress`:. [`4-byte length`, `n/8 byte missigness data`, `n * elementByteSize byte element data`]. # <a name=""parray""></a> PSet. An abstract class for immutable ordered collections where all elements are unique. ## Core Methods. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutable unordered collections of key-value pairs. All keys must have one PType, and all values must have one (possibly different from keys) PType. ## Core Methods. ```scala; def elementType: PStruct; ```. - The PStruct representation of the key/value pair. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:5203,load,loads,5203,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['load'],['loads']
Performance,"r later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.10.0 - 2024-03-27</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <det",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14427:2208,load,loads,2208,https://hail.is,https://github.com/hail-is/hail/pull/14427,1,['load'],['loads']
Performance,r$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182); 	at scala.Option.foreach(Option.scala:407); 	at org.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9736,Load,LoadVCF,9736,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"r, f). /home/hail/hail.zip/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . /home/hail/hail.zip/hail/table.py in _process_joins(self, *exprs); 1459 def broadcast_f(left, data, jt):; 1460 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1461 return process_joins(self, exprs, broadcast_f); 1462 ; 1463 def cache(self):. /home/hail/hail.zip/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 354 for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; 355 if j not in used_joins:; --> 356 left = j.join_func(left); 357 all_uids.extend(j.temp_vars); 358 used_joins.add(j). /home/hail/hail.zip/hail/table.py in joiner(obj); 1448 else:; 1449 assert isinstance(obj, Table); -> 1450 return Table(Env.jutils().joinGlobals(obj._jt, self._jt, uid)); 1451 ; 1452 ast = Join(Select(TopLevelReference('global', Indices()), uid),. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3728:3554,cache,cache,3554,https://hail.is,https://github.com/hail-is/hail/issues/3728,1,['cache'],['cache']
Performance,r-tmp-4day/o/bge-wave-1-VQSR%2FparallelizeAndComputeWithIndex%2FgCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=%2Fresult.2706?alt=media; No such object: wes-bipolar-tmp-4day/bge-wave-1-VQSR/parallelizeAndComputeWithIndex/gCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=/result.2706. Java stack trace:; is.hail.relocated.com.google.cloud.storage.StorageException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/wes-bipolar-tmp-4day/o/bge-wave-1-VQSR%2FparallelizeAndComputeWithIndex%2FgCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=%2Fresult.2706?alt=media; No such object: wes-bipolar-tmp-4day/bge-wave-1-VQSR/parallelizeAndComputeWithIndex/gCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=/result.2706; 	at is.hail.relocated.com.google.cloud.storage.StorageException.translate(StorageException.java:163); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:297); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.load(HttpStorageRpc.java:730); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.lambda$readAllBytes$24(StorageImpl.java:574); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:60); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1476); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:574); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:563); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:288); 	at is.hail.services.package$.retryTransientErrors(package.scala:163); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:286); 	at is.hail.io.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409:5391,load,load,5391,https://hail.is,https://github.com/hail-is/hail/issues/13409,1,['load'],['load']
Performance,"r==2.8.2|protobuf==3.20.2|py4j==0.10.9.5|pyasn1==0.5.1|pyasn1-modules==0.3.0|pycares==4.4.0|pycparser==2.21|pygments==2.17.2|pyjwt[crypto]==2.8.0|python-dateutil==2.8.2|python-json-logger==2.0.7|pytz==2023.3.post1|pyyaml==6.0.1|regex==2023.10.3|requests==2.31.0|requests-oauthlib==1.3.1|rich==12.6.0|rsa==4.9|s3transfer==0.8.0|scipy==1.11.4|six==1.16.0|sortedcontainers==2.4.0|tabulate==0.9.0|tenacity==8.2.3|tornado==6.3.3|typer==0.9.0|typing-extensions==4.8.0|tzdata==2023.3|urllib3==1.26.18|uvloop==0.19.0;sys_platform!=""win32""|wrapt==1.16.0|xyzservices==2023.10.1|yarl==1.9.3 \; ---; > '--metadata=^|||^WHEEL=gs://hail-30-day/hailctl/dataproc/dking-dev/0.2.126-a51eabd65859/hail-0.2.126-py3-none-any.whl|||PKGS=aiodns==2.0.0|aiohttp==3.9.1|aiosignal==1.3.1|async-timeout==4.0.3|attrs==23.1.0|avro==1.11.3|azure-common==1.1.28|azure-core==1.29.5|azure-identity==1.15.0|azure-mgmt-core==1.4.0|azure-mgmt-storage==20.1.0|azure-storage-blob==12.19.0|bokeh==3.3.1|boto3==1.33.1|botocore==1.33.1|cachetools==5.3.2|certifi==2023.11.17|cffi==1.16.0|charset-normalizer==3.3.2|click==8.1.7|commonmark==0.9.1|contourpy==1.2.0|cryptography==41.0.7|decorator==4.4.2|deprecated==1.2.14|dill==0.3.7|frozenlist==1.4.0|google-auth==2.23.4|google-auth-oauthlib==0.8.0|humanize==1.1.0|idna==3.6|isodate==0.6.1|janus==1.0.0|jinja2==3.1.2|jmespath==1.0.1|jproperties==2.1.1|markupsafe==2.1.3|msal==1.25.0|msal-extensions==1.0.0|msrest==0.7.1|multidict==6.0.4|nest-asyncio==1.5.8|numpy==1.26.2|oauthlib==3.2.2|orjson==3.9.10|packaging==23.2|pandas==2.1.3|parsimonious==0.10.0|pillow==10.1.0|plotly==5.18.0|portalocker==2.8.2|protobuf==3.20.2|py4j==0.10.9.5|pyasn1==0.5.1|pyasn1-modules==0.3.0|pycares==4.4.0|pycparser==2.21|pygments==2.17.2|pyjwt[crypto]==2.8.0|python-dateutil==2.8.2|python-json-logger==2.0.7|pytz==2023.3.post1|pyyaml==6.0.1|regex==2023.10.3|requests==2.31.0|requests-oauthlib==1.3.1|rich==12.6.0|rsa==4.9|s3transfer==0.8.0|scipy==1.11.4|six==1.16.0|sortedcontainers==2.4.0|tabulate==0.9.0|tenacity=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14127:3693,cache,cachetools,3693,https://hail.is,https://github.com/hail-is/hail/pull/14127,1,['cache'],['cachetools']
Performance,rVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-08a1543; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:10061,concurren,concurrent,10061,https://hail.is,https://github.com/hail-is/hail/issues/2743,2,['concurren'],['concurrent']
Performance,rator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:12544,concurren,concurrent,12544,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['concurren'],['concurrent']
Performance,rator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5211,concurren,concurrent,5211,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['concurren'],['concurrent']
Performance,raversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5409,Optimiz,Optimize,5409,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,"rbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`.; - The `query` user was missing from bootstrap-create-accounts.; - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:1783,load,loader,1783,https://hail.is,https://github.com/hail-is/hail/pull/10279,2,['load'],"['load', 'loader']"
Performance,readLocal: perform read before file is closed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3512:11,perform,perform,11,https://hail.is,https://github.com/hail-is/hail/pull/3512,1,['perform'],['perform']
Performance,"readPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13308,Load,LoadVCF,13308,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,"reemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which internal namespaces (dev and PR) are currently active. This requires relatively small changes to CI to track active namespaces but overall is a pretty small change. Note that this does not introduce a dependency on CI to support production traffic, only development traffic.; 3. Deployments that run more than 1 replica (but really can be all of them) are run behind Headless Services, which expose the underlying pod IPs so Envoy can handle load-balancing instead of kube-proxy. This allows Envoy to make smart load-balancing decisions and correctly enforce rate-limiting when using connection pools. The namespace tracking in CI in Point ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:4214,load,load,4214,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['load'],['load']
Performance,remove loadconda,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5905:7,load,loadconda,7,https://hail.is,https://github.com/hail-is/hail/pull/5905,1,['load'],['loadconda']
Performance,remove race condition where we might observe a complete job before the callback is sent.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5861:7,race condition,race condition,7,https://hail.is,https://github.com/hail-is/hail/pull/5861,1,['race condition'],['race condition']
Performance,remove unused LoadVCF.lineRef,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3568:14,Load,LoadVCF,14,https://hail.is,https://github.com/hail-is/hail/pull/3568,1,['Load'],['LoadVCF']
Performance,removed required alleles from LoadPlink,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2962:30,Load,LoadPlink,30,https://hail.is,https://github.com/hail-is/hail/pull/2962,1,['Load'],['LoadPlink']
Performance,"rence_data.drop('END'); + if 'END' in reference_data.entry:; + reference_data = reference_data.drop('END'); + else: # if END is missing, add it, _add_end is a no-op if END is already present; + reference_data = VariantDataset._add_end(reference_data); +; vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; ```. There was nothing in the IR that stood out when I examined it, but I will admit that I'm not the best at digging into it. ### Version. https://github.com/chrisvittal/hail/tree/vds/repro-example. ### Relevant log output. ```shell; E hail.utils.java.FatalError: RuntimeException: invalid memory access: 140a68008/00000001: not in 140a58008/00010000; E; E Java stack trace:; E java.lang.RuntimeException: invalid memory access: 140a68008/00000001: not in 140a58008/00010000; E 	at is.hail.annotations.Memory.checkAddress(Memory.java:226); E 	at is.hail.annotations.Memory.loadByte(Memory.java:130); E 	at is.hail.annotations.Region$.loadByte(Region.scala:28); E 	at is.hail.annotations.Region$.loadBit(Region.scala:86); E 	at __C23148collect_distributed_array_matrix_native_writer.__m23333split_ToArray(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region478_486(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region16_503(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region14_529(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89); E 	",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14705:2582,load,loadByte,2582,https://hail.is,https://github.com/hail-is/hail/issues/14705,1,['load'],['loadByte']
Performance,resolves some memory problems related to task results queueing up.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6823:54,queue,queueing,54,https://hail.is,https://github.com/hail-is/hail/issues/6823,1,['queue'],['queueing']
Performance,rfc-0000: Cache CollectDistributedArray Executions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13194:10,Cache,Cache,10,https://hail.is,https://github.com/hail-is/hail/pull/13194,1,['Cache'],['Cache']
Performance,rg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.sche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9425,Load,LoadVCF,9425,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"rg.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:922); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:915); at is.hail.utils.package$.using(package.scala:577); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:915); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:12129,Load,LoadVCF,12129,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['Load'],['LoadVCF']
Performance,richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAG,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:7626,concurren,concurrent,7626,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['concurren'],['concurrent']
Performance,"riter = ir.MatrixNativeWriter(output, overwrite, stage_locally, _codec_spec, _partitions, _partitions_type); -> 2734 Env.backend().execute(ir.MatrixWrite(self._mir, writer)). File /opt/conda/lib/python3.10/site-packages/hail/backend/backend.py:180, in Backend.execute(self, ir, timed); 178 result, timings = self._rpc(ActionTag.EXECUTE, payload); 179 except FatalError as e:; --> 180 raise e.maybe_user_error(ir) from None; 181 if ir.typ == tvoid:; 182 value = None. File /opt/conda/lib/python3.10/site-packages/hail/backend/backend.py:178, in Backend.execute(self, ir, timed); 176 payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); 177 try:; --> 178 result, timings = self._rpc(ActionTag.EXECUTE, payload); 179 except FatalError as e:; 180 raise e.maybe_user_error(ir) from None. File /opt/conda/lib/python3.10/site-packages/hail/backend/py4j_backend.py:213, in Py4JBackend._rpc(self, action, payload); 211 if resp.status_code >= 400:; 212 error_json = orjson.loads(resp.content); --> 213 raise fatal_error_from_java_error_triplet(error_json['short'], error_json['expanded'], error_json['error_id']); 214 return resp.content, resp.headers.get('X-Hail-Timings', ''). FatalError: HailException: cannot set missing field for required type +PFloat64. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 6.0 failed 4 times, most recent failure: Lost task 5.3 in stage 6.0 (TID 67) (saturn-machinenumber.c.terra-code.internal executor 4): is.hail.utils.HailException: gs://path/to/bucket/chrY.0002.hard_filtered_with_genotypes.vcf.gz:offset 23933331019603: error while parsing line; chrY	113	.	GG	G,*,AG,CG	596	PASS	AC=2,4,6,1;AF=1.23e-03,5.550e-05,4.44e-05,2.00e-04;AN=265;AS_AltDP=10,0,3,10;AS_BaseQRankSum=0.000,.,0.100,0.500;AS_FS=7.777,.,2.144,8.001;AS_MQ=55.75,.,38.98,40.20;AS_MQRankSum=0.200,.,-1.050,-0.500;AS_QD=0.50,0.00,0.25,0.52;AS_ReadPosRankSum=-0.200,.,0.500,-0.220;AS_SOR=2.300,.,1.600,3.000;BaseQRankSum=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:5654,load,loads,5654,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['load'],['loads']
Performance,rk.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5846,Load,LoadMatrix,5846,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"rks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this better; 3. Performance. NodeJS is faster than Flask, React is ~fastest JS view layer. Next makes it really easy to split app into page bundles, and (on localhost) achieves DOMContentLoaded of ~70-100ms, and faster interactivity: first loaded page (the page of the current route) is ~6-10ms.; * [Techempower]: https://www.techempower.com/benchmarks/; * [Node vs , ](https://medium.com/@mihaigeorge.c/web-rest-api-benchmark-on-a-real-life-application-ebb743a5d7a3). * React vs other client side micro bench (pay attention to ""Non-keyed""): https://krausest.github.io/js-framework-benchmark/current.html; 4. Structure, aforementioned; 5. Path to relatively performant desktop and mobile applications, via [Electron](https://getstream.io/blog/takeaways-on-building-a-react-based-app-with-electron/). [Visual Studio Code](https://github.com/Microsoft/vscode) and [Slack](https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview-2759690d9c7b) are good examples. Facebook Messenger written in React Native, which we have an even more straightforward path to.; 6. Low cognitive cost (relative to Angular, others. React is just a view layer, and has a tiny API. I've develop a large application in AngularJS, and have spent a bit of time with Angular2+. There is no comparison: Angular takes months to know well, React days at worst. Also, by not buying into a full framework, we achieve modularity: If we end up finding React too slow, even with [planned 2019 improvements](https://reactjs.org/blog/2018/11/27/react-16-roadmap.html), there are plenty of others view layers we can migrate to, without gutting our entire app. Next makes this somewhat trickier, but s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:3985,perform,performant,3985,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['perform'],['performant']
Performance,"ror_id = tpl._1(), tpl._2(), tpl._3(); ---> 35 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 36 except pyspark.sql.utils.CapturedException as e:; 37 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 38 'Hail version: %s\n'; 39 'Error summary: %s' % (e.desc, e.stackTrace, hail.__version__, e.desc)) from None. FatalError: ClassCastException: class is.hail.types.physical.stypes.concrete.SIndexablePointer cannot be cast to class is.hail.types.physical.stypes.concrete.SJavaArrayString (is.hail.types.physical.stypes.concrete.SIndexablePointer and is.hail.types.physical.stypes.concrete.SJavaArrayString are in unnamed module of loader 'app'). Java stack trace:; java.lang.ClassCastException: class is.hail.types.physical.stypes.concrete.SIndexablePointer cannot be cast to class is.hail.types.physical.stypes.concrete.SJavaArrayString (is.hail.types.physical.stypes.concrete.SIndexablePointer and is.hail.types.physical.stypes.concrete.SJavaArrayString are in unnamed module of loader 'app'); 	at is.hail.expr.ir.functions.RegistryFunctions.unwrapReturn(Functions.scala:364); 	at is.hail.expr.ir.Emit.$anonfun$emitI$85(Emit.scala:1173); 	at is.hail.expr.ir.IEmitCodeGen.map(Emit.scala:352); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:1153); 	at is.hail.expr.ir.streams.EmitStream$.is$hail$expr$ir$streams$EmitStream$$emit$1(EmitStream.scala:148); 	at is.hail.expr.ir.streams.EmitStream$.produce(EmitStream.scala:321); 	at is.hail.expr.ir.Emit.emitStream$2(Emit.scala:821); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:1177); 	at is.hail.expr.ir.Emit.$anonfun$emitSplitMethod$1(Emit.scala:607); 	at is.hail.expr.ir.Emit.$anonfun$emitSplitMethod$1$adapted(Emit.scala:605); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:19); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:29); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1086); 	at is.hail.expr.ir.Emit.emitSplitMethod(Emit.scala:605); 	at is.hail.e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:4470,load,loader,4470,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['load'],['loader']
Performance,"rt). I leave these as future tasks, because enough changes are present in this PR. Summary of changes:; * Refined homepage styles, ensured navbar matches to pixel between docs and hail.is root (surprisingly difficult); * Improved mobile styles, especially mobile nav menu (much smoother animation, larger, easier to click on links); * Optimized icon sizes (50KB -> 3.3KB); * Removed all use of bootstrap on hail.is/*.html pages (bootstrap remains on docs, future pr).; * Removed index.md contents. The markdown format is pretty limited. To have a richly-marked up site with consistent styling, the syntax it provides is not enough. The solution is either to add html to index.md, or just write html in a the index.xslt file. I chose the latter, because it's simpler. Future reorganization should simplify this and docs further, though I think I still recommend NextJS and the build system that provides.; * Added threeR115.min.js. This is regrettably large, but doesn't impact page rendering performance in any meaningful way, because it is loaded after all html content (and is cached after the first visit). Future work can go to webgl directly, potentially. There is also ongoing work by the ThreeJS maintainers to allow tree-shaking and smaller builds.; * Added heavily modified fork of VantaJS. Because we are not using something like NextJS, there is no package manager to rely on, so I just checked the file in manually (I have this in a separate repo, we can use that if preferred). License is in line with a note about modifications. Vanta performs very, very poorly (order of 40% CPU usage, old/slow ways of observing whether animated element is in view, unnecessary object generation, etc), this does not, removes a bunch of totally unnecessary OO abstractions, and provides some additional effects (hover highlighting of vertices), hence the fork. License is MIT, no issue for us.; * Made sure this all works, looks nice with docs. Future works will bring doc style in line with homepage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8634:1327,perform,performance,1327,https://hail.is,https://github.com/hail-is/hail/pull/8634,4,"['cache', 'load', 'perform']","['cached', 'loaded', 'performance', 'performs']"
Performance,"rted due to stage failure: Task 35 in stage 7.0 failed 20 times, most recent failure: Lost task 35.19 in stage 7.0 (TID 6963, gnomad-prod-sw-m8lk.c.broad-mpg-gnomad.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.sca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202:4869,concurren,concurrent,4869,https://hail.is,https://github.com/hail-is/hail/issues/1202,1,['concurren'],['concurrent']
Performance,"ructural since they rarely influence the bytecode generated. Here is an exception, and that's where the method splitter comes in. Method splitting exists in the Hail compiler because not only does the JVM have limits on how large methods can be, but also the JIT compiler handles small methods much more effectively than large methods (and so splitting a large method into two small ones can make an order of magnitude or more in performance difference). We have three forms of method splitting in the Hail Query compiler. The first is a heuristic and greedy IR-level method splitter that generates new methods every X IR nodes, simply based on node count. However, the size of code generated by each IR can vary widely (`I32` vs `LowerBoundOnOrderedCollection` for instance), and so we have two other kinds of splitting that operate on the LIR level. The first is region splitting, which is used to split large blocks of LIR. In order to insert a split, any variables on the stack are stored in local variables before the split and loaded from those locals after the split. The second is method splitting, which is used to split large single methods. A single-exit group of blocks can be split into a separate method, and we have some machinery for replacing control flow instructions (which I will not go into here, for they are not relevant now), as well as handling local variables that are used across a method split. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:2164,load,loaded,2164,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['load'],['loaded']
Performance,"run(Thread.java:748)org.apache.spark.SparkException: Job aborted due to stage failure: Task 754 in stage 1.0 failed 1 times, most recent failure: Lost task 754.0 in stage 1.0 (TID 1625, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:234); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(Ordere",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2407:5045,concurren,concurrent,5045,https://hail.is,https://github.com/hail-is/hail/issues/2407,1,['concurren'],['concurrent']
Performance,"rver will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, in init_server_extensions; Mar 01 19:59:04 dk-m python[5149]: func(self); Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/__init__.py"", line 30, in load_jupyter_server_extension; Mar 01 19:59:04 dk-m python[5149]: from .handlers import SparkHandler; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 8, in <module>; Mar 01 19:59:04 dk-m python[5149]: class SparkHandler(IPythonHandler):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 13, in SparkHandler; Mar 01 19:59:04 dk-m python[5149]: @tornado.web.asynchronous; Mar 01 19:59:04 dk-m python[5149]: AttributeError: module 'tornado.web' has no attribute 'asynchronous'; ```. It appears that Jupyter starts even though on",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5505:1788,load,loading,1788,https://hail.is,https://github.com/hail-is/hail/issues/5505,1,['load'],['loading']
Performance,"ry-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main$.main(Main.scala:14) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 		at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 		at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; 	Caused by: com.google.api.client.http.HttpResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:25257,concurren,concurrent,25257,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"ry:; driver_output = await self._async_fs.open(output_uri); except FileNotFoundError as exc:; raise FatalError('Hail internal error. Please contact the Hail team and provide the following information.\n\n' + yamlx.dump({; 'service_backend_debug_info': self.debug_info(),; 'batch_debug_info': await self._batch.debug_info(); })) from exc; ; async with driver_output as outfile:; success = await read_bool(outfile); if success:; return await read_bytes(outfile); ; short_message = await read_str(outfile); expanded_message = await read_str(outfile); error_id = await read_int(outfile); ; reconstructed_error = fatal_error_from_java_error_triplet(short_message, expanded_message, error_id); if ir is None:; raise reconstructed_error; > raise reconstructed_error.maybe_user_error(ir); E hail.utils.java.FatalError: RuntimeException: Stream is already closed.; E ; E Java stack trace:; E java.util.concurrent.ExecutionException: java.lang.RuntimeException: Stream is already closed.; E 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); E 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); E 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:150); E 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:44); E 	at __C256669Compiled.__m256730split_CollectDistributedArray(Emit.scala); E 	at __C256669Compiled.__m256689split_Let(Emit.scala); E 	at __C256669Compiled.apply(Emit.scala); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:74); E 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:13807,concurren,concurrent,13807,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,"ry>; <ul>; <li><a href=""https://github.com/PyCQA/pylint/commit/95cbd2bd14576cb5d9eade4798e73e8601c884de""><code>95cbd2b</code></a> Bump pylint to 2.13.5, update changelog</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/2e9b33b13264a3cc229e879e7c03b36acd523554""><code>2e9b33b</code></a> Bump black from 22.1.0 to 22.3.0 (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6176"">#6176</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/f251131eaf88c0a6b30983b9ccd8d2924e28fe38""><code>f251131</code></a> Add <code>subclassed-final-class</code> message to the <code>check_messages</code> decorator (#...</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/a03b6e77bef920a7c72be9f3e2c2babddecd2fd2""><code>a03b6e7</code></a> Prevent <code>used-before-assignment</code> for assignment via nonlocal after type annot...</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/074131312977fbd423fe4faff004d4fa8dbba4e5""><code>0741313</code></a> Only emit <code>lru-cache-decorating-method</code> when <code>maxsize</code> is <code>None</code> (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6181"">#6181</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/702474327d7c756b61b82a1805efdd32c2d78ca8""><code>7024743</code></a> Fix false positive for <code>unused-import</code> when disabling both ``used-before-as...</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/4213b3c9a1d4ea7213636b67954dfbd95e290e91""><code>4213b3c</code></a> Fix handling of &quot;for x in x&quot; homonyms (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6154"">#6154</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/22b5dc1ae716e870873ac0b7d8b3369ca9896c38""><code>22b5dc1</code></a> Account for more node types in handling of except block homonyms with compreh...</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/05990167978b3acbb1fbf37b079602a057ee4774""><code>0599016</code></a> <code>redefined-slots-in-subclas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11739:3481,cache,cache-decorating-method,3481,https://hail.is,https://github.com/hail-is/hail/pull/11739,1,['cache'],['cache-decorating-method']
Performance,"s < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/backend/backend.py"", line 108, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/BROAD.MIT.EDU/cvittal/.local/opt/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/utils/java.py"", line 221, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: locus<GRCh37> (of class is.hail.expr.types.virtual.TLocus). Java stack trace:; scala.MatchError: locus<GRCh37> (of class is.hail.expr.types.virtual.TLocus); 	at is.hail.expr.ir.ExtractIntervalFilters$.minimumValueByType(ExtractIntervalFilters.scala:42); 	at is.hail.expr.ir.ExtractIntervalFilters$.openInterval(ExtractIntervalFilters.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:2068,load,loads,2068,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['load'],['loads']
Performance,"s is not the only problem. Each ""upstream"" Service in Kubernetes may consist of multiple underlying pods but Kubernetes Services as we use them don't provide proper load-balancing when mixed with persistent connections. When we declare a Service for say, batch in default, Kubernetes adds a DNS record for `batch.default` that resolves to a single IP pointing at kube-proxy. When a new TCP connection is established with kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:3486,load,load-balance,3486,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['load'],['load-balance']
Performance,"s""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (Let g; (ArrayRef; (Ref global))); (SelectFields (locus alleles); (Ref row)))); 2019-01-08 18:19:48 root: INFO: optimize: after:; (TableCount; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (ArrayRef; (GetField `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (Ref row)); (Ref i))))))); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5100:1335,optimiz,optimize,1335,https://hail.is,https://github.com/hail-is/hail/issues/5100,1,['optimiz'],['optimize']
Performance,s$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5837,Optimiz,Optimize,5837,https://hail.is,https://github.com/hail-is/hail/issues/9128,2,['Optimiz'],['Optimize']
Performance,s$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(D,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9653,Load,LoadVCF,9653,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"s%20only%20(no%2Daddress)%20is%20set%20by%20default%20when%20creating%20a%20Dataproc%202.2%20image%20version%20cluster.%20You%20can%20use%20the%20gcloud%20dataproc%20clusters%20create%20%2D%2Dpublic%2Dip%2Daddress%20flag%20to%20enable%20public%20IP%20addresses.), clusters are created without public internet access by default. A workaround is to pass the `--public-ip-address` flag to the command. Error message:. ```python; pip packages are ['setuptools', 'mkl<2020', 'lxml<5', 'https://github.com/hail-is/jgscm/archive/v0.1.13+hail.zip', 'ipykernel==6.22.0', 'ipywidgets==8.0.6', 'jupyter-console==6.6.3', 'nbconvert==7.3.1', 'notebook==6.5.6', 'qtconsole==5.4.2', 'aiodns==2.0.0', 'aiohttp==3.9.5', 'aiosignal==1.3.1', 'async-timeout==4.0.3', 'attrs==23.2.0', 'avro==1.11.3', 'azure-common==1.1.28', 'azure-core==1.30.2', 'azure-identity==1.17.1', 'azure-mgmt-core==1.4.0', 'azure-mgmt-storage==20.1.0', 'azure-storage-blob==12.20.0', 'bokeh==3.3.4', 'boto3==1.34.138', 'botocore==1.34.138', 'cachetools==5.3.3', 'certifi==2024.6.2', 'cffi==1.16.0', 'charset-normalizer==3.3.2', 'click==8.1.7', 'commonmark==0.9.1', 'contourpy==1.2.1', 'cryptography==42.0.8', 'decorator==4.4.2', 'deprecated==1.2.14', 'dill==0.3.8', 'frozenlist==1.4.1', 'google-auth==2.31.0', 'google-auth-oauthlib==0.8.0', 'humanize==1.1.0', 'idna==3.7', 'isodate==0.6.1', 'janus==1.0.0', 'jinja2==3.1.4', 'jmespath==1.0.1', 'jproperties==2.1.1', 'markupsafe==2.1.5', 'msal==1.29.0', 'msal-extensions==1.2.0', 'msrest==0.7.1', 'multidict==6.0.5', 'nest-asyncio==1.6.0', 'numpy==1.26.4', 'oauthlib==3.2.2', 'orjson==3.10.6', 'packaging==24.1', 'pandas==2.2.2', 'parsimonious==0.10.0', 'pillow==10.4.0', 'plotly==5.22.0', 'portalocker==2.10.0', 'protobuf==3.20.2', 'py4j==0.10.9.7', 'pyasn1==0.6.0', 'pyasn1-modules==0.4.0', 'pycares==4.4.0', 'pycparser==2.22', 'pygments==2.18.0', 'pyjwt==2.8.0', 'python-dateutil==2.9.0.post0', 'python-json-logger==2.0.7', 'pytz==2024.1', 'pyyaml==6.0.1', 'regex==2024.5.15', 'requests==2.32.3'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14652:1219,cache,cachetools,1219,https://hail.is,https://github.com/hail-is/hail/issues/14652,1,['cache'],['cachetools']
Performance,"s. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:4756,load,load,4756,https://hail.is,https://github.com/hail-is/hail/pull/8513,2,['load'],['load']
Performance,s.hail.backend.BackendUtils.$anonfun$collectDArray$16(BackendUtils.scala:91); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90); 	at is.hail.backend.service.Worker$.$anonfun$main$9(Worker.scala:172); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:171); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.service.Worker$.main(Worker.scala:169); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.lang.NullPointerException: null; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionPutTask.call(JsonResumableSessionPutTask.java:201); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.lambda$put$0(JsonResumableSession.java:81); 	at is.hail.relocated.com.google.cloud.storage.Retrying.lambda$run$0(Retrying.java:102); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13937:5313,concurren,concurrent,5313,https://hail.is,https://github.com/hail-is/hail/issues/13937,1,['concurren'],['concurrent']
Performance,s.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.exp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5811,Optimiz,Optimize,5811,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,s.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_matrix_ir$2(SparkBackend.scala:689); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:69); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:69); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.annotations.RegionPool$.scoped(Reg,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:6557,Load,LoadPlink,6557,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,s.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-c37301a; Error summary: IllegalArgumentException: requirement failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465:18282,concurren,concurrent,18282,https://hail.is,https://github.com/hail-is/hail/issues/3465,2,['concurren'],['concurrent']
Performance,s.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:456); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:456); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor90.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.se,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:7761,concurren,concurrent,7761,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['concurren'],['concurrent']
Performance,s.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.net.SocketTimeoutException: connect timed out; E 	at java.net.PlainSocketImpl.socketConnect(Native Method); E 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); E 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); E 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); E 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); E 	at java.net.Socket.connect(Socket.java:607); E 	at is.hail.relocated.org.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75); E 	at is.hail.relocated.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConn,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:7475,concurren,concurrent,7475,https://hail.is,https://github.com/hail-is/hail/issues/13074,1,['concurren'],['concurrent']
Performance,"s: array<str>,; impact: str,; minimised: int32,; regulatory_feature_id: str,; variant_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; appris: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; tsl: int32,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'xpos': int64; 'xstart': int64; 'xstop': int64; ----------------------------------------; Entry fields:; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'GT': call; 'PL': array<int32>; 'BX': array<str>; 'PS': int32; 'PQ': int32; 'JQ': int32; 'MIN_DP': int32; 'PGT': call; 'PID': str; 'RGQ': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 4:===================================================> (480 + 20) / 500]2020-04-05 14:09:48 Hail: INFO: Coerced almost-sorted dataset; [Stage 5:======================================================>(498 + 2) / 500]2020-04-05 14:09:50 Hail: INFO: Coerced almost-sorted dataset; [Stage 7:> (0 + 108) / 500]ERROR: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) failed SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:46930,load,loading-cluster-m,46930,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['load'],['loading-cluster-m']
Performance,"s://redirect.github.com/sphinx-doc/sphinx/issues/11418"">#11418</a>)</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/db546189ce1d2a345f4399367ced6ecdd538be5d""><code>db54618</code></a> Support Docutils 0.20 (<a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11411"">#11411</a>)</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/8942a1dddf2355928f088d6b631db8658034eaae""><code>8942a1d</code></a> Test with Docutils 0.20</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/c9d0933e5d8e34aa9d2c1d88c5a80b46b575730e""><code>c9d0933</code></a> linkcheck: Use context managers for HTTP requests (<a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11318"">#11318</a>)</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/2b1c106bbff5265e8a6076318db5d083c329d575""><code>2b1c106</code></a> Update documentation workflow</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/ba7408209e84ee413f240afc20f3c6b484a81f8f""><code>ba74082</code></a> Change concurrency groups for GitHub workflows</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/b546879539200ec4128bcc6d0ed911ebf28bb3cb""><code>b546879</code></a> Bump version</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/d568b2f4f7cca743fcbf70814d15602d8129b790""><code>d568b2f</code></a> Bump to 7.0.0 final</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/ff79edf353f5cc6e02036f58e0295dc704c5e681""><code>ff79edf</code></a> Remove <code>jsdump</code> references post removal</li>; <li>Additional commits viewable in <a href=""https://github.com/sphinx-doc/sphinx/compare/v6.2.1...v7.0.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinx&package-manager=pip&previous-version=6.2.1&new-version=7.0.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13054:6025,concurren,concurrency,6025,https://hail.is,https://github.com/hail-is/hail/pull/13054,1,['concurren'],['concurrency']
Performance,"scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichR",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:12948,Load,LoadVCF,12948,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,"scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/b09ec92a-49f4-4d16-ad6d-efc5a5805e92/05_variant_qc.py"", line 201, in <module>; cumcounts = {'step0': rt.aggregate(hl.agg.sum(hl.cond(rt.qccum.step0, 1, 0))),; File ""<decorator-gen-519>"", line 2, in aggregate; File ""/home/hail/hail.zip/hail/utils/java.py"", line 191, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3063:11232,concurren,concurrent,11232,https://hail.is,https://github.com/hail-is/hail/issues/3063,1,['concurren'],['concurrent']
Performance,scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3063:4968,concurren,concurrent,4968,https://hail.is,https://github.com/hail-is/hail/issues/3063,2,['concurren'],['concurrent']
Performance,"self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:299); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:2537,load,loadField,2537,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['load'],['loadField']
Performance,"series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:14844,Perform,Performance,14844,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['Perform'],['Performance']
Performance,"set; 2018-01-17 18:47:04 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:2139,load,loadInt,2139,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['load'],['loadInt']
Performance,"sets of intervals. The visual conception of the partitioning of this matrix table (with its globals, column margin data, row margin data, and entry data) might look like:. ```; +--+ +-----+--+--++---+------+------+; | | | | | || | | |; +--+ +-----+--+--++---+------+------+. ck1 ck2 ...; +--+ rk1 +-----+--+--++---+------+------+; | | | | | || | | |; | | | | | || | | |; +--+ rk2 +-----+--+--++---+------+------+; | | | | | || | | |; +--+ ... +-----+--+--++---+------+------+; | | | | | || | | |; | | | | | || | | |; | | | | | || | | |; +--+ +-----+--+--++---+------+------+; | | | | | || | | |; | | | | | || | | |; +--+ +-----+--+--++---+------+------+; +--+ +-----+--+--++---+------+------+; | | | | | || | | |; | | | | | || | | |; | | | | | || | | |; +--+ +-----+--+--++---+------+------+; ```. The first row-key interval is `[rk1, rk2)`. The first col-key interval is `[ck1, ck2)`. These intervals are define a ""rectangle"" corresponding to the first partition. . All the partitions in the fourth partition column are empty (perhaps these column keys are absent in this dataset). Likewise, all the partitions in the fifth partition row are empty. ---. Global values are still global and must be stored in memory for each partition. The columns table becomes a distributed table like the rows table. Its partitioning must match the column partitioning of the blocks. `annotate_cols` and `aggregate_cols` become distributed operations. The rows table is mostly unchanged. `annotate_rows` and `annotate_cols`, when used with aggregation, become symmetric. They both aggregate across partitions as column aggregation did before. `annotate_entries`, `annotate_rows` with aggregation, and `annotate_cols` with aggregation now read the corresponding row and column partition. Each row partition is read once per column block. Similarly for column partitions. In particular, we need not load the entire column table into RAM for each partition. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13800:2737,load,load,2737,https://hail.is,https://github.com/hail-is/hail/issues/13800,1,['load'],['load']
Performance,since the lowering is not actually a C++ step. (also pulled out the table lowering step into an explicit step so that I could add an optimization pass afterwards),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6053:133,optimiz,optimization,133,https://hail.is,https://github.com/hail-is/hail/pull/6053,1,['optimiz'],['optimization']
Performance,"site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5213,cache,cached,5213,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,sk.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:13260,Load,LoadMatrix,13260,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"sn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:7190,cache,cached,7190,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,software.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); ... 18 more; Caused by: java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:5109,load,loadClass,5109,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['load'],['loadClass']
Performance,"spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.Mappark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iteratoadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartiti288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scat org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at ) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.rception: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.jnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.Abst",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:20197,concurren,concurrent,20197,https://hail.is,https://github.com/hail-is/hail/issues/8106,2,['concurren'],['concurrent']
Performance,"spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSet",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:6059,Load,LoadVCF,6059,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,"spilled to a class field on the spills class. Our incorrectly-Boolean local becomes an incorrectly-Boolean **field**, and this is where things go wrong -- it seems as though Boolean class fields (appropriately) truncate on store and load a single bit. Our value of `3` was stored as a class Boolean, and came out `1`. The fact that a single field's missingness was flipped was a red herring -- all higher bits are flipped to 0 (defined)! Here's a look at the LIR looks like, though it was ultimately the JVM class file printout that tipped me off to the problem:. ```code. # I2B is stored as a class field on spills. The Z at the end of the next line indicates this field is a Boolean, not a byte. 31017 (PutFieldX PUTFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 31018 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;); 31019 25774 (InsnX I2B; 31020 25775 (InsnX IOR; 31021 25776 (InsnX IOR; 31022 25777 (InsnX IOR; 31023 25778 (InsnX IOR; 31024 25779 (LdcX 0 I); 31025 25780 (InsnX ISHL; 31026 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2346null Z; 31027 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31028 25782 (LdcX 0 I))); 31029 25783 (InsnX ISHL; 31030 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2348null Z; 31031 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31032 25785 (LdcX 1 I))); 31033 25786 (InsnX ISHL; 31034 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2350null Z; 31035 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31036 25788 (LdcX 2 I))); 31037 25789 (InsnX ISHL; 31038 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills._",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:4241,Load,LoadX,4241,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['Load'],['LoadX']
Performance,"ss org.broadinstitute.hail.driver.Main ***/hail-all-spark.jar --master yarn-client importvcf /user/hail/sample.vcf splitmulti write -o /user/hail/sample_1.vds exportvcf -o /user/hail/sample_1.vcf. Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/solr/client/solrj/SolrClient : Unsupported major.minor version 52.0; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:800); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:449); at java.net.URLClassLoader.access$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). I think this may relate to",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825:1124,load,loadClass,1124,https://hail.is,https://github.com/hail-is/hail/issues/825,1,['load'],['loadClass']
Performance,"ss-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:972); 	at org.apache.hadoop.util.Shell.run(Shell.java:869); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libco",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053:3567,concurren,concurrent,3567,https://hail.is,https://github.com/hail-is/hail/issues/3053,1,['concurren'],['concurrent']
Performance,"st <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-xdist/issues/738"">#738</a> from pytest-dev/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/a25c14bef59ad728e39cabc64f71190aaad73b0a""><code>a25c14b</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/110c114025202d11570737be823de158d1bb8d99""><code>110c114</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-xdist/issues/734"">#734</a> from nicoddemus/revamp-readme</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/83bdbf4b95c914a889d1faa8fba8d506bcc2f8c7""><code>83bdbf4</code></a> Revamp README</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/630c1eb6f2c31dcb4c38c75bb62f868237cdde94""><code>630c1eb</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-xdist/issues/733"">#733</a> from baekdohyeop/feature-loadgroup</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/62e50d00977b41e175b5f119381f9db760459ddc""><code>62e50d0</code></a> Address review</li>; <li>Additional commits viewable in <a href=""https://github.com/pytest-dev/pytest-xdist/compare/v2.2.1...v2.5.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pytest-xdist&package-manager=pip&previous-version=2.2.1&new-version=2.5.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11491:5408,load,loadgroup,5408,https://hail.is,https://github.com/hail-is/hail/pull/11491,2,['load'],['loadgroup']
Performance,"st succeed to install hail on AWS but still have some environment issue:. * I am trying to install Hail v0.2.124; * on AWS EMR v6.9.1 (latest version with Spark 3.3.0 suggested on hail doc); * I upgrade to python 3.9.18; ```sh; $ python --version; Python 3.9.18; ```; I activate java 11.0.20.1; ```sh; $ java -version; openjdk version ""11.0.20.1"" 2023-08-22 LTS; OpenJDK Runtime Environment Corretto-11.0.20.9.1 (build 11.0.20.1+9-LTS); OpenJDK 64-Bit Server VM Corretto-11.0.20.9.1 (build 11.0.20.1+9-LTS, mixed mode); ```; * I clone hail; ```sh; $ cd /tmp; $ git clone --branch 0.2.124 --depth 1 https://github.com/broadinstitute/hail.git; ```; * I build hail; ```sh; $ cd hail/hail/; $ make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.0; [...]; Successfully installed hail-0.2.124; hailctl config set query/backend spark; ```; * At this point Hail seems correcly installed; ```sh; $ pip show hail; Name: hail; Version: 0.2.124; Summary: Scalable library for exploring and analyzing genomic data.; Home-page: https://hail.is; Author: Hail Team; Author-email: hail@broadinstitute.org; License: UNKNOWN; Location: /home/hadoop/.local/lib/python3.9/site-packages; ```; * For sake of configuration I create a symlink of the hail backend; ```sh; sudo ln -sf /home/hadoop/.local/lib/python3.9/site-packages/hail/backend /opt/hail/backend; ```; * Confident of the. installation I try to run spark shell; ```sh; $ spark-shell; [...]; Exception in thread ""main"" java.lang.NoSuchMethodError: 'scala.reflect.internal.settings.MutableSettings ; ```. I am out of idea on how to solve the current situation. ; Thanks. ### Version. 0.2.124. ### Relevant log output. ```shell; $ spark-shell; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignorin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837:1026,Scalab,Scalable,1026,https://hail.is,https://github.com/hail-is/hail/issues/13837,1,['Scalab'],['Scalable']
Performance,"st, I took the JAR URL out of the ""command"" of the job spec. This ""command"" is just an array of strings. The fact that certain parts of that array *must* be the JAR URL and the SHA-1 is confusing. Instead, there are now two keys in a JVM process specification:; 1. `jar_spec`, which may be either `{""type"": ""jar_url"", ""value"": ""gs://..../abc123....jar""}` or `{""type"":""git_revision"", ""value"": ""abc123...""}`.; 2. `argv`, an opaque list of strings which are passed, by the JVMEntryway, along with a few more args, to `is.hail.backend.service.Main`. The `Main` class dispatches to either `ServiceBackendSocketAPI2` or the `Worker` based on the first element of `argv`. Each class expects different contents in `argv` that suits its needs. Second, I completely eliminated the HAIL_SHA/revision from the Worker and Hail Query Java code. This was only ever used as unique name for the JAR. Instead, I just use the full JAR URL as a unique name for the JAR. If you need to defeat the cache, just create a new git commit before running `make -C query ipython`. If defeating the cache becomes a common problem, we can add a ""reload_jar"" parameter or similar to the job spec. Third, I renamed `push-jar` in `query/Makefile` to `upload-query-jar` to mirror the build.yaml step. Fourth, I embraced the use of `NAMEPSACE` in `query/Makefile` instead of relying on the minor hack that our laptop usernames match our namespace names. This does mean you need to always specify NAMESPACE when uploading a jar. Finally, a pleasant outcome of this change is the elimination of a bunch of conditional build.yaml logic in the service backend tests!. I think this will simplify the use of Hail Query by Australia et al. because I've isolated the use of hail-specific data to `query/Makefile`. If there's a way to access the relevant global-config variables from `query/Makefile`, I can also fix the `query/Makefile` to be deployment-independent. cc: @lgruen @illusional @tpoterba . [1] For our default namespace deployment,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11645:2272,cache,cache,2272,https://hail.is,https://github.com/hail-is/hail/pull/11645,1,['cache'],['cache']
Performance,"stacked on #7446. This is the first step toward broad integration of the timer throughout the compiler. This gives us output like:; ```; 2019-11-06 18:44:11 root: INFO: Timer: all timings:; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 5.811ms, total 29.474ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:259,Optimiz,Optimize,259,https://hail.is,https://github.com/hail-is/hail/pull/7476,6,['Optimiz'],['Optimize']
Performance,"stination.; 1998 """"""; 2000 hl.current_backend().validate_file(output); -> 2002 Env.backend().execute(; 2003 ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec)); 2004 ). File ~/projects/hail/hail/python/hail/backend/backend.py:190, in Backend.execute(self, ir, timed); 188 result, timings = self._rpc(ActionTag.EXECUTE, payload); 189 except FatalError as e:; --> 190 raise e.maybe_user_error(ir) from None; 191 if ir.typ == tvoid:; 192 value = None. File ~/projects/hail/hail/python/hail/backend/backend.py:188, in Backend.execute(self, ir, timed); 186 payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); 187 try:; --> 188 result, timings = self._rpc(ActionTag.EXECUTE, payload); 189 except FatalError as e:; 190 raise e.maybe_user_error(ir) from None. File ~/projects/hail/hail/python/hail/backend/py4j_backend.py:223, in Py4JBackend._rpc(self, action, payload); 221 if resp.status_code >= 400:; 222 error_json = orjson.loads(resp.content); --> 223 raise fatal_error_from_java_error_triplet(; 224 error_json['short'], error_json['expanded'], error_json['error_id']; 225 ); 226 return resp.content, resp.headers.get('X-Hail-Timings', ''). FatalError: NoSuchElementException: Ref with name __iruid_1834 could not be resolved in env BindingEnv((__iruid_1832 -> struct{},__iruid_2157 -> struct{}),None,None,()). Java stack trace:; is.hail.utils.HailException: error after applying LowerToDistributedArray; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:23); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:23); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:32); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245:2903,load,loads,2903,https://hail.is,https://github.com/hail-is/hail/issues/14245,1,['load'],['loads']
Performance,"stomers and partners can automate and simplify their SAP system administration tasks such as backup/restore of SAP HANA.</li>; <li>api-change:<code>stepfunctions</code>: [<code>botocore</code>] Update stepfunctions client to latest version</li>; <li>api-change:<code>transfer</code>: [<code>botocore</code>] Adds a NONE encryption algorithm type to AS2 connectors, providing support for skipping encryption of the AS2 message body when a HTTPS URL is also specified.</li>; </ul>; <h1>1.26.12</h1>; <ul>; <li>api-change:<code>amplify</code>: [<code>botocore</code>] Adds a new value (WEB_COMPUTE) to the Platform enum that allows customers to create Amplify Apps with Server-Side Rendering support.</li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow simplifies the preparation and cataloging of SaaS data into the AWS Glue Data Catalog where your data can be discovered and accessed by AWS analytics and ML services. AppFlow now also supports data field partitioning and file size optimization to improve query performance and reduce cost.</li>; <li>api-change:<code>appsync</code>: [<code>botocore</code>] This release introduces the APPSYNC_JS runtime, and adds support for JavaScript in AppSync functions and AppSync pipeline resolvers.</li>; <li>api-change:<code>dms</code>: [<code>botocore</code>] Adds support for Internet Protocol Version 6 (IPv6) on DMS Replication Instances</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/f38ce50a317baf6715870b2706100d43b80b0c73""><code>f38ce50</code></a> Merge branch 'release-1.26.16'</li>; <li><a href=""https://github.com/boto/boto3/commit/33d7d6f020510890b93edf49de3f81c0ba208cb3""><code>33d7d6f</code></a> Bumping version to 1.26.16</li>; <li><a href=""https://github.com/boto/boto3/commit/fb642196bd5dda0f48636e3eeae5f983835fcef5""><code>fb64219</code></a> Add changelog entries from botocore</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12502:6171,optimiz,optimization,6171,https://hail.is,https://github.com/hail-is/hail/pull/12502,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"stomers and partners can automate and simplify their SAP system administration tasks such as backup/restore of SAP HANA.</li>; <li>api-change:<code>stepfunctions</code>: [<code>botocore</code>] Update stepfunctions client to latest version</li>; <li>api-change:<code>transfer</code>: [<code>botocore</code>] Adds a NONE encryption algorithm type to AS2 connectors, providing support for skipping encryption of the AS2 message body when a HTTPS URL is also specified.</li>; </ul>; <h1>1.26.12</h1>; <ul>; <li>api-change:<code>amplify</code>: [<code>botocore</code>] Adds a new value (WEB_COMPUTE) to the Platform enum that allows customers to create Amplify Apps with Server-Side Rendering support.</li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow simplifies the preparation and cataloging of SaaS data into the AWS Glue Data Catalog where your data can be discovered and accessed by AWS analytics and ML services. AppFlow now also supports data field partitioning and file size optimization to improve query performance and reduce cost.</li>; <li>api-change:<code>appsync</code>: [<code>botocore</code>] This release introduces the APPSYNC_JS runtime, and adds support for JavaScript in AppSync functions and AppSync pipeline resolvers.</li>; <li>api-change:<code>dms</code>: [<code>botocore</code>] Adds support for Internet Protocol Version 6 (IPv6) on DMS Replication Instances</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds a new optional parameter &quot;privateIpAddress&quot; for the CreateNatGateway API. PrivateIPAddress will allow customers to select a custom Private IPv4 address instead of having it be auto-assigned.</li>; <li>api-change:<code>elbv2</code>: [<code>botocore</code>] Update elbv2 client to latest version</li>; <li>api-change:<code>emr-serverless</code>: [<code>botocore</code>] Adds support for AWS Graviton2 based applications. You can now select CPU architecture when creating new applications or updating exist",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12498:5700,optimiz,optimization,5700,https://hail.is,https://github.com/hail-is/hail/pull/12498,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,stractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5741,Load,LoadPlink,5741,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,support TableHead(TableOrderBy) optimization with descending sort,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6758:32,optimiz,optimization,32,https://hail.is,https://github.com/hail-is/hail/issues/6758,1,['optimiz'],['optimization']
Performance,t is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.Lowering,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5975,Optimiz,Optimize,5975,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,"t org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anon",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13142,Load,LoadVCF,13142,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,t org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apach,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9491,Load,LoadVCF,9491,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"t sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf spark.executor.memory=40G \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/backend/hail-all-spark.jar\"" \; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:18615,load,load,18615,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['load'],['load']
Performance,t(Kryo.java:651); at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:270); at org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:321); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:323); at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:140); at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95); at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34); at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:75); at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1539); at is.hail.backend.spark.SparkBackend.broadcast(SparkBackend.scala:411); at is.hail.io.plink.MatrixPLINKReader.executeGeneric(LoadPlink.scala:390); at is.hail.io.plink.MatrixPLINKReader.lower(LoadPlink.scala:561); at is.hail.expr.ir.TableReader.lower(TableIR.scala:663); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1062); at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:728); at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1021); at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:27); at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:11); at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:91); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:27); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:4767,Load,LoadPlink,4767,https://hail.is,https://github.com/hail-is/hail/issues/14168,3,['Load'],['LoadPlink']
Performance,t.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Itera,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:10508,Load,LoadMatrix,10508,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"t.scala:65); at is.hail.backend.service.ServiceBackend.$anonfun$withExecuteContext$1(ServiceBackend.scala:426); at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); at is.hail.backend.service.ServiceBackend.withExecuteContext(ServiceBackend.scala:415); at is.hail.backend.service.ServiceBackendAPI.$anonfun$doAction$1(ServiceBackend.scala:608); at is.hail.services.package$.retryTransientErrors(package.scala:186); at is.hail.backend.service.ServiceBackendAPI.doAction(ServiceBackend.scala:585); at is.hail.backend.service.ServiceBackendAPI.executeOneCommand(ServiceBackend.scala:662); at is.hail.backend.service.ServiceBackendAPI$.main(ServiceBackend.scala:497); at is.hail.backend.service.Main$.main(Main.scala:10); at is.hail.backend.service.Main.main(Main.scala); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.base/java.lang.reflect.Method.invoke(Method.java:566); at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515); at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264); at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515); at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.128-17247d8990c6; Error summary: RuntimeException: IR is.hail.expr.ir.StreamFlatMap of type stream<struct{oldContext: str, nRows: int64, nCols: int64}> is not realizable; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14537:11478,concurren,concurrent,11478,https://hail.is,https://github.com/hail-is/hail/issues/14537,6,['concurren'],['concurrent']
Performance,"t_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2620,cache,cached,2620,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"table_location). ### determine the file locations of the pca variants; if (generate_contig_row_dict):; mt = hl.methods.import_bgen(bgen_files,; [],; contig_recoding=contigs,; _row_fields=['file_row_idx']); pca_rows = mt.filter_rows(hl.is_defined(pcloadings[mt.row_key])).rows(); print('about to collect'); # remove all unnecessary data, dropping keys and other irrelevant fields; pca_rows = pca_rows.key_by(); pca_rows = pca_rows.select(pca_rows.locus.contig, pca_rows.file_row_idx); contig_row_list = pca_rows.collect(); print('finished collecting'); contig_reformed = [(x['contig'], x['file_row_idx']) for x in contig_row_list]; print('reformed'); from collections import defaultdict; contig_row_dict = defaultdict(list); for k, v in contig_reformed:; contig_row_dict[k].append(v); print('dictionary created'). with hl.hadoop_open(contig_row_dict_location, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3953:2936,load,load,2936,https://hail.is,https://github.com/hail-is/hail/issues/3953,1,['load'],['load']
Performance,"tage 5:=====================================================> (344 + 8) / 352]; 	[PASS] with 352 partitions: (50000, 1000); 	2020-06-10 10:30:13 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 7:=================================> (222 + 80) / 353]; 	[PASS] with 353 partitions: (50000, 973); 	2020-06-10 10:30:15 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 9:> (0 + 18) / 18]; 	[FAIL] with 354 partitions; 	Traceback (most recent call last):; 	 File ""test_11_cluster_sampleqc.py"", line 20, in <module>; 		print(""\n[PASS] with"", N, ""partitions:"", Y.count()); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/matrixtable.py"", line 2426, in count; 		return Env.backend().execute(count_ir); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 296, in execute; 		result = json.loads(self._jhc.backend().executeJSON(jir)); 	 File ""/bmrn/apps/spark/2.4.5/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 41, in deco; 		'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 	hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutput",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:3354,load,loads,3354,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['load'],['loads']
Performance,tageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1234); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAG,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:5494,concurren,concurrent,5494,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['concurren'],['concurrent']
Performance,"tandard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:4633,load,load,4633,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['load'],['load']
Performance,"tarting a dataproc cluster with VEP, e.g.; ```{bash}; hailctl dataproc start hail-test --region australia-southeast1 --project my-project --vep GRCh38 --packages gnomad --num-workers 2; ```; the dataproc cluster command would be provided the following environment variable through the `--metadata` flag: `VEP_REPLICATE=aus-sydney`. This variable is used within the script `gs://hail-common/hailctl/dataproc/0.2.115/vep-GRCh38.sh` to determine which bucket to pull the VEP cache data from. In more recent versions (tested with 0.2.130), this `VEP_REPLICATE` variable has been changed to `VEP_REPLICATE=australia-southeast1`, however the Australian bucket containing the VEP cache data is still `aus-sydney`, meaning that the VEP data is not copied into the dataproc cluster, and when trying to run VEP I get the error `No cache found for homo_sapiens, version 95`. ### Version. 0.2.130. ### Relevant log output. ```shell; FatalError: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:1103,cache,cache,1103,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['cache'],['cache']
Performance,"tation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutable unordered collections of key-value pairs. All keys must have one PType, and all values must have one (possibly different from keys) PType. ## Core Methods. ```scala; def elementType: PStruct; ```. - The PStruct representation of the key/value pair. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalDict. A PCanonicalArray-backed implementation of PDict. # <a name=""parray""></a> PNDArray. An abstract class for multidimensional arrays (tensors) that have a row-major or column-major layout. ## Core Methods. ```scala; val shape: StaticallyKnownField[PTuple, Long]; val strides: StaticallyKnownField[PTuple, Long]; ```. - Defines the tensor shape. ```scala; def loadElementToIRIntermediate(indices: Array[Code[Long]], ndAddress: Code[Long], mb: MethodBuilder): Code[_]; ```. - Load the element's primitive representation, as indexed by `indices`, which specifies the element index at every dimension in the PNDArray's shape. ```scala; def linearizeIndicesRowMajor(indices: Array[Code[Long]], shapeArray: Array[Code[Long]], mb: MethodBuilder): Code[Long]; ```. - Get the off-heap index of the element (since NDArray elements are stored as a 1D series of bytes off-heap). ```scala; def unlinearizeIndexRowMajor(index: Code[Long], shapeArray: Array[Code[Long]], mb: MethodBuilder): (Code[Unit], Array[Code[Long]]); ```. - Generate the index path that represents the virtual, shape-dependent index into an arbitrary tensor. ```scala; def copyRowMajorToColumnMajor(rowMajorAddress: Code[Long], targetAddress: Code[Long], nRows: Code[Long], nCols: Code[Long], mb: MethodBuilder): Code[Unit]. def copyColumnMajorToRowMajor(colMajorAddress: Code[Long], targetAddress: Code[Long], nRows: Code[Long], nCols: Code[Long], mb: MethodBuilder): Code[Unit]; ```. - Interconvert between column and row major. ```scala; def construct(flags: Code[Int], offset: Code[In",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:6727,Load,Load,6727,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['Load'],['Load']
Performance,"tation</strong></p>; <ul>; <li>Various typo fixes and doc improvements.</li>; </ul>; <p><strong>Packaging</strong></p>; <ul>; <li>Requests has started adopting some modern packaging practices.; The source files for the projects (formerly <code>requests</code>) is now located; in <code>src/requests</code> in the Requests sdist. (<a href=""https://redirect.github.com/psf/requests/issues/6506"">#6506</a>)</li>; <li>Starting in Requests 2.33.0, Requests will migrate to a PEP 517 build system; using <code>hatchling</code>. This should not impact the average user, but extremely old; versions of packaging utilities may have issues with the new packaging format.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/psf/requests/commit/d6ebc4a2f1f68b7e355fb7e4dd5ffc0845547f9f""><code>d6ebc4a</code></a> v2.32.0</li>; <li><a href=""https://github.com/psf/requests/commit/9a40d1277807f0a4f26c9a37eea8ec90faa8aadc""><code>9a40d12</code></a> Avoid reloading root certificates to improve concurrent performance (<a href=""https://redirect.github.com/psf/requests/issues/6667"">#6667</a>)</li>; <li><a href=""https://github.com/psf/requests/commit/0c030f78d24f29a459dbf39b28b4cc765e2153d7""><code>0c030f7</code></a> Merge pull request <a href=""https://redirect.github.com/psf/requests/issues/6702"">#6702</a> from nateprewitt/no_char_detection</li>; <li><a href=""https://github.com/psf/requests/commit/555b870eb19d497ddb67042645420083ec8efb02""><code>555b870</code></a> Allow character detection dependencies to be optional in post-packaging steps</li>; <li><a href=""https://github.com/psf/requests/commit/d6dded3f00afcf56a7e866cb0732799045301eb0""><code>d6dded3</code></a> Merge pull request <a href=""https://redirect.github.com/psf/requests/issues/6700"">#6700</a> from franekmagiera/update-redirect-to-invalid-uri-test</li>; <li><a href=""https://github.com/psf/requests/commit/bf24b7d8d17da34be720c19e5978b2d3bf94a53b""><code>bf24b7d</code></a> Use an ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14555:7816,concurren,concurrent,7816,https://hail.is,https://github.com/hail-is/hail/pull/14555,2,"['concurren', 'perform']","['concurrent', 'performance']"
Performance,"te `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. 7. BLAS and LAPACK need to be thread-local. 8. The LSM-tree used by the Shuffler needs to permit multiple values for the same key. There was; some subtlety here around fine-grained locking. Unfortunately, the LSM doesn't expose the right; operations (putIfAbsent) to make this easy, so I had to use a concurrent hash map of locks. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - Remove the auto-scaling and remove the k8s grace period. Neither of these really works right. We; will live without the auto-scaling for now. I have to fix the grace period thing before we let; users run real pipelines.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10390:3590,concurren,concurrent,3590,https://hail.is,https://github.com/hail-is/hail/pull/10390,1,['concurren'],['concurrent']
Performance,"te sphinx requirement (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/175"">#175</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/f4e11b7223e546515e99c984f9948b6caa06622a""><code>f4e11b7</code></a> chore: update version</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/bf4a9a8eb24149cd68efbc6ae61a6445121f4b70""><code>bf4a9a8</code></a> chore: update setup.py</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/f028cc793e3a2c519be6a52a49fb77ff0b014c9b""><code>f028cc7</code></a> [feat] regenerate client for v1.19.15 (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/172"">#172</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/c876bce774cebdb1eec90c8c957a7b45ec3c1404""><code>c876bce</code></a> chore: update changlog</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/e625e8d296aa8f68d5b0a285e0414c43877f63f5""><code>e625e8d</code></a> [feat] Load kubeconfig from dict (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/169"">#169</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/1722b0b9e19d8f25eb653a2f27bdc97c28a1c713""><code>1722b0b</code></a> chore(deps): bump actions/setup-python from 2.3.0 to 2.3.1 (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/168"">#168</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/tomplus/kubernetes_asyncio/compare/v9.1.0...v19.15.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=kubernetes-asyncio&package-manager=pip&previous-version=9.1.0&new-version=19.15.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:16110,Load,Load,16110,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['Load'],['Load']
Performance,"te(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 840, in create; await self._run_until_done_or_deleted(self.image.pull); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1014, in _run_until_done_or_deleted; raise ContainerDeletedError from e; ContainerDeletedError. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1887, in run_container; await container.run(on_completion); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 920, in run; await on_completion(*args, **kwargs); File ""/usr/lib/python3.9/contextlib.py"", line 137, in __exit__; self.gen.throw(typ, value, traceback); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 1154, in step; yield; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1873, in on_completion; await self.worker.fs.read(container.log_path),; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/fs/fs.py"", line 281, in read; async with await self.open(url) as f:; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/router_fs.py"", line 76, in open; return await fs.open(url); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 252, in open; f = await blocking_to_async(self._thread_pool, open, self._get_path(url), 'rb'); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 181, in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 58, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 182, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); FileNotFoundError: [Errno 2] No such file or directory: '/batch/35055eff18f547de9c77b9e80744e362/main/container.log'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13906:2489,concurren,concurrent,2489,https://hail.is,https://github.com/hail-is/hail/issues/13906,1,['concurren'],['concurrent']
Performance,teBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(Low,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5449,Optimiz,Optimize,5449,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,teBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5440,Optimiz,Optimize,5440,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Optimiz'],['Optimize']
Performance,teOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3235:5512,concurren,concurrent,5512,https://hail.is,https://github.com/hail-is/hail/issues/3235,1,['concurren'],['concurrent']
Performance,"tegories, new fields to the API response, &quot;aliases&quot; and &quot;categories&quot;</li>; <li>api-change:<code>securityhub</code>: [<code>botocore</code>] Documentation updates for Security Hub</li>; <li>api-change:<code>ssm-incidents</code>: [<code>botocore</code>] RelatedItems now have an ID field which can be used for referencing them else where. Introducing event references in TimelineEvent API and increasing maximum length of &quot;eventData&quot; to 12K characters.</li>; </ul>; <h1>1.26.7</h1>; <ul>; <li>api-change:<code>autoscaling</code>: [<code>botocore</code>] This release adds a new price capacity optimized allocation strategy for Spot Instances to help customers optimize provisioning of Spot Instances via EC2 Auto Scaling, EC2 Fleet, and Spot Fleet. It allocates Spot Instances based on both spare capacity availability and Spot Instance price.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds a new price capacity optimized allocation strategy for Spot Instances to help customers optimize provisioning of Spot Instances via EC2 Auto Scaling, EC2 Fleet, and Spot Fleet. It allocates Spot Instances based on both spare capacity availability and Spot Instance price.</li>; <li>api-change:<code>ecs</code>: [<code>botocore</code>] This release adds support for task scale-in protection with updateTaskProtection and getTaskProtection APIs. UpdateTaskProtection API can be used to protect a service managed task from being terminated by scale-in events and getTaskProtection API to get the scale-in protection status of a task.</li>; <li>api-change:<code>es</code>: [<code>botocore</code>] Amazon OpenSearch Service now offers managed VPC endpoints to connect to your Amazon OpenSearch Service VPC-enabled domain in a Virtual Private Cloud (VPC). This feature allows you to privately access OpenSearch Service domain without using public IPs or requiring traffic to traverse the Internet.</li>; <li>api-change:<code>resource-explorer-2</code>: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12458:2342,optimiz,optimized,2342,https://hail.is,https://github.com/hail-is/hail/pull/12458,4,['optimiz'],"['optimize', 'optimized']"
Performance,terOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed. /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:477: FatalError; ------------------------------ Captured log call -------------------------------; INFO batch_client.aioclient:aioclient.py:753 created batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:7,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:22980,concurren,concurrent,22980,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,"terOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed.; ```. ### Version. 0.2.115-f6017673dbb6. ### Relevant log output. ```shell; ________________________________ test_spectra_4 ________________________________; [gw2] linux -- Python 3.8.10 /usr/bin/python3. def test_spectra_4():; > spectra_helper(spec4). test/hail/methods/test_pca.py:229: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/hail/methods/test_pca.py:172: in spectra_helper; hail_V = (np.array(scores.scores.collect()) / singulars).T; <decorator-gen-538>:2: in collect; ???; /usr/local/lib/python3.8/dist-packages/hail/typecheck/check.py:584: in wrapper; return __original_func(*args_, **kwargs_); /usr/local/lib/python3.8/dist-packages/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:9362,concurren,concurrent,9362,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,"teratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Under ""Failed Stages"", these were the details for what I was running:; ```; org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:456); is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:433); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:285); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:21); is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:243); is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); is.hail.rvd.RVD$class.take(RVD.scala:247); is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); is.hail.table.Table.take(Table.scala:990); is.hail.table.Table.showString(Table.scala:1031); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.refle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508:6043,concurren,concurrent,6043,https://hail.is,https://github.com/hail-is/hail/issues/3508,1,['concurren'],['concurrent']
Performance,teratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3516:3233,concurren,concurrent,3233,https://hail.is,https://github.com/hail-is/hail/issues/3516,1,['concurren'],['concurrent']
Performance,"tes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital. Re-run analyses without the full available code. See if they can replicate (not p-values, but order). Could generate multiple-hypothesis-tes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:8146,perform,performed,8146,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['perform'],['performed']
Performance,"test-dev/pytest-xdist/blob/master/CHANGELOG.rst"">pytest-xdist's changelog</a>.</em></p>; <blockquote>; <h1>pytest-xdist 2.5.0 (2021-12-10)</h1>; <h2>Deprecations and Removals</h2>; <ul>; <li><code>[#468](https://github.com/pytest-dev/pytest-xdist/issues/468) &lt;https://github.com/pytest-dev/pytest-xdist/issues/468&gt;</code>_: The <code>--boxed</code> command line argument is deprecated. Install pytest-forked and use <code>--forked</code> instead. pytest-xdist 3.0.0 will remove the <code>--boxed</code> argument and pytest-forked dependency.</li>; </ul>; <h2>Features</h2>; <ul>; <li>; <p><code>[#722](https://github.com/pytest-dev/pytest-xdist/issues/722) &lt;https://github.com/pytest-dev/pytest-xdist/issues/722&gt;</code>_: Full compatibility with pytest 7 - no deprecation warnings or use of legacy features.</p>; </li>; <li>; <p><code>[#733](https://github.com/pytest-dev/pytest-xdist/issues/733) &lt;https://github.com/pytest-dev/pytest-xdist/issues/733&gt;</code>_: New <code>--dist=loadgroup</code> option, which ensures all tests marked with <code>@pytest.mark.xdist_group</code> run in the same session/worker. Other tests run distributed as in <code>--dist=load</code>.</p>; </li>; </ul>; <h2>Trivial Changes</h2>; <ul>; <li>; <p><code>[#708](https://github.com/pytest-dev/pytest-xdist/issues/708) &lt;https://github.com/pytest-dev/pytest-xdist/issues/708&gt;</code>_: Use <code>@pytest.hookspec</code> decorator to declare hook options in <code>newhooks.py</code> to avoid warnings in <code>pytest 7.0</code>.</p>; </li>; <li>; <p><code>[#719](https://github.com/pytest-dev/pytest-xdist/issues/719) &lt;https://github.com/pytest-dev/pytest-xdist/issues/719&gt;</code>_: Use up-to-date <code>setup.cfg</code>/<code>pyproject.toml</code> packaging setup.</p>; </li>; <li>; <p><code>[#720](https://github.com/pytest-dev/pytest-xdist/issues/720) &lt;https://github.com/pytest-dev/pytest-xdist/issues/720&gt;</code>_: Require pytest&gt;=6.2.0.</p>; </li>; <li>; <p><code>[#721](https://",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11491:1175,load,loadgroup,1175,https://hail.is,https://github.com/hail-is/hail/pull/11491,2,['load'],['loadgroup']
Performance,"that the local filesystem can, infrequently, stall when executing `rmtree`. Note that the error about the directory being non-empty is because we have a bug in `rm_dir`: we try to remove the directory even if the children tasks failed. It oddly seems to have happened on both a deploy batch and a PR batch:; - PR: https://ci.hail.is/batches/7706444/jobs/170; - deploy: https://ci.hail.is/batches/7707793/jobs/172. ```; [2023-08-02 05:33:14] test/hail/utils/test_hl_hadoop_and_hail_fs.py::test_hadoop_methods_3[local] PASSED; +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_1 (139802083059456) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_0 (139802091452160) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-0_0 (139802205742848) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = wo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:1017,concurren,concurrent,1017,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,"the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:1587,load,load,1587,https://hail.is,https://github.com/hail-is/hail/pull/10279,1,['load'],['load']
Performance,"the current structure is easier to follow:. 1. Fit the null model.; 2. If wald, assume the beta for the genotypes is zero and use the rest of the parameters from the null model fit to compute the score (i.e. the gradient of the likelihood). Recall calculus: gradient near zero => value near the maximum. Return: this is the test.; 3. Otherwise, fit the full model starting at the null fit parameters.; 4. Test the ""goodness"" of this new & full fit. ---. Poisson regression is similar but with a different likelihood function and gradient thereof. Notice that I `key_cols_by()` to indicate to Hail that the order of the cols is irrelevant (the result is a locus-keyed table after all). This is necessary at least until #12753 merges. I think it's generally a good idea though: it indicates to Hail that the ordering of the columns is irrelevant, which is potentially useful information for the optimizer!. ---. Both logistic and Poisson regression can benefit from BLAS3 by running at least the score test for multiple variants at once. ---. I'll attach an image in the comments, but I spend ~6 seconds compiling this trivial model and ~140ms testing it. ```python3; import hail as hl; mt = hl.utils.range_matrix_table(1, 3); mt = mt.annotate_entries(x=hl.literal([1, 3, 10, 5])); ht = hl.poisson_regression_rows(; 'wald', y=hl.literal([0, 1, 1, 0])[mt.col_idx], x=mt.x[mt.col_idx], covariates=[1], max_iterations=2); ht.collect(); ```. I grabbed some [sample code from; scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PoissonRegressor.html) for Poisson regression (doing a score test rather than a wald test) and timed it. It takes ~8ms. So we're 3 orders of magnitude including the compiler, and ~1.2 orders of magnitude off without the compiler. Digging in a bit:; - ~65ms for class loading.; - ~15ms for region allocation.; - ~20ms various little spots. Leaving about 40ms strictly executing generated code That's about 5x which is starting to feel reasonable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12793:2832,load,loading,2832,https://hail.is,https://github.com/hail-is/hail/pull/12793,1,['load'],['loading']
Performance,"the following tests causes a segfault:. ```python; def test_agg_table_take(self):; ht = hl.utils.range_table(10).annotate(x = 'a'); ht.aggregate(agg.take(ht.x, 2)); ```. *only* as long as you run the test `test_init_hail_context_twice` in the same execution, i.e. ```; hail/python $ pytest -k 'test_init_hail_context_twice or test_agg_table_take'; platform darwin -- Python 3.6.0, pytest-4.5.0, py-1.8.0, pluggy-0.12.0; collected 653 items / 651 deselected / 2 selected ; test/hail/test_context.py . [ 50%]; test/hail/expr/test_expr.py F [100%]. .... # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ac8bbe2, pid=92110, tid=0x0000000000013d03; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 5335 C1 is.hail.annotations.Region$.loadInt(J)I (5 bytes) @ 0x000000010ac8bbe2 [0x000000010ac8bb40+0xa2]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid92110.log; Compiled method (c1) 4061 5335 3 is.hail.annotations.Region$::loadInt (5 bytes); total in heap [0x000000010ac8b9d0,0x000000010ac8bd78] = 936; relocation [0x000000010ac8baf8,0x000000010ac8bb28] = 48; main code [0x000000010ac8bb40,0x000000010ac8bc60] = 288; stub code [0x000000010ac8bc60,0x000000010ac8bcf0] = 144; oops [0x000000010ac8bcf0,0x000000010ac8bcf8] = 8; metadata [0x000000010ac8bcf8,0x000000010ac8bd08] = 16; scopes data [0x000000010ac8bd08,0x000000010ac8bd30] = 40; scopes pcs [0x000000010ac8bd30,0x000000010ac8bd70] = 64; dependencies [0x000000010ac8bd70,0x000000010ac8bd78] = 8; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp. ....; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6860:944,load,loadInt,944,https://hail.is,https://github.com/hail-is/hail/issues/6860,2,['load'],['loadInt']
Performance,"the worker. The worker starts one jvm per core on; startup. The mainclass is a new class called `JVMEntryway`. This entryway starts a UNIX socket and; speaks a very simple binary protocol. It accepts only one type of message:. ```; int32 the number of strings to expect; (; int32 the number of bytes in the next string; byte* UTF-8 string; )*; ```. The array of strings is interpreted as:. ```; comma-spearated-classpath; main-class-name; arg0; arg1; ...; ```. The entryway constructs a URLClassLoader with the given classpath, reflectively allocates an; instance of the mainclass and invokes the `main` method with the remaining arguments. This is; obviously a security risk. The system bans JARs from locations not controlled (and locked down) by; Hail Team. You should require me to hardcode the mainclass as; `is.hail.backend.service.ServiceBackendSocketAPI2` before we merge; however, this flexibility was; useful during development. The JVMEntryway will eventually be useful because we will keep a ClassLoader full of a bunch of; JIT-optimized Hail classes. I did not include that in this PR because we need to finish eliminating; global state used by Hail. Currently, two executions would try to re-use compiled class names for; different code, leading to very weird errors. # Changes to File Systems. Hail has three four file system interfaces:. | File System Interface | Public | Language | Async |; | ----------------------- | ------ | -------- | ----- |; | hail.utils.hadoop_utils | Yes | Python | no |; | hail.fs | Yes | Python | no |; | hailtop.aiotools.fs | No | Python | yes |; | is.hail.io.fs | No | Scala | no |. `hail.fs` is technically in the public API (via `hl.current_backend().fs`), but I doubt anyone uses; it. `hail.utils.hadoop_utils` is a shim over `hail.fs`, there are no direct concrete implementations of; it. This PR adds `hail.fs.RouterFS` to `hail.fs`, a synchronous wrapper around; `hailtop.aiotools.fs.AsyncRouterFS`. A ""router"" file system is one which operates on ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194:4001,optimiz,optimized,4001,https://hail.is,https://github.com/hail-is/hail/pull/11194,1,['optimiz'],['optimized']
Performance,thread-2; 2023-09-22 19:11:13.138 : ERROR: GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=/result.0; From is.hail.relocated.com.google.cloud.storage.StorageException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=/result.0; 	at is.hail.relocated.com.google.cloud.storage.StorageException.translate(StorageException.java:165); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:298); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.load(HttpStorageRpc.java:729); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.lambda$readAllBytes$20(StorageImpl.java:610); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1515); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:610); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:599); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:280); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:278); 	at is.hail.io.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:4901,load,load,4901,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['load'],['load']
Performance,"thub-redirect.dependabot.com/bokeh/bokeh/issues/11422"">#11422</a> [component: bokehjs] [BUG] <code>DeserializationError</code> when trying to change a <code>DataTable</code>'s columns with <code>CustomJS</code></li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11800"">#11800</a> [BUG] DeserializationError when plotting graphs</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11801"">#11801</a> [component: bokehjs] [BUG] Log axis figures don't render if they're not visible at start</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11807"">#11807</a> [component: bokehjs] Work around issues with initialization-time change discovery</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11808"">#11808</a> Don't unnecessarily update node/edge renderers in graphs</li>; </ul>; </li>; <li>; <p>tasks:</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11613"">#11613</a> [component: docs] Cache-bust custom.css for docs</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11791"">#11791</a> [component: docs] Update issue template to use new GH forms</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11761"">#11761</a> [component: docs] Clarify use of color in first steps guide</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11762"">#11762</a> [component: docs] Replace slash with backslash for PS commands</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11767"">#11767</a> [component: bokehjs] Upgrade jquery-ui to resolve security concerns</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11781"">#11781</a> [component: examples] fix transform jitter example</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11786"">#11786</a> bokeh 2.4.2 backports</li>; <li><a href=""https://github-redirect.dependabot.c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11540:1382,Cache,Cache-bust,1382,https://hail.is,https://github.com/hail-is/hail/pull/11540,1,['Cache'],['Cache-bust']
Performance,til.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)sun.reflect.generics.reflectiveObjects.NotImplementedException: null; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Hail version: devel-438801a84105; Error summary: NotImplementedException: null; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:7946,concurren,concurrent,7946,https://hail.is,https://github.com/hail-is/hail/issues/4215,2,['concurren'],['concurrent']
Performance,tils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_matrix_ir$2(SparkBackend.scala:689); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:69); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:69); 	at is.hail.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:2608,Load,LoadPlink,2608,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,"tils/utils.py:402: in run_and_cleanup; retval = await f(*args, **kwargs); /usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py:367: in rm_file; await self.remove(path); /usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py:348: in remove; return await blocking_to_async(self._thread_pool, os.remove, path); /usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:162: in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; /usr/lib/python3.9/asyncio/base_events.py:819: in run_in_executor; executor.submit(func, *args), loop=self); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <concurrent.futures.thread.ThreadPoolExecutor object at 0x7f263d862100>; fn = <function blocking_to_async.<locals>.<lambda> at 0x7f263d781040>, args = (); kwargs = {}. def submit(self, fn, /, *args, **kwargs):; > with self._shutdown_lock, _global_shutdown_lock:; E Failed: Timeout >600.0s. /usr/lib/python3.9/concurrent/futures/thread.py:162: Failed; ---------------------------- Captured log teardown -----------------------------; INFO hailtop.utils:utils.py:450 discarding exception; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 378, in rm_dir; await self.rmdir(path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 352, in rmdir; return await blocking_to_async(self._thread_pool, os.rmdir, path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 162, in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; File ""/usr/lib/python3.9/asyncio/futures.py"", line 284, in __await__; yield self # This tells Task to wait for completion.; File ""/usr/lib/python3.9/asyncio/tasks.py"", line 328, in __wakeup; future.result(); File ""/usr/lib/python3.9/asyncio/futures.py"", line 201, in result; raise self._exception; File ""/usr/lib/python3.9/concurrent/futures/thread.py"", li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:12099,concurren,concurrent,12099,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,time.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:456); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor90.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:7912,concurren,concurrent,7912,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['concurren'],['concurrent']
Performance,time.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.net.SocketTimeoutException: connect timed out; E 	at java.net.PlainSocketImpl.socketConnect(Native Method); E 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); E 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); E 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); E 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); E 	at java.net.Socket.connect(Socket.java:607); E 	at is.hail.relocated.org.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75); E 	at is.hail.relocated.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142); E 	at is.hail.relocated.org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManag,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:7626,concurren,concurrent,7626,https://hail.is,https://github.com/hail-is/hail/issues/13074,1,['concurren'],['concurrent']
Performance,"ting D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e61c-4fa9-9419-2278528f3c86; 2023-05-04 01:04:37.559 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=132096, peakBytesReadable=129.00 KiB, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.560 : INFO: RegionPool: FREE: 129.0K allocated (129.0K blocks / 0 chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.561 : ERROR: error while applying lowering 'LowerAndExecuteShuffles'; 2023-05-04 01:04:37.600 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:37.601 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.601 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.601 : INFO: RegionPool: FREE: 128.0K allocated (128.0K blocks / 0 chunks), regions.size = 2, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.603 : ERROR: SocketException: Connection reset; From javax.net.ssl.SSLException: Connection reset; 	at sun.security.ssl.Alert.createSSLException(Alert.java:127); 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:324); 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:267); 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:262); 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:138); 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1400); 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1368); 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:23187,cache,cache,23187,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['cache'],['cache']
Performance,"ting decisions dynamically circumvents this limitation. However, this prevents usage of NGINX [upstream](http://nginx.org/en/docs/http/ngx_http_upstream_module.html) blocks that provide connection pooling, at least in the community edition, and as a result the gateways will create and terminate a TCP connection per http request. This likely causes minor delays on the front-end through gateway, but this hampers performance greatly in job scheduling. The batch driver is forced to establish a new TCP connection and do an SSL handshake with the internal-gateway multiple times per job, which is expensive and slow. We currently have to dedicate a 2-core NGINX sidecar for the batch-driver just to terminate TLS with internal-gateway and free up cycles in the batch-driver python process. By using proper persistent connections, we can reduce the TLS overhead to single-digit percents of a core. This leads to the first goal of this transition: configure our load balancers to know the full cluster configuration at any point in time so they can properly maintain connection pools with upstream services. However, this is not the only problem. Each ""upstream"" Service in Kubernetes may consist of multiple underlying pods but Kubernetes Services as we use them don't provide proper load-balancing when mixed with persistent connections. When we declare a Service for say, batch in default, Kubernetes adds a DNS record for `batch.default` that resolves to a single IP pointing at kube-proxy. When a new TCP connection is established with kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:2253,load,load,2253,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['load'],['load']
Performance,tingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.for,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:7839,Load,LoadVCF,7839,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,"tingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:768); at is.hail.utils.package$.using(package.scala:575); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$ano",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:11716,Load,LoadVCF,11716,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['Load'],['LoadVCF']
Performance,tion.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8545:9139,concurren,concurrent,9139,https://hail.is,https://github.com/hail-is/hail/issues/8545,1,['concurren'],['concurrent']
Performance,tion.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:81966,concurren,concurrent,81966,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['concurren'],['concurrent']
Performance,"tion.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-a1d6ecc; Error summary: NoSuchElementException: key not found: GT; ```; The file has `GT` in the format field, but it's missing the corresponding header line. Passing a custom `header_file=` fixes the problem, but it's unfortunate that that's required (especially on such a widely used publicly available dataset).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:13732,concurren,concurrent,13732,https://hail.is,https://github.com/hail-is/hail/issues/3467,2,['concurren'],['concurrent']
Performance,tion: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); ... 18 more; Caused by: java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:5054,load,loadClass,5054,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['load'],['loadClass']
Performance,tions$.filterGenotypes$extension(VariantDataset.scala:463); at is.hail.variant.VariantDatasetFunctions.filterGenotypes(VariantDataset.scala:449); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: is.hail.asm4s.AsmFunction2; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.ha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:13599,load,loadClass,13599,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['load'],['loadClass']
Performance,"to a /opt/venv installation + added to PATH to be available to non-root users (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3202"">#3202</a>)</li>; </ul>; <h3>Output</h3>; <ul>; <li>Change from deprecated <code>asyncio.get_event_loop()</code> to create our event loop which removes DeprecationWarning (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3164"">#3164</a>)</li>; <li>Remove logging from internal <code>blib2to3</code> library since it regularly emits error logs about failed caching that can and should be ignored (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3193"">#3193</a>)</li>; </ul>; <h3>Parser</h3>; <ul>; <li>Type comments are now included in the AST equivalence check consistently so accidental deletion raises an error. Though type comments can't be tracked when running on PyPy 3.7 due to standard library limitations. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2874"">#2874</a>)</li>; </ul>; <h3>Performance</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/psf/black/blob/main/CHANGES.md"">black's changelog</a>.</em></p>; <blockquote>; <h2>22.8.0</h2>; <h3>Highlights</h3>; <ul>; <li>Python 3.11 is now supported, except for <em>blackd</em> as aiohttp does not support 3.11 as; of publishing (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3234"">#3234</a>)</li>; <li>This is the last release that supports running <em>Black</em> on Python 3.6 (formatting 3.6; code will continue to be supported until further notice)</li>; <li>Reword the stability policy to say that we may, in rare cases, make changes that; affect code that was not previously formatted by <em>Black</em> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3155"">#3155</a>)</li>; </ul>; <h3>Stable style</h3>; <ul>; <li>Fix an infinite loop when using <code># fmt: on/off</code",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12174:4975,Perform,Performance,4975,https://hail.is,https://github.com/hail-is/hail/pull/12174,1,['Perform'],['Performance']
Performance,"to complete before it starts the next iteration of the scheduler. This leaves the scheduler vulnerable to problematic workers or workers that happen to be preempted during the scheduling process. So, the driver sets a [2 second timeout](https://github.com/hail-is/hail/blob/b27737f67bf9e69f1abed2fec07fc7c921790ef8/batch/batch/driver/job.py#L585) on the call to `/api/v1alpha/batches/jobs/create`. Additionally, this general design means that in the event of a request timeout or transient error, Batch cannot guarantee that there is always at most one concurrent running attempt for a given job. This ends up being a fine (and intentional) concession in practice because the idempotent design of preemptible jobs tends to cover this scenario, but it is regardless wasted compute and cost to users. Nevertheless, we strive to minimize cases where we might halt the scheduling loop or double-schedule work, and one way to do that in the current design is to minimize the variance in latency of `/api/v1alpha/batches/jobs/create`. The largest source of this latency is the request to blob storage. While GCS and ABS are relatively fast and highly available, Batch in Azure Terra requires first obtaining SAS tokens from the Terra control plane, which can introduce much higher and more variable latency. There have also been occurrences in the past of corrupted or deleted specs, which introduce unexpected failure modes that should error the job but instead disrupt the scheduling loop. Many of these problems would be mitigated by moving the read from object storage outside of the `/api/v1alpha/batches/jobs/create` endpoint. The endpoint should push this read into the asynchronous task that ultimately runs the job and therefore return its acknowledgement to the driver faster. If the worker encounters errors later on while reading the spec, those should result in `error`ing the job instead of raising a 500 in the scheduling request. ### Version. 0.2.129. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14456:2001,latency,latency,2001,https://hail.is,https://github.com/hail-is/hail/issues/14456,2,['latency'],['latency']
Performance,"toBlockMatrixDense method did not specify size of matrix in constructor, meaning that querying its size later performed an RDD action.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1969:110,perform,performed,110,https://hail.is,https://github.com/hail-is/hail/pull/1969,1,['perform'],['performed']
Performance,"tools/htsjdk/issues/1525"">#1525</a>); d40fe5412 Beta implementation of Bundles. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1546"">#1546</a>)</p>; <p>CRAM; 489c4192d Support CRAM reference regions. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1605"">#1605</a>); 22aec6782 Fix decoding of CRAM Scores read feature during normalization. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1592"">#1592</a>); 6507249a4 Make the CRAM MD5 failure message more user friendly. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1607"">#1607</a>); b5af659e6 Fix restoration of read base feature code. <a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1379"">#1379</a> (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1590"">#1590</a>); e63c34a92 Ignore TC, TN on CRAM read (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1578"">#1578</a>)</p>; <p>BAM/SAM; 1449dec45 Support loading of CSI from URLs/streams. <a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1507"">#1507</a> (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1595"">#1595</a>); a38c78d6c Add an option to SAMFileWriter to disable checking of ordering of rec (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1599"">#1599</a>); 51aa6ed2b Validate that SAM header tag keys are exactly 2 characters long (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1561"">#1561</a>); fbd9e96d5 Deprecate OTHER as a PL value (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1552"">#1552</a>); d5f7e106b Adding PL Tag 'DNBSEQ' as the Platform/Technology for BGI/MGI (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1547"">#1547</a>)</p>; <p>Misc Improvements; f461401e3 Silence AsciiLineReader warning when creating a FASTA sequence index (<a href=""https://github-redirect.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12229:2937,load,loading,2937,https://hail.is,https://github.com/hail-is/hail/pull/12229,2,['load'],['loading']
Performance,tor(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. Thanks!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:50926,concurren,concurrent,50926,https://hail.is,https://github.com/hail-is/hail/issues/7044,2,['concurren'],['concurrent']
Performance,tor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19279,Load,LoadVCF,19279,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"tor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:6225,Load,LoadVCF,6225,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,"tors here: http://dev.hail.is/t/proposal-for-aggregators/93/3. Builds on: https://github.com/hail-is/hail/pull/3552. I still need to finish converting some tests from ExtractAggregatorSuite (they are currently commented out). Old-style aggregators (filter, map, flatMap) are expanded at toIR conversion time to ApplyAggOp and SeqOp. ApplyAggOp returns the result of the aggregation. ApplyAggOp's first argument, which must be of type TVoid, is the expression to run for each element being aggregated over. SeqOp merges a computed value into the RegionValueAggregator. I added a AggSignature that holds the AggOp, type being aggregated over and the RegionValueAggregator constructor argument types. This is stored by the ApplyAggOp and the SeqOp. @tpoterba I believe TAggregable is no longer used in the IR code and can go away when the AST gets ripped out. @danking I added some IR testing logic to TestUtils. Namely, `eval` evaluates an IR with environments, args and/or aggregations with a single call (and verifies that the interpret with and without optimization and compiler all agree). There are also functions for asserting the result of aggregations, for example:. ```; val aggSig = AggSignature(Sum(), TFloat64(), FastSeq()); assertEvalsTo(ApplyAggOp(; SeqOp(ApplyBinaryPrimOp(Multiply(), Ref(""a"", TFloat64()), Ref(""b"", TFloat64())), I32(0), aggSig),; FastSeq(), aggSig),; (FastIndexedSeq(Row(1.0, 10.0), Row(10.0, 10.0), Row(null, 10.0)), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),; 110.0); ```. The line:. > (FastIndexedSeq(Row(1.0, 10.0), ...), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),. is the IndexedSeq of values to aggregate over, along with their signature. The struct type is used to build the scope in which aggregators are evaluated. (A little noisy because of the aggregator syntax. It's noisier than I'd like it to be.). This nicely paves the way for aggregators with multi-argument seqOps (like takeBy) that were previously handled by lambdas and we didn't have a cl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3570:1101,optimiz,optimization,1101,https://hail.is,https://github.com/hail-is/hail/pull/3570,1,['optimiz'],['optimization']
Performance,"tpStorageOptions deserialized (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2153"">#2153</a>) (<a href=""https://github.com/googleapis/java-storage/commit/68ad8e7357097e3dd161c2ab5f7a42a060a3702c"">68ad8e7</a>)</li>; <li>Update grpc default metadata projection to include acl same as json (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2150"">#2150</a>) (<a href=""https://github.com/googleapis/java-storage/commit/330e795040592e5df22d44fb5216ad7cf2448e81"">330e795</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.14.0 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2151"">#2151</a>) (<a href=""https://github.com/googleapis/java-storage/commit/eba8b6a235919a27d1f6dadf770140c7d143aa1a"">eba8b6a</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.24.0...v2.25.0"">2.25.0</a> (2023-07-24)</h2>; <h3>Features</h3>; <ul>; <li>BlobWriteChannelV2 - same throughput less GC (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2110"">#2110</a>) (<a href=""https://github.com/googleapis/java-storage/commit/1b52a1053130620011515060787bada10c324c0b"">1b52a10</a>)</li>; <li>Update Storage.createFrom(BlobInfo, Path) to have 150% higher throughput (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2059"">#2059</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4c2f44e28a1ff19ffb2a02e3cefc062a1dd98fdc"">4c2f44e</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>Update BlobWriteChannelV2 to properly carry forward offset after incremental flush (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2125"">#2125</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c099a2f4f8ea9afa6953270876653916b021fd9f"">c099a2f</a>)</li>; <li>Update GrpcStorageImpl.createFrom(BlobInfo, Path) to use RewindableContent (<a href=""https://redirect.github.com/googleapis/java-storage/issue",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13605:7837,throughput,throughput,7837,https://hail.is,https://github.com/hail-is/hail/pull/13605,1,['throughput'],['throughput']
Performance,"tps://github-redirect.dependabot.com/psf/black/issues/3242"">#3242</a>)</li>; <li><a href=""https://github.com/psf/black/commit/767604e03f5e454ae5b5c268cd5831c672f46de8""><code>767604e</code></a> Use .gitignore files in the initial source directories (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3237"">#3237</a>)</li>; <li><a href=""https://github.com/psf/black/commit/2c90480e1a102ab0fac57737d2ba5143d82abed7""><code>2c90480</code></a> Use strict mypy checking (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3222"">#3222</a>)</li>; <li><a href=""https://github.com/psf/black/commit/ba618a307a30a119b4fafe526ebf7d5f092ba981""><code>ba618a3</code></a> Add parens around implicit string concatenations where it increases readabili...</li>; <li><a href=""https://github.com/psf/black/commit/c0cc19b5b3371842d696875897bebefebd5e1596""><code>c0cc19b</code></a> Delay worker count determination</li>; <li><a href=""https://github.com/psf/black/commit/afed2c01903465f9a486ac481a66aa3413cc1b01""><code>afed2c0</code></a> Load .gitignore and exclude regex at time of use</li>; <li><a href=""https://github.com/psf/black/commit/e269f44b25737360e0dc65379f889dfa931dc68a""><code>e269f44</code></a> Lazily import parallelized format modules</li>; <li><a href=""https://github.com/psf/black/commit/c47b91f513052cd39b818ea7c19716423c85c04e""><code>c47b91f</code></a> Fix misdetection of project root with <code>--stdin-filename</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3216"">#3216</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/psf/black/compare/22.3.0...22.8.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=black&package-manager=pip&previous-version=22.3.0&new-version=22.8.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12174:10639,Load,Load,10639,https://hail.is,https://github.com/hail-is/hail/pull/12174,1,['Load'],['Load']
Performance,"tps://github.com/psf/requests) from 2.31.0 to 2.32.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/psf/requests/releases"">requests's releases</a>.</em></p>; <blockquote>; <h2>v2.32.0</h2>; <h2>2.32.0 (2024-05-20)</h2>; <h2> PYCON US 2024 EDITION </h2>; <p><strong>Security</strong></p>; <ul>; <li>Fixed an issue where setting <code>verify=False</code> on the first request from a; Session will cause subsequent requests to the <em>same origin</em> to also ignore; cert verification, regardless of the value of <code>verify</code>.; (<a href=""https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56"">https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56</a>)</li>; </ul>; <p><strong>Improvements</strong></p>; <ul>; <li><code>verify=True</code> now reuses a global SSLContext which should improve; request time variance between first and subsequent requests. It should; also minimize certificate load time on Windows systems when using a Python; version built with OpenSSL 3.x. (<a href=""https://redirect.github.com/psf/requests/issues/6667"">#6667</a>)</li>; <li>Requests now supports optional use of character detection; (<code>chardet</code> or <code>charset_normalizer</code>) when repackaged or vendored.; This enables <code>pip</code> and other projects to minimize their vendoring; surface area. The <code>Response.text()</code> and <code>apparent_encoding</code> APIs; will default to <code>utf-8</code> if neither library is present. (<a href=""https://redirect.github.com/psf/requests/issues/6702"">#6702</a>)</li>; </ul>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Fixed bug in length detection where emoji length was incorrectly; calculated in the request content-length. (<a href=""https://redirect.github.com/psf/requests/issues/6589"">#6589</a>)</li>; <li>Fixed deserialization bug in JSONDecodeError. (<a href=""https://redirect.github.com/psf/requests/issues/6629"">#6629</a>)</li>; <li>Fixed bug where an ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14555:999,load,load,999,https://hail.is,https://github.com/hail-is/hail/pull/14555,1,['load'],['load']
Performance,"trout</code></a>.</li>; </ul>; </li>; <li>Prioritize binary builds for R dependencies.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2277"">#2277</a> PR by <a href=""https://github.com/lorenzwalthert""><code>@lorenzwalthert</code></a>.</li>; </ul>; </li>; <li>Fix handling of git worktrees.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2252"">#2252</a> PR by <a href=""https://github.com/daschuer""><code>@daschuer</code></a>.</li>; </ul>; </li>; <li>Fix handling of <code>$R_HOME</code> for R hooks.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2301"">#2301</a> PR by <a href=""https://github.com/jeff-m-sullivan""><code>@jeff-m-sullivan</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2300"">#2300</a> issue by <a href=""https://github.com/jeff-m-sullivan""><code>@jeff-m-sullivan</code></a>.</li>; </ul>; </li>; <li>Fix a rare race condition in change stashing.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2323"">#2323</a> PR by <a href=""https://github.com/asottile""><code>@asottile</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2287"">#2287</a> issue by <a href=""https://github.com/ian-h-chamberlain""><code>@ian-h-chamberlain</code></a>.</li>; </ul>; </li>; </ul>; <h3>Updating</h3>; <ul>; <li>Remove python3.6 support. Note that pre-commit still supports running hooks written in older versions, but pre-commit itself requires python 3.7+.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2215"">#2215</a> PR by <a href=""https://github.com/asottile""><code>@asottile</code></a>.</li>; </ul>; </li>; <li>pre-commit has migrated from the <code>master</code> branch to <code>main</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2302"">#2302</a> ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11731:4122,race condition,race condition,4122,https://hail.is,https://github.com/hail-is/hail/pull/11731,1,['race condition'],['race condition']
Performance,"trout</code></a>.</li>; </ul>; </li>; <li>Prioritize binary builds for R dependencies.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2277"">#2277</a> PR by <a href=""https://github.com/lorenzwalthert""><code>@lorenzwalthert</code></a>.</li>; </ul>; </li>; <li>Fix handling of git worktrees.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2252"">#2252</a> PR by <a href=""https://github.com/daschuer""><code>@daschuer</code></a>.</li>; </ul>; </li>; <li>Fix handling of <code>$R_HOME</code> for R hooks.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2301"">#2301</a> PR by <a href=""https://github.com/jeff-m-sullivan""><code>@jeff-m-sullivan</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2300"">#2300</a> issue by <a href=""https://github.com/jeff-m-sullivan""><code>@jeff-m-sullivan</code></a>.</li>; </ul>; </li>; <li>Fix a rare race condition in change stashing.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2323"">#2323</a> PR by <a href=""https://github.com/asottile""><code>@asottile</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2287"">#2287</a> issue by <a href=""https://github.com/ian-h-chamberlain""><code>@ian-h-chamberlain</code></a>.</li>; </ul>; </li>; </ul>; <h3>Updating</h3>; <ul>; <li>Remove python3.6 support. Note that pre-commit still supports running hooks; written in older versions, but pre-commit itself requires python 3.7+.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2215"">#2215</a> PR by <a href=""https://github.com/asottile""><code>@asottile</code></a>.</li>; </ul>; </li>; <li>pre-commit has migrated from the <code>master</code> branch to <code>main</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2302"">#2302</a>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11731:9301,race condition,race condition,9301,https://hail.is,https://github.com/hail-is/hail/pull/11731,1,['race condition'],['race condition']
Performance,"trying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1515); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:610); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:599); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:280); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:278); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:25); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$15(ServiceBackend.scala:225); 	at scala.util.Try$.apply(Try.scala:213); 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:995); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); ```. The driver will have log output like this:; ```; 2023-09-22 19:11:13.051 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/8042383 response 200; 2023-09-22 19:11:13.052 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=: reading results; 2023-09-22 19:11:13.125 ServiceBackend$: INFO: all results read. 0.072746861 s. 0.0 result/s. 0.0 MiB/s.; 2023-09-22 19:11:13.125 : INFO: [collectDArray|table_native_writer]: executed 5 tasks in 1.822s; 2023-09-22 19:11:13.126 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:13.126 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-09-22 19:11:13.126 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 10: pool",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:2344,concurren,concurrent,2344,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,trying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1515); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:610); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:599); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:280); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:278); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:25); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$15(ServiceBackend.scala:225); 	at scala.util.Try$.apply(Try.scala:213); 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:995); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); ```. but the worker looks like this:; ```; 2023-09-22 19:11:12.125 JVMEntryway: INFO: is.hail.JVMEntryway received arguments:; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 0: /hail-jars/gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 1: is.hail.backend.service.Main; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 2: /batch/fe537a243a3046d29d76861ffee94b92; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 3: /batch/fe537a243a3046d29d76861ffee94b92/log; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 4: gs://hail-query-ger0g/jars/b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 5: worker; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 6: gs://1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 7: 0; 2023-09-22 19:11:12.1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:6342,concurren,concurrent,6342,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"ts, like; <code>case Foo(bar=baz as quux)</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2749"">#2749</a>)</li>; <li>Tuple unpacking on <code>return</code> and <code>yield</code> constructs now implies 3.8+ (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2700"">#2700</a>)</li>; <li>Unparenthesized tuples on annotated assignments (e.g; <code>values: Tuple[int, ...] = 1, 2, 3</code>) now implies 3.8+ (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2708"">#2708</a>)</li>; <li>Fix handling of standalone <code>match()</code> or <code>case()</code> when there is a trailing newline or a; comment inside of the parentheses. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2760"">#2760</a>)</li>; <li><code>from __future__ import annotations</code> statement now implies Python 3.7+ (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2690"">#2690</a>)</li>; </ul>; <h3>Performance</h3>; <ul>; <li>Speed-up the new backtracking parser about 4X in general (enabled when; <code>--target-version</code> is set to 3.10 and higher). (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2728"">#2728</a>)</li>; <li><em>Black</em> is now compiled with <a href=""https://github.com/mypyc/mypyc"">mypyc</a> for an overall 2x; speed-up. 64-bit Windows, MacOS, and Linux (not including musl) are supported. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/1009"">#1009</a>,; <a href=""https://github-redirect.dependabot.com/psf/black/issues/2431"">#2431</a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/psf/black/commits/22.1.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=black&package-manager=pip&previous-version=20.8b1&new-version=2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11468:10997,Perform,Performance,10997,https://hail.is,https://github.com/hail-is/hail/pull/11468,1,['Perform'],['Performance']
Performance,ttomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5606,Optimiz,Optimize,5606,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,"ttps://github.com/aio-libs/aiorwlock) from 1.0.0 to 1.3.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiorwlock/releases"">aiorwlock's releases</a>.</em></p>; <blockquote>; <h2>aiorwlock 1.2.0</h2>; <h1>Changes</h1>; <ul>; <li>Fix a bug that makes concurrent writes possible under some (rare) conjunctions (<a href=""https://github-redirect.dependabot.com/aio-libs/aiorwlock/issues/235"">#235</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiorwlock/blob/master/CHANGES.rst"">aiorwlock's changelog</a>.</em></p>; <blockquote>; <p>1.3.0 (2022-1-18); ^^^^^^^^^^^^^^^^^^</p>; <ul>; <li>Dropped Python 3.6 support</li>; <li>Python 3.10 is officially supported</li>; <li>Drop deprecated <code>loop</code> parameter from <code>RWLock</code> constructor</li>; </ul>; <p>1.2.0 (2021-11-09); ^^^^^^^^^^^^^^^^^^</p>; <ul>; <li>Fix a bug that makes concurrent writes possible under some (rare) conjunctions (<a href=""https://github-redirect.dependabot.com/aio-libs/aiorwlock/issues/235"">#235</a>)</li>; </ul>; <p>1.1.0 (2021-09-27); ^^^^^^^^^^^^^^^^^^</p>; <ul>; <li>Remove explicit loop usage in <code>asyncio.sleep()</code> call, make the library forward; compatible with Python 3.10</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/6599d10ba16f95f19d5b5963a00aa857bc98f656""><code>6599d10</code></a> Bump to 1.3.0</li>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/d4b41f54b57caf316c41c3973ab82bd53a418ff8""><code>d4b41f5</code></a> Drop deprecated 'loop' parameter from RWLock constructor</li>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/3edb2a1bc1636832df12671f035e21dd74440824""><code>3edb2a1</code></a> Fix tests</li>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/45a7418474a55defe9c53fd8e38df60af514cf84""><code>45a7418</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11514:1009,concurren,concurrent,1009,https://hail.is,https://github.com/hail-is/hail/pull/11514,1,['concurren'],['concurrent']
Performance,tune ImportVCFs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5507:0,tune,tune,0,https://hail.is,https://github.com/hail-is/hail/pull/5507,1,['tune'],['tune']
Performance,"ub-redirect.dependabot.com/chardet/chardet/issues/254"">#254</a> from chardet/master</li>; <li><a href=""https://github.com/chardet/chardet/commit/322229573173307e1380eb151ea446b8c6fe2c3b""><code>3222295</code></a> Linter fixes (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/253"">#253</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/85c96d3449afedf0e9fe57bd01eada92e0dd11b4""><code>85c96d3</code></a> Bump version to 5.0.0</li>; <li><a href=""https://github.com/chardet/chardet/commit/57abbca866a41758f7c26e1bb26a0126e28575c2""><code>57abbca</code></a> Rebased and cleaned up version of UTF-16/32 BE/LE PR (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/206"">#206</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/eca9558cf7569c1f7689bd66e5aaf965a56e903c""><code>eca9558</code></a> Fix missing black formatting</li>; <li><a href=""https://github.com/chardet/chardet/commit/f1f9d4280e11fb3a9b2d9eaf1827dac9263cb1cb""><code>f1f9d42</code></a> slight increase in performance (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/252"">#252</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/f9ef56cfd6c9b24b9c865eae6dc2285c67ffb75c""><code>f9ef56c</code></a> Use Python-3 super() syntax in Latin1Prober (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/240"">#240</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/c5e5d5a8f1b6e135a8bffd8d60b2f726bb168339""><code>c5e5d5a</code></a> Simple maintenance improvements (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/244"">#244</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/49b8341f507bed68f7d3ff7138bb97047a0e04f0""><code>49b8341</code></a> Configure setuptools using the declarative syntax in setup.cfg (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/239"">#239</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/5c73bfcdf819251d1a1d0de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12107:4261,perform,performance,4261,https://hail.is,https://github.com/hail-is/hail/pull/12107,1,['perform'],['performance']
Performance,"ue, ; ...: sep=' ', ; ...: min_partitions=16) ; ...: m = m.key_rows_by(locus=hl.parse_locus(m.f0)) ; ...: m._force_count_rows() ; ```. `/tmp/foo.tsv.gz` is a gzipped (not blocked) 1GB of 1000 rows each containing one row column and 500k sample columns. The entries are the integers from 0 to 499,999. The first column is the first run (when the JIT is warmed) and the second column is the mean of two subsequent runs. All times in seconds. Everything is necessarily executed on one core. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Aside: using the staged stuff is hard, especially when using multiple methods. A couple thoughts:; - Because SRVB generates a fresh SRVB for arrays and structs, you must thread the srvb through your code gen rather than using a single field, this is annoying and error prone; - `init` is not a first class thing in `FunctionBuilder` and I've arguably made the whole situation more ugly by exposing `addInitInstructions`. Without the ability to place code in the constructor, it is hard to coordinate work between multiple methods.; - When using lots of methods, there's a lot of bookkeeping. I would like a way to define a ""staged class"" that wraps up some of the boilerplate. Not totally clear what I want here, just less boilerplate. Aside2: This is still pretty",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6987:1808,perform,performance,1808,https://hail.is,https://github.com/hail-is/hail/pull/6987,1,['perform'],['performance']
Performance,ulateConcordance.scala:108); 	at is.hail.methods.CalculateConcordance.apply(CalculateConcordance.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:7884,load,loadLength,7884,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['load'],['loadLength']
Performance,"uld remain open if the client did not close it. This; change ensures the transport is closed when the client does not close; it.</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/blob/master/CHANGES.rst"">aiohttp's changelog</a>.</em></p>; <blockquote>; <h1>3.9.4 (2024-04-11)</h1>; <h2>Bug fixes</h2>; <ul>; <li>; <p>The asynchronous internals now set the underlying causes; when assigning exceptions to the future objects; -- by :user:<code>webknjaz</code>.</p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>8089</code>.</p>; </li>; <li>; <p>Treated values of <code>Accept-Encoding</code> header as case-insensitive when checking; for gzip files -- by :user:<code>steverep</code>.</p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>8104</code>.</p>; </li>; <li>; <p>Improved the DNS resolution performance on cache hit -- by :user:<code>bdraco</code>.</p>; <p>This is achieved by avoiding an :mod:<code>asyncio</code> task creation in this case.</p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>8163</code>.</p>; </li>; <li>; <p>Changed the type annotations to allow <code>dict</code> on :meth:<code>aiohttp.MultipartWriter.append</code>,; :meth:<code>aiohttp.MultipartWriter.append_json</code> and; :meth:<code>aiohttp.MultipartWriter.append_form</code> -- by :user:<code>cakemanny</code></p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>7741</code>.</p>; </li>; <li>; <p>Ensure websocket transport is closed when client does not close it; -- by :user:<code>bdraco</code>.</p>; <p>The transport could remain open if the client did not close it. This; change ensures the transport is closed when the client does not close; it.</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14477:2782,perform,performance,2782,https://hail.is,https://github.com/hail-is/hail/pull/14477,12,"['cache', 'perform']","['cache', 'performance']"
Performance,"ule named 'pyspark'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); /tmp/ipykernel_233/4275665471.py in <module>; ----> 1 combined_pandas = pd.read_pickle(gwas_pandas_file). /opt/conda/lib/python3.7/site-packages/pandas/io/pickle.py in read_pickle(filepath_or_buffer, compression, storage_options); 220 # ""No module named 'pandas.core.sparse.series'""; 221 # ""Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib""; --> 222 return pc.load(handles.handle, encoding=None); 223 except UnicodeDecodeError:; 224 # e.g. can occur for files written in py27; see GH#28645 and GH#31988. /opt/conda/lib/python3.7/site-packages/pandas/compat/pickle_compat.py in load(fh, encoding, is_verbose); 272 up.is_verbose = is_verbose; 273 ; --> 274 return up.load(); 275 except (ValueError, TypeError):; 276 raise. /opt/conda/lib/python3.7/pickle.py in load(self); 1086 raise EOFError; 1087 assert isinstance(key, bytes_types); -> 1088 dispatch[key[0]](self); 1089 except _Stop as stopinst:; 1090 return stopinst.value. /opt/conda/lib/python3.7/pickle.py in load_stack_global(self); 1383 if type(name) is not str or type(module) is not str:; 1384 raise UnpicklingError(""STACK_GLOBAL requires str""); -> 1385 self.append(self.find_class(module, name)); 1386 dispatch[STACK_GLOBAL[0]] = load_stack_global; 1387 . /opt/conda/lib/python3.7/site-packages/pandas/compat/pickle_compat.py in find_class(self, module, name); 204 key = (module, name); 205 module, name = _class_locations_map.get(key, key); --> 206 return super().find_class(module, name); 207 ; 208 . /opt/conda/lib/python3.7/pickle.py in find_class(self, module, name); 1424 elif module in _compat_pickle.IMPORT_MAPPING:; 1425 module = _compat_pickle.IMPORT_MAPPING[module]; -> 1426 __import__(module, level=0); 1427 if self.proto >= 4:; 1428 return _getattribute(sys.modules[module], name)[0]. /opt/conda/lib/python3.7/site-packages/hail/__init__.py in <module>; 31 # F401",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14004:3110,load,load,3110,https://hail.is,https://github.com/hail-is/hail/issues/14004,1,['load'],['load']
Performance,uler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Opt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:2452,Load,LoadVCF,2452,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,uler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:21); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:17); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:498); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskSto,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806:4625,Load,LoadVCF,4625,https://hail.is,https://github.com/hail-is/hail/issues/1806,1,['Load'],['LoadVCF']
Performance,uler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:29); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:25); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:557); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.Ra,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:10491,Load,LoadVCF,10491,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['Load'],['LoadVCF']
Performance,un$main$6(ServiceBackend.scala:462); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6$adapted(ServiceBackend.scala:461); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:461); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:460); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:459); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); ```. ### Version. 0.2.116. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:15220,concurren,concurrent,15220,https://hail.is,https://github.com/hail-is/hail/issues/13074,6,['concurren'],['concurrent']
Performance,un$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(Loweri,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5935,Optimiz,Optimize,5935,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,un(Thread.java:745); Caused by: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); ... 18 more; Caused by: java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:4985,load,loadClass,4985,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['load'],['loadClass']
Performance,"unWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1515); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:610); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:599); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:280); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:278); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:25); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$15(ServiceBackend.scala:225); 	at scala.util.Try$.apply(Try.scala:213); 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:995); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); ```. The driver will have log output like this:; ```; 2023-09-22 19:11:13.051 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/8042383 response 200; 2023-09-22 19:11:13.052 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=: reading results; 2023-09-22 19:11:13.125 ServiceBackend$: INFO: all results read. 0.072746861 s. 0.0 result/s. 0.0 MiB/s.; 2023-09-22 19:11:13.125 : INFO: [collectDArray|table_native_writer]: executed 5 tasks in 1.822s; 2023-09-22 19:11:13.126 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:13.126 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-09-22 19:11:13.126 : INFO: RegionPool: FREE: 0 alloc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:2259,concurren,concurrent,2259,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,unWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1515); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:610); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:599); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:280); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:278); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:25); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$15(ServiceBackend.scala:225); 	at scala.util.Try$.apply(Try.scala:213); 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:995); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); ```. but the worker looks like this:; ```; 2023-09-22 19:11:12.125 JVMEntryway: INFO: is.hail.JVMEntryway received arguments:; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 0: /hail-jars/gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 1: is.hail.backend.service.Main; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 2: /batch/fe537a243a3046d29d76861ffee94b92; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 3: /batch/fe537a243a3046d29d76861ffee94b92/log; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 4: gs://hail-query-ger0g/jars/b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 5: worker; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 6: gs://1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:6257,concurren,concurrent,6257,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"unk$4(ServiceBackend.scala:664); at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$parseInputToCommandThunk$3(ServiceBackend.scala:650); at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:822); at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:447); at is.hail.backend.service.Main$.main(Main.scala:15); at is.hail.backend.service.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```; </details>. The code is simple and clearly is running against a path that does not already exist:; ```; if not hl.hadoop_exists(get_aou_util_path('mt_sample_qc')):; print('Run sample qc MT.....'); mt = hl.read_matrix_table(ACAF_MT_PATH); mt = mt.filter_rows(mt.locus.in_autosome()); # mt = mt.filter_rows(mt.locus.contig == 'chr1'); ht = hl.sample_qc(mt, name='mt_sample_qc'); ht.write(get_aou_util_path('mt_sample_qc'), overwrite=args.overwrite); ```. Job lo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:6564,concurren,concurrent,6564,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['concurren'],['concurrent']
Performance,"uote>; <h2>1.26.5</h2>; <p>:warning: <strong>IMPORTANT: urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting the authority component.</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.5 (2021-05-26)</h2>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting; the authority component.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/d1616473df94b94f0f5ad19d2a6608cfe93b7cdf""><code>d161647</code></a> Release 1.26.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2d4a3fee6de2fa45eb82169361918f759269b4ec""><code>2d4a3fe</code></a> Improve performance of sub-authority splitting in URL</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2698537d52f8ff1f0bbb1d45cf018b118e91f637""><code>2698537</code></a> Update vendored six to 1.16.0</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Python 3.10 ssl module</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/d725a9b56bb8baf87c9e6eee0e9edf010034b63b""><code>d725a9b</code></a> Add Python 3.10 to GitHub Actions</li>; <li><a href=""https://github.com/urllib3/urllib",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10544:1213,perform,performance,1213,https://hail.is,https://github.com/hail-is/hail/pull/10544,1,['perform'],['performance']
Performance,"us output like:; ```; 2019-11-06 18:44:11 root: INFO: Timer: all timings:; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 5.811ms, total 29.474ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:1136,Optimiz,Optimize,1136,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,"ut. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 293, in run; await self.get_container_log()); File ""/usr/local/lib/python3.6/site-packages/batch/log_store.py"", line 36, in write_log_file; return await self.gcs.write_gs_file(path, data); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 37, in write_gs_file; return await self._wrapped_write_gs_file(self, uri, string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 56, in wrapped; **kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8053:3137,concurren,concurrent,3137,https://hail.is,https://github.com/hail-is/hail/issues/8053,2,['concurren'],['concurrent']
Performance,ute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at org.apache.hadoop.fs.Path.initialize(Path.java:205); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.net.URI.checkPath(URI.java:1823); 	at java.net.URI.<init>(URI.java:745); 	at org.apache.hadoop.fs.Path.initialize(Path.java:202); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:5487,concurren,concurrent,5487,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,"uth-library-python/compare/v2.5.0...v2.6.0"">2.6.0</a> (2022-01-31)</h2>; <h3>Features</h3>; <ul>; <li>ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/962"">#962</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/52c8ef90058120d7d04d3d201adc111664be526c"">52c8ef9</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>revert &quot;feat: add api key support (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/826"">#826</a>)&quot; (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/964"">#964</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/f9f23f4370f2a7a5b2c66ee56a5e700ef03b5b06"">f9f23f4</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/google-auth-library-python/compare/v2.4.1...v2.5.0"">2.5.0</a> (2022-01-25)</h2>; <h3>Features</h3>; <ul>; <li>ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/956"">#956</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/a8eb4c8693055a3420cfe9c3420aae2bc8cd465a"">a8eb4c8</a>)</li>; </ul>; <h3><a href=""https://github.com/googleapis/google-auth-library-python/compare/v2.4.0...v2.4.1"">2.4.1</a> (2022-01-21)</h3>; <h3>Bug Fixes</h3>; <ul>; <li>urllib3 import (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/953"">#953</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/c8b5cae3da5eb9d40067d38dac51a4a8c1e0763e"">c8b5cae</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/google-auth-library-python/compare/v2.3.3...v2.4.0"">2.4.0</a> (2022-01-20)</h2>; <h3>Features</h3>; <ul>; <li>add 'py.typed' declaration (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546:6219,load,load,6219,https://hail.is,https://github.com/hail-is/hail/pull/11546,1,['load'],['load']
Performance,utor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAG,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:2517,Load,LoadVCF,2517,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,utor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGSch,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9613,Load,LoadVCF,9613,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,va.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.e,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19451,Load,LoadVCF,19451,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,verified this successfully loads natives on linux and osx. @jbloom22 FYI,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3474:27,load,loads,27,https://hail.is,https://github.com/hail-is/hail/pull/3474,1,['load'],['loads']
Performance,"voke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: hybrid.m37m.vcf.bgz: caught htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; offending line: 3	60830534	.	M	C	40	.	.	GT:AD	1/1:0,40; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:6138,Load,LoadVCF,6138,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,ware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:15029,concurren,concurrent,15029,https://hail.is,https://github.com/hail-is/hail/issues/3342,2,['concurren'],['concurrent']
Performance,"we; timeout substantially more frequently. I have observed this myself on my laptop. Just this; morning I saw it happen to Daniel. 2. When using an `aiohttp.AsyncIterablePayload`, it is *critical* to always check if the coroutine; which actually writes to GCS (which is stashed in the variable `request_task`) is still; alive. In the current `main`, we do not do this which causes hangs (in particular the timeout; exceptions are never thrown ergo we never retry). To understand the second problem, you must first recall how writing works in aiogoogle. There are; two Tasks and an `asyncio.Queue`. The terms ""writer"" and ""reader"" are somewhat confusing, so let's; use left and right. The left Task has the owning reference to both the source ""file"" and the; destination ""file"". In particular, it is the *left* Task which closes both ""files"". Moreover, the; left Task reads chunks from the source file and places those chunks on the `asyncio.Queue`. The; right Task takes chunks off the queue and writes those chunks to the destination file. This situation can go awry in two ways. First, if the right Task encounters any kind of failure, it will stop taking chunks off of the; queue. When the queue (which has a size limit of one) is full, the left Task will hang. The system; is stuck. The left Task will wait forever for the right Task to empty the queue. The second scenario is exactly the same except that the left Task is trying to add the ""stop""; message to the queue rather than a chunk. In either case, it is critical that the left Task waits simultaneously on the queue operation *and*; on the right Task completing. If the right Task has died, no further writes can occur and the left; Task must raise an exception. In the first scenario, we do not observe the right Task's exception; because that will be done when we close the `InsertObjectStream` (which represents the destination; ""file""). ---. I also added several types, assertions, and a few missing `async with ... as resp:` blocks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11830:1396,queue,queue,1396,https://hail.is,https://github.com/hail-is/hail/pull/11830,5,['queue'],['queue']
Performance,"whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJW",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:7024,cache,cached,7024,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4330,cache,cached,4330,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2889,cache,cached,2889,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"wip. A few remaining tests fail. Includes a number of fixes to InferPType, InferType. Plan is to get this working before optimize, since that is the simple case, and then move to pre-simplify",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063:121,optimiz,optimize,121,https://hail.is,https://github.com/hail-is/hail/pull/8063,1,['optimiz'],['optimize']
Performance,within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRPar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:6051,Load,LoadPlink,6051,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,"without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which internal namespaces (dev and PR) are currently active. This requires relatively small changes to CI to track active namespaces but overall is a pretty small change. Note that this does not introduce a dependency on CI to support production traffic, only development traffic.; 3. Deployments that run more than 1 replica (but really can be all of them) are run behind Headless Services, which expose the underlying pod IPs so Envoy can handle load-balancing instead of kube-proxy. This allows Envoy to make smart load-balancing decisions and correctly enforce rate-limiting when using connection pools. The namespace tracking in CI in Point 2 is possible before we make any changes to our networking, so that comes first in #12093. Point 3 is taken care of in #12094, and the rest of Point 2 and Point 1, everything to do with Envoy, is in this PR. ### Additional QoL improvements; - Envoy by default exposes Prometheus metrics that we can use to easily monitor things like rate-limiting, request failures and durations; - Since all Envoy configuration is in the configmap, we don't need to build any images. I suppose we could have done this with NGINX, so this isn't something to fault NGINX for. Just another small win buried in these changes. cc @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:5068,load,load-balancing,5068,https://hail.is,https://github.com/hail-is/hail/pull/12095,2,['load'],['load-balancing']
Performance,"wnload.cse.ucsc.edu/gbdb/hg19/1000Genomes/phase3/ALL.chrY.phase3_integrated_v1a.20130502.genotypes.vcf.gz"", force_bgz=True); ----------------------------------------------------------------------; Initializing Hail with default parameters...; 2022-10-06 15:56:03 WARN Utils:69 - Your hostname, nid resolves to a loopback address: 127.0.1.1; using 192.168.248.80 instead (on interface wlp0s20f3); 2022-10-06 15:56:03 WARN Utils:69 - Set SPARK_LOCAL_IP if you need to bind to another address; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/med/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 2022-10-06 15:56:03 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.1.3; SparkUI available at http://192.168.248.80:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.100-2ea2615a797a; LOGGING: writing to /; --------------------------------------------------------------------------; mt.filter_rows(mt.locus.position==2867101).count_rows(); ```; ### Expected ; Return a count of rows with that condition. ### Error ; ```; FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:208); at is.hail.expr.ir.LoweredTableReader$.makeCoercer(TableIR.scala:135); at is.hail.expr.ir.Gen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12280:1326,load,load,1326,https://hail.is,https://github.com/hail-is/hail/issues/12280,1,['load'],['load']
Performance,"writing a simple command such as read and write produces different type of error, but all related to memory issues. ; The most explicit error is: . ```; error='Cannot allocate memory' (errno=12); # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 7376207872 bytes for committing reserved memory.; ```. But also get this issue in other occasions:. ```; ExecutorLostFailure (executor 2265 exited caused by one of the running tasks) Reason: Container marked as failed: container_1481808189977_0001_01_002296 on host: gnomadpsychk-sw-wpt8.c.wgspd-147615.internal. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1481808189977_0001_01_002296; Exit code: 50; Stack trace: ExitCodeException exitCode=50:; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582); 	at org.apache.hadoop.util.Shell.run(Shell.java:479); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 50. Driver stacktrace:; ```. Log for this last error is here: https://storage.googleapis.com/wgspd_urv/hailNEW.log. I tried different projects and get the same error, both with pyhail and native hail. I don't see the same error when running on cray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186:1383,concurren,concurrent,1383,https://hail.is,https://github.com/hail-is/hail/issues/1186,3,['concurren'],['concurrent']
Performance,"ws() ; ```. `/tmp/foo.tsv.gz` is a gzipped (not blocked) 1GB of 1000 rows each containing one row column and 500k sample columns. The entries are the integers from 0 to 499,999. The first column is the first run (when the JIT is warmed) and the second column is the mean of two subsequent runs. All times in seconds. Everything is necessarily executed on one core. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Aside: using the staged stuff is hard, especially when using multiple methods. A couple thoughts:; - Because SRVB generates a fresh SRVB for arrays and structs, you must thread the srvb through your code gen rather than using a single field, this is annoying and error prone; - `init` is not a first class thing in `FunctionBuilder` and I've arguably made the whole situation more ugly by exposing `addInitInstructions`. Without the ability to place code in the constructor, it is hard to coordinate work between multiple methods.; - When using lots of methods, there's a lot of bookkeeping. I would like a way to define a ""staged class"" that wraps up some of the boilerplate. Not totally clear what I want here, just less boilerplate. Aside2: This is still pretty slow!? Splitting into multiple methods allowed me to interrogate where time was spent. The answer is ""method4"" which is `pa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6987:1930,perform,performance,1930,https://hail.is,https://github.com/hail-is/hail/pull/6987,1,['perform'],['performance']
Performance,x$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Ut,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:12549,load,loadClass,12549,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['load'],['loadClass']
Performance,xecutor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:13390,Load,LoadMatrix,13390,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"xecutors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.115-71fc978b5c22; Error summary: SocketException: Connection reset. -------------------. Some more content from the failing worker job:. ...; 2023-05-04 01:04:35.959 : INFO: executing D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e61c-4fa9-9419-2278528f3c86; 2023-05-04 01:04:37.559 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=132096, peakBytesReadable=129.00 KiB, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.560 : INFO: RegionPool: FREE: 129.0K allocated (129.0K blocks / 0 chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.561 : ERROR: error while applying lowering 'LowerAndExecuteShuffles'; 2023-05-04 01:04:37.600 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:37.601 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.601 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.601 : INFO: RegionPool: FREE: 128.0K allocated (128.0K blocks / 0 chunks), regions.size = 2, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.603 : ERROR: SocketException: Connection reset; From javax.net.ssl.SSLException: Connection reset; 	at sun.security.ssl.Alert.createSSLException(Alert.java:127)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:22696,cache,cache,22696,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['cache'],['cache']
Performance,"xomes_v8/daly_atgu_finnish_swedish_exomes_99prosAndPSC.vds'). vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). intervals = KeyTable.import_interval_list('gs://ibd-exomes/5k.txt'); vds = vds.filter_variants_table(intervals, keep=True). vds = vds.variant_qc(); vds = vds.filter_variants_expr('va.qc.AF>0.01 && va.qc.callRate>0.99', keep=True). table = hc.import_table('gs://daly_atgu_finnish_swedish_exomes_v8/finnibdPSC_isCase.txt', impute=True).key_by('s'); vds = vds.annotate_samples_table(table, root='sa.pheno'). vds = vds.repartition(100); vds = vds.ibd_prune(0.35, tiebreaking_expr=""if (sa1.pheno.iscase) 1 else 0""). vds.export_samples('gs://daly_atgu_finnish_swedish_exomes_v8/99pros_psc_relBelowPIHAT35.tsv','s=s'). ```. But this results in an error:. ```hail: info: Reading table to impute column types; hail: info: Finished type imputation; Loading column `s' as type String (imputed); Loading column `case' as type Int (imputed); Struct{; pheno: Int; }; [Stage 6:====================================================>(3347 + 1) / 3348]Verify Output for is/hail/codegen/generated/C3:; Traceback (most recent call last):; File ""/tmp/0a89b0df-1299-4db1-9e90-0efc77501684/99pros_psc_relatedness_short.py"", line 26, in <module>; vds = vds.ibd_prune(0.35, tiebreaking_expr=""if (sa1.pheno) 1 else 0""); File ""<decorator-gen-322>"", line 2, in ibd_prune; File ""/home/ec2-user/BuildAgent/work/179f3a9ad532f105/python/hail/java.py"", line 112, in handle_py4j; hail.java.FatalError: IllegalStateException: Bytecode failed verification 2. Java stack trace:; java.lang.IllegalStateException: Bytecode failed verification 2; 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:196); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:208); 	at is.hail.expr.CM.runWithDelayedValues(CM.scala:78); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$evalNoTypeCheck(Parser.scala:49); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:62); 	at is.hail.expr.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2178:1082,Load,Loading,1082,https://hail.is,https://github.com/hail-is/hail/issues/2178,2,['Load'],['Loading']
Performance,xt$.$anonfun$scoped$3(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:63); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:350); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:495); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:494); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182); 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.120-f00f916faf78; Error summary: ClassCastException: class is.hail.types.physical.stypes.concrete.SIndexablePointer cannot be cast to class is.hail.types.physical.stypes.concrete.SJavaArrayString (is.hail.types.physical.stypes.concrete.SIndexablePointer and is.hail.types.physical.stypes.concrete.SJavaArrayString are in unnamed module of loader 'app'); ```. ### Version. 0.2.122. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:13912,load,loader,13912,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['load'],['loader']
Performance,xt(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-425cc84a9997; Error summary: HailException: array index out of bounds: 2 / 2; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3653:7815,concurren,concurrent,7815,https://hail.is,https://github.com/hail-is/hail/issues/3653,2,['concurren'],['concurrent']
Performance,xt(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:247); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1275:3439,concurren,concurrent,3439,https://hail.is,https://github.com/hail-is/hail/issues/1275,1,['concurren'],['concurrent']
Performance,xt(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508:10823,concurren,concurrent,10823,https://hail.is,https://github.com/hail-is/hail/issues/3508,2,['concurren'],['concurrent']
Performance,xt.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mut,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:6248,Optimiz,OptimizePass,6248,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['OptimizePass']
Performance,xtRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1234); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:5579,concurren,concurrent,5579,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['concurren'],['concurrent']
Performance,xtras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3901:15109,concurren,concurrent,15109,https://hail.is,https://github.com/hail-is/hail/issues/3901,2,['concurren'],['concurrent']
Performance,"y but we; timeout substantially more frequently. I have observed this myself on my laptop. Just this; morning I saw it happen to Daniel. 2. When using an `aiohttp.AsyncIterablePayload`, it is *critical* to always check if the coroutine; which actually writes to GCS (which is stashed in the variable `request_task`) is still; alive. In the current `main`, we do not do this which causes hangs (in particular the timeout; exceptions are never thrown ergo we never retry). To understand the second problem, you must first recall how writing works in aiogoogle. There are; two Tasks and an `asyncio.Queue`. The terms ""writer"" and ""reader"" are somewhat confusing, so let's; use left and right. The left Task has the owning reference to both the source ""file"" and the; destination ""file"". In particular, it is the *left* Task which closes both ""files"". Moreover, the; left Task reads chunks from the source file and places those chunks on the `asyncio.Queue`. The; right Task takes chunks off the queue and writes those chunks to the destination file. This situation can go awry in two ways. First, if the right Task encounters any kind of failure, it will stop taking chunks off of the; queue. When the queue (which has a size limit of one) is full, the left Task will hang. The system; is stuck. The left Task will wait forever for the right Task to empty the queue. The second scenario is exactly the same except that the left Task is trying to add the ""stop""; message to the queue rather than a chunk. In either case, it is critical that the left Task waits simultaneously on the queue operation *and*; on the right Task completing. If the right Task has died, no further writes can occur and the left; Task must raise an exception. In the first scenario, we do not observe the right Task's exception; because that will be done when we close the `InsertObjectStream` (which represents the destination; ""file""). ---. I also added several types, assertions, and a few missing `async with ... as resp:` b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11830:1205,queue,queue,1205,https://hail.is,https://github.com/hail-is/hail/pull/11830,1,['queue'],['queue']
Performance,"y keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions) to multi-way zip join? I don't see where that happens.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5424:1794,optimiz,optimization,1794,https://hail.is,https://github.com/hail-is/hail/pull/5424,1,['optimiz'],['optimization']
Performance,y$3.apply(RowStore.scala:808); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:807); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:807); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:804); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:804); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-df317f3; Error summary: HailException: found non-left aligned variant: 18:76051965:C:G; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3040:14580,concurren,concurrent,14580,https://hail.is,https://github.com/hail-is/hail/issues/3040,2,['concurren'],['concurrent']
Performance,y(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5524,optimiz,optimizeIR,5524,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['optimiz'],['optimizeIR']
Performance,"y.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4417,cache,cached,4417,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"y.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; U",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3768,cache,cached,3768,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"y3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5974,cache,cached,5974,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"y3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4500,cache,cached,4500,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"y4j/java_gateway.py"", line 1304, in __call__; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 31, in deco; raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; hail.utils.java.FatalError: ClassFormatError: Too many arguments in method signature in class file __C2866stream. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 8.0 failed 20 times, most recent failure: Lost task 3.19 in stage 8.0 (TID 54368) (leo-test-w-8.australia-southeast1-a.c.ourdna-browser.internal executor 14): java.lang.ClassFormatError: Too many arguments in method signature in class file __C2866stream; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:635); 	at is.hail.asm4s.HailClassLoader.liftedTree1$1(HailClassLoader.scala:10); 	at is.hail.asm4s.HailClassLoader.loadOrDefineClass(HailClassLoader.scala:6); 	at is.hail.asm4s.ClassesBytes.$anonfun$load$1(ClassBuilder.scala:64); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:3713,load,loadOrDefineClass,3713,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['load'],['loadOrDefineClass']
Performance,yWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19220,Load,LoadVCF,19220,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"ypes are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non). PStruct. - Concrete implementations (canonical/non). PString. - Concrete implementations (canonical/non). PBinary. - Concrete implementations (canonical/non). ## <a name=""complex-ptypes""></a> Complex PTy",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:1180,load,loading,1180,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['load'],['loading']
Performance,"ython3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:3137,Load,LoadMatrix,3137,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"ze -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEval",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:2158,Optimiz,Optimize,2158,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,"|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib \; --master-machine-type=n1-highmem-8 \; --master-boot-disk-size=100GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-8 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --labels=creator=weisburd_broadinstitute_org \; --max-idle=12h; Starting cluster 'bw2'...; Waiting on operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08].; Waiting for cluster creation operation...; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; Waiting for cluster creation operation...done.; ERROR: (gcloud.beta.dataproc.clusters.create) Operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08] failed: Initialization action failed. Failed action 'gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py', see output in: gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output.; Traceback (most recent call last):; File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6634:1983,perform,performance,1983,https://hail.is,https://github.com/hail-is/hail/issues/6634,1,['perform'],['performance']
Performance,"~Stacked on #10791~. A few high level changes got mixed up in this PR, since I couldn't predict where a thread would lead once I started pulling. If you would like, I can try to disentangle them. Here are the conceptual changes:; * add a CodeBuilder argument to `getEmitParam`, so that parameters which are pointers to region values can be loaded into `SValue`s with multiple locals; * make `EmitValue` a concrete class, consisting of an optional boolean value and an `SValue`; * add `valueTuple` to both `SValue` and `EmitValue`; * copy the `SCode` interface onto `SValue`, to make it easier to replace `SCode`s with `SValues` ; * add `loadToSValue` to `SingleCodeType`; * add `SStreamValue`. This loses the single use assertion, but that doesn't seem like it asserts much, since the stream producer can be freely accessed without memoizing the `SStreamCode`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10797:340,load,loaded,340,https://hail.is,https://github.com/hail-is/hail/pull/10797,2,['load'],"['loadToSValue', 'loaded']"
Performance,"~Stacked on #10906~. This PR refactors `MethodBuilder.invoke` and `EmitCodeBuilder.invoke(S)Code` to take/return values. * `invoke` now takes a `CodeBuilderLike`. It is used in places where there is only access to a `CodeBuilder` (not an `EmitCodeBuilder`), so I had to use the generic interface, and had to move a couple methods on `EmitCodeBuilder` to `CodeBuilderLike`. I have never understood this Emit/non-Emit split; would be a great simplification if we could collapse it.; * This change pushed some (S)Code->(S)Value refactorings inside some aggregator implementations, which generate and invoke internal methods.; * I had to keep a version of `MethodBuilder.invoke` that doesn't take a CodeBuilder, for use in `ThisLazyFieldRef.get`. Will have to think more about how this should work when Code is (mostly) gone. Maybe lazy fields should not subclass Value, and to access a lazy field requires an explicit `load(cb: CodeBuilder): Value[T]`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10907:916,load,load,916,https://hail.is,https://github.com/hail-is/hail/pull/10907,1,['load'],['load']
Performance,"~Stacked on #12376~. Reworks the staged index reader query algorithm. Before, `queryInterval` did three traversals from root to leaf: one each for the start and end of the interval, and one to find the first record. Now it always does a single traversal, visiting one or two nodes at each level (two if the paths for the start and end keys diverge), and doing a single binary search at each visited node. This is probably not a significant performance improvement when doing a single query per partition, but could be beneficial when using the new `query_table` per row.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12399:440,perform,performance,440,https://hail.is,https://github.com/hail-is/hail/pull/12399,1,['perform'],['performance']
Performance,"~~Stacked on #8172~~. Implement emitters for StreamScan, RunAggScan, and StreamLeftJoinDistinct. These complete the handling in the new emitter for non-root stream nodes (i.e. those which take stream children). Thus it is now safe to delete non-root nodes from the previous EmitStream. This also implements length tracking in the new EmitStream, and adds back the optimizations that take advantage of knowing the length, plus some we weren't doing before, like in ArraySort and CollectDistributedArray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8204:364,optimiz,optimizations,364,https://hail.is,https://github.com/hail-is/hail/pull/8204,1,['optimiz'],['optimizations']
Performance,"~~Stacked on #9320~~. Use the framework introduced in #9320 to make the IR parser stack safe. This touches a lot of lines, but it is a completely mechanical refactoring. I did some preliminary benchmarking by timing the parse of the IR in `test_ld_prune`. (I chose that test fairly arbitrarily, and we can probably find better test cases, or generate synthetic large IRs.) Running a loop parsing the same IR repeatedly, with 10 burn-in rounds, and 60 timed rounds, I got the following results:; * On main; ```; quartiles = [4.55816, 5.209579, 5.647135]; avg = 5.332496950000001, std = 1.13761574944604; ```; * Using `StackSafe`, without the optimization in `repUntil`; ```; quantiles = [4.610798, 5.09793, 7.159075]; avg = 5.7519015000000016, std = 1.612397075594852; ```; * Using `StackSafe, with the optimization which reuses the `cont` closure, instead of allocating a new one for each token.; ```; quantiles = [4.466849, 4.826873, 5.719238]; avg = 5.2787357833333335, std = 1.272006325411254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9332:641,optimiz,optimization,641,https://hail.is,https://github.com/hail-is/hail/pull/9332,2,['optimiz'],['optimization']
Performance,added optimized keyBy in scala / python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1572:7,optimiz,optimized,7,https://hail.is,https://github.com/hail-is/hail/pull/1572,1,['optimiz'],['optimized']
Safety,	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:6336,abort,abortStage,6336,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['abort'],['abortStage']
Safety, 	at is.hail.codegen.generated.C1.apply(Unknown Source); 	at is.hail.codegen.generated.C1.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:270); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:268); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:304); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:300); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1743); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1741); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at is.hail.annotations.UnsafeIndexedSeq.foreach(UnsafeRow.scala:51); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1741); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1734); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1734); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1728); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Ta,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3276:1396,Unsafe,UnsafeIndexedSeq,1396,https://hail.is,https://github.com/hail-is/hail/issues/3276,1,['Unsafe'],['UnsafeIndexedSeq']
Safety," ### What you did:. copied from gitter:. I'm using hail 0.1 on dataproc and trying to import a vcf from the local filesystem; run(""wget ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh{0}/clinvar.vcf.gz -O /tmp/clinvar.vcf.gz"".format(args.genome_version)); run(""ls -l /tmp/clinvar.vcf.gz""); vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); The output is; ls -l /tmp/clinvar.vcf.gz; -rw-r--r-- 1 root root 16218805 Jun 14 20:21 /tmp/clinvar.vcf.gz; Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputF",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:1243,abort,aborted,1243,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['abort'],['aborted']
Safety," (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Itera",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:2251,Unsafe,UnsafeRow,2251,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['Unsafe'],['UnsafeRow']
Safety," /etc/prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /dev/termination-log; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/resolv.conf; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hostname; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecting to localhost:9090 (127.0.0.1:9090); wget: server returned error: HTTP/1.1 503 Service Unavailable; /prometheus $ ; ```. https://github.com/prometheus/prometheus/issues/5727#issuecomment-510818825; https://github.com/prometheus/prometheus/issues/4324#issuecomment-460243182. ```; # k logs -n monitoring prometheus-0 ; level=info ts=2019-07-31T15:45:51.990Z caller=main.go:286 msg=""no time or size retention was set so using the default time retention"" duration=15d; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:322 msg=""Starting Prometheus"" version=""(version=2.10.0, branch=HEAD, revision=d20e84d0fb64aff2f62a977adc8cfb656da4e286)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:323 build_context=""(go=go1.12.5, user=root@a49185acd9b0, date=20190525-12:28:13)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:324 host_details=""(L",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:1868,recover,recover,1868,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['recover'],['recover']
Safety," 0, 0, 2, 1, 0, 4, 2, 0]; ```. A library developer may want to remove keys while preserving order so as to implement the above methods. Because all sorts in Hail are unstable, this is a delicate feat. There are two cases: zero-length key, non-zero-length key. When the key is of zero-length, the data may be sorted in some unknown and arbitrary order. Consider for example:. ```; In [45]: import hail as hl ; ...: mt = hl.utils.range_matrix_table(3,3) ; ...: mt = mt.key_cols_by().choose_cols([1,2,0]) ; ...: mt.cols().collect() ; ...: ; Out[45]: [Struct(col_idx=1), Struct(col_idx=2), Struct(col_idx=0)]; ```. Or importing data with no key. In this case it is crucial to *not* call `order_by()` or `key_by()` because both permit hail to arbitrarily reorder the entire dataset (we are unstably sorting by an empty key, ergo, all values are equal). When the key is of non-zero-length, then data must be sorted by the key and the ordering of rows with equivalent keys is undefined. In this case, it is safe to `order_by(*t.key)`. # User Expectations. There is a subtle difference in how we treat zero-length keyed (ZLK) objects and non-zero-length keyed objects. We try to preserve the data ordering of ZLK objects. In particular, users would be pretty surprised if `import_table(..., key=[])` shuffled the rows. Additionally, users expect (and we document) that `mt.key_cols_by().cols()` does not shuffle the rows. In contrast, we treat a non-zero-length keyed object as if any ordering beyond that defined by the key is irrelevant. Is this surprising to a user? Suppose they had a text file ordered by `family id, sample id`, they might reasonably expect that `import_table(..., key=['family id'])` does not reorder the rows within a family. Hail doesn't guarantee this even though we do make pains to not reorder the same data imported by `import_table(..., key=[])`. # Ordering and the Optimizer. Currently, the Hail optimizer does not remove a `choose_cols` that precedes a `order_by()`. Nor does ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6929:4051,safe,safe,4051,https://hail.is,https://github.com/hail-is/hail/issues/6929,1,['safe'],['safe']
Safety," 233s, 233s; linreg with 10 PCs, 8 cores: 23s, 24s, 24s; pca, 8 cores: 179s. hail command:; ~/hail/build/install/hail/bin/hail read -i ~/data/profile75.vds linreg -c ~/data/profile75.cov -f ~/data/profile.fam -o ~/data/profile75.linreg. read: 1570.888546; linreg: 58496.508588. plink vcf command to create bed/bim/fam:; ./plink --vcf ~/data/profile75.vcf.bgz; - rename plink.bed/bim/fam to profile75.bed/bim/fam; - create covar with FID column by doubling first column of cov file (use cut and paste in bash). plink linreg command:; time ./plink --bfile profile75 --double-id --pheno ~/data/profile.pheno --allow-no-sex --covar ~/data/profile75.covar --linear --out ~/data/profile75.plink. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/profile75.plink.log.; Options in effect:; --allow-no-sex; --bfile profile75; --covar /Users/Jon/data/profile75.covar; --double-id; --linear; --out /Users/Jon/data/profile75.plink; --pheno /Users/Jon/data/profile.pheno. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 74885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/profile75.plink.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you may want; to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.914041.; 74885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model association results to; /Users/Jon/data/profile75.plink.assoc.linear ... done. real 0m38.837s; user 0m38.609s; sys 0m0.187s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/61:1222,detect,detected,1222,https://hail.is,https://github.com/hail-is/hail/pull/61,1,['detect'],['detected']
Safety," 448 # Otherwise it looks like a bug in the code. File /opt/conda/lib/python3.10/http/client.py:1375, in HTTPConnection.getresponse(self); 1374 try:; -> 1375 response.begin(); 1376 except ConnectionError:. File /opt/conda/lib/python3.10/http/client.py:318, in HTTPResponse.begin(self); 317 while True:; --> 318 version, status, reason = self._read_status(); 319 if status != CONTINUE:. File /opt/conda/lib/python3.10/http/client.py:287, in HTTPResponse._read_status(self); 284 if not line:; 285 # Presumably, the server closed the connection before; 286 # sending a valid response.; --> 287 raise RemoteDisconnected(""Remote end closed connection without""; 288 "" response""); 289 try:. RemoteDisconnected: Remote end closed connection without response. During handling of the above exception, another exception occurred:. ProtocolError Traceback (most recent call last); File /opt/conda/lib/python3.10/site-packages/requests/adapters.py:487, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies); 486 try:; --> 487 resp = conn.urlopen(; 488 method=request.method,; 489 url=url,; 490 body=request.body,; 491 headers=request.headers,; 492 redirect=False,; 493 assert_same_host=False,; 494 preload_content=False,; 495 decode_content=False,; 496 retries=self.max_retries,; 497 timeout=timeout,; 498 chunked=chunked,; 499 ); 501 except (ProtocolError, OSError) as err:. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:787, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw); 785 e = ProtocolError(""Connection aborted."", e); --> 787 retries = retries.increment(; 788 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]; 789 ); 790 retries.sleep(). File /opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py:550, in Retry.increment(self, method, url, response, error, _pool, _stacktrace); 549 if read is False or not self._is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:6333,timeout,timeout,6333,https://hail.is,https://github.com/hail-is/hail/issues/13960,1,['timeout'],['timeout']
Safety," <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h2>3.9.11 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faste",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14357:1843,avoid,avoid,1843,https://hail.is,https://github.com/hail-is/hail/pull/14357,3,['avoid'],['avoid']
Safety," <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Dropped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/5128f7f4ff73b165579006a8336978efaeeca07a""><code>5128f7f</code></a> Fix CHANGES</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/18e718a56f9def7dc40bb9ce3a2962a8fb0c883c""><code>18e718a</code></a> Bump to 4.0.2</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/48d1c4ce923b1da75976d7e2a6ca9234b3092c16""><code>48d1c4c</code></a> Setup towncrier</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/2ede7d73e55f8d1a2279d78861af0009d96219fb""><code>2ede7d7</code></a> Fix annotations on <code>__exit__</code> and <code>__aexit__</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/8685c60dc0e82ee246fbe3d1aa272d1dfe57c24c""><code>8685c60</code></a> Bump mypy from 0.910 to 0.920 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/f0a0914345b224448220aeb00d71e6a04a5d24bd""><code>f0a0914</code></a> Bump twine from 3.7.0 to 3.7.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-time",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:4172,timeout,timeout,4172,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety," <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://github.com/aio-libs/async-timeout/issues/259&gt;</code><em>, <code>[#274](https://github.com/aio-libs/async-timeout/issues/274) &lt;https://github.com/aio-libs/async-timeout/issues/274&gt;</code></em></li>; </ul>; <h1>4.0.1 (2121-11-10)</h1>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h1>4.0.0 (2021-11-01)</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Dropped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/5128f7f4ff73b165579006a8336978efaeeca07a""><code>5128f7f</code></a> Fix CHANGES</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/18e718a5",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:3167,timeout,timeout,3167,https://hail.is,https://github.com/hail-is/hail/pull/11465,2,['timeout'],['timeout']
Safety," <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Drooped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/blob/master/CHANGES.rst"">async-timeout's changelog</a>.</em></p>; <blockquote>; <h1>4.0.2 (2021-12-20)</h1>; <h2>Misc</h2>; <ul>; <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://github.com/aio-libs/async-timeout/issues/259&gt;</code><em>, <code>[#274](https://github.com/aio-libs/async-timeout/issues/274) &lt;https://github.com/aio-libs/async-timeout/issues/274&gt;</code></em></li>; </ul>; <h1>4.0.1 (2121-11-10)</h1>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h1>4.0.0 (2021-11-01)</h1>; <ul>; <li>; <p>Implem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:2058,timeout,timeout,2058,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety," [<a href=""https://github.com/evanmiller""><code>@evanmiller</code></a>]</li>; <li>Fixed reading FLI/FLC images with a prefix chunk <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7804"">#7804</a> [<a href=""https://github.com/twolife""><code>@twolife</code></a>]</li>; <li>Updated package name for Tidelift <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7810"">#7810</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Removed unused code <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7744"">#7744</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/main/CHANGES.rst"">pillow's changelog</a>.</em></p>; <blockquote>; <h2>10.3.0 (2024-04-01)</h2>; <ul>; <li>; <p>CVE-2024-28219: Use <code>strncpy</code> to avoid buffer overflow <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7928"">#7928</a>; [radarhere, hugovk]</p>; </li>; <li>; <p>Deprecate <code>eval()</code>, replacing it with <code>lambda_eval()</code> and <code>unsafe_eval()</code> <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7927"">#7927</a>; [radarhere, hugovk]</p>; </li>; <li>; <p>Raise <code>ValueError</code> if seeking to greater than offset-sized integer in TIFF <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7883"">#7883</a>; [radarhere]</p>; </li>; <li>; <p>Add <code>--report</code> argument to <code>__main__.py</code> to omit supported formats <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7818"">#7818</a>; [nulano, radarhere, hugovk]</p>; </li>; <li>; <p>Added RGB to I;16, I;16L, I;16B and I;16N conversion <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7918"">#7918</a>, <a href=""https://redirect.github.com/python-pillow/Pi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:10201,avoid,avoid,10201,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['avoid'],['avoid']
Safety," _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; usr/local/lib/python3.9/dist-packages/hailtop/batch_client/client.py:84: in wait; return async_to_blocking(self._async_job.wait()); usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:156: in async_to_blocking; return loop.run_until_complete(task); usr/lib/python3.9/asyncio/base_events.py:634: in run_until_complete; self.run_forever(); usr/lib/python3.9/asyncio/base_events.py:601: in run_forever; self._run_once(); usr/lib/python3.9/asyncio/base_events.py:1869: in _run_once; event_list = self._selector.select(timeout); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <selectors.EpollSelector object at 0x7fae890f2d30>; timeout = 15.402000000000001. def select(self, timeout=None):; if timeout is None:; timeout = -1; elif timeout <= 0:; timeout = 0; else:; # epoll_wait() has a resolution of 1 millisecond, round away; # from zero to wait *at least* timeout seconds.; timeout = math.ceil(timeout * 1e3) * 1e-3; ; # epoll_wait() expects `maxevents` to be greater than zero;; # we want to make sure that `select()` can be called when no; # FD is registered.; max_ev = max(len(self._fd_to_key), 1); ; ready = []; try:; > fd_event_list = self._selector.poll(timeout, max_ev); E Failed: Timeout >360.0s. usr/lib/python3.9/selectors.py:469: Failed; ------------------------------ Captured log setup ------------------------------; 2023-09-06T21:45:24 INFO test.conftest conftest.py:14:log_before_after starting test; 2023-09-06T21:45:24 INFO hailtop.aiocloud.aioazure.credentials credentials.py:99:default_credentials using credentials file /test-gsa-key/key.json; ------------------------------ Captured log call -------------------------------; 2023-09-06T21:45:25 INFO azure.identity.aio._internal.get_token_mixin get_token_mixin.py:93:get_token ClientSecretCredential.get_token succeeded; 2023-09-06T21:45:25 INFO batch_client.aioclient aioclient.py:809:_submit created batch 1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13582:3293,timeout,timeout,3293,https://hail.is,https://github.com/hail-is/hail/issues/13582,1,['timeout'],['timeout']
Safety," a long or tuple of longs which is guaranteed to be distinct on every execution of `child`.; * Uids are typically created at the leaves of pipelines (`TableRead`, `StreamRange`, etc.), and propagated upwards. There was a phase-ordering conflict that had to be worked around:; * IRs must be given explict rng state and uid semantics as early as possible, to ensure determinism.; * The transformation to explicitly pass rng states and uids must happen during IR construction. If it happened later, it would create new IR objects, which would defeat the python CSE pass (which only recognizes equivalent subexpressions when they are represented by the same python object).; * The rng explication requires some type information.; * Types on the python IR are assigned after the IR is fully constructed. To fix this:; * `Ref`'s must be given a type at construction; * `TopLevelReference`s are the only case that needs to be constructed before a type is known. But they are always constructed wrapped in a `SelectFields` or `GetField`, whose type is known at construction. I added new IR classes `SelectedTopLevelReference` and `ProjectedTopLevelReference` for these two cases, which are thin wrappers which don't appear in the rendered IR.; * `construct_expr` always assigns a type to the ir. Bottom-up type construction will later assert equality with the assigned type. This caught some existing bugs, where expression type and ir type didn't agree.; * At construction of the root node of a stream/table/matrixtable pipeline (i.e. a non-stream value ir with at least one stream/table/matrixtable child), recursively rewrite the contained pipeline(s) to make rng states and uids explicit. This is safe, since stream/table/matrixtable IRs won't be CSE'd, because they may only be evaluated once. Contained value IRs are not rewritten, only wrapped with bindings which define the rng state. These are currently non-functional changes, as `ApplySeeded` still uses the old rng, and will ignore the rng state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11847:2160,safe,safe,2160,https://hail.is,https://github.com/hail-is/hail/pull/11847,1,['safe'],['safe']
Safety," as you need Javascript, the choice becomes Vanilla JS, JQuery, or something more structured. Vanilla JS requires a lot of boilerplate (verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be sl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:1873,avoid,avoid,1873,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['avoid'],['avoid']
Safety," book air (M1) , spark local mode; Rocky Linux 8.5 , spark local mode; Rocky Linux 8.5 , spark yarn cluster mode. - how to reproduce; ```; import os; os.environ['PYSPARK_SUBMIT_ARGS'] = ' \; --jars \; /Users/username/miniforge3/envs/hail/lib/python3.9/site-packages/hail/backend/hail-all-spark.jar \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; pyspark-shell '. from pyspark import SparkContext; sc=SparkContext.getOrCreate(). import hail as hl; hl.init(sc=sc); ```. - Error logs ; ```; 22/05/11 14:31:21 WARN Utils: Your hostname, spacerider.local resolves to a loopback address: 127.0.0.1; using 172.20.10.4 instead (on interface en6); 22/05/11 14:31:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/username/miniforge3/envs/hail/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 22/05/11 14:31:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Input In [2], in <cell line: 6>(); 3 sc = spark._sc; 5 import hail as h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11827:1210,unsafe,unsafe,1210,https://hail.is,https://github.com/hail-is/hail/issues/11827,1,['unsafe'],['unsafe']
Safety," client does not close; it.</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/blob/master/CHANGES.rst"">aiohttp's changelog</a>.</em></p>; <blockquote>; <h1>3.9.4 (2024-04-11)</h1>; <h2>Bug fixes</h2>; <ul>; <li>; <p>The asynchronous internals now set the underlying causes; when assigning exceptions to the future objects; -- by :user:<code>webknjaz</code>.</p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>8089</code>.</p>; </li>; <li>; <p>Treated values of <code>Accept-Encoding</code> header as case-insensitive when checking; for gzip files -- by :user:<code>steverep</code>.</p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>8104</code>.</p>; </li>; <li>; <p>Improved the DNS resolution performance on cache hit -- by :user:<code>bdraco</code>.</p>; <p>This is achieved by avoiding an :mod:<code>asyncio</code> task creation in this case.</p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>8163</code>.</p>; </li>; <li>; <p>Changed the type annotations to allow <code>dict</code> on :meth:<code>aiohttp.MultipartWriter.append</code>,; :meth:<code>aiohttp.MultipartWriter.append_json</code> and; :meth:<code>aiohttp.MultipartWriter.append_form</code> -- by :user:<code>cakemanny</code></p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>7741</code>.</p>; </li>; <li>; <p>Ensure websocket transport is closed when client does not close it; -- by :user:<code>bdraco</code>.</p>; <p>The transport could remain open if the client did not close it. This; change ensures the transport is closed when the client does not close; it.</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/b3397c7ac44fc80206d28",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14477:2868,avoid,avoiding,2868,https://hail.is,https://github.com/hail-is/hail/pull/14477,6,['avoid'],['avoiding']
Safety, com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: sun.reflect.generics.reflectiveObjects.NotImplementedException; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 10 more; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:1720,Unsafe,UnsafeRow,1720,https://hail.is,https://github.com/hail-is/hail/issues/4215,1,['Unsafe'],['UnsafeRow']
Safety," constructor `optional<T>`, i.e. base types don't encode missingness. Emits a single bit in the encoding. Can invert this bit to control whether missing values come first or last in the ordering. If missing, nothing is emitted after.; - sort-order; - treat reversing the default ordering as a type constructor `reverse<T>`; - simply inverts the encoding bitwise; - primitive types; - same as in datafusion, encoding has same size as original type; - signed integers - flip the sign bit; - floating point numbers - if sign bit is set, invert all bits, otherwise only flip the sign bit; - arrays; - before each element and after last element, emit continuation bit (0 if no more elements); - pad before each element. This prevents a variable number of missing bits packing into a byte; - strings and byte-arrays; - simply use null-terminated strings (being careful to do this in a unicode-safe way); - structs; - simply concatenate element encodings. safe because codes are prefix-free; - key structs; - support variable length ""interval endpoints""; - e.g. for a key type `struct<t1, t2>`, the interval `[{a}, {a, b})` contains all keys with first field `a` and second field less than `b`. We break it into two ""interval endpoints"", `({a}, -1)` and `({a, b}, -1)`, which consist of a struct value which is a prefix of the key struct type, and a ""sign"". In this case, both endpoints ""lean left"".; - needed for working with partitioners at runtime; - like an array with fixed but heterogenous types and a max length; - before each element and after last element, emit two continuation bits; - `00` - end of key, leans left (less than all longer keys with this prefix); - `01` - continue, or after last key field of actual key value (not interval endpoint); - unambiguous because key value can't terminate early, and can't continue past last key field (max length); - `11` - end of key, leans right (greater than all longer keys with this prefix); - after each element, pad. # Implementation sketch; Concr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14396:2254,safe,safe,2254,https://hail.is,https://github.com/hail-is/hail/issues/14396,1,['safe'],['safe']
Safety," count in genotypes, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; ##INFO=<ID=BaseQRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities"">; ##INFO=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth; some reads may have been filtered"">; ##INFO=<ID=DS,Number=0,Type=Flag,Description=""Were any of the samples downsampled?"">; ##INFO=<ID=ExcessHet,Number=1,Type=Float,Description=""Phred-scaled p-value for exact test of excess heterozygosity"">; ##INFO=<ID=FS,Number=1,Type=Float,Description=""Phred-scaled p-value using Fisher's exact test to detect strand bias"">; ##INFO=<ID=InbreedingCoeff,Number=1,Type=Float,Description=""Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation"">; ##INFO=<ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">; ##INFO=<ID=MQRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities"">; ##INFO=<ID=QD,Number=1,Type=Float,Description=""Variant Confidence/Quality by Depth"">; ##INFO=<ID=RAW_MQ,Number=1,Type=Float,Description=""Raw data for RMS Mapping Quality"">; ##INFO=<ID=ReadPosRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias"">; ##INFO=<ID=SOR,Number=1,Type=Float,Description=""Symme",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:8090,detect,detect,8090,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['detect'],['detect']
Safety," creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured out the rest of the terraform/bootstrap process should follow pretty quickly. Stacked on #10911",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10919:1235,redund,redundant,1235,https://hail.is,https://github.com/hail-is/hail/pull/10919,1,['redund'],['redundant']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI1ZDhmZDhmZC1mZGUxLTRiYmMtYWMzMi0xOTE1NmY0ZDFjZjIiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjVkOGZkOGZkLWZkZTEtNGJiYy1hYzMyLTE5MTU2ZjRkMWNmMiJ9fQ=="" width=""0"" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/7fad328c-8d01-4768-8813-73d6c644e2d4?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/7fad328c-8d01-4768-8813-73d6c644e2d4?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""5d8fd8fd-fde1-4bbc-ac32-19156f4d1cf2"",""prPublicId"":""5d8fd8fd-fde1-4bbc-ac32-19156f4d1cf2"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""7fad328c-8d01-4768-8813-73d6c644e2d4"",""projectUrl"":""https://app.snyk.io/org/danking/project/7fad328c-8d01-4768-8813-73d6c644e2d4?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13112:3094,remediat,remediationStrategy,3094,https://hail.is,https://github.com/hail-is/hail/pull/13112,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI2YzU3NmY1Yi1lNGM5LTQ4ZjctYmYxNy04YjEzOTIxODlmZDQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjZjNTc2ZjViLWU0YzktNDhmNy1iZjE3LThiMTM5MjE4OWZkNCJ9fQ=="" width=""0"" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""6c576f5b-e4c9-48f7-bf17-8b1392189fd4"",""prPublicId"":""6c576f5b-e4c9-48f7-bf17-8b1392189fd4"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13097:3017,remediat,remediationStrategy,3017,https://hail.is,https://github.com/hail-is/hail/pull/13097,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3ZGVlZGFlMy1mZmE3LTQxYmUtOGY4MS1lNmYwZTA5YTczOTMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjdkZWVkYWUzLWZmYTctNDFiZS04ZjgxLWU2ZjBlMDlhNzM5MyJ9fQ=="" width=""0"" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""7deedae3-ffa7-41be-8f81-e6f0e09a7393"",""prPublicId"":""7deedae3-ffa7-41be-8f81-e6f0e09a7393"",""dependencies"":[{""name"":""urllib3"",""from"":""1.26.16"",""to"":""1.26.17""}],""packageManager"":""pip"",""projectPublicId"":""701495b8-b53d-48af-82fe-1a6c57aa56cb"",""projectUrl"":""https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-URLLIB3-5926907""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[581],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13770:3109,remediat,remediationStrategy,3109,https://hail.is,https://github.com/hail-is/hail/pull/13770,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIwMzdiOGRmZS1hZDA4LTRmZjUtYTFkOC1hNGM4Nzg2N2NkYjAiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjAzN2I4ZGZlLWFkMDgtNGZmNS1hMWQ4LWE0Yzg3ODY3Y2RiMCJ9fQ=="" width=""0"" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""037b8dfe-ad08-4ff5-a1d8-a4c87867cdb0"",""prPublicId"":""037b8dfe-ad08-4ff5-a1d8-a4c87867cdb0"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13159:3109,remediat,remediationStrategy,3109,https://hail.is,https://github.com/hail-is/hail/pull/13159,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIyNWE2ZGYzMi1kYmEzLTQzOTctYmIyNC0zNjdlMzhmZWQ3ZmUiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjI1YTZkZjMyLWRiYTMtNDM5Ny1iYjI0LTM2N2UzOGZlZDdmZSJ9fQ=="" width=""0"" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""25a6df32-dba3-4397-bb24-367e38fed7fe"",""prPublicId"":""25a6df32-dba3-4397-bb24-367e38fed7fe"",""dependencies"":[{""name"":""urllib3"",""from"":""1.26.17"",""to"":""1.26.18""}],""packageManager"":""pip"",""projectPublicId"":""701495b8-b53d-48af-82fe-1a6c57aa56cb"",""projectUrl"":""https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-URLLIB3-6002459""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[496],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13848:3109,remediat,remediationStrategy,3109,https://hail.is,https://github.com/hail-is/hail/pull/13848,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIzM2VkMzM4Ny0zZTVmLTRkZDgtYjIxYy1iYzIyNzk4ODViZjMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjMzZWQzMzg3LTNlNWYtNGRkOC1iMjFjLWJjMjI3OTg4NWJmMyJ9fQ=="" width=""0"" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/b72ce54d-5de3-48e5-a1d4-6f8967681a12?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/b72ce54d-5de3-48e5-a1d4-6f8967681a12?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""33ed3387-3e5f-4dd8-b21c-bc2279885bf3"",""prPublicId"":""33ed3387-3e5f-4dd8-b21c-bc2279885bf3"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""b72ce54d-5de3-48e5-a1d4-6f8967681a12"",""projectUrl"":""https://app.snyk.io/org/danking/project/b72ce54d-5de3-48e5-a1d4-6f8967681a12?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13107:3002,remediat,remediationStrategy,3002,https://hail.is,https://github.com/hail-is/hail/pull/13107,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIzYWEwNDk2OC02NDIxLTRmODktYTBjYy03MjE4MzExNDNiZGQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjNhYTA0OTY4LTY0MjEtNGY4OS1hMGNjLTcyMTgzMTE0M2JkZCJ9fQ=="" width=""0"" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""3aa04968-6421-4f89-a0cc-721831143bdd"",""prPublicId"":""3aa04968-6421-4f89-a0cc-721831143bdd"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13100:3105,remediat,remediationStrategy,3105,https://hail.is,https://github.com/hail-is/hail/pull/13100,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJiNzM2NzI0Yi1hY2RiLTRiOTUtYWQwMy1hYWI3MjkyZGNlYzQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImI3MzY3MjRiLWFjZGItNGI5NS1hZDAzLWFhYjcyOTJkY2VjNCJ9fQ=="" width=""0"" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""b736724b-acdb-4b95-ad03-aab7292dcec4"",""prPublicId"":""b736724b-acdb-4b95-ad03-aab7292dcec4"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13116:3009,remediat,remediationStrategy,3009,https://hail.is,https://github.com/hail-is/hail/pull/13116,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlNDQxZTBmNS1jZDQ4LTQzZDUtYTdkMy1kMTM4YzQ2ZTc2NTgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImU0NDFlMGY1LWNkNDgtNDNkNS1hN2QzLWQxMzhjNDZlNzY1OCJ9fQ=="" width=""0"" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""e441e0f5-cd48-43d5-a7d3-d138c46e7658"",""prPublicId"":""e441e0f5-cd48-43d5-a7d3-d138c46e7658"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13158:3101,remediat,remediationStrategy,3101,https://hail.is,https://github.com/hail-is/hail/pull/13158,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlZjQxMWYxOC1hM2JiLTQ1YzgtODFjOS1hNmNhNjI4MWI1ZjMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImVmNDExZjE4LWEzYmItNDVjOC04MWM5LWE2Y2E2MjgxYjVmMyJ9fQ=="" width=""0"" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""ef411f18-a3bb-45c8-81c9-a6ca6281b5f3"",""prPublicId"":""ef411f18-a3bb-45c8-81c9-a6ca6281b5f3"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""701495b8-b53d-48af-82fe-1a6c57aa56cb"",""projectUrl"":""https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13108:3092,remediat,remediationStrategy,3092,https://hail.is,https://github.com/hail-is/hail/pull/13108,1,['remediat'],['remediationStrategy']
Safety," devel-6bb4670. ### What you did:; A number of variant QC steps, then a `vds.write`; The error is probably caused by one of the previous steps. If it helps I can comment out earlier parts to narrow down what actually triggers the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3063:1239,abort,aborted,1239,https://hail.is,https://github.com/hail-is/hail/issues/3063,1,['abort'],['aborted']
Safety," for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe without using locks.; ntodo = len(self._ready); for i in range(ntodo):; > handle = self._ready.popleft(); E IndexError: pop from an empty deque. /usr/lib/python3.9/asyncio/base_events.py:1890: IndexError; ```. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13997:3581,timeout,timeout,3581,https://hail.is,https://github.com/hail-is/hail/issues/13997,2,"['safe', 'timeout']","['safe', 'timeout']"
Safety," full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe without using locks.; ntodo = len(self._ready); for i in range(ntodo):; > handle = self._ready.popleft(); E IndexError: pop from an empty deque. /usr/lib/python3.9/asyncio/base_events.py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13997:3434,timeout,timeout,3434,https://hail.is,https://github.com/hail-is/hail/issues/13997,1,['timeout'],['timeout']
Safety," full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(hand",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10705:2523,timeout,timeout,2523,https://hail.is,https://github.com/hail-is/hail/pull/10705,1,['timeout'],['timeout']
Safety," in FastPath, not needed.</li>; </ul>; <h1>v4.11.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/369"">#369</a>: Fixed bug where <code>EntryPoint.extras</code> was returning; match objects and not the extras strings.</li>; </ul>; <h1>v4.11.1</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/367"">#367</a>: In <code>Distribution.requires</code> for egg-info, if <code>requires.txt</code>; is empty, return an empty list.</li>; </ul>; <h1>v4.11.0</h1>; <ul>; <li>bpo-46246: Added <code>__slots__</code> to <code>EntryPoints</code>.</li>; </ul>; <h1>v4.10.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/365"">#365</a> and bpo-46546: Avoid leaking <code>method_name</code> in; <code>DeprecatedList</code>.</li>; </ul>; <h1>v4.10.1</h1>; <h1>v2.1.3</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/361"">#361</a>: Avoid potential REDoS in <code>EntryPoint.pattern</code>.</li>; </ul>; <h1>v4.10.0</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/354"">#354</a>: Removed <code>Distribution._local</code> factory. This; functionality was created as a demonstration of the; possible implementation. Now, the; <code>pep517 &lt;https://pypi.org/project/pep517&gt;</code>_ package; provides this functionality directly through; <code>pep517.meta.load &lt;https://github.com/pypa/pep517/blob/a942316305395f8f757f210e2b16f738af73f8b8/pep517/meta.py#L63-L73&gt;</code>_.</li>; </ul>; <h1>v4.9.0</h1>; <ul>; <li>Require Python 3.7 or later.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python/importlib_metadata/commit/99a2ec4489da45407d8224be2804ff323a164ac0""><code>99a2ec4</code></a> Update changelog.</li>; <li><a href=""https://github.com/python/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11596:1479,Avoid,Avoid,1479,https://hail.is,https://github.com/hail-is/hail/pull/11596,1,['Avoid'],['Avoid']
Safety, is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:5956,abort,abortStage,5956,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['abort'],['abortStage']
Safety, is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:8088,abort,abortStage,8088,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['abort'],['abortStage']
Safety," more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJiN2QwMTZlZS0zODA0LTQwMjItOWE0Yi01MzExNjZhNjBjMWQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImI3ZDAxNmVlLTM4MDQtNDAyMi05YTRiLTUzMTE2NmE2MGMxZCJ9fQ=="" width=""0"" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""b7d016ee-3804-4022-9a4b-531166a60c1d"",""prPublicId"":""b7d016ee-3804-4022-9a4b-531166a60c1d"",""dependencies"":[{""name"":""cryptography"",""from"":""41.0.3"",""to"":""41.0.4""}],""packageManager"":""pip"",""projectPublicId"":""701495b8-b53d-48af-82fe-1a6c57aa56cb"",""projectUrl"":""https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CRYPTOGRAPHY-5914629""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[611],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13698:3115,remediat,remediationStrategy,3115,https://hail.is,https://github.com/hail-is/hail/pull/13698,1,['remediat'],['remediationStrategy']
Safety," order in the builder (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2927"">#2927</a>).</li>; <li>[cu2quPen] Add Cu2QuMultiPen that converts multiple outlines at a time in interpolation compatible way; its methods take a list of tuples arguments that would normally be passed to individual segment pens, and at the end it dispatches the converted outlines to each pen (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2912"">#2912</a>).</li>; <li>[reverseContourPen/ttGlyphPen] Add outputImpliedClosingLine option (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2913"">#2913</a>, <a href=""https://redirect.github.com/fonttools/fonttools/issues/2914"">#2914</a>, <a href=""https://redirect.github.com/fonttools/fonttools/issues/2921"">#2921</a>, <a href=""https://redirect.github.com/fonttools/fonttools/issues/2922"">#2922</a>, <a href=""https://redirect.github.com/fonttools/fonttools/issues/2995"">#2995</a>).</li>; <li>[gvar] Avoid expanding all glyphs unnecessarily upon compile (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2918"">#2918</a>).</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/fonttools/fonttools/blob/main/NEWS.rst"">fonttools's changelog</a>.</em></p>; <blockquote>; <h2>4.39.3 (released 2023-03-28)</h2>; <ul>; <li>[sbix] Fixed TypeError when compiling empty glyphs whose imageData is None, regression; was introduced in v4.39 (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3059"">#3059</a>).</li>; <li>[ttFont] Fixed AttributeError on python &lt;= 3.10 when opening a TTFont from a tempfile; SpooledTemporaryFile, seekable method only added on python 3.11 (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3052"">#3052</a>).</li>; </ul>; <h2>4.39.2 (released 2023-03-16)</h2>; <ul>; <li>[varLib] Fixed regression introduced in 4.39.1 whereby an incomplete 'S",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:9953,Avoid,Avoid,9953,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['Avoid'],['Avoid']
Safety," own `config` or `url`. `Dataset.from_name_and_json()` now calls `DatasetVersion.get_region()` method to retrieve the dataset from the bucket in the selected region if `custom_config` is `False`. ; - The `DatasetVersion.get_region()` method takes the dataset `name`, a list of `DatasetVersion` objects, and a `region`, and returns a list of the versions that are available for that region. This method calls the instance method `in_region()` to check if the dataset is available in the requested region.; - If `in_region()` determines the desired region is not available for some dataset that otherwise is available in another region, it will raise a warning. If user still tries to call `db.annotate_rows_db()` using a dataset unavailable in their region, then it will get caught by the `_check_availability` instance method in the `DB` class and raise a `ValueError`.; - Started to add documentation to the classes and methods, still a work in progress. Changes to datasets and datasets API site:; - Added the `ldsc_baselineLD_annotations`, `ldsc_baselineLD_ldscores`, and `ldsc_baseline_ldscores` datasets to the `annotation_db.json` configuration file. Now accessible via `load_dataset()` and `db.annotate_rows_db()` (for the annotations at least).; - New `.rst` files in `hail/python/hail/docs/datasets` have been generated to reflect the available datasets in the config file, and `hail/python/hail/docs/datasets.rst` has been updated with the new files as well. Future updates:; - In next PR can add the functionality to automatically determine the users region.; - Also considering modifying the `load_datasets()` function a bit to only require one `version` parameter, to be consistent with the way the version strings are formatted in `annotation_db.json`, and to avoid having to check if the version and reference genome are available separately. Something like this:. ```; mt_1kg = hl.experimental.load_dataset(name='1000_Genomes_autosomes',; version='phase_3-GRCh37', ; region='us'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9496:2924,avoid,avoid,2924,https://hail.is,https://github.com/hail-is/hail/pull/9496,1,['avoid'],['avoid']
Safety," remove redundant definition of npy_nextafter [wheel build]</li>; </ul>; <h2>Checksums</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/numpy/numpy/commit/85f38ab180ece5290f64e8ddbd9cf06ad8fa4a5e""><code>85f38ab</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/23159"">#23159</a> from charris/prepare-1.24.2-release</li>; <li><a href=""https://github.com/numpy/numpy/commit/124252537f526a059b6a5ee3ac1e3bf1442bbc13""><code>1242525</code></a> REL: Prepare for the NumPy 1.24.2 release</li>; <li><a href=""https://github.com/numpy/numpy/commit/de0ee415e45b09c86d1ddc04f51c11192b1e2fe6""><code>de0ee41</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/23161"">#23161</a> from mattip/npy_nextafter</li>; <li><a href=""https://github.com/numpy/numpy/commit/ed09037473581908f6b52ecc3cabc82a414e2a54""><code>ed09037</code></a> BLD: remove redundant definition of npy_nextafter [wheel build]</li>; <li><a href=""https://github.com/numpy/numpy/commit/bc47a5ba6798a942d4a76e38f1089fc38f81f50d""><code>bc47a5b</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/23150"">#23150</a> from charris/backport-23144</li>; <li><a href=""https://github.com/numpy/numpy/commit/e5452b91b87523853b2e33c0f7f6788a9a22c1b4""><code>e5452b9</code></a> TYP,MAINT: Add a missing explicit <code>Any</code> parameter to the <code>npt.ArrayLike</code> defi...</li>; <li><a href=""https://github.com/numpy/numpy/commit/2433fe5b66016a640dd9337d9e114d7076f55861""><code>2433fe5</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/23149"">#23149</a> from charris/backport-23128</li>; <li><a href=""https://github.com/numpy/numpy/commit/8dfa47db1eb6472490b33d5f380513308f3e1a2d""><code>8dfa47d</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/23148"">#23148</a> from c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12898:4494,redund,redundant,4494,https://hail.is,https://github.com/hail-is/hail/pull/12898,1,['redund'],['redundant']
Safety," return decorator(_typecheck); /home/hail/hail.zip/hail/matrixtable.py in repartition(self, n_partitions, shuffle); 2505 Repartitioned dataset.; 2506 """"""; -> 2507 jvds = self._jvds.coalesce(n_partitions, shuffle); 2508 return MatrixTable(jvds); 2509 ; /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:2175,abort,aborted,2175,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['abort'],['aborted']
Safety," signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal. Exit status: 137. Diagnostics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.u",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287:6619,abort,abortStage,6619,https://hail.is,https://github.com/hail-is/hail/issues/13287,1,['abort'],['abortStage']
Safety," the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI5NmE4NGVhMS1hYzgxLTQxYmEtOGYzNC02MGU1ZTdhYzNjZTMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6Ijk2YTg0ZWExLWFjODEtNDFiYS04ZjM0LTYwZTVlN2FjM2NlMyJ9fQ=="" width=""0"" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""96a84ea1-ac81-41ba-8f34-60e5e7ac3ce3"",""prPublicId"":""96a84ea1-ac81-41ba-8f34-60e5e7ac3ce3"",""dependencies"":[{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-SETUPTOOLS-3180412""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[509],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lessons/redos/javascript/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12896:3266,remediat,remediationStrategy,3266,https://hail.is,https://github.com/hail-is/hail/pull/12896,1,['remediat'],['remediationStrategy']
Safety," timed); 97 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 98 try:; ---> 99 result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); 100 (result, timings) = (result_tuple._1(), result_tuple._2()); 101 value = ir.typ._from_encoding(result). /opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1321 answer = self.gateway_client.send_command(command); 1322 return_value = get_return_value(; -> 1323 answer, self.gateway_client, self.target_id, self.name); 1324 ; 1325 for temp_arg in temp_args:. /opt/conda/lib/python3.7/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 29 tpl = Env.jutils().handleForPython(e.java_exception); 30 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 31 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 32 except pyspark.sql.utils.CapturedException as e:; 33 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal. Exit status: 137. Diagnostics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad no",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287:5006,abort,aborted,5006,https://hail.is,https://github.com/hail-is/hail/issues/13287,1,['abort'],['aborted']
Safety," timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/5128f7f4ff73b165579006a8336978efaeeca07a""><code>5128f7f</code></a> Fix CHANGES</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/18e718a56f9def7dc40bb9ce3a2962a8fb0c883c""><code>18e718a</code></a> Bump to 4.0.2</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/48d1c4ce923b1da75976d7e2a6ca9234b3092c16""><code>48d1c4c</code></a> Setup towncrier</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/2ede7d73e55f8d1a2279d78861af0009d96219fb""><code>2ede7d7</code></a> Fix annotations on <code>__exit__</code> and <code>__aexit__</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/8685c60dc0e82ee246fbe3d1aa272d1dfe57c24c""><code>8685c60</code></a> Bump mypy from 0.910 to 0.920 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/f0a0914345b224448220aeb00d71e6a04a5d24bd""><code>f0a0914</code></a> Bump twine from 3.7.0 to 3.7.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/e5c813173b8811b30f4a30eeba56fa8808ab15bb""><code>e5c8131</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/271"">#271</a>)</li>; <li><a href=""https://github.com/aio-libs/async-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:4472,timeout,timeout,4472,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety," ;  51   if LocalAsyncFS.valid_url(uri): ;  52    fs = LocalAsyncFS(**self._local_kwargs) ;  53   elif aiogoogle.GoogleStorageAsyncFS.valid_url(uri): ;   54    fs = aiogoogle.GoogleStorageAsyncFS( ;  55     **self._gcs_kwargs, ;  56     bucket_allow_list = self._gcs_bucket_allow_list.copy() ;  57    ) ;  ;  /Users/cvittal/src/hail/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py:602 in ;  __init__ ;  ;  599     bucket_allow_list: Optional[List[str]] = None, ;  600     **kwargs): ;  601   if not storage_client: ;   602    storage_client = GoogleStorageClient(**kwargs) ;  603   self._storage_client = storage_client ;  604   if bucket_allow_list is None: ;  605    bucket_allow_list = [] ;  ;  /Users/cvittal/src/hail/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py:309 in ;  __init__ ;  ;  306   if 'timeout' not in kwargs and 'http_session' not in kwargs: ;  307    # Around May 2022, GCS started timing out a lot with our default 5s timeout ;  308    kwargs['timeout'] = aiohttp.ClientTimeout(total=20) ;   309   super().__init__('https://storage.googleapis.com/storage/v1', **kwargs) ;  310   self._gcs_requester_pays_configuration = get_gcs_requester_pays_configuration( ;  311    gcs_requester_pays_configuration=gcs_requester_pays_configuration ;  312   ) ;  ;  /Users/cvittal/src/hail/hail/python/hailtop/aiocloud/aiogoogle/client/base_client.py:15 in ;  __init__ ;  ;  12  def __init__(self, base_url: str, *, session: Optional[GoogleSession] = None, ;  13     rate_limit: Optional[RateLimit] = None, **kwargs): ;  14   if session is None: ;   15    session = GoogleSession(**kwargs) ;  16   super().__init__(base_url, session, rate_limit=rate_limit) ;  17 ;  ;  /Users/cvittal/src/hail/hail/python/hailtop/aiocloud/aiogoogle/session.py:18 in __init__ ;  ;  15     credentials = GoogleCredent",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13793:3839,timeout,timeout,3839,https://hail.is,https://github.com/hail-is/hail/issues/13793,3,['timeout'],['timeout']
Safety,"![Finally.](https://media.giphy.com/media/yIsbuPCEOgNHO/giphy.gif). - update endpoints to handle the ""zen"" that GitHub sends when a web hook is created. - update `make run-local` and friends for the new IP of the `dk-test` micro instance. - remove the unused `refresh_statuses` (this was intended to recover build state from github's commit statuses, but the commit status description is limited to like 120 characters, so I gave up on this a while ago, but never removed the code). - `.strip()` the GitHub token in case there are newlines. - print the SHA being deployed in the log statement. - add `hail-ci-build.sh` to CI, which just invokes `make test-in-cluster`(which in turn runs `test-in-cluster.sh`. - `test-in-cluster.sh` copies the secrets for testing to the expected locations and exposes the pod in which it is running with an internal service, recent changes to `site` [redirect sub URLs of ci.test.is to services named using this scheme](https://github.com/hail-is/hail/blob/master/site/hail.nginx.conf#L38-L41). GitHub uses these URLs to send updates to the CI under test about the watched repositories. - `test-locally.sh` now installs `../batch` into the currently running `pip` before testing (NB: if you edit batch and run the tests without committing the changes you've made to batch, this will pass tests but fail when pushed to a PR!). - `test-locally.sh` activates the `hail-ci` conda environment itself because it was not being propagated from the `Makefile`. I don't know why, but this is a simple fix. - `test-locally.sh` starts the ci after the repository is created. CI will print error messages if a watched repository doesn't exist. - `test/test-ci.py` now uses access tokens for all interaction with GitHub, previously it relied on the latent privileges that I and Cotton had in our environments. - `test/test-ci.py` uses a temporary, but not automatically deleted, directory when the environment variable `IN_CLUSTER` is set to `true` (to which it is set by `test-in-c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4474:300,recover,recover,300,https://hail.is,https://github.com/hail-is/hail/pull/4474,1,['recover'],['recover']
Safety,""" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""4d1e728e-269c-49a2-a2d0-bf1c04966e29"",""prPublicId"":""4d1e728e-269c-49a2-a2d0-bf1c04966e29"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494,539],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr);  [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr);  [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr);  [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14026:5877,remediat,remediationStrategy,5877,https://hail.is,https://github.com/hail-is/hail/pull/14026,1,['remediat'],['remediationStrategy']
Safety,""",""from"":""0.8.4"",""to"":""2.0.3""},{""name"":""nbconvert"",""from"":""5.6.1"",""to"":""6.3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr);  [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr);  [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr);  [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13835:10413,remediat,remediationStrategy,10413,https://hail.is,https://github.com/hail-is/hail/pull/13835,1,['remediat'],['remediationStrategy']
Safety,""",""from"":""0.8.4"",""to"":""2.0.3""},{""name"":""nbconvert"",""from"":""5.6.1"",""to"":""6.3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr);  [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr);  [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr);  [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13866:10439,remediat,remediationStrategy,10439,https://hail.is,https://github.com/hail-is/hail/pull/13866,1,['remediat'],['remediationStrategy']
Safety,""":""13e2c460-fe06-4099-adab-e1f8fa931de0"",""prPublicId"":""13e2c460-fe06-4099-adab-e1f8fa931de0"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""42.0.2""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""SNYK-PYTHON-CRYPTOGRAPHY-5663682"",""SNYK-PYTHON-CRYPTOGRAPHY-5777683"",""SNYK-PYTHON-CRYPTOGRAPHY-5813745"",""SNYK-PYTHON-CRYPTOGRAPHY-5813746"",""SNYK-PYTHON-CRYPTOGRAPHY-5813750"",""SNYK-PYTHON-CRYPTOGRAPHY-5914629"",""SNYK-PYTHON-CRYPTOGRAPHY-6036192"",""SNYK-PYTHON-CRYPTOGRAPHY-6050294"",""SNYK-PYTHON-CRYPTOGRAPHY-6092044"",""SNYK-PYTHON-CRYPTOGRAPHY-6126975"",""SNYK-PYTHON-CRYPTOGRAPHY-6210214"",""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[554,704,509,454,616,584,479,509,509,509,509,589,509,691,399,479,399,539,479,479,616,616,489,519],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Use After Free](https://learn.snyk.io/lesson/use-after-free/?loc&#x3D;fix-pr);  [Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;)](https://learn.snyk.io/lesson/type-confusion/?loc&#x3D;fix-pr);  [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14329:13018,remediat,remediationStrategy,13018,https://hail.is,https://github.com/hail-is/hail/pull/14329,1,['remediat'],['remediationStrategy']
Safety,""":""9f2a0fe3-dbed-46c0-bd20-223771bc1497"",""prPublicId"":""9f2a0fe3-dbed-46c0-bd20-223771bc1497"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""42.0.2""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""SNYK-PYTHON-CRYPTOGRAPHY-5663682"",""SNYK-PYTHON-CRYPTOGRAPHY-5777683"",""SNYK-PYTHON-CRYPTOGRAPHY-5813745"",""SNYK-PYTHON-CRYPTOGRAPHY-5813746"",""SNYK-PYTHON-CRYPTOGRAPHY-5813750"",""SNYK-PYTHON-CRYPTOGRAPHY-5914629"",""SNYK-PYTHON-CRYPTOGRAPHY-6036192"",""SNYK-PYTHON-CRYPTOGRAPHY-6050294"",""SNYK-PYTHON-CRYPTOGRAPHY-6092044"",""SNYK-PYTHON-CRYPTOGRAPHY-6126975"",""SNYK-PYTHON-CRYPTOGRAPHY-6210214"",""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[554,704,509,454,616,584,479,509,509,509,509,589,509,691,399,479,399,539,479,479,616,616,561,519],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Use After Free](https://learn.snyk.io/lesson/use-after-free/?loc&#x3D;fix-pr);  [Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;)](https://learn.snyk.io/lesson/type-confusion/?loc&#x3D;fix-pr);  [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:13030,remediat,remediationStrategy,13030,https://hail.is,https://github.com/hail-is/hail/pull/14327,1,['remediat'],['remediationStrategy']
Safety,"""><code>18e718a</code></a> Bump to 4.0.2</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/48d1c4ce923b1da75976d7e2a6ca9234b3092c16""><code>48d1c4c</code></a> Setup towncrier</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/2ede7d73e55f8d1a2279d78861af0009d96219fb""><code>2ede7d7</code></a> Fix annotations on <code>__exit__</code> and <code>__aexit__</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/8685c60dc0e82ee246fbe3d1aa272d1dfe57c24c""><code>8685c60</code></a> Bump mypy from 0.910 to 0.920 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/f0a0914345b224448220aeb00d71e6a04a5d24bd""><code>f0a0914</code></a> Bump twine from 3.7.0 to 3.7.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/e5c813173b8811b30f4a30eeba56fa8808ab15bb""><code>e5c8131</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/271"">#271</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c7e10db3d0965422122bef263fb36f6fd7572330""><code>c7e10db</code></a> Fix linter configs</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/6af8bf421a799208b27d57c6bf69de0d998fedec""><code>6af8bf4</code></a> Bump pre-commit from 2.15 to 2.16.0 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/269"">#269</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c71bbb5b5d7330f6dabfde7a1adec30a4611c0be""><code>c71bbb5</code></a> Bump docutils from 0.18 to 0.18.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/266"">#266</a>)</li>; <li>Additional commits viewable in <a ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:5205,timeout,timeout,5205,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,"""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIyN2MzNWY4NC0yNDIyLTRmNzUtYWMxYy1mODQxOGJmNzRlMzciLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjI3YzM1Zjg0LTI0MjItNGY3NS1hYzFjLWY4NDE4YmY3NGUzNyJ9fQ=="" width=""0"" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""27c35f84-2422-4f75-ac1c-f8418bf74e37"",""prPublicId"":""27c35f84-2422-4f75-ac1c-f8418bf74e37"",""dependencies"":[{""name"":""cryptography"",""from"":""41.0.7"",""to"":""42.0.2""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CRYPTOGRAPHY-6149518"",""SNYK-PYTHON-CRYPTOGRAPHY-6157248"",""SNYK-PYTHON-CRYPTOGRAPHY-6210214""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[509,581,451],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Use of a Broken or Risky Cryptographic Algorithm](https://learn.snyk.io/lesson/insecure-hash/?loc&#x3D;fix-pr);  [Uncontrolled Resource Consumption (&#x27;Resource Exhaustion&#x27;)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr);  [NULL Pointer Dereference](https://learn.snyk.io/lesson/null-dereference/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14234:4302,remediat,remediationStrategy,4302,https://hail.is,https://github.com/hail-is/hail/pull/14234,2,"['Risk', 'remediat']","['Risky', 'remediationStrategy']"
Safety,"""https://github.com/aio-libs/aioredis-py/commit/224f843bd4b33d657770bded6f86ce33b881257c""><code>224f843</code></a> Release version 2.0.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1247"">#1247</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/a9825c2ac35939b9ad8928e9468335d8efab963f""><code>a9825c2</code></a> Bump py-actions/py-dependency-install from 2.1.0 to 3 (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1239"">#1239</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/7f65c4ccb0e954c17f2a3e1ecc665c62e4a1aaeb""><code>7f65c4c</code></a> Remove <strong>del</strong> from Redis (Fixes <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1115"">#1115</a>) (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1227"">#1227</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/5062740974e493c390fb8db33982f97d6e08df2d""><code>5062740</code></a> Fix typing on blpop (etc) timeout argument (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1224"">#1224</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/dbdd0add63f986f2ed2d56c9736303d133add23c""><code>dbdd0ad</code></a> fix socket.error raises (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1129"">#1129</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/2ba15fb6947fa2347d401ba436e362ad62ed38ff""><code>2ba15fb</code></a> Fix buffer is closed error when using PythonParser class (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1213"">#1213</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/0aa06df10b9531f4ba734ec7567f8621c00e65e9""><code>0aa06df</code></a> Fix typing on evalsha keys_and_args argument (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1215"">#1215</a>)</li>; <li><a href=""https://github.com/aio-lib",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11569:4746,timeout,timeout,4746,https://hail.is,https://github.com/hail-is/hail/pull/11569,1,['timeout'],['timeout']
Safety,"""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[554,704,624,531,604,589,726,434,589,449,696,589,479,519,509,711,701,586,586,384,494,539,589],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr);  [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr);  [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr);  [More lessons are available in Snyk Learn](https://learn.snyk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14108:12750,remediat,remediationStrategy,12750,https://hail.is,https://github.com/hail-is/hail/pull/14108,1,['remediat'],['remediationStrategy']
Safety,"""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JINJA2-6150717"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PROMPTTOOLKIT-6141120"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494,539,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr);  [Cross-site Scripting (XSS)](https://learn.snyk.io/lesson/xss/?loc&#x3D;fix-pr);  [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr);  [More lessons are available in Snyk Learn]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14257:11863,remediat,remediationStrategy,11863,https://hail.is,https://github.com/hail-is/hail/pull/14257,1,['remediat'],['remediationStrategy']
Safety,"""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""40.5.0"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.32.2"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JINJA2-6150717"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PROMPTTOOLKIT-6141120"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494,539,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr);  [Cross-site Scripting (XSS)](https://learn.snyk.io/lesson/xss/?loc&#x3D;fix-pr);  [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr);  [More lessons are available in Snyk Learn]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14364:12216,remediat,remediationStrategy,12216,https://hail.is,https://github.com/hail-is/hail/pull/14364,1,['remediat'],['remediationStrategy']
Safety,"## Change Description; - Added ability to check job status in individual jobs in a JobGroup; - Cancel JobGroup if any jobs in the partition fail; - Added test functionality for detecting cancelled, failed jobs; - Query batch for which jobs have failed within a JobGroup, rather than go through every job in the group. ## Security Assessment. Delete all except the correct answer:; - This change has no security impact. ### Impact Description; Mainly just code for testing, nothing security related. (Reviewers: please confirm the security impact before approving)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14751:177,detect,detecting,177,https://hail.is,https://github.com/hail-is/hail/pull/14751,1,['detect'],['detecting']
Safety,"## `JoinPoint` interface for nontrivial, type-safe control flow in the JVM backend. This PR implements a ""join-point"" abstraction for the JVM backend. Join-points are a primitive; control flow construct that allow sophisticated forms of branching to be implemented in a type safe; way, without having to directly manipulate labels and jumps at the JVM bytecode level. Importantly,; they will enable the kinds of branching needed by stream-deforestation techniques that; @patrick-schultz and I have been discussing, for which while loops and if's were not sufficient. We've also discussed plans for implementing join points as a feature in the IR. ### Notable examples:. * Implementation of `whileLoop` (emits bytecode identical to current version):; ```scala; def whileLoop(cond: Code[Boolean], body: Code[Unit]): Code[Unit] =; JoinPoint.CallCC[Unit] { (jb, break) =>; val continue = jb.joinPoint(); val loopBody = jb.joinPoint(); continue.define { _ => JoinPoint.mux(cond, loopBody, break) }; loopBody.define { _ => Code(body, continue(())) }; continue(()); }; ```. * Mutual recursion:; ```scala; def parity(; n: Code[Int],; even: Code[Ctrl],; odd: Code[Ctrl]; ): Code[Ctrl] = {; val isEven = jb.joinPoint[Code[Int]](mb); val isOdd = jb.joinPoint[Code[Int]](mb); isEven.define { i => (i ceq 0).mux(even, isOdd(i - 1)) }; isOdd.define { i => (i ceq 0).mux(odd, isEven(i - 1)) }; isEven(n); }; ```. ### Classes of interest (tl;dr). - `JoinPoint` - Non-returning function. Used to implement control flow in a type-safe, functional way.; - `ParameterPack` - Trait used for tuple deforesting. Allows join-points to be provided multiple; arguments.; - `JoinPointBuilder` - Used to define new join points.; - `CallCC` - Entry-point for expressions with complex control flow. Provides a `JoinPointBuilder`; and a `JoinPoint` to return a value from the expression. ### `JoinPoint`. A `JoinPoint[A]` acts like a non-returning function with an argument of type `A`. The type of an; applied join point is `Code[C",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7055:46,safe,safe,46,https://hail.is,https://github.com/hail-is/hail/pull/7055,2,['safe'],['safe']
Safety,"### Change Description. This change exists as part of larger refactoring work. Herein, I've exchanged; hard-coded contextual strings passed to `ExecutionTimer.time` with implict; contexts, drawing inspiration from scalatest. These contexts are now supplied after entering functions like `Compile` and; `Emit` instead of before (see `ExecuteContext.time`). By sprinking calls to ; `time` throughout the codebase after entering functions, we obtain a nice trace; of the timings with `sourcecode.Enclosing`, minus the previous verbosity. See [1] for more information about what pre-built macros are available. We can; always build our own later. See comments in [pull request id] for example output.; Note that `ExectionTimer.time` still accepts a string to support uses like; `Optimise` and `LoweringPass` where those contexts are provided already.; It is also exception-safe now. This change exposed many similarities between the implementations of query; execution across all three backends. I've stopped short of full unification; which is a greater work, I've instead simplified and moved duplicated result; encoding into the various backend api implementations. More interesting changes are to `ExecuteContext`, which now supports; - `time`, as discussed above; - `local`, a generalisation for temporarily overriding properties of an ; `ExecuteContext` (inspired by [2]). While I've long wanted this for testing,; we were doing some questionable things when reporting timings back to python,; for which locally overriding the `timer` of a `ctx` has been very useful.; We also follow this pattern for local regions. [1] https://github.com/com-lihaoyi/sourcecode; [2] https://hackage.haskell.org/package/mtl-2.3.1/docs/Control-Monad-Reader.html#v:local. ### Security Assessment. This change has no security impact as it's confined to refactoring of existing non-security-related code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14683:869,safe,safe,869,https://hail.is,https://github.com/hail-is/hail/pull/14683,1,['safe'],['safe']
Safety,"### Description. Today our APIs are ""documented"" only through the list of endpoint handlers in implementation code ([example](https://github.com/hail-is/hail/blob/main/batch/batch/front_end/front_end.py#L239)). We can and should:; - Create OpenAPI documentation for our APIs (maybe per-service, maybe once in the gateway?); - Host swagger page/pages for exploring and testing out APIs . ### Security Impact. High. ### Security Impact Description. ""None"" for the creation of documentation, since we do not believe that documenting our APIs is inherently risky. ""High"" for hosting a new functional component on our web endpoints. Mitigating factor: swagger pages are loaded as static html with no need (or ability) to interact with other functional components, except through the same public APIs as are already accessible. ### Appsec Signoff. - [ ] Reviewed and approved",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14725:553,risk,risky,553,https://hail.is,https://github.com/hail-is/hail/issues/14725,1,['risk'],['risky']
Safety,"### Hail version:. eb5d13fe97fc. ### What you did:. ```; t1 = hl.Table.parallelize([; {'a': 'foo', 'b': 1},; {'a': 'bar', 'b': 2},; {'a': 'bar', 'b': 2}],; hl.tstruct(a=hl.tstr, b=hl.tint32),; key='a'); t2 = hl.Table.parallelize([; {'t': 'foo', 'x': 3.14},; {'t': 'bar', 'x': 2.78},; {'t': 'bar', 'x': -1},; {'t': 'quam', 'x': 0}],; hl.tstruct(t=hl.tstr, x=hl.tfloat64),; key='t'). t1.join(t2, how='outer').show(). # or. t1.join(t2, how='right').show(); ```. ### What went wrong (all error messages here, including the full java stack trace):. FatalError: HailException: OrderedRVD error! Unexpected PK in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Invalid PK: [quam]; Full key: [quam]. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 24, localhost, executor driver): is.hail.utils.HailException: OrderedRVD error! Unexpected PK in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Invalid PK: [quam]; Full key: [quam]; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1031); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1012); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.RVD$$anonfun$4$$anon$1.hasNext(RVD.scala:226); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1015); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.utils.richUtils",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055:758,abort,aborted,758,https://hail.is,https://github.com/hail-is/hail/issues/4055,1,['abort'],['aborted']
Safety,"### Hail version:. f2b0dca9f506. ### What you did:; ```; ht = hl.Table.parallelize([; {'a': '1', 'c': .5,'d': 'foo'},; {'a': '1', 'c': .6,'d': 'foo'},; ], hl.tstruct(a=hl.tstr,; c=hl.tfloat32, d=hl.tstr)); mt = ht.to_matrix_table(['a'], ['d']). mt.entries().show(); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 7, localhost, executor driver): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.endArray(RegionValueBuilder.scala:167); 	at is.hail.expr.ir.TableToMatrixTable$$anonfun$75$$anonfun$apply$25.apply(MatrixIR.scala:1878); 	at is.hail.expr.ir.TableToMatrixTable$$anonfun$75$$anonfun$apply$25.apply(MatrixIR.scala:1849); 	at is.hail.utils.FlipbookIterator$$anon$4.<init>(FlipbookIterator.scala:133); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:131); 	at is.hail.expr.ir.TableToMatrixTable$$anonfun$75.apply(MatrixIR.scala:1849); 	at is.hail.expr.ir.TableToMatrixTable$$anonfun$75.apply(MatrixIR.scala:1840); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$18.apply(ContextRDD.scala:293); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$18.apply(ContextRDD.scala:293); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1014); 	at scala.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4114:463,abort,aborted,463,https://hail.is,https://github.com/hail-is/hail/issues/4114,1,['abort'],['aborted']
Safety,"### Hail version:; 18d0195e6; ### What you did:; ```; import hail as hl; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}; mt = hl.methods.import_bgen('gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr22_v3.bgen',; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; min_partitions=100); sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'); og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])); og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2); og_sample._force_count_rows(); ```; The important part is that I used `annotate_rows` on a sufficiently large dataset.; ### What went wrong (all error messages here, including the full java stack trace):; Container failures; ```; Job aborted due to stage failure: Task 5 in stage 9.0 failed 20 times, most recent failure: Lost task 5.19 in stage 9.0 (TID 603, dk-w-0.c.broad-ctsa.internal, executor 63): ExecutorLostFailure (executor 63 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 15.9 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; ```; This is due to `collectPerPartition` allowing regions to grow without bound.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3920:856,abort,aborted,856,https://hail.is,https://github.com/hail-is/hail/issues/3920,1,['abort'],['aborted']
Safety,"### Hail version:; `a230321`; ### What you did:; `mt.GT[1]` on a haploid call; ### What went wrong (all error messages here, including the full java stack trace):; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 3.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 3.0 (TID 10515, pbt-sw-nkpn.c.broad-mpg-gnomad.internal, executor 306): java.lang.IllegalArgumentException: requirement failed; at scala.Predef$.require(Predef.scala:212); at is.hail.variant.Call$.alleleByIndex(Call.scala:128); at is.hail.expr.FunctionRegistry$$anonfun$11.apply$mcIII$sp(FunctionRegistry.scala:682); at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:682); at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:682); at is.hail.expr.BinaryFun.apply(Fun.scala:122); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3713:206,abort,aborted,206,https://hail.is,https://github.com/hail-is/hail/issues/3713,1,['abort'],['aborted']
Safety,"### Problem Description. `ExportPlink` was previously modified to resiliently handle failure of a; Spark task by including a per-task UUID. `copyMerge` was not modified to; correctly handle the directories generated by this modified; `ExportPlink`. For example, if exactly one task out of N fails during `ExportPlink`, the; temporary output directory will contain N+1 partition files. One of; these partition files is corrupted and invalid. The other N are the; output of successful task completion. The invalid file should simply be; ignored by `copyMerge`. ### Changes Made. This PR modifies `copyMerge` to take an optional list of files to; merge. If that argument is set to `None`, the original behavior; persists. The original behavior is used by `RichRDD.writeTable` and; `RichRDDByteArray.saveFromByteArrays`. These two methods use the default; Spark parallel export system. This system is not resilient to all task; failures, but *does* ensure failed tasks do not generate garbage; partitions in the output directory. Ergo, they can safely use the; original behavior of `copyMerge`. ### On Testing. I do not test this behavior because failing a task during write is a; little bit tricky. I have verified that all users of `copyMerge` now use; `copyMerge` correctly. Adding a test to `ExportPlink` would not save us; from incorrectly using `copyMerge` in the future. A longer term testing strategy that includes a Chaos Monkey that kills; entire containers during Hail Scala tests execution would protect; against this type of bug. ---. Fixes #4932",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4938:1041,safe,safely,1041,https://hail.is,https://github.com/hail-is/hail/pull/4938,1,['safe'],['safely']
Safety,"### What happened?. After I ran the ""make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0"", I get the following error. > Configure project :; WARNING: Hail primarily tested with Spark 3.3.2, use other versions at your own risk. > Task :compileScala; [Error] /gpfs/fs1/home/jl/Hail2/hail/hail/src/main/scala/is/hail/HailContext.scala:127:21: value implOpMulMatrix_DMD_DVD_eq_DVD is not a member of object breeze.linalg.DenseMatrix; one error found. > Task :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; > Run with --info option to get more log output.; > Run with --scan to get full insights. BUILD FAILED in 4m 52s; 2 actionable tasks: 2 executed; make: *** [build/libs/hail-all-spark.jar] Error 1. ### Version. Hail 0.2.13. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14235:258,risk,risk,258,https://hail.is,https://github.com/hail-is/hail/issues/14235,1,['risk'],['risk']
Safety,"### What happened?. After sorting our costs into ""cost of goods"", ""operating expenses"", and ""capital expenses"", I realized there are four ""operating expenses"" that are not tracked and reported with the other expenses. I regressed these costs against the core-hours to estimate the cost per core-hour. resource | intercept (USD) | cost (USD/core-hour); -- | -- | --; GCP Support Variable fee | 3.46403 +- 0.49155 | 0.00123 +- 0.00007; System logs costs SKU#1 | 13.09991 +- 3.13991 | 0.00093 +- 0.00039; System logs costs SKU#2 | 7.87838 +- 0.81695 | 0.00027 +- 0.00012; Job specifications | 5.41150 +- 0.36608 | 0.00025 +- 0.00005; Firewall policy | 0.51216 +- 0.03185 | 0.00012 +- 0.00000. To fully recover the operating expenses at our current revenue, we need an additional 0.005 USD per core-hour (which is 0.002 more than the sum of intercepts). This issue is complete after we add a new product:. resource | cost (USD/core-hour); -- | --; support-logs-specs-and-firewall-fees/1 | 0.005. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13526:699,recover,recover,699,https://hail.is,https://github.com/hail-is/hail/issues/13526,1,['recover'],['recover']
Safety,"### What happened?. Although it is not possible to avoid all cross-region access (and thus costs), there are some obvious preventable misuses. For example, the following pipeline should error:. ```; b = hb.Batch(regions=['us-east1']); x = b.read_input('gs://bucket-in-central1/'); b.new_job(f'cat {x}'); b.run(); ```; But the following pipeline should not error:; ```; b = hb.Batch(regions=['us-east1']); x = b.read_input('gs://bucket-in-central1/'); j = b.new_job(f'cat {x}'); j.regions(['us-central1']); ```; The following should error because the job *could* be in us-east1:; ```; b = hb.Batch(regions=['us-east1', 'us-central1']); x = b.read_input('gs://bucket-in-central1/'); b.new_job(f'cat {x}'); b.run(); ```; The following should error:; ```; b = hb.Batch(regions=['us-east1']) # remote_tmpdir is set in config file as a us-centra1 bucket; j = b.new_job(f'echo hi > {j.f}'); j2 = b.new_job(f'cat {j.f}'); b.run(); ```. ### Version. 0.2.119. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13232:51,avoid,avoid,51,https://hail.is,https://github.com/hail-is/hail/issues/13232,1,['avoid'],['avoid']
Safety,"### What happened?. Double quote `""""` is frequently used to mean just one `""` when it appears inside a quoted field a la:. ```; a	b; 1	""""""""; ```; This contains one row whose value for column a is `1` and whose value for column b is `""`. The outer quotes are redundant indicators of the bounds of that column for that row. A less trivial case involves having a tab inside the column:. ```; a	b; 1	""	""""	""; ```; In this file, the b column's value in the first row has length three and consists of a tab, a quote character and a tab: `	""	`. Another test case. The `test.txt` contains:; ```; a	b	c; ""hello"",""a""""b"",""goodbye""; ```; This code,; ```python3; hl.import_table(""test.txt"", quote='""').collect(); ```; should return:; ```python3; [hl.Struct(a=""hello"", b=""a\""b"", c=""goodbye"")]; ```. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13563:258,redund,redundant,258,https://hail.is,https://github.com/hail-is/hail/issues/13563,1,['redund'],['redundant']
Safety,"### What happened?. Hail should support integer types like int8 & int16. In SEQR, we have sets of values that are known to range from 0 to a small fixed integer which would fit in 8 or 16 bytes. Using such a smaller type would avoid allocating unnecessary memory and may also improve size since the compressor need not clean up excess bytes. ### Version. 0.2.115. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13466:227,avoid,avoid,227,https://hail.is,https://github.com/hail-is/hail/issues/13466,1,['avoid'],['avoid']
Safety,"### What happened?. Hail's google/azure credential classes do not require the caller to specify scopes when requesting access tokens, and thus default to a [very wide set of scopes](https://github.com/hail-is/hail/blob/91f5a0bfc30927014b60b11a353a4d95db009427/hail/python/hailtop/aiocloud/aiogoogle/credentials.py#L140), making those access tokens excessively powerful. An access token does not need to have the `https://www.googleapis.com/auth/appengine.admin` scope to read a blob from GCS. This poses an unnecessary risk if such a token were leaked. These classes should instead require that scopes be specified when requesting an access token, and call sights should specify the minimum set of scopes necessary to perform their function. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13530:519,risk,risk,519,https://hail.is,https://github.com/hail-is/hail/issues/13530,1,['risk'],['risk']
Safety,"### What happened?. In particular, VDS is doubly sparse:; 1. Sparse columns. Reference blocks are a form of run-length compression of homozygous reference calls.; 2. Sparse alleles. Local alleles is a form of compressed-sparse-column (albeit on arrays rather than matrices). All sparsely encoded allele-indexed fields share the same index array (the ""local alleles"" (`LA`) field). `hl.vds.to_dense_mt` only desifies the sparse columns, it *does not* densify the sparse alleles. This ticket is complete when:; 1. We have written a careful analysis, for ourselves, both of (1) the possible terms for describing sparse columns, sparse allele-indexed fields, densification of columns, and densification of allele-indexed fields as well as of (2) how to structure the code to avoid foot guns, make clear what is being densified, and facilitating selective densification of only what is necessary.; 2. We have merged a (short) RFC realizing part (1).; 3. We have merged a PR the realizes part (2).; 4. We have improved the documentation of the densification and local alleles methods to use this new language and to clearly describe what is and is not densified and the growth of size caused by these densifications. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14009:771,avoid,avoid,771,https://hail.is,https://github.com/hail-is/hail/issues/14009,1,['avoid'],['avoid']
Safety,"### What happened?. It seems that the local filesystem can, infrequently, stall when executing `rmtree`. Note that the error about the directory being non-empty is because we have a bug in `rm_dir`: we try to remove the directory even if the children tasks failed. It oddly seems to have happened on both a deploy batch and a PR batch:; - PR: https://ci.hail.is/batches/7706444/jobs/170; - deploy: https://ci.hail.is/batches/7707793/jobs/172. ```; [2023-08-02 05:33:14] test/hail/utils/test_hl_hadoop_and_hail_fs.py::test_hadoop_methods_3[local] PASSED; +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_1 (139802083059456) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_0 (139802091452160) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-0_0 (139802205742848) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:590,Timeout,Timeout,590,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['Timeout'],['Timeout']
Safety,"### What happened?. Julia Sealock reported this https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/vep.20issue/near/352790173. We also saw it in test_dataproc. Cal also reported it.; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 56 in stage 4.0 failed 20 times, most recent failure: Lost task 56.19 in stage 4.0 (TID 48622) (jsealock-schema-sw-43bq.c.daly-neale-sczmeta.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 125; VEP Error output:; docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.; See 'docker run --help'. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.methods.VEP$.waitFor(VEP.scala:73); 	at is.hail.methods.VEP.$anonfun$execute$5(VEP.scala:231); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.hasNext(RichContextRDD.scala:69); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collectio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:254,abort,aborted,254,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['abort'],['aborted']
Safety,"### What happened?. Most stored procedures take either a shared or exclusive lock on a relevant row of the `jobs` table near the start of the procedure, but not all. This appears to interact poorly with the `attempts_after_update` trigger as it attempts to take an exclusive lock on rows in the `jobs` table in the below join with the attempt resources tables. It's not clear exactly what the right fix is. It should be simple enough not to join on the jobs table in the `FOR UPDATE`, but we should also evaluate when in our various transactions a lock should be taken on the jobs table and whether it should be an X or S lock. ### Version. 0.2.128. ### Relevant log output. ```shell; ------------------------; LATEST DETECTED DEADLOCK; ------------------------; 2024-02-29 15:07:05 140331971655424; *** (1) TRANSACTION:; TRANSACTION 2486515, ACTIVE 0 sec inserting; mysql tables in use 27, locked 27; LOCK WAIT 16 lock struct(s), heap size 1128, 9 row lock(s), undo log entries 1; MySQL thread id 703, OS thread handle 140330830395136, query id 4745489 10.32.3.39 dgoldste-batch-user executing; INSERT INTO aggregated_billing_project_user_resources_v3 (billing_project, user, resource_id, token, `usage`); SELECT cur_billing_project, cur_user,; attempt_resources.deduped_resource_id,; rand_token,; msec_diff_rollup * quantity; FROM attempt_resources; WHERE attempt_resources.batch_id = NEW.batch_id AND attempt_resources.job_id = NEW.job_id AND attempt_id = NEW.attempt_id; FOR UPDATE; ON DUPLICATE KEY UPDATE `usage` = aggregated_billing_project_user_resources_v3.`usage` + msec_diff_rollup * quantity. *** (1) HOLDS THE LOCK(S):; RECORD LOCKS space id 351 page no 4 n bits 248 index PRIMARY of table `dgoldste-batch`.`instances_free_cores_mcpu` trx id 2486515 lock_mode X locks rec but not gap; Record lock, heap no 176 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d64676f6c647374652d7374616e64617264; asc batch-worker-dgoldste-standard; (total ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14380:718,DETECT,DETECTED,718,https://hail.is,https://github.com/hail-is/hail/issues/14380,1,['DETECT'],['DETECTED']
Safety,"### What happened?. Notify these threads on completion:; - https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/exporting.20sites.20only.20VCF/near/376801844. Using QoB, reading out of GCS, we encounter corrupted blocks on this simple pipeline. ```; Caused by: com.github.luben.zstd.ZstdException: Corrupted block detected; at com.github.luben.zstd.ZstdDecompressCtx.decompressByteArray(ZstdDecompressCtx.java:157) ~[zstd-jni-1.5.2-1.jar:1.5.2-1]; at com.github.luben.zstd.Zstd.decompressByteArray(Zstd.java:409) ~[zstd-jni-1.5.2-1.jar:1.5.2-1]; at is.hail.io.ZstdInputBlockBuffer.readBlock(InputBuffers.scala:649) ~[gs:__hail-query-ger0g_jars_f00f916faf783b89cc2fc00bfc3e39df5485d8b0.jar.jar:0.0.1-SNAPSHOT]; at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:384) ~[gs:__hail-query-ger0g_jars_f00f916faf783b89cc2fc00bfc3e39df5485d8b0.jar.jar:0.0.1-SNAPSHOT]; at is.hail.io.BlockingInputBuffer.readByte(InputBuffers.scala:402) ~[gs:__hail-query-ger0g_jars_f00f916faf783b89cc2fc00bfc3e39df5485d8b0.jar.jar:0.0.1-SNAPSHOT]; ....; ```. A simplified version of the script:. ```python3; import hail as hl; import gnomad.utils.sparse_mt. tmp_dir = 'gs://bucket/'; vds_file = 'gs://neale-bge/bge-wave-1.vds'; out = 'gs://bucket/foo.vcf.bgz'. hl.init(default_reference = 'GRCh38',; tmp_dir = tmp_dir). vds = hl.vds.read_vds(vds_file); mt = hl.vds.to_dense_mt(vds); t = gnomad.utils.sparse_mt.default_compute_info(mt); t = t.annotate(info=t.info.drop('AS_SB_TABLE')); t = t.annotate(info = t.info.drop(; 'AS_QUALapprox', 'AS_VarDP', 'AS_SOR', 'AC_raw', 'AC', 'AS_SB'; )); t = t.drop('AS_lowqual'). hl.methods.export_vcf(dataset = t, output = out, tabix = True); ```. [batch-7751958-2713-main.log](https://github.com/hail-is/hail/files/12314207/batch-7751958-2713-main.log). ### Version. 0.2.120. ### Relevant log output. ```shell; Traceback (most recent call last):; File ""/Users/rye/Projects/VQSR/formatting-VQSR-vcf.py"", line 102, in <module>; main(args); File ""/Users/rye/Proj",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409:335,detect,detected,335,https://hail.is,https://github.com/hail-is/hail/issues/13409,1,['detect'],['detected']
Safety,"### What happened?. On the driver (but this could happen anywhere), a `read` call failed in GoogleStorageFS. In particular line 205:; ```; if (reader != null) {; reader.read(bb); } else {; ```; We don't retry transient errors here or below in the other call to `read`. We only retry on the initial creation of the stream. I think we are concerned that the stream is in a bad state, possible advanced a few bytes. If we were to read from it, we might drop some data. The safe thing to do is to `seek` to the correct position. This will likely initiate a new HTTP request to GCS, which is fine, because we almost certainly lost the old connection due to the transient error. I also think we need to remove `lazyPosition`. I think we can achieve the requester pays nonsense by just relying on the `pos` from the parent class (see FS.scala). ### Version. 0.2.115-71fc978b5c22. ### Relevant log output. ```shell; Traceback (most recent call last):; File ""/usr/local/lib/python3.10/site-packages/reanalysis/summarise_clinvar_entries.py"", line 531, in <module>; main(subs=args.s, date=processed_date, variants=args.v, out=args.o); File ""/usr/local/lib/python3.10/site-packages/reanalysis/summarise_clinvar_entries.py"", line 505, in main; parse_into_table(json_path=temp_output, out_path=out); File ""/usr/local/lib/python3.10/site-packages/reanalysis/summarise_clinvar_entries.py"", line 439, in parse_into_table; ht.write(out_path, overwrite=True); File ""<decorator-gen-1106>"", line 2, in write; File ""/usr/local/lib/python3.10/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/usr/local/lib/python3.10/site-packages/hail/table.py"", line 1392, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/usr/local/lib/python3.10/site-packages/hail/backend/service_backend.py"", line 490, in execute; return self._cancel_on_ctrl_c(self._async_execute(ir, timed=timed)); File",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:470,safe,safe,470,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['safe'],['safe']
Safety,"### What happened?. One of our unit tests recently changed from taking around 20 seconds to being aborted by a time out after six hours  see populationgenomics/production-pipelines#352. This change turned out to coincide with the release of hail 0.2.113 and the unit test's `pip` selecting the new release. PR #12780 added a recursive `add_dependents` function to `LocalBackend`, that appears to be used to compute the transitive dependencies of each job. Profiling our unit test indicates that it is spending six hours inside this function with no end in sight. Running the job locally for a few seconds with more logging shows that it is calling `add_dependents` with the same `ancestor` and `child` millions of times. I'm not sure whether it's in an actual infinite loop or merely a combinatorial disaster than might terminate after a few months of runtime. The following change, for example,. ```diff; --- a/hail/python/hailtop/batch/backend.py; +++ b/hail/python/hailtop/batch/backend.py; @@ -268,7 +268,7 @@ class LocalBackend(Backend[None]):; def add_dependents(ancestor, child):; dependent_jobs[ancestor].add(child); for ancestor_parent in ancestor._dependencies:; - add_dependents(ancestor_parent, child); + if child not in dependent_jobs[ancestor_parent]: add_dependents(ancestor_parent, child); ; for j in jobs:; for parent in j._dependencies:; ```. reduces it to calling it only once or twice for each `ancestor`/`child` combination, and returns the unit test to completing in ~20 seconds. I am not familiar enough with the data structure to say if that is a correct fix, but something of this nature appears to be needed to return this transitive dependency computation to a sensible runtime. ### Version. 0.2.113. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12915:98,abort,aborted,98,https://hail.is,https://github.com/hail-is/hail/issues/12915,1,['abort'],['aborted']
Safety,"### What happened?. See [Batch Metadata Server RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst) for background. The objective of this issue is to fully remove GSA key files from Batch job filesystems, preventing possible exfiltration of long-lived credentials. Each remaining task should get its own issue if there isn't already one. Breakdown of tasks:. - [X] Implement a Batch metadata server and expose it in GCP `DockerJob`s (#14019); - [ ] Add metadata server support for `JVMJob`s aka Query-on-Batch in GCP (#14487); - [ ] Add metadata server support in Azure; - [ ] Deprecate and remove support for key files in `DockerJob`s; - [ ] Deprecate and remove support for key files in `JVMJob`s. This requires dropping support for old versions of hail that depend on the key file (up to and including at least 0.2.130). These steps get us past the security milestone of not exposing GSA key files to jobs and risking exfiltration. We might be able to go even further and get rid of key files entirely, which would reduce our operational burden of securing and rotating them.; - [ ] In GCP, use Service Account Impersonation to have the Batch Worker identity impersonate user GSAs, allowing it to create metadata server access tokens without the key files themselves; - [ ] In Azure, investigate if something like the above is even possible. At time of writing, it does not appear that there is an alternative other than storing credentials or adding users to the VM's metadata server. It is unclear whether this can be done dynamically and with what frequency and feels like not their intended use case. ### Version. 0.2.130. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14486:943,risk,risking,943,https://hail.is,https://github.com/hail-is/hail/issues/14486,1,['risk'],['risking']
Safety,"### What happened?. Since we guarantee a job will run at least once, there are two issues that can happen:. 1. A user can write a pipeline in which two jobs race to write the same file, e.g.; ```; j = b.new_job(); j.command('echo hello > {j.out}'); j.write_output(j.out, ""gs://bucket/final-output""); ```; 2. Or, a clever user can avoid this race with some randomness:; ```; j = b.new_job(); j.command('echo hello gsutil cp - gs://bucket/final-output-$RANDOM'); ```. The former is a really common pattern and a bit of a footgun! The latter is rare (I don't know anyone who does it) and hard to work with: how would you know the output file of the *successful* attempt?. Hail should provide some mechanism for a user to get the list of successful attempts and their outputs. One simple option is to include some kind of seeded randomness which the user can access and to return either the seed or all the draws of the successful attempt for each job in `/jobs` or for the one job in `/job/{job_id}`. For example, consider:. ```; j = b.new_job(); j.command('echo hello gsutil cp - gs://bucket/final-output-$(/hail-random-str)'); ```. Where `/hail-random-str` is a binary we mount into the container that randomly generates numbers seeded by `(batch id, job id, attempt id)`. Hail should use the same randomness to ensure that `write_output` is reliable. We might also want a way to automatically remove the output files of the non-successful (e.g. preempted) attempts. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13502:330,avoid,avoid,330,https://hail.is,https://github.com/hail-is/hail/issues/13502,1,['avoid'],['avoid']
Safety,"### What happened?. Struct decoding currently uses `Region.loadBit` which:; 1. Calculates the address of the byte has this bit (e.g. the 65th bit is in the second byte).; 2. Loads the byte out of memory.; 3. Masks the bit out of the byte.; 4. Compares to zero. We don't have concrete data, but we suspect that the JVM can't avoid loading the byte out of memory 8 times. If we can instead load it once per 8 missing fields, there may be a speed up for structs that are frequently decoded (e.g. an entry struct). ### See also. - https://github.com/hail-is/hail/issues/13792#issuecomment-1761652107 . ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13811:324,avoid,avoid,324,https://hail.is,https://github.com/hail-is/hail/issues/13811,1,['avoid'],['avoid']
Safety,"### What happened?. The basic problem is:; 1. The type for `ArrayMaximalIndependentSet` is `TArray(...)` where `...` is whatever the node type is. ; 2. We choose a PType based on the Type.; 3. We choose an SType based on the PType.; 4. `unwrapReturn` makes an incorrect assumption about which SType corresponds to a `TArray(String)`. In particular,; ```; Code.invokeScalaObject1[UnsafeIndexedSeq, IndexedSeq[Any]](Graph.getClass, ""maximalIndependentSet"", jEdges); ```; returns a Java array of whatever `svalueToJavaValue` returns. In that case, that's a `String[]` which we call `SJavaArrayString`. However, the SType chosen for `TArray(TString)` is `SIndexablePointer(SBinary)`. **I think the real fix here is to just pass region pointers into MaximalIndependentSet.** Just get an `elementIterator` from `PCanonicalArray` and use `loadElement`, etc. to populate the `Graph`. ```; In [1]: import hail as hl; ...: ht = hl.Table.parallelize([hl.Struct(i='A', j='B', kin=0.25), hl.Struct(i='A', j='C', kin=0.25), hl.Struct(i='D', j='E', kin=0.5)]); ...: hl.maximal_independent_set(ht.i, ht.j, False).collect(); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); Cell In[1], line 3; 1 import hail as hl; 2 ht = hl.Table.parallelize([hl.Struct(i='A', j='B', kin=0.25), hl.Struct(i='A', j='C', kin=0.25), hl.Struct(i='D', j='E', kin=0.5)]); ----> 3 hl.maximal_independent_set(ht.i, ht.j, False).collect(). File <decorator-gen-1148>:2, in collect(self, _localize, _timed). File ~/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py:584, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 581 @decorator; 582 def wrapper(__original_func, *args, **kwargs):; 583 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 584 return __original_func(*args_, **kwargs_). File ~/miniconda3/lib/python3.10/site-packages/hail/table.py:2162, in Table.collect(self, _localize, _timed);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:379,Unsafe,UnsafeIndexedSeq,379,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['Unsafe'],['UnsafeIndexedSeq']
Safety,"### What happened?. The following fails:; ```; import hail as hl; hl.init(); ````; with the error:; ```; ImportError: cannot import name 'getargspec' from 'inspect' (/usr/local/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/inspect.py); ```; when running Python 3.11. The code importing `getargspec` is the Parsimonious library (see stacktrace below). ### Version. 0.2.109. ### Relevant log output. ```shell; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); Cell In[1], line 1; ----> 1 import hail as hl; 2 hl.init(). File /usr/local/Cellar/jupyterlab/3.6.1/libexec/lib/python3.11/site-packages/hail/__init__.py:33; 14 __doc__ = r""""""; 15 __ __ <>__; 16 / /_/ /__ __/ /; (...); 27 To report a bug, please open an issue: https://github.com/hail-is/hail/issues; 28 """"""; 30 # F403 'from .expr import *' used; unable to detect undefined names; 31 # F401 '.expr.*' imported but unused; 32 # E402 module level import not at top of file; ---> 33 from .table import Table, GroupedTable, asc, desc # noqa: E402; 34 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 35 from .expr import * # noqa: F401,F403,E402. File /usr/local/Cellar/jupyterlab/3.6.1/libexec/lib/python3.11/site-packages/hail/table.py:8; 5 import pyspark; 6 from typing import Optional, Dict, Callable, Sequence; ----> 8 from hail.expr.expressions import Expression, StructExpression, \; 9 BooleanExpression, expr_struct, expr_any, expr_bool, analyze, Indices, \; 10 construct_reference, to_expr, construct_expr, extract_refs_by_indices, \; 11 ExpressionException, TupleExpression, unify_all, NumericExpression, \; 12 StringExpression, CallExpression, CollectionExpression, DictExpression, \; 13 IntervalExpression, LocusExpression, NDArrayExpression, expr_stream; 14 from hail.expr.types import hail_type, tstruct, types_match, tarray, tset, dtypes_from_pandas; 15 from hail.expr.table_type import ttable. Fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12759:928,detect,detect,928,https://hail.is,https://github.com/hail-is/hail/issues/12759,1,['detect'],['detect']
Safety,"### What happened?. Try running a job with `_machine_type: 'n1-highmem-64'`. This is necessary to get enough memory for some larger jobs (> ~200GB). Startup on the batch worker fails because the job is calculating how many theoretical network namespaces it could support (4 per CPU, 64 CPUS, plus some for JVMs), but not considering that the IPv4 schema puts a hard limit of 255 on namespaces if only one subnet value is changing each time. ### Version. Live 7/30/24. ### Relevant log output. _No response_. ### Security considerations:. Low risk of impacting security. High CPU machine types are not materially different from others with respect to security considerations, and the bug is a simple logic error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14644:542,risk,risk,542,https://hail.is,https://github.com/hail-is/hail/issues/14644,1,['risk'],['risk']
Safety,### What happened?. Users need a way to control their risk tolerance for preemptions. ### Version. 0.2.120. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13395:54,risk,risk,54,https://hail.is,https://github.com/hail-is/hail/issues/13395,1,['risk'],['risk']
Safety,"### What happened?. We appear to have lost the run_until_done_or_deleted on [borrow_jvm](https://github.com/hail-is/hail/pull/11397/files#diff-f8ba97f763395908a5b67f47a630c98e8d223ca5914f18d588f405d629d52197L1703-R1811) and [download_jar](https://github.com/hail-is/hail/pull/11397/files#diff-f8ba97f763395908a5b67f47a630c98e8d223ca5914f18d588f405d629d52197L1724-R1832). We should understand why we made this change (is there some bad interaction?) and determine how to rectify it. It seems to me that, basically any `await` during the runtime of a job must be able to abort because the job could be cancelled at any time. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13950:569,abort,abort,569,https://hail.is,https://github.com/hail-is/hail/issues/13950,1,['abort'],['abort']
Safety,"### What happened?. We have reported to their GitHub, but we don't have a simple enough repro for them to make progress. https://github.com/Azure/azure-sdk-for-java/issues/35125. Personal correspondence with some MSFT researchers suggested there could be an issue with threading:; > It sort of reminds me of an issue we saw with Cromwell where their old akka pool code caused a bunch of unexpected network behavior that broke their API in certain cases. I've asked if BlobServiceClient is thread-safe or not. We share an object of that class, but none of the things it produces (e.g. blobs). We know that the java.io libraries can improperly drop an HTTP response if it is followed by a TCP RST. In particular, we've seen this happen when a server is load shedding and sends an HTTP ""429 Too Many Requests"" rapidly followed by a TCP RST. This might explain the ""Connection reset"" errors that we sometimes see. We have fewer intuitions about the ""Stream is already closed"". That specific error was reported to Azure in the aforementioned GitHub issue. We treat both stream is closed and connection reset as ""limited retry"" errors. We might retry too quickly. Our initial delay is `100ms * x` where `x` is drawn uniformly from `[0, 1]`. Perhaps we should try an initial delay of at least 1s? . For example, [Azure gives as an example retrying after 2s, 4s, 10s, and 30s](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-performance-checklist#timeout-and-server-busy-errors). Google's [code examples](https://cloud.google.com/storage/docs/retry-strategy#client-libraries_1) suggest an initial delay of 1s with a multiplier of 2. AWS seems to use 500ms as the [default base backoff for ""throttled"" exceptions](https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-core/src/main/java/com/amazonaws/retry/PredefinedBackoffStrategies.java#L39). ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13351:496,safe,safe,496,https://hail.is,https://github.com/hail-is/hail/issues/13351,2,"['safe', 'timeout']","['safe', 'timeout-and-server-busy-errors']"
Safety,"### What happened?. When the QoB client on a user's laptop sends a request to create a QoB job, it sends a `jar_spec` parameter as part of the job spec that is either:; - `git_revision`: the git SHA that the hail was built with. The Batch front end takes this and resolves a URL for the published JAR that was created when that commit was merged to `main`.; - `jar_url`: A blob storage URL that points directly to the JAR to use. The Batch front end ensures that this URL is trusted. The `jar_url` setting is mainly for development and debugging purposes, allowing a dev or user to set a URL to a development JAR instead of using a merged commit. In normal configuration fashion, it is possible to set `jar_url` in `hailctl config`. This is an enormous footgun, as users may forget to unset this configuration and continue using the dev jar *even after they install a different hail wheel*. We must do two things:; 1. Remove the ability to set the jar_url through `hailctl` so as to avoid this footgun. Batch should also fully remove support for `jar_url`s so that any users who might be inadvertently using it are loudly alerted (though I suspect there are few if any such users now).; 2. Remove entirely the ability to specify a JAR other than that which was built along with the installed wheel. The proposed plan is to always send `git_revision` for QoB jobs. In order to enable development JARs, Batch should be augmented to search first for production JARs matching a certain revision, and then if that fails search a specified `dev/` subdirectory for the requested revision. These development JARs should not be cached on workers so as to enable debugging development without constant committing. ### Version. 0.2.130. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14539:983,avoid,avoid,983,https://hail.is,https://github.com/hail-is/hail/issues/14539,1,['avoid'],['avoid']
Safety,"### What happened?. When the image cannot be pulled, the exception can trigger a FileNotFoundError reading the main container log. https://cloudlogging.app.goo.gl/5h9Q9MUG7KdZRVXN9. ### Version. 0.2.124. ### Relevant log output. ```shell; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 551, in pull; await docker_call_retry(; File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 840, in retry_transient_errors_with_debug_string; return await f(*args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 460, in timed_out_f; return await asyncio.wait_for(f(*args, **kwargs), timeout); File ""/usr/lib/python3.9/asyncio/tasks.py"", line 479, in wait_for; return fut.result(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 484, in _pull_with_auth_refresh; return await docker.images.pull(image_ref_str, auth=credentials); File ""/usr/local/lib/python3.9/dist-packages/aiodocker/images.py"", line 133, in _handle_list; async with cm as response:; File ""/usr/local/lib/python3.9/dist-packages/aiodocker/utils.py"", line 309, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.9/dist-packages/aiodocker/docker.py"", line 275, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(500, 'Head ""https://us-docker.pkg.dev/v2/1/does-not-exist/manifests/latest"": denied: Permission ""artifactregistry.repositories.downloadArtifacts"" denied on resource ""projects/1/locations/us/repositories/does-not-exist"" (or it may not exist)'). The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 915, in run; await self.create(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 840, in create; await self._run_until_done_or_deleted(sel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13907:694,timeout,timeout,694,https://hail.is,https://github.com/hail-is/hail/issues/13907,1,['timeout'],['timeout']
Safety,"### What happened?. When using logistic regression, the null model tells me about the relationship between my covariates and the phenotype(s). In particular, if my covariates perfectly predict my phenotype, the model will fail to converge on every row. Investigating this situation demands access to the null model.; ```; import hail as hl; mt = hl.utils.range_matrix_table(3,3); mt = mt.annotate_entries(prod = mt.row_idx * mt.col_idx); hl.logistic_regression_rows('wald', y=[hl.bool(mt.col_idx)], x=mt.prod, covariates=[1.0]).describe(); ```. When using the Query-on-Spark backend, I receive no access to the null model parameters:; ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'row_idx': int32 ; 'logistic_regression': array<struct {; beta: float64, ; standard_error: float64, ; z_stat: float64, ; p_value: float64, ; fit: struct {; n_iterations: int32, ; converged: bool, ; exploded: bool; }; }> ; ----------------------------------------; Key: ['row_idx']; ----------------------------------------; ```. In contrast, the Query-on-Batch backend exposes this information:; ```; Global fields:; 'null_fits': array<struct {; b: ndarray<float64, 1>, ; score: ndarray<float64, 1>, ; fisher: ndarray<float64, 2>, ; mu: ndarray<float64, 1>, ; n_iterations: int32, ; log_lkhd: float64, ; converged: bool, ; exploded: bool; }> ; ```. The Query-on-Spark backend should expose the same information for the benefit of the user. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13789:185,predict,predict,185,https://hail.is,https://github.com/hail-is/hail/issues/13789,1,['predict'],['predict']
Safety,"### What happened?. `hailctl dataproc start` fails with an error message like the one below because [in Dataproc 2.2](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network#:~:text=Internal%20addresses%20only%20(no%2Daddress)%20is%20set%20by%20default%20when%20creating%20a%20Dataproc%202.2%20image%20version%20cluster.%20You%20can%20use%20the%20gcloud%20dataproc%20clusters%20create%20%2D%2Dpublic%2Dip%2Daddress%20flag%20to%20enable%20public%20IP%20addresses.), clusters are created without public internet access by default. A workaround is to pass the `--public-ip-address` flag to the command. Error message:. ```python; pip packages are ['setuptools', 'mkl<2020', 'lxml<5', 'https://github.com/hail-is/jgscm/archive/v0.1.13+hail.zip', 'ipykernel==6.22.0', 'ipywidgets==8.0.6', 'jupyter-console==6.6.3', 'nbconvert==7.3.1', 'notebook==6.5.6', 'qtconsole==5.4.2', 'aiodns==2.0.0', 'aiohttp==3.9.5', 'aiosignal==1.3.1', 'async-timeout==4.0.3', 'attrs==23.2.0', 'avro==1.11.3', 'azure-common==1.1.28', 'azure-core==1.30.2', 'azure-identity==1.17.1', 'azure-mgmt-core==1.4.0', 'azure-mgmt-storage==20.1.0', 'azure-storage-blob==12.20.0', 'bokeh==3.3.4', 'boto3==1.34.138', 'botocore==1.34.138', 'cachetools==5.3.3', 'certifi==2024.6.2', 'cffi==1.16.0', 'charset-normalizer==3.3.2', 'click==8.1.7', 'commonmark==0.9.1', 'contourpy==1.2.1', 'cryptography==42.0.8', 'decorator==4.4.2', 'deprecated==1.2.14', 'dill==0.3.8', 'frozenlist==1.4.1', 'google-auth==2.31.0', 'google-auth-oauthlib==0.8.0', 'humanize==1.1.0', 'idna==3.7', 'isodate==0.6.1', 'janus==1.0.0', 'jinja2==3.1.4', 'jmespath==1.0.1', 'jproperties==2.1.1', 'markupsafe==2.1.5', 'msal==1.29.0', 'msal-extensions==1.2.0', 'msrest==0.7.1', 'multidict==6.0.5', 'nest-asyncio==1.6.0', 'numpy==1.26.4', 'oauthlib==3.2.2', 'orjson==3.10.6', 'packaging==24.1', 'pandas==2.2.2', 'parsimonious==0.10.0', 'pillow==10.4.0', 'plotly==5.22.0', 'portalocker==2.10.0', 'protobuf==3.20.2', 'py4j==0.10.9.7', 'pyasn1==0.6.0', 'pyasn1-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14652:952,timeout,timeout,952,https://hail.is,https://github.com/hail-is/hail/issues/14652,1,['timeout'],['timeout']
Safety,"### What happened?. `hl.maximal_independent_set` should return the same independent set regardless of the ordering of the input table. gnomAD team reports that the returned set can differ depending on whether or not the input table had been written or came directly from PC-Relate. I have yet to create a simple reproducible example. Permuting the entries in this array does not change the output. I always get 'a' and 'b'. I suspect this is because what really matters is the order in which we traverse the entries of the multi map which depends on the hash of the nodes. I think a durable fix might be to eliminate the MultiMap, insert all the nodes into the binary heap, then increment priority for each edge detected. This will perform more reflows of the heap, but eliminates the non-determinism of MultiMap iteration order. ```; import hail as hl; ht = hl.Table.parallelize([; hl.Struct(i=hl.Struct(s=x[0]), j=hl.Struct(s=x[1])); for x in [('c', 'a'), ('a', 'b'), ('b', 'c'), ]; ]); hl.maximal_independent_set(ht.i, ht.j, False).collect(); ```. ### Version. 0.2.122. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13635:712,detect,detected,712,https://hail.is,https://github.com/hail-is/hail/issues/13635,1,['detect'],['detected']
Safety,"### What happened?. https://hail.zulipchat.com/#narrow/stream/128581-Cloud-support/topic/Inefficient.20computing.20in.20AoU.20workbench. At least the first one seems to be a genuine missed optimization in Hail; ```; import hail as hl; import os; bucket = os.getenv(""WORKSPACE_BUCKET""); vds_srwgs_path = os.getenv(""WGS_VDS_PATH""); vds = hl.vds.read_vds(vds_srwgs_path); vds = hl.vds.split_multi(vds, filter_changed_loci=False); vmt = vds.variant_data; vht = vmt.rows(); vht = vht.select('filters'); vht.write(f'{bucket}/aou_vat_with_filter_wlu.ht', overwrite=True); ```; The `vmt.rows()` should have avoided all entry-level work. This should really just explode the alleles array and write that to a file. That should be relatively quick. We should be able to reproduce this on any VDS we have, and see that the IR we actually execute still references the entry data. . ### Version. 0.2.107-2387bb00ceee. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13312:599,avoid,avoided,599,https://hail.is,https://github.com/hail-is/hail/issues/13312,1,['avoid'],['avoided']
Safety,"#13008 Started using the ci-utils from the CI pipeline for the database jobs, but we actually can't use it safely for the database cleanup step because we might untag the image before the database cleanup jobs run.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13014:107,safe,safely,107,https://hail.is,https://github.com/hail-is/hail/pull/13014,1,['safe'],['safely']
Safety,"#450</a>)</li>; <li><a href=""https://github.com/googleapis/python-logging/commit/97e32b67603553fe350b6327455fc9f80b8aa6ce""><code>97e32b6</code></a> fix: allow reading logs from non-project paths (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/444"">#444</a>)</li>; <li><a href=""https://github.com/googleapis/python-logging/commit/a760e02371a55d6262e42de9e0222fffa2c7192b""><code>a760e02</code></a> feat: add json_fields extras argument for adding to jsonPayload (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/447"">#447</a>)</li>; <li><a href=""https://github.com/googleapis/python-logging/commit/83d9ca8521fe7c470bb6755a48a97496515d7abc""><code>83d9ca8</code></a> feat!: make logging API more friendly to use (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/422"">#422</a>)</li>; <li><a href=""https://github.com/googleapis/python-logging/commit/818213e143d6a1941211a48e0b23069a426ac300""><code>818213e</code></a> feat: avoid importing grpc when explicitly disabled (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/416"">#416</a>)</li>; <li><a href=""https://github.com/googleapis/python-logging/commit/e1506fa9030776353878048ce562c53bf6ccf7bf""><code>e1506fa</code></a> fix!: api consistency between HTTP and Gapic layers (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/375"">#375</a>)</li>; <li><a href=""https://github.com/googleapis/python-logging/commit/6fa17735fe3edb45483ec5e3abd1f53c24ffa881""><code>6fa1773</code></a> feat!: support string-encoded json (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/339"">#339</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/googleapis/python-logging/compare/v1.12.1...v3.0.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11574:13203,avoid,avoid,13203,https://hail.is,https://github.com/hail-is/hail/pull/11574,1,['avoid'],['avoid']
Safety,"#4659 teaches CI to recover from a build job gone missing, but; I neglected to teach CI how to recover from a deploy job gone; missing. This follows the same strategy but for deploy jobs. If a deploy job is not found in the list of refreshed jobs; it is simply removed from the deploy_jobs map. The next heal; stage of CI will kick off a new batch job for whatever the; latest undeployed SHA is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4683:20,recover,recover,20,https://hail.is,https://github.com/hail-is/hail/pull/4683,2,['recover'],['recover']
Safety,"#9634 Introduced a large performance regression in the `linear_regression_rows_nd` benchmark (making it about 4x slower). This PR fixes that by doing two things:. 1. Move all the global into one single `annotate_globals` expression, so that CSE can work properly. This required fixing a bug in some ndarray expressions that were not correctly tracking their source tables. To make sure I was only referencing the global versions of this computed things, rather accidentally recomputing, I wrapped the global setup in a function to scope the variables. This improvement was minor, didn't hit the real root of the problem. 2. Much more significantly, and not 100% clear why: `process_y_group` is now a function that returns a python dictionary, instead of a hail struct. I can guess that the allocation required by making a struct was wasteful, but it seems crazy that it was ""make the benchmark 4x slower"" amounts of wasteful. . While this is not user facing yet, would be good to get this in before an eventual 0.2.60 release if we want to avoid benchmarks regressing between versions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9666:1040,avoid,avoid,1040,https://hail.is,https://github.com/hail-is/hail/pull/9666,1,['avoid'],['avoid']
Safety,$1.run(JVMEntryway.java:107); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); ... 7 more; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/rwalters-hail-tmp/o?name=merged_round2_sumstats.fix_lowconf.mt/entries/rows/parts/part-15801-2fde3786-67cb-42ed-8aac-f900cfcc4c00&uploadType=resumable&upload_id=ADPycduMEzX6d_uX4CiP6_XItJKmP8UnUnYBfyPoselMbyLUkxs1wDLPnxWl5gXr5LnBaVntYR_i7jchyxgVsRb_5PknvcCIcfDJ; chunkOffset: 16777216; chunkLength: 0; localOffset: 0; remoteOffset: 16777216; lastChunk: false. at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); at java.util.concurrent.Executo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:2296,recover,recover,2296,https://hail.is,https://github.com/hail-is/hail/issues/12950,1,['recover'],['recover']
Safety,$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/120:4475,abort,abortStage,4475,https://hail.is,https://github.com/hail-is/hail/issues/120,3,['abort'],['abortStage']
Safety,$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1234); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:5864,abort,abortStage,5864,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['abort'],['abortStage']
Safety,"&quot;+&quot; by their names contributed a patch for the first time.; This list of names is automatically generated, and may not be fully complete.</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/scipy/scipy/commit/656076ca6b490f587e9bd9c4cd10cb259a687c5b""><code>656076c</code></a> MAINT: wheel push 1.9.2 [wheel build]</li>; <li><a href=""https://github.com/scipy/scipy/commit/ad0d0f907010fbc8b66cdbe8ce0af2683881a309""><code>ad0d0f9</code></a> REL: set 1.9.2 released [wheel build]</li>; <li><a href=""https://github.com/scipy/scipy/commit/d9ad9801323653a2015b4d3e80d6d3ea93b6c021""><code>d9ad980</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17150"">#17150</a> from tylerjereddy/treddy_scipy_192_more_backports</li>; <li><a href=""https://github.com/scipy/scipy/commit/6b098c25223e224ff44101f86bbc86efecffe1d9""><code>6b098c2</code></a> TST: optimize.milp: remove problematic timeout/iteration test</li>; <li><a href=""https://github.com/scipy/scipy/commit/24dce9760b87934f1be046ec817c758b0f3952dc""><code>24dce97</code></a> DOC: stats.pearsonr: typo in coeffic<em>i</em>ent (<a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17153"">#17153</a>)</li>; <li><a href=""https://github.com/scipy/scipy/commit/a6ba7cad3b54c35d2ccb55c595691689004742c1""><code>a6ba7ca</code></a> MAINT: misc 1.9.2 updates</li>; <li><a href=""https://github.com/scipy/scipy/commit/ed9760e60a28b8f13e5644494033e2dab9aafbcd""><code>ed9760e</code></a> MAINT: stats.pearson3: fix ppf for negative skew (<a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17055"">#17055</a>)</li>; <li><a href=""https://github.com/scipy/scipy/commit/6fb67007dd7105755057f3379fb7ef423eae524e""><code>6fb6700</code></a> FIX: optimize.milp: return feasible solution if available on timeout/node lim...</li>; <li><a href=""https://github.com/scipy/scipy/commit/bcfce27fc061cbde6ac6531799362e0420ea4796""><code>bcfce27</cod",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12352:1965,timeout,timeout,1965,https://hail.is,https://github.com/hail-is/hail/pull/12352,1,['timeout'],['timeout']
Safety,"': /hail-is/jgscm/archive/v0.1.13+hail.zip; WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979cc310>, 'Connection to github.com timed out. (connect timeout=15)')': /hail-is/jgscm/archive/v0.1.13+hail.zip; WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979cce10>, 'Connection to github.com timed out. (connect timeout=15)')': /hail-is/jgscm/archive/v0.1.13+hail.zip; ERROR: Could not install packages due to an OSError: HTTPSConnectionPool(host='github.com', port=443): Max retries exceeded with url: /hail-is/jgscm/archive/v0.1.13+hail.zip (Caused by ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979ce290>, 'Connection to github.com timed out. (connect timeout=15)')). Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 61, in <module>; safe_call(*command); File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 17, in safe_call; raise e; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 14, in safe_call; sp.check_output(args, stderr=sp.STDOUT, **kwargs); File ""/opt/conda/default/lib/python3.11/subprocess.py"", line 466, in check_output; return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/opt/conda/default/lib/python3.11/subprocess.py"", line 571, in run; raise CalledProcessError(retcode, process.args,; subprocess.CalledProcessError: Command '('pip', 'install', 'setuptools', 'mkl<2020', 'lxml<5', 'https://github.com/hail-is/jgscm/archive/v0.1.13+hail.zip', 'ipykernel==6.22.0', 'ipywidgets==8.0.6', 'jupyter-console==6.6.3', 'nbconvert==7",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14652:4566,timeout,timeout,4566,https://hail.is,https://github.com/hail-is/hail/issues/14652,1,['timeout'],['timeout']
Safety,'may or may not' is redundant phrasing. The word 'may' is sufficient to indicate the optional nature of glob expressions in the `path` argument to `import_vcf`. ## Security Assessment; - This change has no security impact. ### Impact Description; Docs only,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14746:20,redund,redundant,20,https://hail.is,https://github.com/hail-is/hail/pull/14746,1,['redund'],['redundant']
Safety,"'s the only event loop that will exist forever. Pytest (and newer version of IPython, afaict) violate this pretty liberally. ~~pytest_asyncio has [explicit instructions on how to run every test in the same event loop](https://pytest-asyncio.readthedocs.io/en/latest/how-to-guides/run_session_tests_in_same_loop.html). I've implemented those here.~~ [These instructions don't work](https://github.com/pytest-dev/pytest-asyncio/issues/744). It seems that the reliable way to ensure we're using one event loop everywhere is to use pytest-asyncio < 0.23 and to define an event_loop fixture with scope `'session'`. I also switched test_batch.py into pytest-only style. This allows me to use session-scoped fixtures so that they exist exactly once for the entire test suite execution. Also:; - `RouterAsyncFS` methods must either be a static method or an async method. We must not create an FS in a sync method. Both `parse_url` and `copy_part_size` now both do not allocate an FS.; - `httpx.py` now eagerly errors if the running event loop in `request` differs from that at allocation time. Annoying but much better error message than this nonsense about timeout context managers.; - `hail_event_loop` either gets the current thread's event loop (running or not, doesn't matter to us) or creates a fresh event loop and sets it as the current thread's event loop. The previous code didn't guarantee we'd get an event loop b/c `get_event_loop` fails if `set_event_loop` was previously called.; - `conftest.py` is inherited downward, so I lifted fixtures out of test_copy.py and friends and into a common `hailtop/conftest.py`; - I added `make -C hail pytest-inter-cloud` for testing the inter cloud directory. You still need appropriate permissions and authn.; - I removed extraneous pytest.mark.asyncio since we use auto mode everywhere.; - `FailureInjectingClientSession` creates an `aiohttp.ClientSession` and therefore must be used while an event loop is running. Easiest fix was to make the test async.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14097:1334,timeout,timeout,1334,https://hail.is,https://github.com/hail-is/hail/pull/14097,1,['timeout'],['timeout']
Safety,"().write(); ```. ```; Traceback (most recent call last):; File ""/tmp/a913d6ce5b814a63ad7af31060416237/pyscripts_Xr0D99.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/a913d6ce5b814a63ad7af31060416237/generate_qc_annotations.py"", line 247, in main; generate_call_stats(mt).write(annotations_mt_path(data_type, 'call_stats'), args.overwrite); File ""<decorator-gen-556>"", line 2, in write; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/matrixtable.py"", line 2027, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: IllegalArgumentException: requirement failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 1.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 1.0 (TID 10060, exomes3-sw-dfpw.c.broad-mpg-gnomad.internal, executor 134): java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:212); 	at is.hail.variant.Call$.alleleByIndex(Call.scala:128); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply$mcIII$sp(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.BinaryFun.apply(Fun.scala:122); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465:1606,abort,aborted,1606,https://hail.is,https://github.com/hail-is/hail/issues/3465,1,['abort'],['aborted']
Safety,(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.hail.rvd.RVD.combine(RV,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:11091,abort,abortStage,11091,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['abort'],['abortStage']
Safety,(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806:3167,abort,abortStage,3167,https://hail.is,https://github.com/hail-is/hail/issues/1806,1,['abort'],['abortStage']
Safety,(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:5128,abort,abortStage,5128,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['abort'],['abortStage']
Safety,(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:9033,abort,abortStage,9033,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['abort'],['abortStage']
Safety,"(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-0_0 (139802205742848) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~~~~~~~~ Stack of asyncio_0 (139802248206080) ~~~~~~~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ------------------------------ live log teardown -------------------------------; INFO hailtop.utils:utils.py:450 discarding exception; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 378, in rm_dir; await self.rmdir(path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 352, in rmdir; return await blocking_to_async(self._thread_pool, os.rmdir, path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 162, in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; File ""/usr/lib/python3.9/asyncio/futures.py"", line 284, in __await__; yield self # This tells Task to wait for completion.; File ""/usr/lib/python3.9/asyncio/tasks.py"", line 328, in __wakeup; future.result(); File ""/usr/lib/python3.9/asyncio/futures.py"", line 201, in result; raise self._exception; File ""/usr/lib/python3.9/concurrent/futures/th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:2564,Timeout,Timeout,2564,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['Timeout'],['Timeout']
Safety,"(body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1234, in endheaders\n self._send_output(message_body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1026, in _send_output\n self.send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 168, in _new_conn\n self, \""Failed to establish a new connection: %s\"" % e)\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 449, in send\n timeout=timeout\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 638, in urlopen\n _stacktrace=sys.exc_info()[2])\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\"", line 399, in increment\n raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6754:2523,timeout,timeout,2523,https://hail.is,https://github.com/hail-is/hail/issues/6754,2,['timeout'],['timeout']
Safety,"(https://github.com/pytest-dev/pytest) from 6.2.5 to 7.0.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pytest-dev/pytest/releases"">pytest's releases</a>.</em></p>; <blockquote>; <h2>7.0.1</h2>; <h1>pytest 7.0.1 (2022-02-11)</h1>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9608"">#9608</a>: Fix invalid importing of <code>importlib.readers</code> in Python 3.9.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9610"">#9610</a>: Restore [UnitTestFunction.obj]{.title-ref} to return unbound rather than bound method.; Fixes a crash during a failed teardown in unittest TestCases with non-default [__init__]{.title-ref}.; Regressed in pytest 7.0.0.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9636"">#9636</a>: The <code>pythonpath</code> plugin was renamed to <code>python_path</code>. This avoids a conflict with the <code>pytest-pythonpath</code> plugin.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9642"">#9642</a>: Fix running tests by id with <code>::</code> in the parametrize portion.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9643"">#9643</a>: Delay issuing a <code>~pytest.PytestWarning</code>{.interpreted-text role=&quot;class&quot;} about diamond inheritance involving <code>~pytest.Item</code>{.interpreted-text role=&quot;class&quot;} and; <code>~pytest.Collector</code>{.interpreted-text role=&quot;class&quot;} so it can be filtered using <code>standard warning filters &lt;warnings&gt;</code>{.interpreted-text role=&quot;ref&quot;}.</li>; </ul>; <h2>7.0.0</h2>; <h1>pytest 7.0.0 (2022-02-03)</h1>; <p>(<strong>Please see the full set of changes for this release also in the 7.0.0rc1 notes below</strong>)</p>; <h2>Deprecations</h2>; <ul>; <li>; <p><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9488",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:985,avoid,avoids,985,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['avoid'],['avoids']
Safety,"(mt.locus, mt.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(mt.fam.pat_id),; mt.GT.num_alt_alleles())),; meta={'group': 'adj'}). per_sample = per_sample.annotate(adj=adj_per_sample[per_sample.s]). mt = mt.annotate_rows(family_stats=mt.family_stats.append(family_stats_adj)); mt.write(); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:394); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1433); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1421); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60$$anonfun$apply$4.apply$mcV$sp(MatrixTable.scala:1723); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:197); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:186); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTable.scala:1722); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTable.scala:1718); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rv",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3074:1919,unsafe,unsafeInsert,1919,https://hail.is,https://github.com/hail-is/hail/issues/3074,1,['unsafe'],['unsafeInsert']
Safety,"(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37; ```. ### Traces No.2:; ```java; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 220, ip-172-31-2-255.ec2.internal, executor 2): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:879); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:877); 	at scala.Option.map(Option.scala:146); 	at is.hail.HailContext$$anon$3.compute(HailContext.scala:877); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.sp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:30138,abort,aborted,30138,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['abort'],['aborted']
Safety,"), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 3, altAlleles), ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false)))) AS variant#8; ```. Attached is a toy test.in.vds that reproduces the problem [test.in.vds.tar.gz](https://github.com/hail-is/hail/files/709524/test.in.vds.tar.gz",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1260:9707,unsafe,unsafe,9707,https://hail.is,https://github.com/hail-is/hail/issues/1260,1,['unsafe'],['unsafe']
Safety,")._convert_to_j(index_file_map); 1956; -> 1957 Env.hc()._jhc.indexBgen(jindexed_seq_args(path), index_file_map, joption(rg), contig_recoding, skip_invalid_loci); 1958; 1959. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: GC overhead limit exceeded. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.immutable.VectorBuilder.<init>(Vector.scala:713); at scala.collection.immutable.Vector$.newBuilder(Vector.scala:22); at scala.collection.immutable.IndexedSeq$.newBuilder(IndexedSeq.scala:46); at scala.collection.IndexedSeq$.newBuilder(IndexedSeq.scala:36); at scala.collection.IndexedSeq$$anon$1.apply(IndexedSeq.scala:34); at com.twitter.chill.TraversableSerializer.read(Traversable.scala:39); at com.twitter.chill.TraversableSerializer.read(Traversable.scala:21); at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396); at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307); at com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:6952,abort,aborted,6952,https://hail.is,https://github.com/hail-is/hail/issues/4780,1,['abort'],['aborted']
Safety,); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:8283,Unsafe,UnsafeRow,8283,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['Unsafe'],['UnsafeRow']
Safety,"); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10705:2722,timeout,timeout,2722,https://hail.is,https://github.com/hail-is/hail/pull/10705,1,['timeout'],['timeout']
Safety,"* Add a lightweight DSL for writing IR in Scala, which made the lowerings much easier to write, and read. It is implemented in `IRBuilder`, and can be used by importing `IRBuilder._` into scope. It's not complete, and I want to make it eagerly typecheck eventually, but we can build on it.; * Make `execute` protected on `MatrixIR` and `TableIR`, making `Interpret` the official place to execute IR.; * Add a compiler pass lowering some `MatrixIR` to `TableIR`. The `Interpret` gateway to `execute` always lowers, so we can safely remove the execute methods of IR nodes which are rewritten by the lowering.; * Fix `LoadBgen` to not create entries arrays when `dropCols` is true.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4707:524,safe,safely,524,https://hail.is,https://github.com/hail-is/hail/pull/4707,1,['safe'],['safely']
Safety,* Added a quick check in minRep to avoid copmuting things if ref is already minimal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1664:35,avoid,avoid,35,https://hail.is,https://github.com/hail-is/hail/pull/1664,1,['avoid'],['avoid']
Safety,"* Correct spelling of ""decommissioning"" in help for `--graceful-decommission-timeout`.; * Add space between ""match"" and ""the"" in help for `--update-hail-version`.; * Add punctuation to help for `--update-hail-version` for consistency with other arguments.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7335:77,timeout,timeout,77,https://hail.is,https://github.com/hail-is/hail/pull/7335,1,['timeout'],['timeout']
Safety,* Improves pruning of BlockMatrix.write_from_entry_expr to avoid large; allocation in the slow test.; * Delete the slow test.; * Add two benchmarks that should catch this,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9150:59,avoid,avoid,59,https://hail.is,https://github.com/hail-is/hail/pull/9150,1,['avoid'],['avoid']
Safety,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2329:1090,avoid,avoid,1090,https://hail.is,https://github.com/hail-is/hail/pull/2329,1,['avoid'],['avoid']
Safety,", <a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a></li>; </ul>; <h2>v4.0.1</h2>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h2>async-timeout 4.0.0</h2>; <h1>Changes</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Drooped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/blob/master/CHANGES.rst"">async-timeout's changelog</a>.</em></p>; <blockquote>; <h1>4.0.2 (2021-12-20)</h1>; <h2>Misc</h2>; <ul>; <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://github.com/aio-libs/async-timeout/issues/259&gt;</code><em>, <code>[#274](https://github.com/aio-libs/async-timeout/issue",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:1425,timeout,timeout,1425,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,", full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 31 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 32 except pyspark.sql.utils.CapturedException as e:; 33 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal. Exit status: 137. Diagnostics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal. Exit status: 137. Diagnostics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287:5719,abort,aborted,5719,https://hail.is,https://github.com/hail-is/hail/issues/13287,1,['abort'],['aborted']
Safety,", only depending on the semantics. It also correctly handles missing key fields, where the previous implementation often produced an unsound transformation of the IR. Also adds a much more thorough test suite than we had before. At the top level, the analysis takes a boolean typed IR `cond` in an environment where there is a reference to some `key`, and produces a set `intervals`, such that `cond` is equivalent to `cond & intervals.contains(key)` (in other words `cond` implies `intervals.contains(key)`, or `intervals` contains all rows where `cond` is true). This means for instance it is safe to replace `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond)`. Then in a second pass it rewrites `cond` to `cond2`, such that `cond & (intervals.contains(key))` is equivalent to `cond2 & intervals.contains(key)` (in other words `cond` implies `cond2`, and `cond2 & intervals.contains(key)` implies `cond`). This means it is safe to replace the `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond2)`. A common example is when `cond` can be completely captured by the interval filter, i.e. `cond` is equivant to `intervals.contains(key)`, in which case we can take `cond2 = True`, and the `TableFilter` can be optimized away. This all happens in the function; ```scala; def extractPartitionFilters(ctx: ExecuteContext, cond: IR, ref: Ref, key: IndexedSeq[String]): Option[(IR, IndexedSeq[Interval])] = {; if (key.isEmpty) None; else {; val extract = new ExtractIntervalFilters(ctx, ref.typ.asInstanceOf[TStruct].typeAfterSelectNames(key)); val trueSet = extract.analyze(cond, ref.name); if (trueSet == extract.KeySetLattice.top); None; else {; val rw = extract.Rewrites(mutable.Set.empty, mutable.Set.empty); extract.analyze(cond, ref.name, Some(rw), trueSet); Some((extract.rewrite(cond, rw), trueSet)); }; }; }; ```; `trueSet` is the set of intervals which contains all rows where `cond` is true. This set is passed back into `analyze` in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13355:1456,safe,safe,1456,https://hail.is,https://github.com/hail-is/hail/pull/13355,1,['safe'],['safe']
Safety,", with only the core interface methods and without any runtime checking. It is intended to be used only for defining a new `FlipbookIterator` or `StagingIterator`, through the factory methods on their respective companion objects which take a `StateMachine`. I implemented `fliterWhere`, `map`, and `flatMap` on `FlipbookIterator`, allowing the use of forcomprehensions, and on top of that I defined all the varieties of join. Some of the join methods take `OrderingView` arguments. An `OrderingView` is a small abstraction on top of an `Ordering` which can take one element `a`, copy data (such as key-fields) from `a` if necessary, then later (after `a` might have been destroyed or mutated) compare `a` to other elements using the copied data. The potentially producting (non-distinct assuming) join methods also take a buffer argument, which is anything that can make a copy of an iterator and then iterate over the copy multiple times. To avoid allocating tuples in the output iterators of the join methods, I made `Muple`, which is just a mutable tuple. I turned the existing `JoinedRegionValue` into an alias of `Muple[RegionValue, RegionValue]`. All of this core `FlipbookIterator` and `StagingIterator` behavior has no dependencies on anything else in Hail, so I want to thoroughly test everything at this level, and treat it like a small external iterator library living inside the repo. As such, I think this level should be quite stable going forwards. At the higher level, I lifted all the join methods on `FlipbookIterator` to `OrderedRVIterator`, which is a `Iterator[RegionValue]` together with an `OrderedRVDType`. I think `OrderedRVIterator` should be replaced by something better soon: see future work below. I've replaced the old implementations of `innerJoinDistinct`, `leftJoinDistinct`, and `orderedZipJoin` using the new infrastructure (see OrderedRDD2.scala), which I think is a good example of the kind of simplifications possible. The existing JoinSuite tests also serve as",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3016:4116,avoid,avoid,4116,https://hail.is,https://github.com/hail-is/hail/pull/3016,1,['avoid'],['avoid']
Safety,",A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String; }; [u'va.varid']; +--------+-------+--------------------+--------------------+--------------------+; |v.contig|v.start| v.ref| v.altAlleles| va.varid|; +--------+-------+--------------------+--------------------+--------------------+; | 01| 10013| A| [[A,C]]| 1:10013_A_C|; | 01| 10179| G| [[G,T]]| 1:10179_G_T|; | 01| 10259| C| [[C,A]]| 1:10259_C_A|; | 01| 10292| C| [[C,T]]| 1:10292_C_T|; | 01| 10402| G| [[G,A]]| 1:10402_G_A|; | 01| 10527| T| [[T,A]]| 1:10527_T_A|; | 01| 10611| G| [[G,A]]| 1:10611_G_A|; | 01| 10754| G| [[G,C]]| 1:10754_G_C|; | 01| 11099| T| [[T,G]]| 1:11099_T_G|; | 01| 11115| C| [[C,A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String,; C1: Double,; C2: Double; }; [u'va.varid']; [Stage 6:====================================================>(1640 + 1) / 1641]Traceback (most recent call last):; File ""/tmp/ec5f6e42-0ea7-404d-8311-f97f7ec26ad6/kt_troubleshooting_issue_042617.py"", line 31, in <module>; kt2.to_dataframe().show(10); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o448.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 20 times, most recent failure: Lost task 0.19 in stage 8.0 (TID 3406, cluster-mh-sw-xn3h.c.practice.internal): java.lang.ClassCastException: java.lang.String cannot be cast to is.hail.variant.Variant; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1725:2974,abort,aborted,2974,https://hail.is,https://github.com/hail-is/hail/issues/1725,1,['abort'],['aborted']
Safety,"- **Requires the python modules ** `nbsphinx`, `matplotlib`, `pandas`, `numpy`, and `seaborn`.; - Use property `-Dtutorial.home=/path/to/tutorial/files` with `gradle` to avoid downloading tutorial files with `wget`.; - Added new tgz file with tutorial files (reduced number of samples to 248 from 2535) https://storage.googleapis.com/hail-tutorial/Hail_Tutorial_Data-v2.tgz; - Edited tutorial to reflect smaller input file.; - Added iPython notebook to repository (this should be edited from now on); - Added tutorial to Sphinx docs.; - Changed tutorial location on website.; - Removed old tutorial infrastructure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1374:170,avoid,avoid,170,https://hail.is,https://github.com/hail-is/hail/pull/1374,1,['avoid'],['avoid']
Safety,"- Add a flush after writing the first log statement. This log statement is; displayed before any network requests, the flush ensures we always see it.; - Set the retries for in-cluster synchronous requests to 1.; - Change all external (ones that go through the gateway) HTTP(S) requests to use; a centrally defined session. This session improves the situation in two ways:; 1. It prevents urllib from retrying requests, which ensures Hail's retry; infrastructure is the only retry infrastructure.; 2. It sets a timeout, ensuring that all requests will timeout. Previously,; requests could hang forever.; 3. It permits setting headers that are used for all requests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9478:511,timeout,timeout,511,https://hail.is,https://github.com/hail-is/hail/pull/9478,2,['timeout'],['timeout']
Safety,"- Also moved the location of where buckets are mounted to not be in /batch so as to avoid accidentally deleting entire buckets.; - The file mode didn't do what I expected (allowed you to write to a bucket), but now that I think about it, we probably do want to expose this and my first intuition was right. We probably want files to be specified as read only when they're created on the local file system. I can make this a separate PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8979:84,avoid,avoid,84,https://hail.is,https://github.com/hail-is/hail/pull/8979,1,['avoid'],['avoid']
Safety,"- Create a cache that stores an instance's token which can be looked up by the instance's name; - Use this cache in the active_instances_only decorator to avoid making DB request on every invocation; - Add monitoring of caches' hits, misses, evictions, and load latencies",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11346:155,avoid,avoid,155,https://hail.is,https://github.com/hail-is/hail/pull/11346,1,['avoid'],['avoid']
Safety,"- Eigen, EigenDistributed in Scala and Python; - EigenSuite; - assertVectorEqualityUpToSignDouble in TestUtils for comparing eigenvectors; - eigen methods in LDMatrix and KinshipMatrix; - toLocalMatrix on IRM avoiding BlockMatrix and test; - RichSparkMatrix with asBreeze to avoid copy; - asSpark and toArrayShallow on RichBreezeDenseMatrix. I think there is opportunity to have Eigen and EigenDistributed abstract a common class to replication. And for Kinship and LDMatrix as well, so that Kinship can also use the read and write that LDMatrix has.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2160:209,avoid,avoiding,209,https://hail.is,https://github.com/hail-is/hail/pull/2160,2,['avoid'],"['avoid', 'avoiding']"
Safety,"- On *deploys*, makes sure that whatever is in our third-party images is in our private registry before starting builds like hail-ubuntu that might depend on those images. This means that we can update our ubuntu base image without the australians needing to deploy any images by hand. However, this does not run in PRs because I 1) didn't want to add that kind of latency for PRs and 2) we don't do any kind of namespacing for our images so if we did include this for a PR that ultimately wasn't merged we would have to manually remove the image anyway so why not manually add it if you're going to PR it I think point 2 is a little weak but I recall this being what we agreed on a couple months back when we discussed this. I'm wondering if we should just eat the minute or so latency at the beginning of PRs to be safe but it also feels like a shame for something that changes so infrequently. . - Again on deploys, upload the hailgenetics/* images to the private registry if they don't already exist there. This way any deployments that aren't hail team's GCP deployment can get these images automatically when they deploy a new SHA instead of uploading them manually. It won't backfill skipped versions, but we decided that was ok. This seems less relevant for testing on PRs as it will get triggered on releases and we can easily dev deploy to rectify the image if this breaks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12818:818,safe,safe,818,https://hail.is,https://github.com/hail-is/hail/pull/12818,1,['safe'],['safe']
Safety,"- Signup page with web socket and spinner while waiting for account to create; - Upon account creation, a billing project named `{username}-trial` is created with $10 limit and a user `{username}`; - When deleting an account, the billing project is reopened if it's closed, then remove the user, and finally close the billing project. This behavior might be debatable. We may not need to remove the user from the billing project. I think it's better to be safe in case the billing project is reopened. Auth-driver is implicitly dependent on the batch front end, but this dependency isn't stated in build.yaml. An event queue is a future solution. I can get rid of my personal email being whitelisted, but I think it will be useful if we need to debug later and for a possible demo on Monday (although I'll probably just use some screenshots). If you want to test it in your namespace, make sure to comment out all the create and delete steps that are not related to billing projects.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9658:456,safe,safe,456,https://hail.is,https://github.com/hail-is/hail/pull/9658,1,['safe'],['safe']
Safety,"- Values of types other than `Array` and `Boolean` get output in VCF format (e.g. `.` instead of `NA` for missing values); - `NaN` values are converted to missing (`.`) when exporting VCF since VCF doesn't handle `NaN`; - Changes to handling of filters:; - `.` <=> `NA:Set[String]`; - `PASS` <=> `{}:Set[String]`; - `other` <=> `{""other""}:Set[String]""`; - Removed `va.pass` entirely (redundant with `va.filters` and needs constant synchronization)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1517:384,redund,redundant,384,https://hail.is,https://github.com/hail-is/hail/pull/1517,1,['redund'],['redundant']
Safety,"- [ ] (@tpoterba) caf1e1e673 add fails_service_backend; - [ ] (@tpoterba, @cseed) a979dfba58 [hail] introduce and use mktemp and mktempd; - [ ] (@tpoterba) dcf026b01c [hail] make is.hail.expr.ir.functions threadsafe; - [ ] (@tpoterba) 807f38c20e [hail] fix use of row requiredness in lowerDistributedSort; - [ ] (@catoverdrive) 12df8eb456 [query-service] handle void-typed IRs in query-service; - [ ] (@catoverdrive) 03357ee83d [query-service] make user cache thread-safe; - [ ] (@tpoterba) 6c6734bc71 [query-service] bugfix: preserve globals through a shuffle; - [ ] (@catoverdrive) a3d2572ce7 [shuffler] log ShuffleCodecSpec anytime it is created; - [ ] (@daniel-goldstein) 8949dfec3c [scala-lsm] bugfix: least key may equal greatest key; - [ ] (@daniel-goldstein) 6067bd8e51 [services] discovered new transient error; - [ ] (@daniel-goldstein) c8356d30bb [shuffler] more assertions in ShuffleClient; - [ ] (@daniel-goldstein) 9991da90f0 [shuffler] bugfix: shuffler needs a HailContext to decode loci; - [ ] (@daniel-goldstein) bc0140ab6f [query-service] move hail.jar earlier in Dockerfile; - [ ] (@daniel-goldstein) f96c28174d [query-service] permit pod scaling and remove cpu limit; - [ ] (@catoverdrive) 6ae26339fe [query-service] simplify socket handling; - [ ] (@jigold) f3db30e23f [batch] teach JVMJob where to find the hail configuration files; - [ ] (@daniel-goldstein) b5c6d85554 [query-service] switch to services team approved logging; - [ ] (@tpoterba) 35a306c066 [query-service] query workers need a hail context; - [ ] (@daniel-goldstein, @catoverdrive) 051c89b8e7 [query-service] use a UNIX Domain Socket for Py-Scala communication; - [ ] (@daniel-goldstein, @catoverdrive) ad9ea73d7a [query-service] run tests against query service",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10072:467,safe,safe,467,https://hail.is,https://github.com/hail-is/hail/pull/10072,1,['safe'],['safe']
Safety,- [ ] attributes PR (per-batch attributes); - [ ] per-job attributes; - [ ] use queue and concurrent worker pool for all k8s communication; - [ ] batch avoids scheduling more than 150k pods,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6493:152,avoid,avoids,152,https://hail.is,https://github.com/hail-is/hail/issues/6493,1,['avoid'],['avoids']
Safety,"- [x] Width of navbar doesn't match #body; * Caused by using em to determine max-width of a child where the parent (#body) had a font-size set differently from root (html). Fixed by using rem, and to avoid changing so many em's, removing font-size on #body. - [x] Better dropdown: width, box shadow, padding. - [x] Some apparently unnecessary styles. Before (narrow):; <img width=""677"" alt=""Screenshot 2019-08-01 17 03 06"" src=""https://user-images.githubusercontent.com/5543229/62328170-61bb7700-b480-11e9-838a-43229ee955c3.png"">. After (narrow):; <img width=""712"" alt=""Screenshot 2019-08-01 17 02 53"" src=""https://user-images.githubusercontent.com/5543229/62328172-641dd100-b480-11e9-9be1-f3ff67035cd0.png"">. (more views):; ; <img width=""894"" alt=""Screenshot 2019-08-01 17 37 00"" src=""https://user-images.githubusercontent.com/5543229/62329162-19518880-b483-11e9-9cfd-12ca8c1a52dc.png"">; <img width=""854"" alt=""Screenshot 2019-08-01 17 37 05"" src=""https://user-images.githubusercontent.com/5543229/62329163-19518880-b483-11e9-8a70-90faa3dcc685.png"">; <img width=""923"" alt=""Screenshot 2019-08-01 17 37 11"" src=""https://user-images.githubusercontent.com/5543229/62329164-19518880-b483-11e9-9871-0ce39c6c7c53.png"">. I did this quickly, so didn't set up local server. If anything becomes ugly I'll fix today. Also didn't test in IE, just chrome, safari, and I expect Firefox to be fine as well. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6789:200,avoid,avoid,200,https://hail.is,https://github.com/hail-is/hail/pull/6789,1,['avoid'],['avoid']
Safety,"- `TContainer.loadLength` is now on the object so creating byte code to; read the length does not require knowledge of the element type. - in `elementsOffset`, use the non-Code pattern of delegating to; `roundUpAlignment`. - add staged `TContainer.elementOffset`, `TContainer.loadElement`,; `TStruct.fieldOffset`, `UnsafeUtils.roundUpAlignment`. - fix bug: loadElement reads addresses using `loadAddress` instead of; `loadInt`. - add several `loadX` methods to `RichCodeMemoryBuffer`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2347:315,Unsafe,UnsafeUtils,315,https://hail.is,https://github.com/hail-is/hail/pull/2347,1,['Unsafe'],['UnsafeUtils']
Safety,"- created is.hail.services package; - added DeployConfig, Tokens with the necessary functionality to get BatchClient working; - BatchClient is built on Apache HttpComponents; - Synchronous, thread safe. HttpClient is thread safe, BatchClient should be, too.; - Simple hello, world! test; - Added build step for Java services tests. FYI @jigold this might be a possible model if we ever rework the Python BatchClient. Also, if there are Batch changes going forward this code will also need to updated. The client is incredibly light weight, so that shouldn't be often, similar to the aiogoogle clients I wrote recently. Next up: Query Batch backend!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8779:197,safe,safe,197,https://hail.is,https://github.com/hail-is/hail/pull/8779,2,['safe'],['safe']
Safety,"- improved and fleshed out documentation of current BlockMatrix python functionality. Note`from_matrix_table` renamed to `from_entry_expr`, `from_numpy_matrix(numpy_matrix, ...)` renamed to `from_numpy(ndarray, ...)`, and similarly for `to_numpy_matrix`.; - renamed `matrix.py` as `blockmatrix.py`; - renamed `toLocalMatrix` to the more specific `toBreezeMatrix` on BlockMatrix and RowMatrix. This is prep for filling out the BlockMatrix interface w/ NumPy broadcast rules (model is LocalMatrix) and speeding up `to_numpy` and `from_numpy` (for now, by passing bytes via temp files rather than trough py4j) so that NumPy ndarrays serve as local matrix on Python side and interact predictably with BlockMatrices.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3072:680,predict,predictably,680,https://hail.is,https://github.com/hail-is/hail/pull/3072,1,['predict'],['predictably']
Safety,"- methods to go from `BlockMatrix` to `Table` and `MatrixTable` in a row-major representation. The table has fields for row index and array of entries for a row, and the matrix table has a row index, col index and the value at those indexes as the entry. This operation goes through disk to avoid a shuffle.; - method to go from a `Table` to a `MatrixTable` with a similar representation, where selected fields of the same type from the `Table` become column fields in the matrix table, with their values making the fields of the matrix table. Motivation for these conversions can be found at the issue below.; Resolves #5504. Big thanks to @patrick-schultz and @jigold for taking the time to teach me about region-based mem management!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5568:291,avoid,avoid,291,https://hail.is,https://github.com/hail-is/hail/pull/5568,1,['avoid'],['avoid']
Safety,- remove some redundant project git ignores; - add a few more file types,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8192:14,redund,redundant,14,https://hail.is,https://github.com/hail-is/hail/pull/8192,1,['redund'],['redundant']
Safety,- two network requests instead of four now that we know the name of both the service name and pod name; - use try-catch to avoid 500ing if a user hits `GET /new` and their service or pod is already gone (for whatever reason),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4863:123,avoid,avoid,123,https://hail.is,https://github.com/hail-is/hail/pull/4863,1,['avoid'],['avoid']
Safety,"------------------- live log setup --------------------------------; 2023-09-06T21:45:24 INFO test.conftest conftest.py:14:log_before_after starting test; 2023-09-06T21:45:24 INFO hailtop.aiocloud.aioazure.credentials credentials.py:99:default_credentials using credentials file /test-gsa-key/key.json; -------------------------------- live log call ---------------------------------; 2023-09-06T21:45:25 INFO azure.identity.aio._internal.get_token_mixin get_token_mixin.py:93:get_token ClientSecretCredential.get_token succeeded; 2023-09-06T21:45:25 INFO batch_client.aioclient aioclient.py:809:_submit created batch 191; submit job bunches  100% 1/1 0:00:00 0:00:00; 2023-09-06T21:47:17 WARNING hailtop.utils utils.py:842:retry_transient_errors_with_debug_string A transient error occured. We will automatically retry. Do not be alarmed. We have thus far seen 2 transient errors (next delay: 3.794s). The most recent error was <class 'asyncio.exceptions.TimeoutError'> . . +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ~~~~~~~~~~~~~~~~~~~~~ Stack of asyncio_0 (140387515627072) ~~~~~~~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++; FAILED; _________________ test_always_run_job_private_instance_cancel __________________. client = <hailtop.batch_client.client.BatchClient object at 0x7fae899806a0>. def test_always_run_job_private_instance_cancel(client: BatchClient):; b = create_batch(client); resources = {'machine_type': smallest_machine_type()}; j = b.create_job(DOCKER_ROOT_IMAG",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13582:1153,Timeout,TimeoutError,1153,https://hail.is,https://github.com/hail-is/hail/issues/13582,1,['Timeout'],['TimeoutError']
Safety,"------------------------------------------------------------------------; To reproduce; ```python; import hail as hl; mt = hl.import_vcf(""http://hgdownload.cse.ucsc.edu/gbdb/hg19/1000Genomes/phase3/ALL.chrY.phase3_integrated_v1a.20130502.genotypes.vcf.gz"", force_bgz=True); ----------------------------------------------------------------------; Initializing Hail with default parameters...; 2022-10-06 15:56:03 WARN Utils:69 - Your hostname, nid resolves to a loopback address: 127.0.1.1; using 192.168.248.80 instead (on interface wlp0s20f3); 2022-10-06 15:56:03 WARN Utils:69 - Set SPARK_LOCAL_IP if you need to bind to another address; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/med/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 2022-10-06 15:56:03 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.1.3; SparkUI available at http://192.168.248.80:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.100-2ea2615a797a; LOGGING: writing to /; --------------------------------------------------------------------------; mt.filter_rows(mt.locus.position==2867101).count_rows(); ```; ### Expected ; Return a count of rows with that condition. ### Error ; ```; FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assert",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12280:1074,unsafe,unsafe,1074,https://hail.is,https://github.com/hail-is/hail/issues/12280,1,['unsafe'],['unsafe']
Safety,"--------------------------; RemoteDisconnected Traceback (most recent call last); File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:703, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw); 702 # Make the request on the httplib connection object.; --> 703 httplib_response = self._make_request(; 704 conn,; 705 method,; 706 url,; 707 timeout=timeout_obj,; 708 body=body,; 709 headers=headers,; 710 chunked=chunked,; 711 ); 713 # If we're going to release the connection in ``finally:``, then; 714 # the response doesn't need to know about the connection. Otherwise; 715 # it will also try to release it and we'll have a double-release; 716 # mess. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:449, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw); 445 except BaseException as e:; 446 # Remove the TypeError from the exception chain in; 447 # Python 3 (including for exceptions like SystemExit).; 448 # Otherwise it looks like a bug in the code.; --> 449 six.raise_from(e, None); 450 except (SocketTimeout, BaseSSLError, SocketError) as e:. File <string>:3, in raise_from(value, from_value). File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:444, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw); 443 try:; --> 444 httplib_response = conn.getresponse(); 445 except BaseException as e:; 446 # Remove the TypeError from the exception chain in; 447 # Python 3 (including for exceptions like SystemExit).; 448 # Otherwise it looks like a bug in the code. File /opt/conda/lib/python3.10/http/client.py:1375, in HTTPConnection.getresponse(self); 1374 try:; -> 1375 response.begin(); 1376 except ConnectionError:. File /opt/conda/lib/python3.10/http/client.py:318, in HTTPResponse.begin(self); 317 while True:; --> 3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:4584,timeout,timeout,4584,https://hail.is,https://github.com/hail-is/hail/issues/13960,1,['timeout'],['timeout']
Safety,"----------------; 2023-09-06T21:45:24 INFO test.conftest conftest.py:14:log_before_after starting test; 2023-09-06T21:45:24 INFO hailtop.aiocloud.aioazure.credentials credentials.py:99:default_credentials using credentials file /test-gsa-key/key.json; -------------------------------- live log call ---------------------------------; 2023-09-06T21:45:25 INFO azure.identity.aio._internal.get_token_mixin get_token_mixin.py:93:get_token ClientSecretCredential.get_token succeeded; 2023-09-06T21:45:25 INFO batch_client.aioclient aioclient.py:809:_submit created batch 191; submit job bunches  100% 1/1 0:00:00 0:00:00; 2023-09-06T21:47:17 WARNING hailtop.utils utils.py:842:retry_transient_errors_with_debug_string A transient error occured. We will automatically retry. Do not be alarmed. We have thus far seen 2 transient errors (next delay: 3.794s). The most recent error was <class 'asyncio.exceptions.TimeoutError'> . . +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ~~~~~~~~~~~~~~~~~~~~~ Stack of asyncio_0 (140387515627072) ~~~~~~~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++; FAILED; _________________ test_always_run_job_private_instance_cancel __________________. client = <hailtop.batch_client.client.BatchClient object at 0x7fae899806a0>. def test_always_run_job_private_instance_cancel(client: BatchClient):; b = create_batch(client); resources = {'machine_type': smallest_machine_type()}; j = b.create_job(DOCKER_ROOT_IMAGE, ['true'], resources=resources, always_run=True);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13582:1208,Timeout,Timeout,1208,https://hail.is,https://github.com/hail-is/hail/issues/13582,1,['Timeout'],['Timeout']
Safety,"--------. ### Hail version: 0.1-6e815ac. ### What you did: hc.import_bgen('*.bgen) X chromosome cannot be imported, which is a major issue when working on phenotypes linked to blood coagulation, for example. ### What went wrong (all error messages here, including the full java stack trace):. [Stage 1:===============================================> (747 + 9) / 871]Traceback (most recent call last):; File ""regression1.py"", line 22, in <module>; hc.import_bgen('/mnt/volume/imputed_genotypes/*.bgen', sample_file='/mnt/volume/imputed_genotypes/MT.sample').split_multi().write('/mnt/volume/imputed_genotypes/MT_intersect_imputed.vds'); File ""<decorator-gen-285>"", line 2, in write; File ""/usr/local/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'. Java stack trace:; org.apache.spark.SparkException: Job aborted.; at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:147); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101); at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58); at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56); at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74); at org.apac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2407:1231,abort,aborted,1231,https://hail.is,https://github.com/hail-is/hail/issues/2407,1,['abort'],['aborted']
Safety,"---. ### Hail version:; 0.2; ### What you did:; users = hl.read_table('data/users.ht'); hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ). ### What went wrong (all error messages here, including the full java stack trace):; Gotten this error even though the elasticsearch IP and port number 32565 is correct. The IP mentioned in the error 192.168.185.157:9200 was not found anywhere in our EMR or elasticsearch cluster. ; >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ); Config Map(es.nodes -> XX.XXX.XXX.XXX, es.port -> 32565, es.batch.size.entries -> 200, es.index.auto.create -> true); [Stage 0:> (0 + 32) / 65]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1122>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2106, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]] . Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 73, ip-172-31-10-234.ap-southeast-1.compute.internal, executor 3): org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5643:1859,abort,aborted,1859,https://hail.is,https://github.com/hail-is/hail/issues/5643,1,['abort'],['aborted']
Safety,"-0-18-2021-10-26</a></p>; <h2>Incompatible changes</h2>; <p>5.0.0 b1</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10031"">#10031</a>: autosummary: <code>sphinx.ext.autosummary.import_by_name()</code> now raises; <code>ImportExceptionGroup</code> instead of <code>ImportError</code> when it failed to import; target object. Please handle the exception if your extension uses the; function to import Python object. As a workaround, you can disable the; behavior via <code>grouped_exception=False</code> keyword argument until v7.0.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9962"">#9962</a>: texinfo: Customizing styles of emphasized text via <code>@definfoenclose</code>; command was not supported because the command was deprecated since texinfo 6.8</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/2068"">#2068</a>: :confval:<code>intersphinx_disabled_reftypes</code> has changed default value; from an empty list to <code>['std:doc']</code> as avoid too surprising silent; intersphinx resolutions.; To migrate: either add an explicit inventory name to the references; intersphinx should resolve, or explicitly set the value of this configuration; variable to an empty list.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10197"">#10197</a>: html theme: Reduce <code>body_min_width</code> setting in basic theme to 360px</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9999"">#9999</a>: LaTeX: separate terms from their definitions by a CR (refs: <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9985"">#9985</a>)</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10062"">#10062</a>: Change the default language to <code>'en'</code> if any language is not set in; <code>conf.py</code></li>; </ul>; <p>5.0.0 final</p>; <ul>; <li><a href=""https://github-redirect",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11871:2435,avoid,avoid,2435,https://hail.is,https://github.com/hail-is/hail/pull/11871,2,['avoid'],['avoid']
Safety,"-jinja2/blob/master/CHANGES.rst"">aiohttp-jinja2's changelog</a>.</em></p>; <blockquote>; <h2>1.5 (2021-08-21)</h2>; <ul>; <li>Drop support for jinaj2 &lt;3. Add support for 3+.</li>; <li>Don't require <code>typing_extensions</code> on Python 3.8+.</li>; </ul>; <h2>1.4.2 (2020-11-23)</h2>; <ul>; <li>Add CHANGES.rst to MANIFEST.in and sdist <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/402"">#402</a></li>; </ul>; <h2>1.4.1 (2020-11-22)</h2>; <ul>; <li>Document async rendering functions <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/396"">#396</a></li>; </ul>; <h2>1.4.0 (2020-11-12)</h2>; <ul>; <li>; <p>Fix type annotation for <code>context_processors</code> argument <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/354"">#354</a></p>; </li>; <li>; <p>Bump the minimal supported <code>aiohttp</code> version to 3.6.3 to avoid problems; with uncompatibility between <code>aiohttp</code> and <code>yarl</code></p>; </li>; <li>; <p>Add async rendering support <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/393"">#393</a></p>; </li>; </ul>; <h2>1.3.0 (2020-10-30)</h2>; <ul>; <li>; <p>Remove Any from template annotations <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/343"">#343</a></p>; </li>; <li>; <p>Fix type annotation for filters in <code>aiohttp_jinja2.setup</code> <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/330"">#330</a></p>; </li>; <li>; <p>Drop Python 3.5, support Python 3.9</p>; </li>; </ul>; <h2>1.2.0 (2019-10-21)</h2>; <ul>; <li>Add type hints <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/285"">#285</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiohttp-jinja2/commit/d83f081c4c1b10102b53d4b973225c190b91652f""><code>d83f081</code></a> Release 1.5</li>; <li><a href=""https://",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11576:2912,avoid,avoid,2912,https://hail.is,https://github.com/hail-is/hail/pull/11576,1,['avoid'],['avoid']
Safety,"-libs/async-timeout/commit/f0a0914345b224448220aeb00d71e6a04a5d24bd""><code>f0a0914</code></a> Bump twine from 3.7.0 to 3.7.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/e5c813173b8811b30f4a30eeba56fa8808ab15bb""><code>e5c8131</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/271"">#271</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c7e10db3d0965422122bef263fb36f6fd7572330""><code>c7e10db</code></a> Fix linter configs</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/6af8bf421a799208b27d57c6bf69de0d998fedec""><code>6af8bf4</code></a> Bump pre-commit from 2.15 to 2.16.0 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/269"">#269</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c71bbb5b5d7330f6dabfde7a1adec30a4611c0be""><code>c71bbb5</code></a> Bump docutils from 0.18 to 0.18.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/266"">#266</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/aio-libs/async-timeout/compare/v3.0.1...v4.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=async-timeout&package-manager=pip&previous-version=3.0.1&new-version=4.0.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:5974,timeout,timeout,5974,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,"-preview</code> header from :meth:<code>gidgethub.apps.get_installation_access_token</code>; because it is out of preview. The <code>machine-man-preview</code> is <code>no longer required &lt;https://developer.github.com/changes/#--machine-man-and-sailor-v-previews-graduate&gt;</code>_; as of August 20, 2020.</li>; </ul>; <h2>5.0.0</h2>; <ul>; <li>Add :meth:<code>gidgethub.routing.Router.fetch</code> for obtaining a frozenset of functions; registered to the router that the event would be called on.; (<code>Issue [#74](https://github.com/brettcannon/gidgethub/issues/74) &lt;https://github.com/brettcannon/gidgethub/issues/74&gt;</code>_).</li>; <li>Add support for GitHub Actions Environment Files with :meth:<code>gidgethub.actions.setenv</code>; and :meth:<code>gidgethub.actions.addpath</code>.; (<code>Issue [#137](https://github.com/brettcannon/gidgethub/issues/137) &lt;https://github.com/brettcannon/gidgethub/issues/132&gt;</code>_).</li>; <li>Make router callback execution order non-deterministic to avoid relying on; registration order.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/9660d1e1c0187d9def32c473c8ceefcd130fe26f""><code>9660d1e</code></a> Add .DS_Store to .gitignore file</li>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/ef0368998fe40769f4f20a6c4b6ccfea27fe8ca9""><code>ef03689</code></a> Bump the version number</li>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/1f80a51670555acda0db0e42189d00bb58bb3b45""><code>1f80a51</code></a> Release 5.2.1</li>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/89ade8859539212e0663e91f0777ad8a39ecf323""><code>89ade88</code></a> Fix cgi and importlib_resources deprecations (<a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/185"">#185</a>)</li>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/6488",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12328:6911,avoid,avoid,6911,https://hail.is,https://github.com/hail-is/hail/pull/12328,1,['avoid'],['avoid']
Safety,"... in the sense that they always aggregate in partition order. This is achieved by a `AssociativeCombiner` which greedily combOp's aggregator state from adjacent partitions. I think this is the best you can do. Later we should have a `CommutativeCombiner` and choose between the two based on the whether the user-level aggregators are (duh) associative (like collect and prev_nonnull) or commutative (like collectAsSet or count). This is slightly conservative as I converted, with slavish consistency, all aggregators and reducers, included one only used by concordance, which I think is commutative. I'm OK with that mostly because this is safer and concordance really needs to get rewritten in Python (I think @tpoterba has a draft but it needed some performance work). FYI @chrisvittal this should fix any prev_nonnull aggregator/scan issues.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5420:642,safe,safer,642,https://hail.is,https://github.com/hail-is/hail/pull/5420,1,['safe'],['safer']
Safety,".0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3452,timeout,timeout,3452,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['timeout'],['timeout']
Safety,".9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/futures.py"", line 287, in __await__; return self.result() # May raise too.; File ""/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/futures.py"", line 201, in result; raise self._exception; File ""/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py"", line 256, in __step; result = coro.send(None); File ""/usr/local/lib/python3.9/site-packages/hailtop/aiocloud/common/session.py"", line 21, in post; return await self.request('POST', url, **kwargs); File ""/usr/local/lib/python3.9/site-packages/hailtop/aiocloud/common/session.py"", line 105, in request; return await self._http_session.request(method, url, **kwargs); File ""/usr/local/lib/python3.9/site-packages/hailtop/httpx.py"", line 137, in request_and_raise_for_status; resp = await self.client_session._request(method, url, **kwargs); File ""/usr/local/lib/python3.9/site-packages/aiohttp/client.py"", line 535, in _request; conn = await self._connector.connect(; File ""/usr/local/lib/python3.9/site-packages/aiohttp/connector.py"", line 542, in connect; proto = await self._create_connection(req, traces, timeout); File ""/usr/local/lib/python3.9/site-packages/aiohttp/connector.py"", line 907, in _create_connection; _, proto = await self._create_direct_connection(req, traces, timeout); File ""/usr/local/lib/python3.9/site-packages/aiohttp/connector.py"", line 1206, in _create_direct_connection; raise last_exc; File ""/usr/local/lib/python3.9/site-packages/aiohttp/connector.py"", line 1175, in _create_direct_connection; transp, proto = await self._wrap_create_connection(; File ""/usr/local/lib/python3.9/site-packages/aiohttp/connector.py"", line 992, in _wrap_create_connection; raise client_error(req.connection_key, exc) from exc; aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host storage.googleapis.com:443 ssl:default [Too many open files]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13940:6266,timeout,timeout,6266,https://hail.is,https://github.com/hail-is/hail/issues/13940,2,['timeout'],['timeout']
Safety,".</em></p>; <blockquote>; <h2>v2.32.0</h2>; <h2>2.32.0 (2024-05-20)</h2>; <h2> PYCON US 2024 EDITION </h2>; <p><strong>Security</strong></p>; <ul>; <li>Fixed an issue where setting <code>verify=False</code> on the first request from a; Session will cause subsequent requests to the <em>same origin</em> to also ignore; cert verification, regardless of the value of <code>verify</code>.; (<a href=""https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56"">https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56</a>)</li>; </ul>; <p><strong>Improvements</strong></p>; <ul>; <li><code>verify=True</code> now reuses a global SSLContext which should improve; request time variance between first and subsequent requests. It should; also minimize certificate load time on Windows systems when using a Python; version built with OpenSSL 3.x. (<a href=""https://redirect.github.com/psf/requests/issues/6667"">#6667</a>)</li>; <li>Requests now supports optional use of character detection; (<code>chardet</code> or <code>charset_normalizer</code>) when repackaged or vendored.; This enables <code>pip</code> and other projects to minimize their vendoring; surface area. The <code>Response.text()</code> and <code>apparent_encoding</code> APIs; will default to <code>utf-8</code> if neither library is present. (<a href=""https://redirect.github.com/psf/requests/issues/6702"">#6702</a>)</li>; </ul>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Fixed bug in length detection where emoji length was incorrectly; calculated in the request content-length. (<a href=""https://redirect.github.com/psf/requests/issues/6589"">#6589</a>)</li>; <li>Fixed deserialization bug in JSONDecodeError. (<a href=""https://redirect.github.com/psf/requests/issues/6629"">#6629</a>)</li>; <li>Fixed bug where an extra leading <code>/</code> (path separator) could lead; urllib3 to unnecessarily reparse the request URI. (<a href=""https://redirect.github.com/psf/requests/issues/6644"">#6644</a>)</li>; </ul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14555:1214,detect,detection,1214,https://hail.is,https://github.com/hail-is/hail/pull/14555,1,['detect'],['detection']
Safety,.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:5977,abort,abortStage,5977,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['abort'],['abortStage']
Safety,".RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283). Konrad Karczewski @konradjk 16:24; this should work, so i think it's a bug. but in the short run, you could hdfs dfs -cp file:///tmp/clinvar.vcf.gz / and then just load /clinvar.vcf.gz; copy to hdfs; (you shouldn't have to, but \_()_/). bw2 @bw2 16:27; that worked. thanks!. ### What went wrong (all error messages here, including the full java stack trace):. Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); 	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:5078,abort,aborted,5078,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['abort'],['aborted']
Safety,.SparkBackend.withExecuteContext(SparkBackend.scala:229); 			at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); 			at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); 			at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 			at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 			at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 			at java.lang.reflect.Method.invoke(Method.java:498); 			at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 			at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 			at py4j.Gateway.invoke(Gateway.java:282); 			at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 			at py4j.commands.CallCommand.execute(CallCommand.java:79); 			at py4j.GatewayConnection.run(GatewayConnection.java:238); 			at java.lang.Thread.run(Thread.java:748). 	org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:8418,abort,aborted,8418,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['abort'],['aborted']
Safety,".__version__, deepest)) from None; 229 except pyspark.sql.utils.CapturedException as e:; 230 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 20 times, most recent failure: Lost task 0.19 in stage 24.0 (TID 1813, lfrani-sw-hqb8.c.broad-mpg-gnomad.internal, executor 159): is.hail.utils.HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; 	at is.hail.utils.ErrorHandling$class.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:2782,abort,aborted,2782,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['abort'],['aborted']
Safety,".apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 1, start), IntegerType), ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8Str",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1260:7506,unsafe,unsafe,7506,https://hail.is,https://github.com/hail-is/hail/issues/1260,1,['unsafe'],['unsafe']
Safety,".com/aio-libs/async-timeout/commit/e5c813173b8811b30f4a30eeba56fa8808ab15bb""><code>e5c8131</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/271"">#271</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c7e10db3d0965422122bef263fb36f6fd7572330""><code>c7e10db</code></a> Fix linter configs</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/6af8bf421a799208b27d57c6bf69de0d998fedec""><code>6af8bf4</code></a> Bump pre-commit from 2.15 to 2.16.0 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/269"">#269</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c71bbb5b5d7330f6dabfde7a1adec30a4611c0be""><code>c71bbb5</code></a> Bump docutils from 0.18 to 0.18.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/266"">#266</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/aio-libs/async-timeout/compare/v3.0.1...v4.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=async-timeout&package-manager=pip&previous-version=3.0.1&new-version=4.0.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:6268,timeout,timeout,6268,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:8221,Unsafe,UnsafeRow,8221,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['Unsafe'],['UnsafeRow']
Safety,.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:12111,abort,abortStage,12111,https://hail.is,https://github.com/hail-is/hail/issues/7044,2,['abort'],['abortStage']
Safety,.scala:86); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855); 	at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:8337,abort,abortStage,8337,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['abort'],['abortStage']
Safety,.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202:5248,abort,abortStage,5248,https://hail.is,https://github.com/hail-is/hail/issues/1202,1,['abort'],['abortStage']
Safety,.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)sun.reflect.generics.reflectiveObjects.NotImplementedException: null; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:6615,Unsafe,UnsafeRow,6615,https://hail.is,https://github.com/hail-is/hail/issues/4215,1,['Unsafe'],['UnsafeRow']
Safety,"/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 20 times, most recent failure: Lost task 8.19 in stage 1.0 (TID 2899) (hail-test-w-1.australia-southeast1-a.c.pb-dev-312200.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:3409,abort,aborted,3409,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['abort'],['aborted']
Safety,"/Library/Python/3.9/lib/python/site-packages/hail/backend/py4j_backend.py:218, in Py4JBackend._rpc(self, action, payload); 216 path = action_routes[action]; 217 port = self._backend_server_port;  218 resp = self._requests_session.post(fhttp://localhost:{port}{path}', data=data); 219 if resp.status_code >= 400:; 220 error_json = orjson.loads(resp.content). File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:637, in Session.post(self, url, data, json, **kwargs); 626 def post(self, url, data=None, json=None, **kwargs):; 627 r""""Sends a POST request. Returns :class:Response object.; 628; 629 :param url: URL for the new :class:Request object.; (); 634 :rtype: requests.Response; 635 ;  637 return self.request(POST, url, data=data, json=json, **kwargs). File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:589, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json); 584 send_kwargs = {; 585 timeout: timeout,; 586 allow_redirects: allow_redirects,; 587 }; 588 send_kwargs.update(settings);  589 resp = self.send(prep, **send_kwargs); 591 return resp. File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:703, in Session.send(self, request, **kwargs); 700 start = preferred_clock(); 702 # Send the request;  703 r = adapter.send(request, **kwargs); 705 # Total elapsed time of the request (approximately); 706 elapsed = preferred_clock() - start. File ~/Library/Python/3.9/lib/python/site-packages/requests/adapters.py:501, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies); 486 resp = conn.urlopen(; 487 method=request.method,; 488 url=url,; (); 497 chunked=chunked,; 498 ); 500 except (ProtocolError, OSError) as err:;  501 raise ConnectionError(err, request=request); 503 except MaxRetryError as e:; 504 if isinstance(e.reason, ConnectTimeoutError):; 505 # TODO: Remove this in 3.0.0: see #2811. ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14557:2134,timeout,timeout,2134,https://hail.is,https://github.com/hail-is/hail/issues/14557,3,['timeout'],['timeout']
Safety,"/a>) (<a href=""https://github.com/vitejs/vite/commit/1afc1c2"">1afc1c2</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8668"">#8668</a></li>; </ul>; <h2><!-- raw HTML omitted -->2.9.12 (2022-06-10)<!-- raw HTML omitted --></h2>; <ul>; <li>fix: outdated optimized dep removed from module graph (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8534"">#8534</a>) (<a href=""https://github.com/vitejs/vite/commit/c0d6c60"">c0d6c60</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8534"">#8534</a></li>; </ul>; <h2><!-- raw HTML omitted -->2.9.11 (2022-06-10)<!-- raw HTML omitted --></h2>; <ul>; <li>fix: respect server.headers in static middlewares (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8481"">#8481</a>) (<a href=""https://github.com/vitejs/vite/commit/ab7dc1c"">ab7dc1c</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8481"">#8481</a></li>; <li>fix(dev): avoid FOUC when swapping out link tag (fix <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7973"">#7973</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8495"">#8495</a>) (<a href=""https://github.com/vitejs/vite/commit/01fa807"">01fa807</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/7973"">#7973</a> <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8495"">#8495</a></li>; </ul>; <h2><!-- raw HTML omitted -->2.9.10 (2022-06-06)<!-- raw HTML omitted --></h2>; <ul>; <li>feat: treat Astro file scripts as TS (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8151"">#8151</a>) (<a href=""https://github.com/vitejs/vite/commit/9fdd0a3"">9fdd0a3</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8151"">#8151</a></li>; <li>feat: new hook <code>configurePreviewServer</code> (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7658"">#7658</a>) ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12142:3070,avoid,avoid,3070,https://hail.is,https://github.com/hail-is/hail/pull/12142,2,['avoid'],['avoid']
Safety,"/compare/4.0.0...4.0.1"">https://github.com/samtools/htsjdk/compare/4.0.0...4.0.1</a></p>; <h2>4.0.0</h2>; <h2>Moving forward</h2>; <p>This is the first release to be built exclusively for java 17. Java 17 features are now allowed in our source code and we will no longer support older versions of java. We've also updated dependencies to fix security issues. There are several small bug fixes as well.</p>; <h3>JSON dependency:</h3>; <p>We've dropped the MJSON library which was no longer being updated and replaced it with a similarly small json library from org.json</p>; <h2>What's Changed</h2>; <ul>; <li>Migrate to Java 17 by <a href=""https://github.com/lbergelson""><code>@lbergelson</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1649"">samtools/htsjdk#1649</a></li>; <li>Remove low-value progress logging message by <a href=""https://github.com/nh13""><code>@nh13</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1659"">samtools/htsjdk#1659</a></li>; <li>removed redundant code by <a href=""https://github.com/KleinSamuel""><code>@KleinSamuel</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1664"">samtools/htsjdk#1664</a></li>; <li>Update snappy-java and migrate mjson to org.json to address CVEs by <a href=""https://github.com/bbimber""><code>@bbimber</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1670"">samtools/htsjdk#1670</a></li>; <li>Remove incorrect zero-length-B-array checks <a href=""https://github.com/gileshall""><code>@gileshall</code></a> and <a href=""https://github.com/jmarshall""><code>@jmarshall</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1674"">samtools/htsjdk#1674</a></li>; <li>add SINGULAR platform to read group by <a href=""https://github.com/omicsorama""><code>@omicsorama</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1635"">samtools/htsjdk#1635</a></li>; </ul>; <h2>New Contributors</h2>; <ul>; <li><a href=""https://gith",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13576:2222,redund,redundant,2222,https://hail.is,https://github.com/hail-is/hail/pull/13576,1,['redund'],['redundant']
Safety,"/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:2387,Unsafe,UnsafeRow,2387,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['Unsafe'],['UnsafeRow']
Safety,"/github.com/aio-libs/aiohttp/issues/6396&gt;</code>_</p>; </li>; <li>; <p>Remove a deprecated usage of pytest.warns(None); <code>[#6663](https://github.com/aio-libs/aiohttp/issues/6663) &lt;https://github.com/aio-libs/aiohttp/issues/6663&gt;</code>_</p>; </li>; <li>; <p>Fix regression where <code>asyncio.CancelledError</code> occurs on client disconnection.; <code>[#6719](https://github.com/aio-libs/aiohttp/issues/6719) &lt;https://github.com/aio-libs/aiohttp/issues/6719&gt;</code>_</p>; </li>; <li>; <p>Export :py:class:<code>~aiohttp.web.PrefixedSubAppResource</code> under; :py:mod:<code>aiohttp.web</code> -- by :user:<code>Dreamsorcerer</code>.</p>; <p>This fixes a regression introduced by :pr:<code>3469</code>.; <code>[#6889](https://github.com/aio-libs/aiohttp/issues/6889) &lt;https://github.com/aio-libs/aiohttp/issues/6889&gt;</code>_</p>; </li>; <li>; <p>Dropped the :class:<code>object</code> type possibility from; the :py:attr:<code>aiohttp.ClientSession.timeout</code>; property return type declaration.; <code>[#6917](https://github.com/aio-libs/aiohttp/issues/6917) &lt;https://github.com/aio-libs/aiohttp/issues/6917&gt;</code>_,</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/30b7a4e99677b4014dda2372504343bb05fc983e""><code>30b7a4e</code></a> Add a yanking caution message to v3.8.2 changelog</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/18279443d4081a02585739d52c5822340068a13f""><code>1827944</code></a> Stop including an empty changelog draft in Sphinx</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/58a4733a17f7d1a29ceda6d8cabd8d4204039038""><code>58a4733</code></a> Mention that v3.8.2 has been yanked</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/13f50f949b8eca81c3809bc79f106e2336d49781""><code>13f50f9</code></a> Move the Python 3.6 attention box to v3.8.3</li>; <li><a h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12296:5289,timeout,timeout,5289,https://hail.is,https://github.com/hail-is/hail/pull/12296,1,['timeout'],['timeout']
Safety,"/importlib_metadata/commit/c8d7285af792d6851227212d4261ce7ae180a87c""><code>c8d7285</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/391"">#391</a> from python/ghpython-93259/from-name-arg-validation-s...</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/91b71494226a95251134c4fe6ea65a1dd25f495c""><code>91b7149</code></a> Update changelog</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/c96dc1e77f032315bfc78f0c1d13c9a61fb68c3f""><code>c96dc1e</code></a> Merge branch 'main' into ghpython-93259/from-name-arg-validation-simple</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/f52757d0c8a9a555d0591a86b334a17028e2ead9""><code>f52757d</code></a> In Distribution.from_name, re-use discover.</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/344a6ffc612eec611592e7686264ced72f64da5a""><code>344a6ff</code></a> Refactor Distribution.from_name to avoid return in loop and unnecessary None ...</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/eb19c647519c754dd93b42a0c421101af73cf7a4""><code>eb19c64</code></a> In Distribution.from_name, require a non-empty string. Fixes <a href=""https://github-redirect.dependabot.com/python/cpython/issues/9"">python/cpython#9</a>...</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/d3fe031dbad4590896829f18ecbd8d9d8a132f53""><code>d3fe031</code></a> Add comment about the compatibility factor.</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/a4ae953dca38da768bcd1786aeba84bada32efb4""><code>a4ae953</code></a> Add xfail test capturing new expectation.</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/e5b7d8759214feedd0c49a7859ebb124473bcfc3""><code>e5b7d87</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/390"">#390</a> from python/bugfix/noisy-coverage</li>; <li>Additional comm",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12000:3422,avoid,avoid,3422,https://hail.is,https://github.com/hail-is/hail/pull/12000,1,['avoid'],['avoid']
Safety,"/li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.24 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2158"">#2158</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f5682a4f6d6d5372a2d382ae3e47dace490ca0d"">4f5682a</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.25.0...v2.26.0"">2.26.0</a> (2023-08-03)</h2>; <h3>Features</h3>; <ul>; <li>Implement BufferToDiskThenUpload BlobWriteSessionConfig (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2139"">#2139</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4dad2d5c3a81eda7190ad4f95316471e7fa30f66"">4dad2d5</a>)</li>; <li>Introduce new BlobWriteSession (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2123"">#2123</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e0191b518e50a49fae0691894b50f0c5f33fc6af"">e0191b5</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li><strong>grpc:</strong> Return error if credentials are detected to be null (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2142"">#2142</a>) (<a href=""https://github.com/googleapis/java-storage/commit/b61a9764a9d953d2b214edb2b543b8df42fbfa06"">b61a976</a>)</li>; <li>Possible NPE when HttpStorageOptions deserialized (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2153"">#2153</a>) (<a href=""https://github.com/googleapis/java-storage/commit/68ad8e7357097e3dd161c2ab5f7a42a060a3702c"">68ad8e7</a>)</li>; <li>Update grpc default metadata projection to include acl same as json (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2150"">#2150</a>) (<a href=""https://github.com/googleapis/java-storage/commit/330e795040592e5df22d44fb5216ad7cf2448e81"">330e795</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.14.0 (<a href=""https://redirect.github.com/google",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13605:6569,detect,detected,6569,https://hail.is,https://github.com/hail-is/hail/pull/13605,1,['detect'],['detected']
Safety,"/mrabarnett/mrab-regex) from 2023.3.23 to 2023.5.5.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/mrabarnett/mrab-regex/blob/hg/changelog.txt"">regex's changelog</a>.</em></p>; <blockquote>; <p>Version: 2023.5.5</p>; <pre><code>Removed semicolon after 'else' in 'munge_name'.; </code></pre>; <p>Version: 2023.5.4</p>; <pre><code>Fixed pyproject.toml and setup.py.; </code></pre>; <p>Version: 2023.5.3</p>; <pre><code>pyproject.toml was missing.; </code></pre>; <p>Version: 2023.5.2</p>; <pre><code>Added pyproject.toml.; </code></pre>; <p>Version: 2023.3.23</p>; <pre><code>Git issue 495: Running time for failing fullmatch increases rapidly with input length; Re-enabled modified repeat guards due to regression in speed caused by excessive backtracking.; </code></pre>; <p>Version: 2023.3.22</p>; <pre><code>Git issue 494: Backtracking failure matching regex `^a?(a?)b?c\1$` against string `abca`; Disabled repeat guards. They keep causing issues, and it's just simpler to rely on timeouts.; </code></pre>; <p>Version: 2022.10.31</p>; <pre><code>Updated text for supported Unicode and Python versions.; </code></pre>; <p>Version: 2022.9.13</p>; <pre><code>Updated to Unicode 15.0.0.; </code></pre>; <p>Version: 2022.9.11</p>; <pre><code>Updated version.; </code></pre>; <p>Version: 2022.8.17</p>; <pre><code>Git issue 477: \v for vertical spacing; <p>Added \p{HorizSpace} (\p{H}) and \p{VertSpace} (\p{V}).; </code></pre></p>; <p>Version: 2022.7.25</p>; <pre><code>Git issue 475: 2022.7.24 improperly released; <p>The file <a href=""https://pypi.org/pypi/regex/2022.7.24/json"">https://pypi.org/pypi/regex/2022.7.24/json</a> was missing references to most of the wheels, so this is a new release in the hope that it was just a glitch in GitHub Actions.; </code></pre></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/mrabarnett/mrab-regex/commit/9",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12989:1058,timeout,timeouts,1058,https://hail.is,https://github.com/hail-is/hail/pull/12989,1,['timeout'],['timeouts']
Safety,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8083:5262,timeout,timeout,5262,https://hail.is,https://github.com/hail-is/hail/issues/8083,1,['timeout'],['timeout']
Safety,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8053:5272,timeout,timeout,5272,https://hail.is,https://github.com/hail-is/hail/issues/8053,2,['timeout'],['timeout']
Safety,"1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:334); at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507); at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481); at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420); at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEven",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:2416,safe,safeSetFailure,2416,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['safe'],['safeSetFailure']
Safety,1); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)sun.reflect.generics.reflectiveObjects.NotImplementedException: null; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.ja,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:6554,Unsafe,UnsafeRow,6554,https://hail.is,https://github.com/hail-is/hail/issues/4215,1,['Unsafe'],['UnsafeRow']
Safety,"1. Add options to store the scores as sample annotations and the loadings as variant annotations (or, eventually, store by default and write out optionally); 2. Once LD-pruning is implemented, should it be performed first automatically? Probably not, but perhaps an option and the doc should mention the issue.; 3. PLINK has an option to use X-chromosome variants. What is it doing exactly? There are several decisions around encoding hemizygous sites for males. More importantly, does anyone use it? Should we support it?; 4. What about PCA of things other than genotypes, such as missingness? Analysts have mentioned applications to QC and flagged the latter specifically, which is implemented in GCTA.; 5. Extension to multiallelics? Probably not so important as few variants have more than two common alleles and each individual variant generally contributes little. If we did it, a good approach is probably a one-hot encoding although the variance normalization needs some care. For microsatellites/STRs a quantitative rather than categorical encoding may be better.; 6. Support for outlier detection a la SmartPCA and/or EIGENSTRAT?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/205:1097,detect,detection,1097,https://hail.is,https://github.com/hail-is/hail/issues/205,1,['detect'],['detection']
Safety,"1. If we receive an error other than 404 from Google when asking about an instance, we should raise. This is unexpected. (The later lines will fail anyway because spec is `None`); 2. (the main issue) if the instance is not active, do not bother contacting it and, crucially, continue `check_on_instance` eventually learning the instance does not exist.; 3. Drop timeout to 5s to talk to a batch agent. Fixes the zombie instance issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8023:362,timeout,timeout,362,https://hail.is,https://github.com/hail-is/hail/pull/8023,1,['timeout'],['timeout']
Safety,"1. Makefile is a bit more resilient to changes in the `dk-test` instance that is used to route traffic from GitHub to a local laptop test. It now looks up the ip. The zone is still hardcoded and it's moved to zone `us-central1-a`. The name is also hardcoded to `dk-test`.; 2. I renamed `is_running` to `is_building`; 3. When a job refresh happens, it is now `PRS` responsibility to determine what to do. It starts the same as it always does, updating existing PRs with new job information. The difference is that it tracks which (believed to be) currently building jobs are not seen in the job list. All such jobs are re-built, under the assumption that the job must have failed. cc: @cseed . This should allow CI to recover from the loss of batch. Fixes #4654.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4659:717,recover,recover,717,https://hail.is,https://github.com/hail-is/hail/pull/4659,1,['recover'],['recover']
Safety,"1. The default log path includes the version and a; timestamp. This will help people avoid overwriting; log files, which will help us.; 2. Echo the full path to the log after the hail logo; 3. Add a function `hl.copy_log` which can be used to; copy the session log to a hadoop-api-compliant; location.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4421:85,avoid,avoid,85,https://hail.is,https://github.com/hail-is/hail/pull/4421,1,['avoid'],['avoid']
Safety,"1. Treat any 500 from Docker as a retryable error.; 2. Move DockerError transiency to is_transient_errors and use retry_transient_errors instead of a hand rolled transient wrapper. The first change also makes us robust to changes in error messages on the GCR side. In particular, we started seeing this error message:. ```; Head https://gcr.io/v2/hail-vdc/ubuntu/manifests/18.04: Get https://gcr.io/v2/token?account=_json_key&scope=repository%3Ahail-vdc%2Fubuntu%3Apull&service=gcr.io: net/http: request canceled (Client.Timeout exceeded while awaiting headers); ```. which is slightly different from the extant messages we check for.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11943:521,Timeout,Timeout,521,https://hail.is,https://github.com/hail-is/hail/pull/11943,1,['Timeout'],['Timeout']
Safety,"1. Until we scale up the memory service's throughput, avoid use on the client; and the worker if there are more than 50 partitions. 2. On the driver, open no more than 100 concurrent connections to Google Cloud; Storage. 3. Set a timeout of five seconds to connect or read from Google Cloud Storage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11688:54,avoid,avoid,54,https://hail.is,https://github.com/hail-is/hail/pull/11688,2,"['avoid', 'timeout']","['avoid', 'timeout']"
Safety,1. Use encoded bytes to transfer result from Scala to Python; 2. Use encoded bytes for RelationalLet literals; 3. Optimize after lifting relational operations to eliminate trivial lets; 4. Avoid the memory service for large jobs (eventually we need to scale memory),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11670:189,Avoid,Avoid,189,https://hail.is,https://github.com/hail-is/hail/pull/11670,1,['Avoid'],['Avoid']
Safety,"1. `test_from_entry_expr_simple` does too much in one test. I split into multiple.; 2. `get_dataset` was fine when we had single-threaded tests, but now we are probably executing that, like, 40 times. I just ran the code and saved it as an MT. We have separate tests for split-multi and vcf import.; 3. `test_big_driver_has_big_memory` might have to spin up a machine to service this request.; 4. `test_billing_project_accrued_costs` can take a long time because its adding up billing info.; 5. split some more randomness tests (should be no mega tests left now); 6. relax pc relate timeouts even further (14 minutes!!)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13147:583,timeout,timeouts,583,https://hail.is,https://github.com/hail-is/hail/pull/13147,1,['timeout'],['timeouts']
Safety,"1. log should include job id not job; 2. `client_session` is only used for k8s-internal requests to worker pods, so; use a very aggressive timeout of 10s; 3. reduce refresh delay to two minutes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7026:139,timeout,timeout,139,https://hail.is,https://github.com/hail-is/hail/pull/7026,1,['timeout'],['timeout']
Safety,1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202:5346,abort,abortStage,5346,https://hail.is,https://github.com/hail-is/hail/issues/1202,1,['abort'],['abortStage']
Safety,"13""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JINJA2-6150717"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PROMPTTOOLKIT-6141120"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[554,704,624,531,556,604,589,726,434,589,449,399,696,589,479,519,509,711,701,586,586,384,494,539,589],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr);  [Cross-site Scripting (XSS)](https://learn.snyk.io/lesson/xss/?loc&#x3D;fix-pr);  [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr);  [More lessons are available in Snyk Learn](h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:13942,remediat,remediationStrategy,13942,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['remediat'],['remediationStrategy']
Safety,"17]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal. Exit status: 137. Diagnostics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287:6527,abort,abortStage,6527,https://hail.is,https://github.com/hail-is/hail/issues/13287,1,['abort'],['abortStage']
Safety,"1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/97"">#97</a>, <a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/284"">#284</a>, <a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/300"">#300</a>: Removed compatibility shims for deprecated entry; point interfaces.</li>; </ul>; <h1>v4.13.0</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/396"">#396</a>: Added compatibility for <code>PathDistributions</code> originating; from Python 3.8 and 3.9.</li>; </ul>; <h1>v4.12.0</h1>; <ul>; <li>py-93259: Now raise <code>ValueError</code> when <code>None</code> or an empty; string are passed to <code>Distribution.from_name</code> (and other; callers).</li>; </ul>; <h1>v4.11.4</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/379"">#379</a>: In <code>PathDistribution._name_from_stem</code>, avoid including; parts of the extension in the result.</li>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/381"">#381</a>: In <code>PathDistribution._normalized_name</code>, ensure names; loaded from the stem of the filename are also normalized, ensuring; duplicate entry points by packages varying only by non-normalized; name are hidden.</li>; </ul>; <h1>v4.11.3</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/372"">#372</a>: Removed cast of path items in FastPath, not needed.</li>; </ul>; <h1>v4.11.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/369"">#369</a>: Fixed bug where <code>EntryPoint.extras</code> was returning; match objects and not the extras strings.</li>; </ul>; <h1>v4.11.1</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/367"">#367</a>: In <code>Distribution.requires</code> for egg-info, if <code>requires.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12391:1292,avoid,avoid,1292,https://hail.is,https://github.com/hail-is/hail/pull/12391,1,['avoid'],['avoid']
Safety,"2, mt.col_idx % 2)); hl.export_vcf(mt, '/tmp/foo.vcf'); ```; It generates a 15GiB file. My initial tests, which used the balding nichols model, had write times of ~8MiB/s. With all my changes, I once saw 177 MIB/s but I think that may have been a fluke. I see pretty consistent ~110MiB/s in the profiler's estimate of bandwidth to the FileOutputStream. When measured by `time python3 test.py` this script writes at ~93MiB/s. Ideally we would hit 250MiB/s (1/8th of an n1-standard-8's network bandwidth), but, considering that we have to split that bandwidth with reading in most cases, ~91 MiB/s ain't so bad. On main, this pipeline writes at 32 MiB/s. The wins in decreasing order of importance were:; 1. Use buffered I/O. All of our exporters should now use buffered I/O because I changed it in the EmitMethodBuilder. I didn't change it in HadoopFS because (a) Hail's native I/O has buffering and (b) buffering and position tracking requires work.; 2. Avoid String allocation, String to UTF8 conversion, and Array[Byte] allocation in VCF writing. In particular, for the most common types of Calls, I just return the UTF8 byte array in a switch statement.; 3. Use a fast path for diploid genotypes. In the worst case, we did 5 branches and now we do 2 which should be well predicted.; 4. Remove an allocation of a lambda in a hot method in Genotype.scala. Future Work:; 1. The randomness stuff still has a lot of low hanging fruit. NumPy can generate random numbers at bandwidths far above what we're managing here.; 2. For VCFs with more entry fields, we should not write ints and floats by generating strings and getting their UTF8 encoding; 3. Invert the writing control flow: serialization methods should take an OutputStream and write bytes directly into it. Contrast that with passing around arrays which are memcopied into a buffer. ---. Range table test:; ```; import hail as hl; hl.init(master='local[1]'); hl._set_flags(write_ir_files='1'); mt = hl.utils.range_matrix_table(n_rows=1000_000,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12733:1791,Avoid,Avoid,1791,https://hail.is,https://github.com/hail-is/hail/pull/12733,1,['Avoid'],['Avoid']
Safety,"22); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 YarnScheduler: ERROR: Lost executor 1 on bw2-sw-dp3j.c.seqr-project.internal: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1574.1 in stage 8.0 (TID 21699, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1559.1 in stage 8.0 (TID 21702, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:3343,safe,safeExecute,3343,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['safe'],['safeExecute']
Safety,23) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.hail.rvd.RVD.combine(RVD.scala:688); 			at is.hail.expr.ir.Interpret$.run(Interpret.scala:804); 			at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 			at is.hail.expr.ir.In,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:11342,abort,abortStage,11342,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['abort'],['abortStage']
Safety,"27745>; ##contig=<ID=chrUn_KI270753v1,length=62944>; ##contig=<ID=chrUn_KI270754v1,length=40191>; ##contig=<ID=chrUn_KI270755v1,length=36723>; ##contig=<ID=chrUn_KI270756v1,length=79590>; ##contig=<ID=chrUn_KI270757v1,length=71251>; ##contig=<ID=chrUn_GL000214v1,length=137718>; ##contig=<ID=chrUn_KI270742v1,length=186739>; ##contig=<ID=chrUn_GL000216v2,length=176608>; ##contig=<ID=chrUn_GL000218v1,length=161147>; ##contig=<ID=chrEBV,length=171823>; ##contig=<ID=hs38d1,length=10560522>; ##bcftools_pluginVersion=1.9+htslib-1.9; ##bcftools_pluginCommand=plugin fill-AN-AC; Date=Sat Dec 29 14:52:44 2018; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=RGQ,Number=1,Type=Integer,Description=""Unconditional reference genotype confidence, encoded as a phred quality -10*log10 p(genotype call is wrong)"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">; ##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output 3P5CH.new.vcf --use-new-qual-calculator true --annotation-group StandardAnnotation --annotation-group StandardHCAnnotation --dbsnp /home/fgc3/dbsnp/150/GRCh38/All_20170710.vcf.gz --variant 3P5CH.new.g.vcf.gz --reference /home/fgc3/10x/refdata-GRCh38-2.1.0/fasta/genome.fa --create-output-variant-index false --verbosity ERROR --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:22877,detect,detect,22877,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['detect'],['detect']
Safety,"2>; <h2>Misc</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/259"">#259</a>, <a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a></li>; </ul>; <h2>v4.0.1</h2>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h2>async-timeout 4.0.0</h2>; <h1>Changes</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Drooped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/blob/master/CHANGES.rst"">async-timeout's changelog</a>.</em></p>; <blockquote>; <h1>4.0.2 (2021-12-20)</h1>; <h2>Misc</h2>; <ul>; <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:1289,timeout,timeout,1289,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,"3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr);  [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr);  [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr);  [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13932:10799,remediat,remediationStrategy,10799,https://hail.is,https://github.com/hail-is/hail/pull/13932,1,['remediat'],['remediationStrategy']
Safety,339); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1431); 	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2276); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2861); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2803); 	at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:5161,abort,abortStage,5161,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['abort'],['abortStage']
Safety,348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.r,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:24077,abort,abortStage,24077,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['abort'],['abortStage']
Safety,36 --gvcf-gq-bands 37 --gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47 --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --indel-size-to-eliminate-in-ref-model 10 --use-alleles-trigger false --disable-optimizations false --just-determine-active-regions false --dont-genotype false --max-mnp-distance 0 --dont-trim-active-regions false --max-disc-ar-extension 25 --max-gga-ar-extension 300 --padding-around-indels 150 --padding-around-snps 20 --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 --recover-dangling-heads false --do-not-recover-dangling-branches false --min-dangling-branch-length 4 --consensus false --max-num-haplotypes-in-population 128 --error-correct-kmers false --min-pruning 2 --debug-graph-transformations false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --base-quality-score-threshold 18 --pair-hmm-gap-continuation-penalty 10 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --native-pair-hmm-threads 4 --native-pair-hmm-use-double-precision false --debug false --use-filtered-reads-for-annotations false --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --capture-assembly-failure-bam false --error-correct-reads false --do-not-run-physical-phasing false --min-base-quality-score 10 --smith-waterman JAVA --use-new-qual-calculator false --annotate-with-num-discovered-alleles false --heterozygosity 0.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:2605,recover,recover-dangling-heads,2605,https://hail.is,https://github.com/hail-is/hail/issues/8469,2,['recover'],"['recover-dangling-branches', 'recover-dangling-heads']"
Safety,"36 ms; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.gbsc-project.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:22",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:2680,abort,abortStage,2680,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['abort'],['abortStage']
Safety,"38-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:33,456	hail_logging.py	log:40	https POST /pr-11438-default-g6cibyji6520/batch-driver/api/v1alpha/instances/activate done in 3.2369999999998527s: 200; ERROR	2022-03-02 19:06:33,492	job.py	schedule_job:473	error while scheduling job (95, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 521, in _request\n req, traces=traces, timeout=real_timeout\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 535, in connect\n proto = await self._create_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 892, in _create_connection\n _, proto = await self._create_direct_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aioht",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:3701,timeout,timeout,3701,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['timeout'],['timeout']
Safety,"3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.10.0 - 2024-03-27</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a hre",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14427:2298,avoid,avoid,2298,https://hail.is,https://github.com/hail-is/hail/pull/14427,1,['avoid'],['avoid']
Safety,"3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""50e1cce8-d68e-4133-a591-e2d1a6257337"",""prPublicId"":""50e1cce8-d68e-4133-a591-e2d1a6257337"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""41.0.4""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""SNYK-PYTHON-CRYPTOGRAPHY-5663682"",""SNYK-PYTHON-CRYPTOGRAPHY-5777683"",""SNYK-PYTHON-CRYPTOGRAPHY-5813745"",""SNYK-PYTHON-CRYPTOGRAPHY-5813746"",""SNYK-PYTHON-CRYPTOGRAPHY-5813750"",""SNYK-PYTHON-CRYPTOGRAPHY-5914629"",""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Use After Free](https://learn.snyk.io/lesson/use-after-free/?loc&#x3D;fix-pr);  [Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;)](https://learn.snyk.io/lesson/type-confusion/?loc&#x3D;fix-pr);  [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13887:9621,remediat,remediationStrategy,9621,https://hail.is,https://github.com/hail-is/hail/pull/13887,1,['remediat'],['remediationStrategy']
Safety,"3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""d900f7ca-fce7-41d4-a16d-bad109338beb"",""prPublicId"":""d900f7ca-fce7-41d4-a16d-bad109338beb"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""41.0.4""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""SNYK-PYTHON-CRYPTOGRAPHY-5663682"",""SNYK-PYTHON-CRYPTOGRAPHY-5777683"",""SNYK-PYTHON-CRYPTOGRAPHY-5813745"",""SNYK-PYTHON-CRYPTOGRAPHY-5813746"",""SNYK-PYTHON-CRYPTOGRAPHY-5813750"",""SNYK-PYTHON-CRYPTOGRAPHY-5914629"",""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Use After Free](https://learn.snyk.io/lesson/use-after-free/?loc&#x3D;fix-pr);  [Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;)](https://learn.snyk.io/lesson/type-confusion/?loc&#x3D;fix-pr);  [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13736:9621,remediat,remediationStrategy,9621,https://hail.is,https://github.com/hail-is/hail/pull/13736,1,['remediat'],['remediationStrategy']
Safety,"3e55f8d1a2279d78861af0009d96219fb""><code>2ede7d7</code></a> Fix annotations on <code>__exit__</code> and <code>__aexit__</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/8685c60dc0e82ee246fbe3d1aa272d1dfe57c24c""><code>8685c60</code></a> Bump mypy from 0.910 to 0.920 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/f0a0914345b224448220aeb00d71e6a04a5d24bd""><code>f0a0914</code></a> Bump twine from 3.7.0 to 3.7.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/e5c813173b8811b30f4a30eeba56fa8808ab15bb""><code>e5c8131</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/271"">#271</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c7e10db3d0965422122bef263fb36f6fd7572330""><code>c7e10db</code></a> Fix linter configs</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/6af8bf421a799208b27d57c6bf69de0d998fedec""><code>6af8bf4</code></a> Bump pre-commit from 2.15 to 2.16.0 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/269"">#269</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c71bbb5b5d7330f6dabfde7a1adec30a4611c0be""><code>c71bbb5</code></a> Bump docutils from 0.18 to 0.18.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/266"">#266</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/aio-libs/async-timeout/compare/v3.0.1...v4.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=async-timeout&package-mana",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:5472,timeout,timeout,5472,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,4); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:5227,abort,abortStage,5227,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['abort'],['abortStage']
Safety,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8473:3269,timeout,timeout,3269,https://hail.is,https://github.com/hail-is/hail/issues/8473,1,['timeout'],['timeout']
Safety,5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8545:9563,abort,abortStage,9563,https://hail.is,https://github.com/hail-is/hail/issues/8545,1,['abort'],['abortStage']
Safety,5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:82390,abort,abortStage,82390,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['abort'],['abortStage']
Safety,"56"">https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56</a>)</li>; </ul>; <p><strong>Improvements</strong></p>; <ul>; <li><code>verify=True</code> now reuses a global SSLContext which should improve; request time variance between first and subsequent requests. It should; also minimize certificate load time on Windows systems when using a Python; version built with OpenSSL 3.x. (<a href=""https://redirect.github.com/psf/requests/issues/6667"">#6667</a>)</li>; <li>Requests now supports optional use of character detection; (<code>chardet</code> or <code>charset_normalizer</code>) when repackaged or vendored.; This enables <code>pip</code> and other projects to minimize their vendoring; surface area. The <code>Response.text()</code> and <code>apparent_encoding</code> APIs; will default to <code>utf-8</code> if neither library is present. (<a href=""https://redirect.github.com/psf/requests/issues/6702"">#6702</a>)</li>; </ul>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Fixed bug in length detection where emoji length was incorrectly; calculated in the request content-length. (<a href=""https://redirect.github.com/psf/requests/issues/6589"">#6589</a>)</li>; <li>Fixed deserialization bug in JSONDecodeError. (<a href=""https://redirect.github.com/psf/requests/issues/6629"">#6629</a>)</li>; <li>Fixed bug where an extra leading <code>/</code> (path separator) could lead; urllib3 to unnecessarily reparse the request URI. (<a href=""https://redirect.github.com/psf/requests/issues/6644"">#6644</a>)</li>; </ul>; <p><strong>Deprecations</strong></p>; <ul>; <li>Requests has officially added support for CPython 3.12 (<a href=""https://redirect.github.com/psf/requests/issues/6503"">#6503</a>)</li>; <li>Requests has officially added support for PyPy 3.9 and 3.10 (<a href=""https://redirect.github.com/psf/requests/issues/6641"">#6641</a>)</li>; <li>Requests has officially dropped support for CPython 3.7 (<a href=""https://redirect.github.com/psf/requests/issues/6642"">#6642</a>)</l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14555:1697,detect,detection,1697,https://hail.is,https://github.com/hail-is/hail/pull/14555,2,['detect'],['detection']
Safety,"61cd8""><code>acada32</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/readthedocs/sphinx_rtd_theme/issues/1220"">#1220</a> from readthedocs/nienn/fix-sphinx-4-pre-overflow</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/e319184c89b0e11dba77441241fe9a735855fedb""><code>e319184</code></a> Merge branch 'master' into nienn/fix-sphinx-4-pre-overflow</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/20f205fc2a5c7dc813faa0b05ff1152e6ba5a4a6""><code>20f205f</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/readthedocs/sphinx_rtd_theme/issues/1217"">#1217</a> from readthedocs/agj/release-labels</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/2774670572c44555c6d6ecc280dfe34827ef62d4""><code>2774670</code></a> Add CSS max-width to dl.property</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/b557851511509c75cfffd10e1ede1e2266249c0a""><code>b557851</code></a> Make section labels verbose to avoid numeric labels</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/73d1707e791712efb837167065c4173ce9b380f8""><code>73d1707</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/readthedocs/sphinx_rtd_theme/issues/1088"">#1088</a> from readthedocs/Blendify/fix-717</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/3a031121ed86bc2b857f734eede0b48d8164545b""><code>3a03112</code></a> Fix build</li>; <li>Additional commits viewable in <a href=""https://github.com/readthedocs/sphinx_rtd_theme/compare/0.4.2...1.0.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinx-rtd-theme&package-manager=pip&previous-version=0.4.2&new-version=1.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11464:6323,avoid,avoid,6323,https://hail.is,https://github.com/hail-is/hail/pull/11464,2,['avoid'],['avoid']
Safety,"64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a8b81aa0,0x00007fe4a8b82100] = 1632; stub code [0x00007fe4a8b82100,0x00007fe4a8b822b8] = 440; oops [0x00007fe4a8b822b8,0x00007fe4a8b822c0] = 8; metadata [0x00007fe4a8b822c0,0x00007fe4a8b82338] = 120; scopes data [0x00007fe4a8b82338,0x00007fe4a8b82f30] = 3064; scopes pcs [0x00007fe4a8b82f30,0x00007fe4a8b83340] = 1040; dependencies [0x00007fe4a8b83340,0x00007fe4a8b83348] = 8; nul chk table [0x00007fe4a8b83348,0x00007fe4a8b83430] = 232; #; FATAL: caught signal 6 SIGABRT; # If you would like to submit a bug report, please visit:; # http://bugreport.sun.com/bugreport/; #; /tmp/libhail8122447512081932366.so(+0x18f5f)[0x7fe3a7bf0f5f]; /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7fe4be507f20]; /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7fe4be507e97]; /lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7fe4be509801]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8e80b9)[0x7fe4bd7f00b9]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0xaaed23)[0x7fe4bd9b6d23]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(JVM_handle_linux_signal+0x1b4)[0x7fe4bd7fa694]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4718:10553,Unsafe,UnsafeRow,10553,https://hail.is,https://github.com/hail-is/hail/issues/4718,1,['Unsafe'],['UnsafeRow']
Safety,"6eb448e-db52-4a21-b596-3e3ad42aaea1"",""prPublicId"":""46eb448e-db52-4a21-b596-3e3ad42aaea1"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""42.0.2""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""SNYK-PYTHON-CRYPTOGRAPHY-5663682"",""SNYK-PYTHON-CRYPTOGRAPHY-5777683"",""SNYK-PYTHON-CRYPTOGRAPHY-5813745"",""SNYK-PYTHON-CRYPTOGRAPHY-5813746"",""SNYK-PYTHON-CRYPTOGRAPHY-5813750"",""SNYK-PYTHON-CRYPTOGRAPHY-5914629"",""SNYK-PYTHON-CRYPTOGRAPHY-6036192"",""SNYK-PYTHON-CRYPTOGRAPHY-6050294"",""SNYK-PYTHON-CRYPTOGRAPHY-6092044"",""SNYK-PYTHON-CRYPTOGRAPHY-6126975"",""SNYK-PYTHON-CRYPTOGRAPHY-6210214"",""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Use After Free](https://learn.snyk.io/lesson/use-after-free/?loc&#x3D;fix-pr);  [Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;)](https://learn.snyk.io/lesson/type-confusion/?loc&#x3D;fix-pr);  [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14296:11356,remediat,remediationStrategy,11356,https://hail.is,https://github.com/hail-is/hail/pull/14296,1,['remediat'],['remediationStrategy']
Safety,"7819/icon/m.png ""medium severity"") | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5914629](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Missing Cryptographic Step <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6036192](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6036192) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6092044](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6092044) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | Use of a Broken or Risky Cryptographic Algorithm <br/>[SNYK-PYTHON-PYJWT-2840625](https://snyk.io/vuln/SNYK-PYTHON-PYJWT-2840625) | `pyjwt:` <br> `1.7.1 -> 2.4.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Timing Attack <br/>[SNYK-PYTHON-RSA-1038401](https://snyk.io/vuln/SNYK-PYTHON-RSA-1038401) | `rsa:` <br> `4.5 -> 4.7` <br> | No | No Known Exploit . Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14134:7040,Risk,Risky,7040,https://hail.is,https://github.com/hail-is/hail/pull/14134,1,['Risk'],['Risky']
Safety,"78efaeeca07a""><code>5128f7f</code></a> Fix CHANGES</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/18e718a56f9def7dc40bb9ce3a2962a8fb0c883c""><code>18e718a</code></a> Bump to 4.0.2</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/48d1c4ce923b1da75976d7e2a6ca9234b3092c16""><code>48d1c4c</code></a> Setup towncrier</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/2ede7d73e55f8d1a2279d78861af0009d96219fb""><code>2ede7d7</code></a> Fix annotations on <code>__exit__</code> and <code>__aexit__</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/8685c60dc0e82ee246fbe3d1aa272d1dfe57c24c""><code>8685c60</code></a> Bump mypy from 0.910 to 0.920 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/f0a0914345b224448220aeb00d71e6a04a5d24bd""><code>f0a0914</code></a> Bump twine from 3.7.0 to 3.7.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/e5c813173b8811b30f4a30eeba56fa8808ab15bb""><code>e5c8131</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/271"">#271</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c7e10db3d0965422122bef263fb36f6fd7572330""><code>c7e10db</code></a> Fix linter configs</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/6af8bf421a799208b27d57c6bf69de0d998fedec""><code>6af8bf4</code></a> Bump pre-commit from 2.15 to 2.16.0 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/269"">#269</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c71bbb5b5d7330f6dabfde7a1adec30a4611c0be""><code>c71bbb5</code></a> Bump docutils",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:5028,timeout,timeout,5028,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,"869: in _run_once; event_list = self._selector.select(timeout); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <selectors.EpollSelector object at 0x7fae890f2d30>; timeout = 15.402000000000001. def select(self, timeout=None):; if timeout is None:; timeout = -1; elif timeout <= 0:; timeout = 0; else:; # epoll_wait() has a resolution of 1 millisecond, round away; # from zero to wait *at least* timeout seconds.; timeout = math.ceil(timeout * 1e3) * 1e-3; ; # epoll_wait() expects `maxevents` to be greater than zero;; # we want to make sure that `select()` can be called when no; # FD is registered.; max_ev = max(len(self._fd_to_key), 1); ; ready = []; try:; > fd_event_list = self._selector.poll(timeout, max_ev); E Failed: Timeout >360.0s. usr/lib/python3.9/selectors.py:469: Failed; ------------------------------ Captured log setup ------------------------------; 2023-09-06T21:45:24 INFO test.conftest conftest.py:14:log_before_after starting test; 2023-09-06T21:45:24 INFO hailtop.aiocloud.aioazure.credentials credentials.py:99:default_credentials using credentials file /test-gsa-key/key.json; ------------------------------ Captured log call -------------------------------; 2023-09-06T21:45:25 INFO azure.identity.aio._internal.get_token_mixin get_token_mixin.py:93:get_token ClientSecretCredential.get_token succeeded; 2023-09-06T21:45:25 INFO batch_client.aioclient aioclient.py:809:_submit created batch 191; 2023-09-06T21:47:17 WARNING hailtop.utils utils.py:842:retry_transient_errors_with_debug_string A transient error occured. We will automatically retry. Do not be alarmed. We have thus far seen 2 transient errors (next delay: 3.794s). The most recent error was <class 'asyncio.exceptions.TimeoutError'> . ------------------------------ live log teardown -------------------------------; 2023-09-06T21:51:25 INFO test.conftest conftest.py:16:log_before_after ending test. ```. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13582:4591,Timeout,TimeoutError,4591,https://hail.is,https://github.com/hail-is/hail/issues/13582,1,['Timeout'],['TimeoutError']
Safety,"88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.Or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:2319,Unsafe,UnsafeRow,2319,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['Unsafe'],['UnsafeRow']
Safety,"89 ); 790 retries.sleep(). File /opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py:550, in Retry.increment(self, method, url, response, error, _pool, _stacktrace); 549 if read is False or not self._is_method_retryable(method):; --> 550 raise six.reraise(type(error), error, _stacktrace); 551 elif read is not None:. File /opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py:769, in reraise(tp, value, tb); 768 if value.__traceback__ is not tb:; --> 769 raise value.with_traceback(tb); 770 raise value. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:703, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw); 702 # Make the request on the httplib connection object.; --> 703 httplib_response = self._make_request(; 704 conn,; 705 method,; 706 url,; 707 timeout=timeout_obj,; 708 body=body,; 709 headers=headers,; 710 chunked=chunked,; 711 ); 713 # If we're going to release the connection in ``finally:``, then; 714 # the response doesn't need to know about the connection. Otherwise; 715 # it will also try to release it and we'll have a double-release; 716 # mess. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:449, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw); 445 except BaseException as e:; 446 # Remove the TypeError from the exception chain in; 447 # Python 3 (including for exceptions like SystemExit).; 448 # Otherwise it looks like a bug in the code.; --> 449 six.raise_from(e, None); 450 except (SocketTimeout, BaseSSLError, SocketError) as e:. File <string>:3, in raise_from(value, from_value). File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:444, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw); 443 try:; --> 444 httplib_response = conn.getresponse(); 445 except Ba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:8057,timeout,timeout,8057,https://hail.is,https://github.com/hail-is/hail/issues/13960,1,['timeout'],['timeout']
Safety,"9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:2412,Unsafe,UnsafeRow,2412,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['Unsafe'],['UnsafeRow']
Safety,": 'E'}` for machine types in Azure. This corresponds to 2Gi/core, 4Gi/core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances that are no longer present and then do a deep delete as then we'll have some redundancy and faster response times.; - Figure out how to do a deep-delete as much as possible for VMs when using the create VM REST API. This is essential for cleaning up resources for idled out workers when the driver is down for a long period of time.; - User billing based on resources used based on the `AzureInstanceConfig`; - Spot billing strategy; - Check network IP settings in the worker; - Add garbage collection CLI commands to build.yaml to clean up VMs, disks, nics, public ip addresses, and network security groups based on a tag; - Fix batch tests to be cloud agnostic. ## Infrastructure. - Create a shared SSH public key on the VMs for development purposes; - Consider having every PR / namespace deploy resources in a separate resource group rather than having one resource group for all Batch resources! We'd need something to name the resource groups something like `hail-dev_jigold` and `jigold_jigold` for example to handle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10970:2078,redund,redundancy,2078,https://hail.is,https://github.com/hail-is/hail/pull/10970,1,['redund'],['redundancy']
Safety,": 10}. # Group by the array directly (gives an expected error); > mt.aggregate_rows(hl.agg.counter(mt.alleles)); TypeError: unhashable type: 'list'. # Aggregate sorted arrays (works but gives wrong result); > mt.aggregate_rows(hl.agg.counter(hl.delimit(hl.sorted(mt.alleles), '|'))); {'A|A|A|C|\x0b\x00\x00': 2, 'A|A|A|C|C|C': 8}. # Aggregate the sorted arrays directly (segfault); # *This should probably throw ""unhashable type list"" like it does without the sort*; mt.aggregate_rows(hl.agg.counter(hl.sorted(mt.alleles))); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/opt/conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty; ...; Py4JError: An error occurred while calling o59.executeJSON; ```. Here is the full [stack trace](https://github.com/hail-is/hail/files/4187400/stacktrace.txt) and [core dump](https://github.com/hail-is/hail/files/4187399/coredump.txt). I think some related questions that arise from this are:. 1. What's the best way to group by an array to avoid the conversion to a delimited string? In this case I could do something like ```mt.aggregate_rows(hl.agg.counter(hl.tuple([mt.alleles[0], mt.alleles[1]])))``` but I can't find a solution for getting a tuple from an array without knowing the length of it beforehand for every row. Is there a more fundamental reason why the API doesn't allow aggregation by arrays even if Spark does?; 2. When the Py4J server crashes, it's no longer reachable from the python clients so I have to restart my process and re-initialize Hail. Is there already functionality implemented for bringing that server up if it's down? I'd imagine segfaults aren't the only reason it could down, so it would be nice if there was a way to bring it back up either automatically or manually. Hail version: 0.2.30-2ae07d872f43; Spark version: 2.4.4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8076:1535,avoid,avoid,1535,https://hail.is,https://github.com/hail-is/hail/issues/8076,1,['avoid'],['avoid']
Safety,": BatchClient):; b = create_batch(client); resources = {'machine_type': smallest_machine_type()}; j = b.create_job(DOCKER_ROOT_IMAGE, ['true'], resources=resources, always_run=True); b.submit(); b.cancel(); > status = j.wait(). io/test/test_batch.py:1487: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; usr/local/lib/python3.9/dist-packages/hailtop/batch_client/client.py:84: in wait; return async_to_blocking(self._async_job.wait()); usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:156: in async_to_blocking; return loop.run_until_complete(task); usr/lib/python3.9/asyncio/base_events.py:634: in run_until_complete; self.run_forever(); usr/lib/python3.9/asyncio/base_events.py:601: in run_forever; self._run_once(); usr/lib/python3.9/asyncio/base_events.py:1869: in _run_once; event_list = self._selector.select(timeout); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <selectors.EpollSelector object at 0x7fae890f2d30>; timeout = 15.402000000000001. def select(self, timeout=None):; if timeout is None:; timeout = -1; elif timeout <= 0:; timeout = 0; else:; # epoll_wait() has a resolution of 1 millisecond, round away; # from zero to wait *at least* timeout seconds.; timeout = math.ceil(timeout * 1e3) * 1e-3; ; # epoll_wait() expects `maxevents` to be greater than zero;; # we want to make sure that `select()` can be called when no; # FD is registered.; max_ev = max(len(self._fd_to_key), 1); ; ready = []; try:; > fd_event_list = self._selector.poll(timeout, max_ev); E Failed: Timeout >360.0s. usr/lib/python3.9/selectors.py:469: Failed; ------------------------------ Captured log setup ------------------------------; 2023-09-06T21:45:24 INFO test.conftest conftest.py:14:log_before_after starting test; 2023-09-06T21:45:24 INFO hailtop.aiocloud.aioazure.credentials credentials.py:99:default_credentials using credentials file /test-gsa-key/key.json; ------------------------------ Captured log ca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13582:3044,timeout,timeout,3044,https://hail.is,https://github.com/hail-is/hail/issues/13582,1,['timeout'],['timeout']
Safety,": INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 36) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:1995,abort,aborted,1995,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['abort'],['aborted']
Safety,":issue:<code>2235</code></li>; </ul>; <h2>Version 8.1.1</h2>; <p>Released 2022-03-30</p>; <ul>; <li>Fix an issue with decorator typing that caused type checking to; report that a command was not callable. :issue:<code>2227</code></li>; </ul>; <h2>Version 8.1.0</h2>; <p>Released 2022-03-28</p>; <ul>; <li>; <p>Drop support for Python 3.6. :pr:<code>2129</code></p>; </li>; <li>; <p>Remove previously deprecated code. :pr:<code>2130</code></p>; <ul>; <li><code>Group.resultcallback</code> is renamed to <code>result_callback</code>.</li>; <li><code>autocompletion</code> parameter to <code>Command</code> is renamed to; <code>shell_complete</code>.</li>; <li><code>get_terminal_size</code> is removed, use; <code>shutil.get_terminal_size</code> instead.</li>; <li><code>get_os_args</code> is removed, use <code>sys.argv[1:]</code> instead.</li>; </ul>; </li>; <li>; <p>Rely on :pep:<code>538</code> and :pep:<code>540</code> to handle selecting UTF-8 encoding; instead of ASCII. Click's locale encoding detection is removed.; :issue:<code>2198</code></p>; </li>; <li>; <p>Single options boolean flags with <code>show_default=True</code> only show; the default if it is <code>True</code>. :issue:<code>1971</code></p>; </li>; <li>; <p>The <code>command</code> and <code>group</code> decorators can be applied with or; without parentheses. :issue:<code>1359</code></p>; </li>; <li>; <p>The <code>Path</code> type can check whether the target is executable.; :issue:<code>1961</code></p>; </li>; <li>; <p><code>Command.show_default</code> overrides <code>Context.show_default</code>, instead; of the other way around. :issue:<code>1963</code></p>; </li>; <li>; <p>Parameter decorators and <code>@group</code> handles <code>cls=None</code> the same as; not passing <code>cls</code>. <code>@option</code> handles <code>help=None</code> the same as; not passing <code>help</code>. :issue:<code>[#1959](https://github.com/pallets/click/issues/1959)</code></p>; </li>; </ul>; <!-- raw HTML omitted -->; </bloc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11801:3474,detect,detection,3474,https://hail.is,https://github.com/hail-is/hail/pull/11801,1,['detect'],['detection']
Safety,":issue:<code>2235</code></li>; </ul>; <h2>Version 8.1.1</h2>; <p>Released 2022-03-30</p>; <ul>; <li>Fix an issue with decorator typing that caused type checking to; report that a command was not callable. :issue:<code>2227</code></li>; </ul>; <h2>Version 8.1.0</h2>; <p>Released 2022-03-28</p>; <ul>; <li>; <p>Drop support for Python 3.6. :pr:<code>2129</code></p>; </li>; <li>; <p>Remove previously deprecated code. :pr:<code>2130</code></p>; <ul>; <li><code>Group.resultcallback</code> is renamed to <code>result_callback</code>.</li>; <li><code>autocompletion</code> parameter to <code>Command</code> is renamed to; <code>shell_complete</code>.</li>; <li><code>get_terminal_size</code> is removed, use; <code>shutil.get_terminal_size</code> instead.</li>; <li><code>get_os_args</code> is removed, use <code>sys.argv[1:]</code> instead.</li>; </ul>; </li>; <li>; <p>Rely on :pep:<code>538</code> and :pep:<code>540</code> to handle selecting UTF-8 encoding; instead of ASCII. Click's locale encoding detection is removed.</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/click/commit/9c6f4c8e1bb8670ce827c98559f57f6ee5935cd0""><code>9c6f4c8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/click/issues/2262"">#2262</a> from pallets/release-8.1.3</li>; <li><a href=""https://github.com/pallets/click/commit/5ec77494bdf2c294d3b082bed429ebce78321431""><code>5ec7749</code></a> release version 8.1.3</li>; <li><a href=""https://github.com/pallets/click/commit/2ac3211cb79a63bae8e6f0441136b432ec2126bc""><code>2ac3211</code></a> update requirements</li>; <li><a href=""https://github.com/pallets/click/commit/5fd87bdf80ed450334b37344f6c99890c217d3db""><code>5fd87bd</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/click/issues/2248"">#2248</a> from jreese/8.1.x</li>; <li><a href=""https://github.com/pal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11808:4304,detect,detection,4304,https://hail.is,https://github.com/hail-is/hail/pull/11808,1,['detect'],['detection']
Safety,"; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently schedu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10705:2654,timeout,timeout,2654,https://hail.is,https://github.com/hail-is/hail/pull/10705,1,['timeout'],['timeout']
Safety,"; 707 timeout=timeout_obj,; 708 body=body,; 709 headers=headers,; 710 chunked=chunked,; 711 ); 713 # If we're going to release the connection in ``finally:``, then; 714 # the response doesn't need to know about the connection. Otherwise; 715 # it will also try to release it and we'll have a double-release; 716 # mess. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:449, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw); 445 except BaseException as e:; 446 # Remove the TypeError from the exception chain in; 447 # Python 3 (including for exceptions like SystemExit).; 448 # Otherwise it looks like a bug in the code.; --> 449 six.raise_from(e, None); 450 except (SocketTimeout, BaseSSLError, SocketError) as e:. File <string>:3, in raise_from(value, from_value). File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:444, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw); 443 try:; --> 444 httplib_response = conn.getresponse(); 445 except BaseException as e:; 446 # Remove the TypeError from the exception chain in; 447 # Python 3 (including for exceptions like SystemExit).; 448 # Otherwise it looks like a bug in the code. File /opt/conda/lib/python3.10/http/client.py:1375, in HTTPConnection.getresponse(self); 1374 try:; -> 1375 response.begin(); 1376 except ConnectionError:. File /opt/conda/lib/python3.10/http/client.py:318, in HTTPResponse.begin(self); 317 while True:; --> 318 version, status, reason = self._read_status(); 319 if status != CONTINUE:. File /opt/conda/lib/python3.10/http/client.py:287, in HTTPResponse._read_status(self); 284 if not line:; 285 # Presumably, the server closed the connection before; 286 # sending a valid response.; --> 287 raise RemoteDisconnected(""Remote end closed connection without""; 288 "" response""); 289 try:. ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection witho",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:9029,timeout,timeout,9029,https://hail.is,https://github.com/hail-is/hail/issues/13960,1,['timeout'],['timeout']
Safety,"; 707 timeout=timeout_obj,; 708 body=body,; 709 headers=headers,; 710 chunked=chunked,; 711 ); 713 # If we're going to release the connection in ``finally:``, then; 714 # the response doesn't need to know about the connection. Otherwise; 715 # it will also try to release it and we'll have a double-release; 716 # mess. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:449, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw); 445 except BaseException as e:; 446 # Remove the TypeError from the exception chain in; 447 # Python 3 (including for exceptions like SystemExit).; 448 # Otherwise it looks like a bug in the code.; --> 449 six.raise_from(e, None); 450 except (SocketTimeout, BaseSSLError, SocketError) as e:. File <string>:3, in raise_from(value, from_value). File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:444, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw); 443 try:; --> 444 httplib_response = conn.getresponse(); 445 except BaseException as e:; 446 # Remove the TypeError from the exception chain in; 447 # Python 3 (including for exceptions like SystemExit).; 448 # Otherwise it looks like a bug in the code. File /opt/conda/lib/python3.10/http/client.py:1375, in HTTPConnection.getresponse(self); 1374 try:; -> 1375 response.begin(); 1376 except ConnectionError:. File /opt/conda/lib/python3.10/http/client.py:318, in HTTPResponse.begin(self); 317 while True:; --> 318 version, status, reason = self._read_status(); 319 if status != CONTINUE:. File /opt/conda/lib/python3.10/http/client.py:287, in HTTPResponse._read_status(self); 284 if not line:; 285 # Presumably, the server closed the connection before; 286 # sending a valid response.; --> 287 raise RemoteDisconnected(""Remote end closed connection without""; 288 "" response""); 289 try:. RemoteDisconnected: Remote end closed connection without response. During handling of the abo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:5105,timeout,timeout,5105,https://hail.is,https://github.com/hail-is/hail/issues/13960,1,['timeout'],['timeout']
Safety,"; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/blob/master/CHANGES.rst"">async-timeout's changelog</a>.</em></p>; <blockquote>; <h1>4.0.2 (2021-12-20)</h1>; <h2>Misc</h2>; <ul>; <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://github.com/aio-libs/async-timeout/issues/259&gt;</code><em>, <code>[#274](https://github.com/aio-libs/async-timeout/issues/274) &lt;https://github.com/aio-libs/async-timeout/issues/274&gt;</code></em></li>; </ul>; <h1>4.0.1 (2121-11-10)</h1>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h1>4.0.0 (2021-11-01)</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Dropped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:2876,timeout,timeout,2876,https://hail.is,https://github.com/hail-is/hail/pull/11465,2,"['Timeout', 'timeout']","['Timeout', 'timeout']"
Safety,"; </li>; <li>; <p>Drooped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/blob/master/CHANGES.rst"">async-timeout's changelog</a>.</em></p>; <blockquote>; <h1>4.0.2 (2021-12-20)</h1>; <h2>Misc</h2>; <ul>; <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://github.com/aio-libs/async-timeout/issues/259&gt;</code><em>, <code>[#274](https://github.com/aio-libs/async-timeout/issues/274) &lt;https://github.com/aio-libs/async-timeout/issues/274&gt;</code></em></li>; </ul>; <h1>4.0.1 (2121-11-10)</h1>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h1>4.0.0 (2021-11-01)</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Dro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:2305,timeout,timeout,2305,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,"; <li>1598_, [Windows]: <code>disk_partitions()</code>_ only returns mountpoints on drives; where it first finds one.</li>; <li>1874_, [SunOS]: swap output error due to incorrect range.</li>; <li>1892_, [macOS]: <code>cpu_freq()</code>_ broken on Apple M1.</li>; <li>1901_, [macOS]: different functions, especially <code>Process.open_files()</code>_ and; <code>Process.connections()</code><em>, could randomly raise <code>AccessDenied</code></em> because the; internal buffer of <code>proc_pidinfo(PROC_PIDLISTFDS)</code> syscall was not big enough.; We now dynamically increase the buffer size until it's big enough instead of; giving up and raising <code>AccessDenied</code>_, which was a fallback to avoid crashing.</li>; <li>1904_, [Windows]: <code>OpenProcess</code> fails with <code>ERROR_SUCCESS</code> due to; <code>GetLastError()</code> called after <code>sprintf()</code>. (patch by alxchk)</li>; <li>1913_, [Linux]: <code>wait_procs()</code>_ should catch <code>subprocess.TimeoutExpired</code>; exception.</li>; <li>1919_, [Linux]: <code>sensors_battery()</code>_ can raise <code>TypeError</code> on PureOS.</li>; <li>1921_, [Windows]: <code>swap_memory()</code>_ shows committed memory instead of swap.</li>; <li>1940_, [Linux]: psutil does not handle <code>ENAMETOOLONG</code> when accessing process; file descriptors in procfs. (patch by Nikita Radchenko)</li>; <li>1948_, <strong>[critical]</strong>: <code>memoize_when_activated</code> decorator is not thread-safe.; (patch by Xuehai Pan)</li>; <li>1953_, [Windows], <strong>[critical]</strong>: <code>disk_partitions()</code>_ crashes due to; insufficient buffer len. (patch by MaWe2019)</li>; <li>1965_, [Windows], <strong>[critical]</strong>: fix &quot;Fatal Python error: deallocating None&quot;; when calling <code>users()</code>_ multiple times.</li>; <li>1980_, [Windows]: 32bit / WoW64 processes fails to read <code>Process.name()</code>_ longer</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11459:2698,Timeout,TimeoutExpired,2698,https://hail.is,https://github.com/hail-is/hail/pull/11459,1,['Timeout'],['TimeoutExpired']
Safety,"; at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:7); at is.hail.backend.Backend.execute(Backend.scala:86); at is.hail.backend.Backend.executeJSON(Backend.scala:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the maximum alloreamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_15657888296Exception.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOExcee.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBlockResolvt org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.NettyBloction.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamMt org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.TransportractChannelHandlerContext.java:362) at io.netty.channel.Abst",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:16186,abort,aborted,16186,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['abort'],['aborted']
Safety,; at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413:4287,abort,abortStage,4287,https://hail.is,https://github.com/hail-is/hail/issues/3413,1,['abort'],['abortStage']
Safety,"; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:2458,Unsafe,UnsafeRow,2458,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['Unsafe'],['UnsafeRow']
Safety,"</a>, <a href=""https://github.com/mattcary""><code>@mattcary</code></a>)</li>; <li>Client-go impersonation config can specify a UID to pass impersonated uid information through in requests. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104483"">kubernetes/kubernetes#104483</a>, <a href=""https://github.com/margocrawf""><code>@margocrawf</code></a>)</li>; <li>Create HPA v2 from v2beta2 with some fields changed. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/102534"">kubernetes/kubernetes#102534</a>, <a href=""https://github.com/wangyysde""><code>@wangyysde</code></a>) [SIG API Machinery, Apps, Auth, Autoscaling and Testing]</li>; <li>Ephemeral containers graduated to beta and are now available by default. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105405"">kubernetes/kubernetes#105405</a>, <a href=""https://github.com/verb""><code>@verb</code></a>)</li>; <li>Fix kube-proxy regression on UDP services because the logic to detect stale connections was not considering if the endpoint was ready. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106163"">kubernetes/kubernetes#106163</a>, <a href=""https://github.com/aojea""><code>@aojea</code></a>) [SIG API Machinery, Apps, Architecture, Auth, Autoscaling, CLI, Cloud Provider, Contributor Experience, Instrumentation, Network, Node, Release, Scalability, Scheduling, Storage, Testing and Windows]</li>; <li>If a conflict occurs when creating an object with <code>generateName</code>, the server now returns an &quot;AlreadyExists&quot; error with a retry option. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104699"">kubernetes/kubernetes#104699</a>, <a href=""https://github.com/vincepri""><code>@vincepri</code></a>)</li>; <li>Implement support for recovering from volume expansion failures (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106154"">kubernetes/kubernetes#",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:3747,detect,detect,3747,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['detect'],['detect']
Safety,"</code></a>) [SIG Apps, Auth, Node, Release, Scheduling and Testing]</li>; <li>ServiceAppProtocol feature gate is now beta and enabled by default, adding new AppProtocol field to Services and Endpoints. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/90023"">kubernetes/kubernetes#90023</a>, <a href=""https://github.com/robscott""><code>@robscott</code></a>) [SIG Apps and Network]</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/bcfd4ed2ec3b2f503adc4f2e681f9404216d302c""><code>bcfd4ed</code></a> chore: update version</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/37f5d63425976b463bb83348d592859a82f2b5af""><code>37f5d63</code></a> chore: update changelog</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/ef2fe15d3473a38f1c13558acf05631d909560ce""><code>ef2fe15</code></a> fix: watch returns raw_object if detection of returned objects fail (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/177"">#177</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/59a6e2a60fb7592e596447555c2da2797b7273a9""><code>59a6e2a</code></a> chore(deps): update sphinx requirement (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/175"">#175</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/f4e11b7223e546515e99c984f9948b6caa06622a""><code>f4e11b7</code></a> chore: update version</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/bf4a9a8eb24149cd68efbc6ae61a6445121f4b70""><code>bf4a9a8</code></a> chore: update setup.py</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/f028cc793e3a2c519be6a52a49fb77ff0b014c9b""><code>f028cc7</code></a> [feat] regenerate client for v1.19.15 (<a href=""https://github-redirect.dependabot.com/tomplus/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:14795,detect,detection,14795,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['detect'],['detection']
Safety,"</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/blob/master/CHANGES.rst"">async-timeout's changelog</a>.</em></p>; <blockquote>; <h1>4.0.2 (2021-12-20)</h1>; <h2>Misc</h2>; <ul>; <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://github.com/aio-libs/async-timeout/issues/259&gt;</code><em>, <code>[#274](https://github.com/aio-libs/async-timeout/issues/274) &lt;https://github.com/aio-libs/async-timeout/issues/274&gt;</code></em></li>; </ul>; <h1>4.0.1 (2121-11-10)</h1>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h1>4.0.0 (2021-11-01)</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Dropped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:2585,Timeout,TimeoutError,2585,https://hail.is,https://github.com/hail-is/hail/pull/11465,3,"['Timeout', 'timeout']","['TimeoutError', 'timeout']"
Safety,"</p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/blob/master/CHANGES.rst"">async-timeout's changelog</a>.</em></p>; <blockquote>; <h1>4.0.2 (2021-12-20)</h1>; <h2>Misc</h2>; <ul>; <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://github.com/aio-libs/async-timeout/issues/259&gt;</code><em>, <code>[#274](https://github.com/aio-libs/async-timeout/issues/274) &lt;https://github.com/aio-libs/async-timeout/issues/274&gt;</code></em></li>; </ul>; <h1>4.0.1 (2121-11-10)</h1>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h1>4.0.0 (2021-11-01)</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Dropped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:2445,timeout,timeout,2445,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - ci/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Use of a Broken or Risky Cryptographic Algorithm <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6149518](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6149518) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Uncontrolled Resource Consumption (&#x27;Resource Exhaustion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6157248](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6157248) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **451/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.3 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14236:872,Risk,Risky,872,https://hail.is,https://github.com/hail-is/hail/pull/14236,1,['Risk'],['Risky']
Safety,"= 3.15.0, &lt;4.0.0dev (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-core/issues/385"">#385</a>) (<a href=""https://github.com/googleapis/python-api-core/commit/d84d66c2a4107f5f9a20c53e870a27fb1250ea3d"">d84d66c</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/python-api-core/compare/v2.7.3...v2.8.0"">2.8.0</a> (2022-05-18)</h2>; <h3>Features</h3>; <ul>; <li>adds support for audience in client_options (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-core/issues/379"">#379</a>) (<a href=""https://github.com/googleapis/python-api-core/commit/c97c4980125a86f384cdf12720df7bb1a2adf9d2"">c97c498</a>)</li>; <li>adds support for audience in client_options. (<a href=""https://github.com/googleapis/python-api-core/commit/c97c4980125a86f384cdf12720df7bb1a2adf9d2"">c97c498</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/python-api-core/compare/v2.7.2...v2.7.3"">2.7.3</a> (2022-04-29)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Avoid AttributeError if grpcio-status is not installed (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-core/issues/370"">#370</a>) (<a href=""https://github.com/googleapis/python-api-core/commit/022add16266f9c07f0f88eea13472cc2e0bfc991"">022add1</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/python-api-core/compare/v2.7.1...v2.7.2"">2.7.2</a> (2022-04-13)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>allow grpc without grpcio-status (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-core/issues/355"">#355</a>) (<a href=""https://github.com/googleapis/python-api-core/commit/112049e79f5a5b0a989d85d438a1bd29485f46f7"">112049e</a>)</li>; <li>remove dependency on pkg_resources (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-core/issues/361"">#361</a>) (<a href=""https://github.com/googleapis/python-api-core/commit/523dbd0b10d37ffcf83fa751f0bad313f162abf1"">523dbd0</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/python-api-core/compare/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11970:6834,Avoid,Avoid,6834,https://hail.is,https://github.com/hail-is/hail/pull/11970,1,['Avoid'],['Avoid']
Safety,"=""https://github-redirect.dependabot.com/psf/black/issues/2951"">GH-2951</a>)</li>; <li><a href=""https://github.com/psf/black/commit/bd1e98034907463f5d86f4d87e89202dc6c34dd4""><code>bd1e980</code></a> Remove unnecessary parentheses from <code>except</code> clauses (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2939"">#2939</a>)</li>; <li><a href=""https://github.com/psf/black/commit/14d84ba2e96c5ca1351b8fe4d0d415cc148f4117""><code>14d84ba</code></a> Resolve new flake8-bugbear errors (B020) (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2950"">GH-2950</a>)</li>; <li><a href=""https://github.com/psf/black/commit/14e5ce5412efa53438df0180e735b3834df3b579""><code>14e5ce5</code></a> Remove unnecessary parentheses from tuple unpacking in <code>for</code> loops (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2945"">#2945</a>)</li>; <li><a href=""https://github.com/psf/black/commit/3800ebd81df6a1c31d1eac8cc15899537b9cbb61""><code>3800ebd</code></a> Avoid magic-trailing-comma in single-element subscripts (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2942"">#2942</a>)</li>; <li><a href=""https://github.com/psf/black/commit/062b54931dc3ea35f673e755893fe28ff1f5a889""><code>062b549</code></a> Github now supports .git-blame-ignore-revs (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2948"">GH-2948</a>)</li>; <li><a href=""https://github.com/psf/black/commit/5379d4f3f460ec9b7063dd1cc10f437b0edf9ae3""><code>5379d4f</code></a> stub style: remove some possible future changes (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2940"">#2940</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/psf/black/compare/22.1.0...22.3.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=black&package-manager=pip&previous-version=22.1.0&new-version=22.3.0)](https://docs.git",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11696:8294,Avoid,Avoid,8294,https://hail.is,https://github.com/hail-is/hail/pull/11696,1,['Avoid'],['Avoid']
Safety,"=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527:2061,abort,abortStage,2061,https://hail.is,https://github.com/hail-is/hail/issues/2527,1,['abort'],['abortStage']
Safety,"==2024.6.0', 'yarl==1.9.4']; Collecting https://github.com/hail-is/jgscm/archive/v0.1.13+hail.zip; WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979a53d0>, 'Connection to github.com timed out. (connect timeout=15)')': /hail-is/jgscm/archive/v0.1.13+hail.zip; WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef9797e050>, 'Connection to github.com timed out. (connect timeout=15)')': /hail-is/jgscm/archive/v0.1.13+hail.zip; WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979ba910>, 'Connection to github.com timed out. (connect timeout=15)')': /hail-is/jgscm/archive/v0.1.13+hail.zip; WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979cc310>, 'Connection to github.com timed out. (connect timeout=15)')': /hail-is/jgscm/archive/v0.1.13+hail.zip; WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979cce10>, 'Connection to github.com timed out. (connect timeout=15)')': /hail-is/jgscm/archive/v0.1.13+hail.zip; ERROR: Could not install packages due to an OSError: HTTPSConnectionPool(host='github.com', port=443): Max retries exceeded with url: /hail-is/jgscm/archive/v0.1.13+hail.zip (Caused by ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979ce290>, 'Connection to github.com timed out. (connect timeo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14652:3555,timeout,timeout,3555,https://hail.is,https://github.com/hail-is/hail/issues/14652,1,['timeout'],['timeout']
Safety,">; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h2>async-timeout 4.0.0</h2>; <h1>Changes</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Drooped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/blob/master/CHANGES.rst"">async-timeout's changelog</a>.</em></p>; <blockquote>; <h1>4.0.2 (2021-12-20)</h1>; <h2>Misc</h2>; <ul>; <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://github.com/aio-libs/async-timeout/issues/259&gt;</code><em>, <code>[#274](https://github.com/aio-libs/async-timeout/issues/274) &lt;https://github.com/aio-libs/async-timeout/issues/274&gt;</code></em></li>; </ul>; <h1>4.0.1 (2121-11-10)</h1>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:1506,timeout,timeout,1506,https://hail.is,https://github.com/hail-is/hail/pull/11465,4,['timeout'],['timeout']
Safety,"@cseed @catoverdrive This is how we should do aggregators. I implemented several as examples. It's a bit repetitive, maybe `@specialized` can help here? I'm kind of afraid of it. Some open issues:. - if the result of an aggregator is missing, I can't write it into the region, maybe rewrite `result` in continuation-passing style with a missing and non missing continuation? (is that function call indirection worth avoiding an allocation of a `Some(offset)` per-aggrgator-result?). - how do I take a user supplied function, e.g. `takeBy`? I keep avoiding lambda-like constructs. Do I introduce a new binding form, like `ApplyUnaryFunAggOp`. I don't like this path, but I also think adding a `Lambda` IR that isn't a full-fledged lambda will make the compiler look annoying/ugly too. This issue is basically the continuation of me punting on how to handle lambdas. - How do y'all feel about me eliminating some type-specific aggregators that can be implemented in terms of other ones (see AggOp.scala). If y'all are happy with this, I want to solve the missingness issue, and then farm out the last few (non-lambda-taking) aggregators to the team.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2606:416,avoid,avoiding,416,https://hail.is,https://github.com/hail-is/hail/pull/2606,2,['avoid'],['avoiding']
Safety,"@cseed @tpoterba . There's now an option to disable the Unsafe warnings in javac. You have to `-XDenableSunApiLintControl` and then you can `@SuppressWarnings(""sunapi"")`. The changes that are not in build.gradle and build.sbt are just me fixing warnings. There were some meaningful things, like unused variables and some places we were unnecessarily allocating a tuple.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5951:56,Unsafe,Unsafe,56,https://hail.is,https://github.com/hail-is/hail/pull/5951,1,['Unsafe'],['Unsafe']
Safety,"@cseed I've implemented `readPartitions` on `HailContext` and `writePartitions` on `RichRDD`, re-implemented read/write of blocks on `BlockMatrix` to preserve partitioning using these, and moved MatrixValue and KeyTable row read/write to use these. readPartitions and writePartitions use `unsafeReadPartition` and `writePartition` in RichHadoopConfig. `readRow` includes `close()`, and I've added `close()` to `readBlock` as well. At first I used `using` (see `readPartition` which is remarked out), but this closes the resource before the iterator iterated in the readRows case. Yet somehow, `testWriteRead` in VSMSuite passes even with `readPartition`. This test just creates a random VDS, writes, reads, compares. I added `testWriteRead2` which imports `sample.vcf`, writes, reads, compares. And indeed, the latter fails with `readPartition`. I have no explanation for why the behavior would differ (could the former be somehow cached?). I'm not happy with the asymmetry or lake of read safety if closing isn't propagated backward (I think it is for DataInputStream), but would like to get your feedback before messing around with these functions further. I've left the filename that had ReadRowsRDD with that name so it's easier to compare, but will change prior to merging since that class is gone. No class jumps out, should I just use the first one, `ArrayInputStream`, or a name that's not a class?. `ReadRDDPartition` in file ReadRowsRDDs and `IntPartition` in `BlockMatrix` have the same definition (the latter is used for more than just reading). I'm thinking of just having IntPartition located somewhere other than BlockMatrix. Any thoughts on where?. I added `sqrt` on blockSize in `blockMatrixGen` in order to get more cases with `blockSize < min(rows, cols)`. Only 1 of 10 had this property in `readWriteIdentityRandom` and its an important case to check with respect to the GridPartitioner dealing with block indices corrected (particularly with transpose in the mix).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2367:289,unsafe,unsafeReadPartition,289,https://hail.is,https://github.com/hail-is/hail/pull/2367,2,"['safe', 'unsafe']","['safety', 'unsafeReadPartition']"
Safety,@danking Can you do an initial pass and make sure there aren't any major issues with the Python code? I am most concerned with how I setup the disk manager and the disk creation / deletion code to avoid costly mistakes. I am aware of the FIXMEs. I haven't tested this at all yet. It would be great if I could have feedback on Wednesday so I can have this PR completely done on Friday.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11253:197,avoid,avoid,197,https://hail.is,https://github.com/hail-is/hail/pull/11253,1,['avoid'],['avoid']
Safety,"@danking I rebased in the process of addressing your requested changes, so git won't allow me to re-open the closed branch #4842. I think this is now safe with regard to closing resources. Do you see a way to not include the branch on binary twice? I'd prefer to consider changing the types of streams/buffers as another PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5046:150,safe,safe,150,https://hail.is,https://github.com/hail-is/hail/pull/5046,1,['safe'],['safe']
Safety,@danking I think this is what you meant. I looked at unsafeInserter also but I don't see where/if it tries to expand missing structs?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2424:53,unsafe,unsafeInserter,53,https://hail.is,https://github.com/hail-is/hail/pull/2424,1,['unsafe'],['unsafeInserter']
Safety,"@huy-nguyen is getting a segfault on on current release (0.2.33-5d8cae649505):; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa4b25e18cd, pid=6637, tid=0x00007f9a4f1fc700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-1~deb9u1-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:;  mt = mt.choose_cols(list(range(10))); ; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8240:113,detect,detected,113,https://hail.is,https://github.com/hail-is/hail/issues/8240,1,['detect'],['detected']
Safety,"@jigold I made some changes to the annotation database web page, care to take a look?. Mainly got rid of the tree/query builder thing and moved that functionality to checkboxes in the documentation. Seemed redundant to have both.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2144:206,redund,redundant,206,https://hail.is,https://github.com/hail-is/hail/pull/2144,1,['redund'],['redundant']
Safety,"@konradjk asked for weighted OLS, which is just a transformation of `x` and `y` by `sqrt(w)`. Currently `sqrt` is done `1 + len(x)` per record rather than once because you can't bind inside an aggregate. If that's a bottleneck, I could rework the aggregator to pass the w through to scala and avoid taking sqrt altogether. But for now this simple change at the Python level seems reasonable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4146:293,avoid,avoid,293,https://hail.is,https://github.com/hail-is/hail/pull/4146,1,['avoid'],['avoid']
Safety,"@maccum ran into a problem where a join was failing because the two tables had different requiredness in their key fields. This should resolve the problem, by adding an overload `Type.unsafeOrdering` which takes a `rightType: Type`, requiring that `this isOfType rightType`, but allowing them to have different requiredness.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3256:184,unsafe,unsafeOrdering,184,https://hail.is,https://github.com/hail-is/hail/pull/3256,1,['unsafe'],['unsafeOrdering']
Safety,"@patrick-schultz I'm not sure if this makes sense or not, but I observed it while working on something else. It seems weird but acceptable to import an empty dictionary as any struct. Does this seem reasonable to you? How have we avoided this bug for so long?. I'm not familiar enough with this code to know how to simply reproduce the bug and add a corresponding test. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14202:230,avoid,avoided,230,https://hail.is,https://github.com/hail-is/hail/pull/14202,1,['avoid'],['avoided']
Safety,"@tpoterba in line with your recommendation (https://github.com/hail-is/hail/pull/7712#discussion_r358433830 I put unsafeOrdering) I put unsafeOrdering on PSet, PDict. However I couldn't move the other constructor, because that depends on arrayRep. I can move this constructor from PArrayBackedContainer to PCanonicalSet and PCanonicalDict, or make a protected arrayRep on PSet, PDict and move the constructor there (which I don't think we want, since that implies that all implementations will have an array representation, which I don't think we know).; * That being said, I don't like having 2 constructors for the same method in 2 different places, makes it much harder to understand, to me. I think I would prefer putting unsafeOrdering both on the canonical implementation, or leave them on PArrayBackedContainer (since that is the canonical implementation for the subset of functions implemented there)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7752:114,unsafe,unsafeOrdering,114,https://hail.is,https://github.com/hail-is/hail/pull/7752,3,['unsafe'],['unsafeOrdering']
Safety,@tpoterba you were in here recently for performance so your eyes are appreciated. I simplified things a bit and localized almost all the parsing logic to `BgenRecord`. The contract for `advance` is that it is always called when `bfis` is pointing at the start of a record _or_ at or past the `end`. Advance will return the position to the start of a record or at or past the `end`. It returns true if there was a new record found. False otherwise. I avoided a couple allocating patterns. The rest of the diffs are copy pastes and some indentation changes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3783:450,avoid,avoided,450,https://hail.is,https://github.com/hail-is/hail/pull/3783,1,['avoid'],['avoided']
Safety,"A couple of fixes to batch pool executor to both get rid of orphaned running forever jobs and exception not retrieved errors:. - `asyncio.wait` does not retrieve results. I had to change waits to gathers with return_exceptions=True to get the behavior we want.; - A timeout error with `asyncio.wait_for` cancels the task automatically. Therefore, the previous code would never cancel the batch because the task was already ""cancelled"".; - I made `asyncio_cancel` idempotent and made sure we cancel the batch if the task has been cancelled to address the issue above. I added a check to see if the batch is running before cancelling. I'm ambivalent on whether this change is necessary.; - I added an explicit test now to make sure all batches are terminated. I think this is a good change, but the downstream consequences could be if this runs forever on a deploy (relies on an explicit timeout). Although, `test_hailtop_batch_*` has explicit timeouts. So I think we're good.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10738:266,timeout,timeout,266,https://hail.is,https://github.com/hail-is/hail/pull/10738,3,['timeout'],"['timeout', 'timeouts']"
Safety,"A user reported this error `concurrent.futures._base.TimeoutError` with no stack trace while copying files in a batch job. There's a comment in `is_transient_error` that we should catch this error, but I did not see it caught in the existing function.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817:53,Timeout,TimeoutError,53,https://hail.is,https://github.com/hail-is/hail/pull/11817,1,['Timeout'],['TimeoutError']
Safety,"A very small PR but here's the background and context behind this change. When talking to either GCP or Azure, hail chooses credentials in the following order from highest priority to lowest priority:. 1. An explicit `credential_file` argument passed to the relevant credentials class; 2. An environment variable containing the path to the credentials (`GOOGLE_APPLICATION_CREDENTIALS` or `AZURE_APPLICATION_CREDENTIALS`) (from this you can see why the code that was here is totally redundant); 3. The latent credentials present on the machine. This might be `gcloud` or `az` credentials, or the metadata server if you're on a cloud VM. I'm trying to rid the codebase of most explicit providing of credentials file paths, for two reasons:; - Quality of life. I'm already signed into the cloud with `gcloud` and `az`. I shouldn't need to download some file and provide `AZURE_APPLICATION_CREDENTIALS` to run this test. It should just use the latent credentials.; - We are trying to phase out credentials files altogether for security reasons. These files are long-lived secrets that you really don't want to leak and are currently exposed to users in Batch jobs, so they can be easily exfiltrated. Using the latent credentials on a cloud VM (the metadata server) has the benefit of only issuing short-lived access tokens which last for hours not months, so it's basically always better to use the latent credentials when possible.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13981:483,redund,redundant,483,https://hail.is,https://github.com/hail-is/hail/pull/13981,1,['redund'],['redundant']
Safety,AKA: How can we avoid using Jenkins?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/655:16,avoid,avoid,16,https://hail.is,https://github.com/hail-is/hail/issues/655,1,['avoid'],['avoid']
Safety,Add RVB.unsafeAdvance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3773:8,unsafe,unsafeAdvance,8,https://hail.is,https://github.com/hail-is/hail/pull/3773,1,['unsafe'],['unsafeAdvance']
Safety,"Add a codegen method `SNDArray.coiterate`, with signature; ```; def coiterate(cb: EmitCodeBuilder, region: Value[Region], indexVars: IndexedSeq[String], arrays: IndexedSeq[(SNDArrayCode, IndexedSeq[Int], String)], body: IndexedSeq[SSettable] => Unit, deepCopy: Boolean): Unit; ```; For example, the index expression `A[i, j] += B[j]` would be written; ```; coiterate(cb, region, Seq(""i"", ""j""), Seq((A, Seq(0, 1), ""A""), (B, Seq(1), ""B"")), {; case Seq(a, b) => cb.assign(a, SCode.add(cb, a, b)); }); ```; This generates a loop nest, with one loop per variable in `indexVars`. The inner loop is `indexVars(0)`, so that column major traversal is when index variables are increasing, as in `(A, Seq(0, 1), ""A"")`. Each index variable iterates over a dimension, with the size of the dimension inferred from its use. In the inner loop, each index variable `i0, i1, ...` has a value; `body` is run, with each of the `SSettable`s bound to an element of the corresponding input in `arrays`. For example, if the first element of `arrays` is `(A, IndexedSeq(1, 3), ""A"")`, then the `SSettable` will be the element of `A` at index `(i1, i3)`. However, it avoids computing the address of each element from the indices from scratch in the inner loop. This was motivated by the need to generate operations on ndarrays in the local whitening aggregator. I replaced a few uses of `forEachIndex` with `coiterate`, which may give a performance boost since it avoids index math in the inner loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10583:1140,avoid,avoids,1140,https://hail.is,https://github.com/hail-is/hail/pull/10583,2,['avoid'],['avoids']
Safety,Add note about avoiding duplicate data to export_elasticsearch docs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9953:15,avoid,avoiding,15,https://hail.is,https://github.com/hail-is/hail/pull/9953,1,['avoid'],['avoiding']
Safety,Add option to use reference population frequency -- more accurate and avoid reading genotypes twice; Use annotations as intermediate allele frequency storage,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/432:70,avoid,avoid,70,https://hail.is,https://github.com/hail-is/hail/pull/432,1,['avoid'],['avoid']
Safety,Added a cast toLong before doing multiplication to avoid overflow,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1358:51,avoid,avoid,51,https://hail.is,https://github.com/hail-is/hail/pull/1358,1,['avoid'],['avoid']
Safety,Added convenience functions to UnsafeOrdering.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2293:31,Unsafe,UnsafeOrdering,31,https://hail.is,https://github.com/hail-is/hail/pull/2293,1,['Unsafe'],['UnsafeOrdering']
Safety,Added unsafe comparators,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2101:6,unsafe,unsafe,6,https://hail.is,https://github.com/hail-is/hail/pull/2101,1,['unsafe'],['unsafe']
Safety,Added unsafe serializers.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2218:6,unsafe,unsafe,6,https://hail.is,https://github.com/hail-is/hail/pull/2218,1,['unsafe'],['unsafe']
Safety,Added:; - NDArray class with toJSON; - NDArray to SafeRow.read to go from RV to Annotation; - NDArrayto RegionValueBuilder.addAnnotation to go from Annotation to RV; - NDArray to JSONAnnotationImpex. Next steps:; ndarray 4: genValue on TNDArray in Scala with tests in TypeSuite; ndarray 5: JSON importers/exporters for NumPy NDArrays; ndarray 6: MakeNDArray IR; ndarray 7: Add MakeNDArray to cxx.emit,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5117:50,Safe,SafeRow,50,https://hail.is,https://github.com/hail-is/hail/pull/5117,1,['Safe'],['SafeRow']
Safety,"Adding a new compiler pass (lowering MatrixIR to TableIR) exposed a problem in Simplify. The logic for preventing some simplifications from triggering if they would introduce non-determinism was broken, and fixing it required a pretty complete overhaul. Fortunately, I think it's now a lot simpler. Besides the rewrite of the high level Simplify architecture, I also:; * Changed `testRepartitioningSimplifyRules` to something that failed in the old version.; * Changed the `copy` signature on the IR hierarchy to be more precise (to avoid unnecessary coercions).; * Grouped the Simplify rules into IR, MatrixIR, and TableIR. After the reorganization, a couple of rule redundancies became evident.; * A couple of vals in PruneSuite required running the compiler. When I had a bug in Simplify, this was causing the test runner to fail before any tests were run, on class initialization of PruneSuite, which gives very little help in diagnosing the issue. I made them lazy vals to prevent this in the future.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4564:533,avoid,avoid,533,https://hail.is,https://github.com/hail-is/hail/pull/4564,2,"['avoid', 'redund']","['avoid', 'redundancies']"
Safety,Adding devserver instructions to the developer FAQ to avoid having to search zulip history for this command in future (and also document a little about how it works),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14687:54,avoid,avoid,54,https://hail.is,https://github.com/hail-is/hail/pull/14687,1,['avoid'],['avoid']
Safety,"Adds BlockMatrix sparsify functions for:; - band matrix; - upper/lower triangle (special case of band); - a collection of rectangles. For diagonal band, I switched GridPartitioner.filterBand to go from lower to upper diagonal index, rather than taking a lower and upper bandwidth. This is more general, e.g. the diagonal itself need not be in the band. Band and triangle zero out elements in partially overlapping blocks by default. Rectangles currently only supports dropping whole blocks. Also adds `export_rectangles` for exporting rectangular regions to TSV in parallel.; I use parameters `path_in` and `path_out`, and switched `BlockMatrix.export` to this convention as well from `input` and `output` to avoid using the reserved word `input`. I have not exposed export methods directly on BlockMatrix for now as it'd be very easy for users to needlessly write and read an already written BlockMatrix. I could add these in a later PR with a warning, or we can wait until we've moved to IR and can optimize read followed by export to export on the file. It'd also be good to add compression options (and float formatting options to `export` and `export_rectanges`). Along the way I fixed NaN checking (due to Double.NaN != Double.NaN) on scalar and vector `/` sparse block ops and added NaN and Infinity checking to scalar and vector `*` sparse ops. Together with `sparsify_row_intervals` in the first sparse matrix PR, this PR exposes all the BlockMatrix functionality needed for big LD applications of Kate/Ran and Jacob/Masa.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3539:709,avoid,avoid,709,https://hail.is,https://github.com/hail-is/hail/pull/3539,1,['avoid'],['avoid']
Safety,"Adds retry for specific 500 errors:. ```; aiodocker.exceptions.DockerError: DockerError(500, 'error creating overlay mount to /var/lib/docker/overlay2/545a1337742e0292d9ed197b06fe900146c85ab06e468843cd0461c3f34df50d/merged: device or resource busy'; ```. ```; aiodocker.exceptions.DockerError: DockerError(500, 'Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7715:414,Timeout,Timeout,414,https://hail.is,https://github.com/hail-is/hail/pull/7715,1,['Timeout'],['Timeout']
Safety,"Also fixed getting the logs for a job. - I didn't realize the context manager for asyncio_timeout was throwing an asyncio.TimeoutError. Now, I handle the TimeoutError exception and then throw our own exception after we've uploaded the logs and cleaned up the container. This way it still shows up as an error. - I noticed the logs were being cached when a user gets the logs while the job is running and we don't update the cache until the job is complete. Therefore, I think from the code, if the user asks for the logs part-way through the job running, they wouldn't see any updates until the job is completed. I'm not sure why no-one has complained about this yet, so might be good to double check that this is indeed a bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8280:122,Timeout,TimeoutError,122,https://hail.is,https://github.com/hail-is/hail/pull/8280,2,['Timeout'],['TimeoutError']
Safety,"Also, added hail.vep.extra_plugins for specifying additional plugins beyond the default ones (such as LoF_splice.pm for predicting variants' probability of splice junction disruption or creation)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1712:120,predict,predicting,120,https://hail.is,https://github.com/hail-is/hail/pull/1712,1,['predict'],['predicting']
Safety,"Also, make batch pods eviction-safe. This should allow the cluster autoscaler to scale the cluster down, according to: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7432:31,safe,safe,31,https://hail.is,https://github.com/hail-is/hail/pull/7432,1,['safe'],['safe']
Safety,"An overhaul of our NumPy ndarray and BlockMatrix conversion, proceeding through binary on disk compatibly with the NumPy `tofile` and `fromfile` functions. For expediency, I still use Breeze matrix as an (internal) intermediate, resulting in an extra copy and max 2^31 size. Longer term, we should avoid routing through Breeze, add compression, etc, but this gets us a lot of bang for the buck in the face of intolerable py4j slowness. In fact, on laptop, the local byte transport piece is about 50x faster through disk than the ""faster"" direction of py4j (java to python via byte array), so now 90% of the time is spent localizing and distributing serialized blocks. E.g. reading a 8192 x 8192 block matrix with 4 blocks and converting to NumPy takes about 12s. Converting the local matrix to NumPy takes a bit over 1s. The current ""slower"" py4j direction (python to java) falls over even on tiny matrices. Main changes:; - added `tofile, to_numpy, fromfile, from_numpy` to BlockMatrix, which are explained in the docs with examples and tested in `test_linalg`.; - deleted all breeze related functions in utils.java; - added a `StreamRawBlockBufferSpec` (better name?) which behaves like the `StreamBlockBufferSpec` except that it only writes the (raw) blocks without adding length data to the stream. This allows for re-using readDoubles and writeDoubles as implemented in BlockingBuffer.; - added readDoubles and writeDoubles using this buffer spec on RichDenseMatrixDouble, and a test.; - along the way, added a `hl.tmp_dir()` function to allow users to inspect the tmp_dir used with `BlockMatrix.from_entry_expr`, as noted in that documentation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3114:298,avoid,avoid,298,https://hail.is,https://github.com/hail-is/hail/pull/3114,1,['avoid'],['avoid']
Safety,"Around 0348 I was executing `curl ci.hail.is/status` and repeatedly getting error responses, unfortunately I lost the error responses (that curl was piping into something that blew up on non-json data). The most recent response was a gateway timeout. The most recent logs are:. ```; INFO	| 2018-10-23 03:41:29,166 	| prs.py 	| heal_target:139 | deploying Nealelab/cloudtools:master; INFO	| 2018-10-23 03:41:29,350 	| prs.py 	| try_deploy:179 | already deployed c49bb905d3ba4d791150c3627c3c9ebde006a55a; INFO	| 2018-10-23 03:41:29,351 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /heal HTTP/1.1"" 200 -; INFO	| 2018-10-23 03:42:04,032 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""POST /test-ci-6oi3jysu.batch-pods/push HTTP/1.0"" 404 -; INFO	| 2018-10-23 03:42:04,196 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""POST /test-ci-6oi3jysu.batch-pods/pull_request HTTP/1.0"" 404 -; INFO	| 2018-10-23 03:42:04,677 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""POST /test-ci-6oi3jysu.batch-pods/pull_request_review HTTP/1.0"" 404 -; INFO	| 2018-10-23 03:42:37,944 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /refresh_github_state HTTP/1.1"" 200 -; ERROR	| 2018-10-23 03:48:38,045 	| ci.py 	| polling_event_loop:357 | Could not poll due to exception: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=360); ```; [hail-ci.log](https://github.com/hail-is/hail/files/2504423/hail-ci.log)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4607:242,timeout,timeout,242,https://hail.is,https://github.com/hail-is/hail/issues/4607,2,['timeout'],['timeout']
Safety,"As a sanity check, we've halved the amount of disk per worker. CPU is $0.01 core hour. So 0.003 / 2 = 0.0015",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7849:5,sanity check,sanity check,5,https://hail.is,https://github.com/hail-is/hail/pull/7849,1,['sanity check'],['sanity check']
Safety,"As much as possible, avoid network requests. In particular, we know the type of tables that are read in checkpoint and Expression.persist.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11677:21,avoid,avoid,21,https://hail.is,https://github.com/hail-is/hail/pull/11677,1,['avoid'],['avoid']
Safety,"As part of our work with generating All of Us datasets, we needed to copy around a million gcs objects. Our `Copier` infrastructure 'should' be able to handle that, but it kept falling with robustness issues. What finally worked was using GCS's [rewrite](https://cloud.google.com/storage/docs/json_api/v1/objects/rewrite) api. This allowed us to copy data without reading it, allowing the copies to complete in a fraction of the time while also reducing bandwidth needs. There are two components to this:; 1. Research what specific APIs we can take advantage of; 2. Update our code to use them when we can, for the `Copier`, and the new sync tool (#14248). Here's the code I used for making the rewrite requests for merging a set of matrix tables together, the progress bar code was for visibility. ```python3; async def rewrite(; gfs: GoogleStorageAsyncFS,; src: str,; dst: str,; progress: Optional[rich.progress.Progress] = None,; file_tid: Optional[rich.progress.TaskID] = None,; requests_tid: Optional[rich.progress.TaskID] = None,; ):; assert (progress is None) == (file_tid is None) == (requests_tid is None); src_bkt, src_name = gfs.get_bucket_and_name(src); dst_bkt, dst_name = gfs.get_bucket_and_name(dst); if not src_name:; raise IsABucketError(src); if not dst_name:; raise IsABucketError(dst); client = gfs._storage_client; path = (; f'/b/{src_bkt}/o/{urllib.parse.quote(src_name, safe="""")}/rewriteTo'; f'/b/{dst_bkt}/o/{urllib.parse.quote(dst_name, safe="""")}'; ); kwargs = {'json': '', 'params': {}}; client._update_params_with_user_project(kwargs, src_bkt); response = await retry_transient_errors(client.post, path, **kwargs); if progress is not None:; progress.update(requests_tid, advance=1); while not response['done']:; kwargs['params']['rewriteToken'] = response['rewriteToken']; response = await retry_transient_errors(client.post, path, **kwargs); if progress is not None:; progress.update(requests_tid, advance=1); if progress is not None:; progress.update(file_tid, advance=1)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14601:1393,safe,safe,1393,https://hail.is,https://github.com/hail-is/hail/issues/14601,2,['safe'],['safe']
Safety,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4974:513,timeout,timeout,513,https://hail.is,https://github.com/hail-is/hail/pull/4974,1,['timeout'],['timeout']
Safety,"At some point we started optimizing the MakeStruct to a SelectFields,; which is great, but not if it breaks important optimizations like the; avoid-a-shuffle rewrite rule!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7073:142,avoid,avoid-a-shuffle,142,https://hail.is,https://github.com/hail-is/hail/pull/7073,1,['avoid'],['avoid-a-shuffle']
Safety,Auto-detect type in sample annotations (and maybe variant annotations),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/463:5,detect,detect,5,https://hail.is,https://github.com/hail-is/hail/issues/463,1,['detect'],['detect']
Safety,Avoid calling a tail -> collect in blanczos pca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9430:0,Avoid,Avoid,0,https://hail.is,https://github.com/hail-is/hail/pull/9430,1,['Avoid'],['Avoid']
Safety,Avoid computation in suite allocation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2262:0,Avoid,Avoid,0,https://hail.is,https://github.com/hail-is/hail/pull/2262,1,['Avoid'],['Avoid']
Safety,Avoid creating List (!) of genotypes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2000:0,Avoid,Avoid,0,https://hail.is,https://github.com/hail-is/hail/pull/2000,1,['Avoid'],['Avoid']
Safety,Avoid end of block checks in int/long en/decoding.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2430:0,Avoid,Avoid,0,https://hail.is,https://github.com/hail-is/hail/pull/2430,1,['Avoid'],['Avoid']
Safety,Avoid query-service races when looking at the cache.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10309:0,Avoid,Avoid,0,https://hail.is,https://github.com/hail-is/hail/pull/10309,1,['Avoid'],['Avoid']
Safety,Avoiding hdfs command,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/56:0,Avoid,Avoiding,0,https://hail.is,https://github.com/hail-is/hail/issues/56,1,['Avoid'],['Avoiding']
Safety,BPE tests with timeouts need to explicitly cancel their jobs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10761:15,timeout,timeouts,15,https://hail.is,https://github.com/hail-is/hail/pull/10761,1,['timeout'],['timeouts']
Safety,"Based off of discussion in #11907, this aims to avoid separate PRs from clobbering the image cache tag and sets up PR-specific cache tags per image. Note that using `ci-intermediate` was also detrimental to the image cache and I don't think different images sharing layers under the common name holds much value. I think we should ultimately get rid of `ci-intermediate` entirely and explicitly name our images so that they don't ruin each other's caches. I tested this in my namespace's CI. Here's the image build times from two consecutive dev deploys:. Before | After; :-------------------------:|:-------------------------:; ![Screen Shot 2022-07-05 at 6 14 36 PM](https://user-images.githubusercontent.com/24440116/177426924-5d5ade8c-0cee-4a0e-b477-2156d4e01e78.png) | ![Screen Shot 2022-07-05 at 6 14 45 PM](https://user-images.githubusercontent.com/24440116/177426882-c0029760-42ae-471d-b48c-daa0eadea448.png). I don't personally see the need for adding more SHAs to the cache as mentioned in #11907, a per-PR cache seems like exactly what you would want. The one drawback I can think of here is that a deploy won't make use of the cache from the PR that resulted in the commit to main. I believe the commit SHAs would be different because we squash so other than devising a way to trace the commit back to the PR I don't see how we can easily connect the two. Still, I feel like it's not a big deal since it will still use the previously deployed commit as a cache, so most deploys will still be very fast and no one's waiting on deploys in the same way as we wait on PRs and dev deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11999:48,avoid,avoid,48,https://hail.is,https://github.com/hail-is/hail/pull/11999,1,['avoid'],['avoid']
Safety,Batch client seems to sometimes use a read timeout of 5s.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5566:43,timeout,timeout,43,https://hail.is,https://github.com/hail-is/hail/issues/5566,1,['timeout'],['timeout']
Safety,Batch eval for tests by putting expressions in a tuple and using `Begin` for write statements. This helps dramatically by avoiding the c++ compilation time on every assert.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6359:122,avoid,avoiding,122,https://hail.is,https://github.com/hail-is/hail/pull/6359,1,['avoid'],['avoiding']
Safety,"Batch lives in a different namespace than it's client and has substantially more credentials. The `batch_callback` URL can be used to make batch POST (without its credentials) to an arbitrary URL. Are there services in the internal namespace that are at risk from malicious, uncredentialed POSTs?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7623:254,risk,risk,254,https://hail.is,https://github.com/hail-is/hail/issues/7623,1,['risk'],['risk']
Safety,"Before open batches, the `n_jobs` of a batch was a constant known before any jobs were added. Moreover, we did not start scheduling jobs until all the jobs were added to the database. Therefore, it was always safe to assume that the final ""bunch"" of jobs in the database was the last ""bunch"" ergo it spanned from its `start_job_id` to the job with id `n_jobs` (nb: job ids are 1-indexed). When open batches were added, the `n_jobs` became a mutable value. Moreover, `n_jobs` includes jobs in bunches *which have not yet been added to the database*. In particular, suppose two clients are each submitting a bunch of size 10. Each client independently ""reserves"" 10 job slots by atomically incrementing `n_jobs` by ten. `n_jobs` is now 20. Further suppose that the first bunch is added to the database and begins scheduling before the second bunch is added to the database. In this case, when calculating the size of this bunch (for use in the bunch cache, and *only* in the bunch cache), we see that this is the last (and only) bunch in the database and assume that `n_jobs` is the last job id in this bunch. This is incorrect because `n_jobs` includes the not-yet-visible second bunch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13399:209,safe,safe,209,https://hail.is,https://github.com/hail-is/hail/pull/13399,1,['safe'],['safe']
Safety,"Before we can simplify the binding structure, we need to stop duplicating it all over the place. This PR rewrites `FreeVariables` so that it no longer needs special logic for particular nodes, hard coding binding structure (redundantly). To do this, it takes advantage of the new `Bindings`, which operates on a `GenericBindingEnv` interface. It adds a new implementation of this interface specifically for computing free variables, then simply does a generic traversal of the IR using this custom binging environment. While I find the new implementation far simpler and more obviously correct than the old, I do expect it to further simplify once I'm able to start modifying the core binding structure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14451:224,redund,redundantly,224,https://hail.is,https://github.com/hail-is/hail/pull/14451,1,['redund'],['redundantly']
Safety,BlockMatrixIR does unsafe casts from long to int,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6325:19,unsafe,unsafe,19,https://hail.is,https://github.com/hail-is/hail/issues/6325,1,['unsafe'],['unsafe']
Safety,BoxedArrayBuilder's type parameter needs to extend AnyRef to avoid; runtime matches on the type for all operations on the array.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10509:61,avoid,avoid,61,https://hail.is,https://github.com/hail-is/hail/pull/10509,1,['avoid'],['avoid']
Safety,"Breeze diag only works on square matrices, whereas BlockMatrix diagonal was written for arbitrary matrices, consistent with NumPy. Update avoids Breeze diag, tests non-square matrix with multiple blocks, and deletes shortened operator `diag` as unnecessary.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3180:138,avoid,avoids,138,https://hail.is,https://github.com/hail-is/hail/pull/3180,1,['avoid'],['avoids']
Safety,"Builds on PC Relate Unified Filtering #2238. I think this practically much more valuable to our users than filtering. This avoids computation of unneeded statistics (they appear as NA in the output). I'm happy to rebase without unified filtering, but it's some work to do that, so I'd rather decide one way or the other on filtering first.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2249:123,avoid,avoids,123,https://hail.is,https://github.com/hail-is/hail/pull/2249,1,['avoid'],['avoids']
Safety,"Builds on https://github.com/hail-is/hail/pull/3852 to avoid a conflict nightmare. You probably don't want to reivew until that goes in. This adds TableIR parser. TableImport is missing because handling the import options requires a bit more work. Tested by pretty printing/parsing example TableIR and verifying the result is the same. Also added (most of the) MatrixIR parser, but it is untested, that will come in part (3). Had to change some Array => IndexedSeq in various places to get proper equality. Rewrote Parser.quotedLiteral to avoid JVM stack depth limits when parsing non-small string literals.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3853:55,avoid,avoid,55,https://hail.is,https://github.com/hail-is/hail/pull/3853,2,['avoid'],['avoid']
Safety,"Builds on: https://github.com/hail-is/hail/pull/2074. Added optimized unsafe row add to region value builder. Tests are faster than toward_fullgeneric_4, 0.1 (8m2s vs 9m18s, 0.1: 8m20s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2081:70,unsafe,unsafe,70,https://hail.is,https://github.com/hail-is/hail/pull/2081,1,['unsafe'],['unsafe']
Safety,"Builds on: https://github.com/hail-is/hail/pull/2299. History of BTT is somewhat obscure. Now we just serialize the type in Unsafe{Row, IndexedSeq}. And some small additional improvements along the way:. Renamed UnsafeIndexedSeqAnnotation => UnsafeIndexedSeq.; Cache specialized Array types in UnsafeRow to avoid allocation.; Fixed ordering disagreement for Variant (added regression test).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2301:124,Unsafe,Unsafe,124,https://hail.is,https://github.com/hail-is/hail/pull/2301,5,"['Unsafe', 'avoid']","['Unsafe', 'UnsafeIndexedSeq', 'UnsafeIndexedSeqAnnotation', 'UnsafeRow', 'avoid']"
Safety,"Builds on: https://github.com/hail-is/hail/pull/2711 (genericintervals7). You probably want to wait until that goes in to review. @konradjk Unfortunately, we don't have automated tests for VEP yet. I'm bump the priority on that to make changing VEP safer, but in the mean time, can I ask you to run this on a small example to make sure we didn't break anything? Thanks!. @jbloom22 Can you do the same for Nirvana?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722:249,safe,safer,249,https://hail.is,https://github.com/hail-is/hail/pull/2722,1,['safe'],['safer']
Safety,Builds on: https://github.com/hail-is/hail/pull/2825. added RVD (should be UnpartitionedRVD) and OrderedRVD; allows to add new rvd types (HashedRVD); added list of partition files to current specs (to support safe object storage write strategy in presence of failure),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2828:209,safe,safe,209,https://hail.is,https://github.com/hail-is/hail/pull/2828,1,['safe'],['safe']
Safety,Bump async-timeout from 3.0.1 to 4.0.2 in /docker,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:11,timeout,timeout,11,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,"Bumps [aiohttp](https://github.com/aio-libs/aiohttp) from 3.8.1 to 3.8.3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/releases"">aiohttp's releases</a>.</em></p>; <blockquote>; <h2>3.8.3</h2>; <p>.. attention::</p>; <p>This is the last :doc:<code>aiohttp &lt;index&gt;</code> release tested under; Python 3.6. The 3.9 stream is dropping it from the CI and the; distribution package metadata.</p>; <h2>Bugfixes</h2>; <ul>; <li>; <p>Increased the upper boundary of the :doc:<code>multidict:index</code> dependency; to allow for the version 6 -- by :user:<code>hugovk</code>.</p>; <p>It used to be limited below version 7 in :doc:<code>aiohttp &lt;index&gt;</code> v3.8.1 but; was lowered in v3.8.2 via :pr:<code>6550</code> and never brought back, causing; problems with dependency pins when upgrading. :doc:<code>aiohttp &lt;index&gt;</code> v3.8.3; fixes that by recovering the original boundary of <code>&lt; 7</code>.; (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp/issues/6950"">#6950</a>)</p>; </li>; </ul>; <hr />; <h1>3.8.2 (2022-09-20, subsequently yanked on 2022-09-21)</h1>; <p>.. note::</p>; <p>This release has some compatibility fixes for Python 3.11 but it may; still have some quirks. Some tests are still flaky in the CI.</p>; <p>.. caution::</p>; <p>This release has been yanked from PyPI. Modern pip will not pick it; up automatically. The reason is that is has <code>multidict &lt; 6</code> set in; the distribution package metadata (see :pr:<code>6950</code>). Please, use; <code>aiohttp ~= 3.8.3, != 3.8.1</code> instead, if you can.</p>; <h2>Bugfixes</h2>; <ul>; <li>Added support for registering :rfc:<code>OPTIONS &lt;9110#OPTIONS&gt;</code>; HTTP method handlers via :py:class:<code>~aiohttp.web.RouteTableDef</code>.; (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp/issues/4663"">#4663</a>)</li>; <li>Started supporting :rfc:<code>authority-form &lt;9112#authority-form&gt;</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12296:929,recover,recovering,929,https://hail.is,https://github.com/hail-is/hail/pull/12296,1,['recover'],['recovering']
Safety,"Bumps [anyio](https://github.com/agronholm/anyio) from 3.6.1 to 3.6.2.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/agronholm/anyio/blob/3.6.2/docs/versionhistory.rst"">anyio's changelog</a>.</em></p>; <blockquote>; <h1>Version history</h1>; <p>This library adheres to <code>Semantic Versioning 2.0 &lt;http://semver.org/&gt;</code>_.</p>; <p><strong>3.6.2</strong></p>; <ul>; <li>Pinned Trio to &lt; 0.22 to avoid incompatibility with AnyIO's <code>ExceptionGroup</code> class; causing <code>AttributeError: 'NonBaseMultiError' object has no attribute '_exceptions'</code>; (AnyIO 4 is unaffected)</li>; </ul>; <p><strong>3.6.1</strong></p>; <ul>; <li>Fixed exception handler in the asyncio test runner not properly handling a context; that does not contain the <code>exception</code> key</li>; </ul>; <p><strong>3.6.0</strong></p>; <ul>; <li>; <p>Fixed <code>TypeError</code> in <code>get_current_task()</code> on asyncio when using a custom <code>Task</code> factory</p>; </li>; <li>; <p>Updated type annotations on <code>run_process()</code> and <code>open_process()</code>:</p>; <ul>; <li><code>command</code> now accepts accepts bytes and sequences of bytes</li>; <li><code>stdin</code>, <code>stdout</code> and <code>stderr</code> now accept file-like objects; (PR by John T. Wodder II)</li>; </ul>; </li>; <li>; <p>Changed the pytest plugin to run both the setup and teardown phases of asynchronous; generator fixtures within a single task to enable use cases such as cancel scopes and; task groups where a context manager straddles the <code>yield</code></p>; </li>; </ul>; <p><strong>3.5.0</strong></p>; <ul>; <li>Added <code>start_new_session</code> keyword argument to <code>run_process()</code> and <code>open_process()</code>; (PR by Jordan Speicher)</li>; <li>Fixed deadlock in synchronization primitives on asyncio which can happen if a task acquiring a; primitive is hit with a native (not AnyIO) cancellation with just the right timing, le",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12362:453,avoid,avoid,453,https://hail.is,https://github.com/hail-is/hail/pull/12362,1,['avoid'],['avoid']
Safety,"Bumps [async-timeout](https://github.com/aio-libs/async-timeout) from 3.0.1 to 4.0.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/releases"">async-timeout's releases</a>.</em></p>; <blockquote>; <h2>v4.0.2</h2>; <h2>Misc</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/259"">#259</a>, <a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a></li>; </ul>; <h2>v4.0.1</h2>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h2>async-timeout 4.0.0</h2>; <h1>Changes</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Drooped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:13,timeout,timeout,13,https://hail.is,https://github.com/hail-is/hail/pull/11465,11,"['Timeout', 'timeout']","['Timeout', 'TimeoutError', 'timeout']"
Safety,"Bumps [black](https://github.com/psf/black) from 22.1.0 to 22.3.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/psf/black/releases"">black's releases</a>.</em></p>; <blockquote>; <h2>22.3.0</h2>; <h3>Preview style</h3>; <ul>; <li>Code cell separators <code>#%%</code> are now standardised to <code># %%</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2919"">#2919</a>)</li>; <li>Remove unnecessary parentheses from <code>except</code> statements (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2939"">#2939</a>)</li>; <li>Remove unnecessary parentheses from tuple unpacking in <code>for</code> loops (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2945"">#2945</a>)</li>; <li>Avoid magic-trailing-comma in single-element subscripts (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2942"">#2942</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Do not format <code>__pypackages__</code> directories by default (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2836"">#2836</a>)</li>; <li>Add support for specifying stable version with <code>--required-version</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2832"">#2832</a>).</li>; <li>Avoid crashing when the user has no homedir (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2814"">#2814</a>)</li>; <li>Avoid crashing when md5 is not available (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2905"">#2905</a>)</li>; <li>Fix handling of directory junctions on Windows (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2904"">#2904</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Update pylint config documentation (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2931"">#2931</a>)</li>; </ul>; <h3>Integrations</h3>; <ul>; <li>Move test to disable plugin in Vim/Neovim, which speeds up loading (<a href=""https://githu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11696:782,Avoid,Avoid,782,https://hail.is,https://github.com/hail-is/hail/pull/11696,1,['Avoid'],['Avoid']
Safety,"Bumps [boto3](https://github.com/boto/boto3) from 1.26.7 to 1.26.17.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/boto/boto3/blob/develop/CHANGELOG.rst"">boto3's changelog</a>.</em></p>; <blockquote>; <h1>1.26.17</h1>; <ul>; <li>bugfix:dynamodb: Fixes duplicate serialization issue in DynamoDB BatchWriter</li>; <li>api-change:<code>backup</code>: [<code>botocore</code>] AWS Backup introduces support for legal hold and application stack backups. AWS Backup Audit Manager introduces support for cross-Region, cross-account reports.</li>; <li>api-change:<code>cloudwatch</code>: [<code>botocore</code>] Update cloudwatch client to latest version</li>; <li>api-change:<code>drs</code>: [<code>botocore</code>] Non breaking changes to existing APIs, and additional APIs added to support in-AWS failing back using AWS Elastic Disaster Recovery.</li>; <li>api-change:<code>ecs</code>: [<code>botocore</code>] This release adds support for ECS Service Connect, a new capability that simplifies writing and operating resilient distributed applications. This release updates the TaskDefinition, Cluster, Service mutation APIs with Service connect constructs and also adds a new ListServicesByNamespace API.</li>; <li>api-change:<code>efs</code>: [<code>botocore</code>] Update efs client to latest version</li>; <li>api-change:<code>iot-data</code>: [<code>botocore</code>] This release adds support for MQTT5 properties to AWS IoT HTTP Publish API.</li>; <li>api-change:<code>iot</code>: [<code>botocore</code>] Job scheduling enables the scheduled rollout of a Job with start and end times and a customizable end behavior when end time is reached. This is available for continuous and snapshot jobs. Added support for MQTT5 properties to AWS IoT TopicRule Republish Action.</li>; <li>api-change:<code>iotwireless</code>: [<code>botocore</code>] This release includes a new feature for customers to calculate the position of their devices by adding three new APIs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12507:876,Recover,Recovery,876,https://hail.is,https://github.com/hail-is/hail/pull/12507,1,['Recover'],['Recovery']
Safety,"Bumps [importlib-metadata](https://github.com/python/importlib_metadata) from 3.10.1 to 4.12.0.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/python/importlib_metadata/blob/main/CHANGES.rst"">importlib-metadata's changelog</a>.</em></p>; <blockquote>; <h1>v4.12.0</h1>; <ul>; <li>py-93259: Now raise <code>ValueError</code> when <code>None</code> or an empty; string are passed to <code>Distribution.from_name</code> (and other; callers).</li>; </ul>; <h1>v4.11.4</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/379"">#379</a>: In <code>PathDistribution._name_from_stem</code>, avoid including; parts of the extension in the result.</li>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/381"">#381</a>: In <code>PathDistribution._normalized_name</code>, ensure names; loaded from the stem of the filename are also normalized, ensuring; duplicate entry points by packages varying only by non-normalized; name are hidden.</li>; </ul>; <h1>v4.11.3</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/372"">#372</a>: Removed cast of path items in FastPath, not needed.</li>; </ul>; <h1>v4.11.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/369"">#369</a>: Fixed bug where <code>EntryPoint.extras</code> was returning; match objects and not the extras strings.</li>; </ul>; <h1>v4.11.1</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/367"">#367</a>: In <code>Distribution.requires</code> for egg-info, if <code>requires.txt</code>; is empty, return an empty list.</li>; </ul>; <h1>v4.11.0</h1>; <ul>; <li>bpo-46246: Added <code>__slots__</code> to <code>EntryPoints</code>.</li>; </ul>; <h1>v4.10.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/365"">#365</a> and bpo-46546: Avoid leakin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12000:669,avoid,avoid,669,https://hail.is,https://github.com/hail-is/hail/pull/12000,1,['avoid'],['avoid']
Safety,"Bumps [kubernetes-asyncio](https://github.com/tomplus/kubernetes_asyncio) from 9.1.0 to 19.15.1.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/tomplus/kubernetes_asyncio/blob/master/CHANGELOG.md"">kubernetes-asyncio's changelog</a>.</em></p>; <blockquote>; <h1>v19.15.1</h1>; <ul>; <li>fix: watch returns <code>raw_object</code> if detection of returned objects fail (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/pull/177"">#177</a>, <a href=""https://github.com/tomplus""><code>@tomplus</code></a>)</li>; </ul>; <h1>v19.15.0</h1>; <ul>; <li>feat: Kubernetes API Version: v1.19.15</li>; </ul>; <h3>API Change</h3>; <ul>; <li>We have added a new Priority &amp; Fairness rule that exempts all probes (/readyz, /healthz, /livez) to prevent; restarting of &quot;healthy&quot; kube-apiserver instance(s) by kubelet. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/101113"">kubernetes/kubernetes#101113</a>, <a href=""https://github.com/tkashem""><code>@tkashem</code></a>) [SIG API Machinery]</li>; <li>Fixes using server-side apply with APIService resources (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/100713"">kubernetes/kubernetes#100713</a>, <a href=""https://github.com/kevindelgado""><code>@kevindelgado</code></a>) [SIG API Machinery, Apps, Scheduling and Testing]</li>; <li>Regenerate protobuf code to fix CVE-2021-3121 (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/100515"">kubernetes/kubernetes#100515</a>, <a href=""https://github.com/joelsmith""><code>@joelsmith</code></a>) [SIG API Machinery, Auth, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation, Node and Storage]</li>; <li>Kubernetes is now built using go1.15.8 (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/99093"">kubernetes/kubernetes#99093</a>, <a href=""https://github.com/cpanato""><code>@cpanato</code></a>) [SIG Cloud Provider, Instrumentat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:375,detect,detection,375,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['detect'],['detection']
Safety,"Bumps [mypy](https://github.com/python/mypy) from 0.780 to 0.942.; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python/mypy/commit/38f1e30e8137ccc1aad6a4f113eb4360c6206539""><code>38f1e30</code></a> Update version to 0.942</li>; <li><a href=""https://github.com/python/mypy/commit/1c836685da13f11287ae6d6931c04337f881ec40""><code>1c83668</code></a> Let overload item have a wider return type than implementation (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12435"">#12435</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/67088e558dc24a2c6c231db542a367923dfdc049""><code>67088e5</code></a> Pin the version of bugbear (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12436"">#12436</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/367b29d4aac16fc7493abffe2df0d8f477c23923""><code>367b29d</code></a> Make order of processing the builtins SCC predictable (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12431"">#12431</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/f81b228e66d8a95cc39247f189e7be7e894f7f92""><code>f81b228</code></a> Fix inheritance false positives with dataclasses/attrs (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12411"">#12411</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/7e09c2a100209072429e290d2f7b9b8007b8629c""><code>7e09c2a</code></a> Support overriding dunder attributes in Enum subclass (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12138"">#12138</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/837543efb616b14e2f800db6962d216621dee4d7""><code>837543e</code></a> Fix crash in match statement if class name is undefined (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12417"">#12417</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/6606dbe98d09170d3ad810bc791a16d99ceb2281""><code>6606dbe</code></a> Allow non-final <strong>match_args</strong> and ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11667:932,predict,predictable,932,https://hail.is,https://github.com/hail-is/hail/pull/11667,2,['predict'],['predictable']
Safety,"Bumps [orjson](https://github.com/ijl/orjson) from 3.9.10 to 3.10.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/releases"">orjson's releases</a>.</em></p>; <blockquote>; <h2>3.10.0</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.10.0 - 2024-03-27</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14427:635,avoid,avoid,635,https://hail.is,https://github.com/hail-is/hail/pull/14427,1,['avoid'],['avoid']
Safety,"Bumps [orjson](https://github.com/ijl/orjson) from 3.9.10 to 3.9.15.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/releases"">orjson's releases</a>.</em></p>; <blockquote>; <h2>3.9.15</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14357:414,avoid,avoid,414,https://hail.is,https://github.com/hail-is/hail/pull/14357,3,['avoid'],['avoid']
Safety,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 10.2.0 to 10.3.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>; <blockquote>; <h2>10.3.0</h2>; <p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/10.3.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/10.3.0.html</a></p>; <h2>Changes</h2>; <ul>; <li>CVE-2024-28219: Use strncpy to avoid buffer overflow <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7928"">#7928</a> [<a href=""https://github.com/hugovk""><code>@hugovk</code></a>]</li>; <li>Use <code>functools.lru_cache</code> for <code>hopper()</code> <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7912"">#7912</a> [<a href=""https://github.com/hugovk""><code>@hugovk</code></a>]</li>; <li>Raise ValueError if seeking to greater than offset-sized integer in TIFF <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7883"">#7883</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Improve speed of loading QOI images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7925"">#7925</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Added RGB to I;16N conversion <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7920"">#7920</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Add --report argument to <strong>main</strong>.py to omit supported formats <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7818"">#7818</a> [<a href=""https://github.com/nulano""><code>@nulano</code></a>]</li>; <li>Added RGB to I;16, I;16L and I;16B conversion <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7918"">#7918</a> [<a href=""https://github.com/radarhere""><code>@radarhere</code></a>]</li>; <li>Fix editable installation with custom build backend and configurati",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:479,avoid,avoid,479,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['avoid'],['avoid']
Safety,"Bumps [sphinx](https://github.com/sphinx-doc/sphinx) from 3.5.4 to 4.5.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/sphinx-doc/sphinx/releases"">sphinx's releases</a>.</em></p>; <blockquote>; <h2>v4.5.0</h2>; <p>Changelog: <a href=""https://www.sphinx-doc.org/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v4.4.0</h2>; <p>Changelog: <a href=""https://www.sphinx-doc.org/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v4.3.1</h2>; <p>No release notes provided.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/sphinx-doc/sphinx/blob/4.x/CHANGES"">sphinx's changelog</a>.</em></p>; <blockquote>; <h1>Release 4.5.0 (released Mar 28, 2022)</h1>; <h2>Incompatible changes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10112"">#10112</a>: extlinks: Disable hardcoded links detector by default</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9993"">#9993</a>, <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10177"">#10177</a>: std domain: Disallow to refer an inline target via; :rst:role:<code>ref</code> role</li>; </ul>; <h2>Deprecated</h2>; <ul>; <li><code>sphinx.ext.napoleon.docstring.GoogleDocstring._qualify_name()</code></li>; </ul>; <h2>Features added</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10260"">#10260</a>: Enable <code>FORCE_COLOR</code> and <code>NO_COLOR</code> for terminal colouring</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10234"">#10234</a>: autosummary: Add &quot;autosummary&quot; CSS class to summary tables</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10125"">#10125</a>: extlinks: Improve suggestion message for a reference having title</li>; <li><a hre",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11714:1010,detect,detector,1010,https://hail.is,https://github.com/hail-is/hail/pull/11714,1,['detect'],['detector']
Safety,"Bumps [sphinx](https://github.com/sphinx-doc/sphinx) from 3.5.4 to 4.5.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/sphinx-doc/sphinx/releases"">sphinx's releases</a>.</em></p>; <blockquote>; <h2>v4.5.0</h2>; <p>Changelog: <a href=""https://www.sphinx-doc.org/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v4.4.0</h2>; <p>Changelog: <a href=""https://www.sphinx-doc.org/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v4.3.1</h2>; <p>No release notes provided.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/sphinx-doc/sphinx/blob/5.x/CHANGES"">sphinx's changelog</a>.</em></p>; <blockquote>; <h1>Release 4.5.0 (released Mar 28, 2022)</h1>; <h2>Incompatible changes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10112"">#10112</a>: extlinks: Disable hardcoded links detector by default</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9993"">#9993</a>, <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10177"">#10177</a>: std domain: Disallow to refer an inline target via; :rst:role:<code>ref</code> role</li>; </ul>; <h2>Deprecated</h2>; <ul>; <li><code>sphinx.ext.napoleon.docstring.GoogleDocstring._qualify_name()</code></li>; </ul>; <h2>Features added</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10260"">#10260</a>: Enable <code>FORCE_COLOR</code> and <code>NO_COLOR</code> for terminal colouring</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10234"">#10234</a>: autosummary: Add &quot;autosummary&quot; CSS class to summary tables</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10125"">#10125</a>: extlinks: Improve suggestion message for a reference having title</li>; <li><a hre",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11730:1010,detect,detector,1010,https://hail.is,https://github.com/hail-is/hail/pull/11730,1,['detect'],['detector']
Safety,"Bumps [tabulate](https://github.com/astanin/python-tabulate) from 0.8.3 to 0.8.9.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/astanin/python-tabulate/blob/master/CHANGELOG"">tabulate's changelog</a>.</em></p>; <blockquote>; <ul>; <li>0.8.9: Bug fix. Revert support of decimal separators.</li>; <li>0.8.8: Python 3.9 support, 3.10 ready.; New formats: <code>unsafehtml</code>, <code>latex_longtable</code>, <code>fancy_outline</code>.; Support lists of UserDicts as input.; Support hyperlinks in terminal output.; Improve testing on systems with proxies.; Migrate to pytest.; Various bug fixes and improvements.</li>; <li>0.8.7: Bug fixes. New format: <code>pretty</code>. HTML escaping.</li>; <li>0.8.6: Bug fixes. Stop supporting Python 3.3, 3.4.</li>; <li>0.8.5: Fix broken Windows package. Minor documentation updates.</li>; <li>0.8.4: Bug fixes.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/astanin/python-tabulate/commits/v0.8.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tabulate&package-manager=pip&previous-version=0.8.3&new-version=0.8.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11478:402,unsafe,unsafehtml,402,https://hail.is,https://github.com/hail-is/hail/pull/11478,2,['unsafe'],['unsafehtml']
Safety,"Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.5 to 1.26.8.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/releases"">urllib3's releases</a>.</em></p>; <blockquote>; <h2>1.26.8</h2>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a>.</strong></p>; <p>:warning: <strong>urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <p>:warning: <strong>This release will be the last release supporting Python 3.5. Please upgrade to a non-EOL Python version.</strong></p>; <ul>; <li>Added extra message to<code>urllib3.exceptions.ProxyError</code> when urllib3 detects that a proxy is configured to use HTTPS but the proxy itself appears to only use HTTP.</li>; <li>Added a mention of the size of the connection pool when discarding a connection due to the pool being full.</li>; <li>Added explicit support for Python 3.11.</li>; <li>Deprecated the <code>Retry.MAX_BACKOFF</code> class property in favor of <code>Retry.DEFAULT_MAX_BACKOFF</code> to better match the rest of the default parameter names. <code>Retry.MAX_BACKOFF</code> is removed in v2.0.</li>; <li>Changed location of the vendored <code>ssl.match_hostname</code> function from <code>urllib3.packages.ssl_match_hostname</code> to <code>urllib3.util.ssl_match_hostname</code> to ensure Python 3.10+ compatibility after being repackaged by downstream distributors.</li>; <li>Fixed absolute imports, all imports are now relative.</li>; </ul>; <h2>1.26.7</h2>; <p>:warning: <strong>IMPORTANT: urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed a bug with HTTPS hostname verification involving IP addresses and lack of SNI</li>; <li>Fixed a bug ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11532:832,detect,detects,832,https://hail.is,https://github.com/hail-is/hail/pull/11532,1,['detect'],['detects']
Safety,"CHANGELOG: Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to specify which cloud regions a job can run in. The default value is a job can run in any available region. Stacked on #12212 . This PR threads through region requests from the user and feeds that information into the scheduler. The architecture of a pool per machine type has not changed. We explicitly chose not to have a new pool per region x machine_type. Instead, the control loop looks at the front of the job queue and tries to predict which jobs are likely to be scheduled. From those jobs, we then find which regions the jobs can run in and create the number of corresponding instances. We use the fair share calculation to estimate how many jobs per user can be scheduled in 2.5 minutes assuming the scheduling loop runs once per second. We then grab this many jobs from the queue for each user and estimate the ""scheduling iteration"" at which each iteration of the scheduler each chunk of user jobs would be scheduled. We sort the overall set of jobs that we've chosen by the ""scheduling iteration"". We also include the regions as part of the sorting queries with None (any region) being sorted last. This is to compact the free cores across jobs so as to avoid fragmentation of instances created and for jobs with no region specifications to fill in the remaining cores in any region. For the hailtop.batch client, I added a new setting in `~/.config/hail` to set the default regions for all jobs in the ServiceBackend and a new method on `Job` that sets the list of regions to run in. Things to double check once everything is working is the sort orders on the scheduling queries are correct. . Once this PR goes in, then we can merge #11840 with some minor changes. There will also be a follow-up PR that gets rid of the CI-specific code in the scheduler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221:538,predict,predict,538,https://hail.is,https://github.com/hail-is/hail/pull/12221,2,"['avoid', 'predict']","['avoid', 'predict']"
Safety,"CHANGELOG: Fix #13356 and fix #13409. In QoB pipelines with 10K or more partitions, transient ""Corrupted block detected"" errors were common. This was caused by incorrect retry logic. That logic has been fixed. I now assume we cannot reuse a ReadChannel after any exception occurs during read. We also do not assume that the ReadChannel ""atomically"", in some sense, modifies the ByteBuffer. In particular, if we encounter any error, we blow away the ByteBuffer and restart our read entirely. As I described in [this comment to #13409](https://github.com/hail-is/hail/issues/13409#issuecomment-1737926184), I have a 10K partition pipeline which was reliably producing this error but now reliably *does not* produce this error (it produces another one, #13721, fix forthcoming for that too).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13730:111,detect,detected,111,https://hail.is,https://github.com/hail-is/hail/pull/13730,1,['detect'],['detected']
Safety,"CHANGELOG: Fix #13979, affecting Query-on-Batch and manifesting most frequently as ""com.github.luben.zstd.ZstdException: Corrupted block detected"". This PR upgrades google-cloud-storage from 2.29.1 to 2.30.1. The google-cloud-storage java library has a bug present at least since 2.29.0 in which simply incorrect data was returned. https://github.com/googleapis/java-storage/issues/2301 . The issue seems related to their use of multiple intremediate ByteBuffers. As far as I can tell, this is what could happen:. 1. If there's no channel, open a new channel with the current position.; 2. Read *some* data from the input ByteChannel into an intermediate ByteBuffer.; 3. While attempting to read more data into a subsequent intermediate ByteBuffer, an retryable exception occurs.; 4. The exception bubbles to google-cloud-storage's error handling, which frees the channel and loops back to (1). The key bug is that the intermediate buffers have data but the `position` hasn't been updated. When we recreate the channel we will jump to the wrong position and re-read some data. Lucky for us, between Zstd and our assertions, this usually crashes the program instead of silently returning bad data. This is the third bug we have found in Google's cloud storage java library. The previous two:. 1. https://github.com/hail-is/hail/issues/13721; 2. https://github.com/hail-is/hail/issues/13937. Be forewarned: the next time we see bizarre networking or data corruption issues, check if updating google-cloud-storage fixes the problem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14080:137,detect,detected,137,https://hail.is,https://github.com/hail-is/hail/pull/14080,1,['detect'],['detected']
Safety,"CHANGELOG: Fix `RuntimeError: This event loop is already running` error when running hail in a Jupyter Notebook. Man this is really complicated. OK, so, things I learned:. 1. [asyncio will not create a new event loop if `set_event_loop` has been called even if `set_event_loop(None)` has since been called.](https://github.com/python/cpython/blob/main/Lib/asyncio/events.py#L676); 2. [asyncio will not create a new event loop in a thread other than the main thread.](https://github.com/python/cpython/blob/main/Lib/asyncio/events.py#L677); 3. `aiohttp.ClientSession` stashes a copy of the event loop present when it starts. This can cause all manner of extremely confusing behavior if you later change the event loop or use that session from a different thread. The fix, in the end, wasn't that complicated. Anywhere Hail explicitly asks for an event loop (so that we can run async code), we apply nest asyncio if the event loop is already running. Otherwise we do nothing. Nest asyncio appears to [no longer require](https://github.com/erdewit/nest_asyncio/tree/master#usage) `apply` to be called before the event loop starts running. This PR *does not* address:; 1. Hail nesting async code in sync code in async code. I think we should avoid this, but the `hailtop.fs` and `hailtop.batch` APIs, among others, need async versions before we can do that.; 2. This `aiohttp.ClientSession` nonsense. We really should take pains to ensure we create one `ClientSession` per loop and we never mix loops.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13899:1238,avoid,avoid,1238,https://hail.is,https://github.com/hail-is/hail/pull/13899,1,['avoid'],['avoid']
Safety,"CHANGELOG: Fix bug introduced in 0.2.117 by commit `c9de81108` which prevented the passing of keyword arguments to Python jobs. This manifested as ""ValueError: too many values to unpack"". We also weren't preserving tuples. They become lists. I fixed that too. I also added some types and avoided an is instance by encoding the necessary knowledge.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13505:288,avoid,avoided,288,https://hail.is,https://github.com/hail-is/hail/pull/13505,1,['avoid'],['avoided']
Safety,CHANGELOG: Fix bug where matrix tables with duplicate col keys do not show properly. Also fix bug where tables and matrix tables with HTML unsafe column headers are rendered wrong in Jupyter.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12582:139,unsafe,unsafe,139,https://hail.is,https://github.com/hail-is/hail/pull/12582,1,['unsafe'],['unsafe']
Safety,"CHANGELOG: Fix long-standing bug wherein `hl.agg.collect_as_set` and `hl.agg.counter` error when applied to types which, in Python, are unhashable. For example, `hl.agg.counter(t.list_of_genes)` will not error when `t.list_of_genes` is a list. Instead, the counter dictionary will use `FrozenList` keys from the `frozenlist` package. Hey @iris-garden ! I figured this was good reviewing practice for you and also a chance to see how we convert data to/from JSON and to/from the JVM (by way of this ""encoded"" representation which is a binary one). The details of that are not super important to this PR, but you might take a peek to understand the change. The main issue here is that in Python, you can't write:; ```; {[1]}; ```; Because sets must contain ""hashable"" data. Python lists are not hashable because they're mutable. This is transitively a problem. For example, the following also fails with the same error because the list inside the tuple is mutable thus the tuple is not (safely) hashable.; ```; {(""tuples"", ""are"", ""immutable"", [""lists"", ""are"", ""not""])}; ```. Hail's internal language is fully immutable, so every type can be placed inside a set (or used as the keys of a dict). When we convert from Hail's internal representation to Python, we cannot use mutable types in hashable positions. Unfortunately, we also need to maintain backwards compatibility with the way the code currently works. You can see this pretty clearly in the difference between `hl.agg.collect` and `hl.agg.collect_as_set`:; ```; t = hl.utils.range_table(1); t = t.annotate(ls = [1, 2, 3]); collected_ls = t.aggregate(hl.agg.collect(t.ls)); collected_as_set_ls = t.aggregate(hl.agg.collect_as_set(t.ls)); ```; `collected_ls` should be `[[1, 2, 3]]` whereas `collected_as_set_ls` necessarily uses hashable types: `{frozenlist([1, 2, 3])}`. Things are particularly subtle with dictionaries whose keys must always be hashable and whose values need only be hashable if the dictionary itself must be hashable. For exa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12265:985,safe,safely,985,https://hail.is,https://github.com/hail-is/hail/pull/12265,1,['safe'],['safely']
Safety,"CHANGELOG: Fixes #13756: operations that collect large results such as `to_pandas` may require up to 3x less memory. This turns all ""actions"", i.e. backend methods supported by QoB into HTTP endpoints on the spark and local backends. This intentionally avoids py4j because py4j was really designed to pass function names and references around and does not handle large payloads well (such as results from a `collect`). Specifically, py4j uses a text-based protocol on top of TCP that substantially inflates the memory requirement for communicating large byte arrays. On the Java side, py4j serializes every binary payload as a Base64-encoded `java.lang.String`, which between the Base64 encoding and `String`'s use of UTF-16 results in a memory footprint of the `String` being `4/3 * 2 = 8/3` nearly three times the size of the byte array on either side of the py4j pipe. py4j also appears to do an entire copy of this payload, which means nearly a 6x memory requirement for sending back bytes. Using our own socket means we can directly send back the response bytes to python without any of this overhead, even going so far as to encode results directly into the TCP output stream. Formalizing the API between python and java also allows us to reuse the same payload schema across all three backends.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13797:253,avoid,avoids,253,https://hail.is,https://github.com/hail-is/hail/pull/13797,1,['avoid'],['avoids']
Safety,"CHANGELOG: Hail Query-on-Batch previously used Class A Operations for all interaction with blobs. This change ensures that QoB only uses Class A Operations when necessary. Inspired by @jigold 's file system improvement campaign, I pursued the avoidance of ""list"" operations. I anticipate this reduces flakiness in Azure (which is tracked in #13351) and cost in Azure. I enforced aiotools.fs terminology on hail.fs and Scala:. 1. `FileStatus`. Metadata about a blob or file. It does not know if a directory exists at this path. 2. `FileListEntry`. Metadata from a list operation. It knows if a directory exists at this path. Variable names were updated to reflect this distinction:. 1. `fileStatus` / `fileStatuses`. 2. `fle`/ `fles` / `fileListEntry` / `fileListEntries`, respectively. `listStatus` renamed to `listDirectory` for clarity. In both Azure and Google, `fileStatus` does not use a list operation. `fileListEntry` can be used when we must know if a directory exists. I just rewrote this from first principles because:; 1. In neither Google nor Azure did it check if the path was a directory and a file.; 2. In Google, if the directory entry wasn't in the first page, it would fail (NB: there are fifteen non-control characters in ASCII before `/`, if the page size is 15 or fewer, we'd miss the first entry with a `/` at the end).; 3. In Azure, we issued both a get and a list. There are now unit tests for this method. ---. 1. `copyMerge` and `concatenateFiles` previously used `O(N_FILES)` list operations, they now use `O(N_FILES)` get operations.; 2. Writers that used `exists` to check for a _SUCCESS file now use a get operation.; 3. Index readers, import BGEN, and import plink all now check file size with a get operation. That said, overall, the bulk of our Class A Operations are probably writes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13885:243,avoid,avoidance,243,https://hail.is,https://github.com/hail-is/hail/pull/13885,1,['avoid'],['avoidance']
Safety,"CHANGELOG: Hail Query-on-Batch previously used Class A Operations for all interaction with blobs. This change ensures that QoB only uses Class A Operations when necessary. Inspired by @jigold 's file system improvement campaign, I pursued the avoidance of ""list"" operations. I anticipate this reduces flakiness in Azure (which is tracked in #13351) and cost in Azure. I enforced aiotools.fs terminology on hail.fs and Scala:. 1. `FileStatus`. Metadata about a blob or file. It does not know if a directory exists at this path. 2. `FileListEntry`. Metadata from a list operation. It knows if a directory exists at this path. Variable names were updated to reflect this distinction:. 1. `fileStatus` / `fileStatuses`. 2. `fle`/ `fles` / `fileListEntry` / `fileListEntries`, respectively. `listStatus` renamed to `listDirectory` for clarity. In both Azure and Google, `fileStatus` does not use a list operation. `fileListEntry` can be used when we must know if a directory exists. I just rewrote this from first principles because:; 1. In neither Google nor Azure did it check if the path was a directory and a file.; 2. In Google, if the directory entry wasn't in the first page, it would fail (NB: there are fifteen non-control characters in ASCII before `/`, if the page size is 15 or fewer, we'd miss the first entry with a `/` at the end).; 3. In Azure, we issued both a get and a list. There are now unit tests for this method. ---. 1. `copyMerge` and `concatenateFiles` previously used `O(N_FILES)` list operations, they now use `O(N_FILES)` get operations.; 2. Writers that used `exists` to check for a _SUCCESS file now use a get operation.; 3. Index readers, import BGEN, and import plink all now check file size with a get operation. That said, overall, the bulk of our Class A Operations are probably writes. fix test failures. passes tests. fixes. fix tests to not use fileStatus for folders. only file vs directory status matters. fix azure. azure dislikes %. finally get azure right. nix e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13883:243,avoid,avoidance,243,https://hail.is,https://github.com/hail-is/hail/pull/13883,1,['avoid'],['avoidance']
Safety,"CHANGELOG: Hail Query-on-Batch previously used Class A Operations for all interaction with blobs. This change ensures that QoB only uses Class A Operations when necessary. Inspired by @jigold 's file system improvement campaign, I pursued the avoidance of ""list"" operations. I anticipate this reduces flakiness in Azure (which is tracked in #13351) and cost in Azure. I enforced aiotools.fs terminology on hail.fs and Scala:. 1. `FileStatus`. Metadata about a blob or file. It does not know if a directory exists at this path. 2. `FileListEntry`. Metadata from a list operation. It knows if a directory exists at this path. Variable names were updated to reflect this distinction:. 1. `fileStatus` / `fileStatuses`. 2. `fle`/ `fles` / `fileListEntry` / `fileListEntries`, respectively. `listStatus` renamed to `listDirectory` for clarity. In both Azure and Google, `fileStatus` does not use a list operation. `getFileListEntry` can be used when we must know if a directory exists. I just rewrote this from first principles because:; 1. In neither Google nor Azure did it check if the path was a directory and a file.; 2. In Google, if the directory entry wasn't in the first page, it would fail (NB: there are fifteen non-control characters in ASCII before `/`, if the page size is 15 or fewer, we'd miss the first entry with a `/` at the end).; 3. In Azure, we issued both a get and a list. There are now unit tests for this method. ---. 1. `copyMerge` and `concatenateFiles` previously used `O(N_FILES)` list operations, they now use `O(N_FILES)` get operations.; 2. Writers that used `exists` to check for a _SUCCESS file now use a get operation.; 3. Index readers, import BGEN, and import plink all now check file size with a get operation. That said, overall, the bulk of our Class A Operations are probably writes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13452:243,avoid,avoidance,243,https://hail.is,https://github.com/hail-is/hail/pull/13452,1,['avoid'],['avoidance']
Safety,CHANGELOG: Require `orjson<3.9.12` to avoid a segfault introduced in orjson 3.9.12. See https://github.com/hail-is/hail/issues/14299 for details.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14300:38,avoid,avoid,38,https://hail.is,https://github.com/hail-is/hail/pull/14300,1,['avoid'],['avoid']
Safety,CHANGELOG: The `batch_size` parameter of `vds.new_combiner` is deprecated in favor of `gvcf_batch_size`. This avoids a confusing error message: https://dev.hail.is/t/vds-new-combiner/269/4?u=dking. @tpoterba do you think this is better?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12213:110,avoid,avoids,110,https://hail.is,https://github.com/hail-is/hail/pull/12213,1,['avoid'],['avoids']
Safety,"CHANGELOG: fix LocalBackend.run() succeeding when intermediate command fails. Stacked on #9219 as that PR is essentially approved, and to avoid a merge conflict. The commit in this PR is https://github.com/hail-is/hail/pull/9297/commits/cbc3bbe7f14c01d44c89995a03375d983fc14f4f. Caused by associativity of the ternary conditional ('set -e' + 'x' is the operand `a` in `a if cond else b`). Easy reproduction case on main:. ```python; def test_single_job_with_mixed_shells(self):; b = self.batch(); j = b.new_job(); j.command(f'echoddd ""hello""'); j2 = b.new_job(); j2.command(f'echo ""world""'). self.assertRaises(Exception, b.run); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9297:138,avoid,avoid,138,https://hail.is,https://github.com/hail-is/hail/pull/9297,1,['avoid'],['avoid']
Safety,"CI had restarted >150 times because every time it came up it tried to start some job pods. It appears that batch did not respond to these job start-ups, causing CI to hang. Unclear why CI didn't timeout. Or, perhaps, the timeout was longer than the health check timeout, so it was always restarted before the HTTP request timed out. [batch-describe.txt](https://github.com/hail-is/hail/files/2496565/batch-describe.txt); [batch.log](https://github.com/hail-is/hail/files/2496566/batch.log); [ci-describe.txt](https://github.com/hail-is/hail/files/2496567/ci-describe.txt); [ci.log](https://github.com/hail-is/hail/files/2496568/ci.log)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4584:195,timeout,timeout,195,https://hail.is,https://github.com/hail-is/hail/issues/4584,3,['timeout'],['timeout']
Safety,Can you point me to where you found `utf8mb4_0900_as_cs` is the correct collation? I'm pretty sure this change doesn't break any foreign key references (I don't think there are any) and just makes it safer for us that a user with a different case name can't impersonate someone else.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12276:200,safe,safer,200,https://hail.is,https://github.com/hail-is/hail/pull/12276,1,['safe'],['safer']
Safety,Change py4j version on getting started to detect any py4j,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1565:42,detect,detect,42,https://hail.is,https://github.com/hail-is/hail/issues/1565,1,['detect'],['detect']
Safety,"Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3MThjYjgyZC1jNGU3LTRlNWEtODgzZi02NjQ0NjlmYzA4MGEiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjcxOGNiODJkLWM0ZTctNGU1YS04ODNmLTY2NDQ2OWZjMDgwYSJ9fQ=="" width=""0"" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""718cb82d-c4e7-4e5a-883f-664469fc080a"",""prPublicId"":""718cb82d-c4e7-4e5a-883f-664469fc080a"",""dependencies"":[{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.11.2""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-JUPYTERSERVER-6099119""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[461],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Generation of Error Message Containing Sensitive Information](https://learn.snyk.io/lesson/error-message-with-sensitive-information/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14070:3569,remediat,remediationStrategy,3569,https://hail.is,https://github.com/hail-is/hail/pull/14070,1,['remediat'],['remediationStrategy']
Safety,Check types in numeric converters to avoid polluting the IR,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3190:37,avoid,avoid,37,https://hail.is,https://github.com/hail-is/hail/pull/3190,1,['avoid'],['avoid']
Safety,"Command:. hail read -i /user/aganna/annotated_test22.vds \; filtervariants expr \; --keep -c 'v.contig == ""22""' \; annotatevariants expr -c 'va.andrea.URV = (va.qc.nNonRef == 1 && va.exac.info.AC.isEmpty)' \; annotatevariants expr -c 'va.andrea.URVEXAC = (va.qc.nNonRef == 1 && (va.exac.info.AC.isEmpty || va.exac.info.AC[1] < 3))' \; exportvariants -c 'v, va.andrea.URV ,va.andrea.URVEXAC, va.qc.nNonRef' -o file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/test. Error:. [Stage 1:> (268 + 184) / 14326]Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 37 in stage 1.0 failed 30 times, most recent failure: Lost task 37.29 in stage 1.0 (TID 3338, dataflow02.broadinstitute.org): java.lang.IndexOutOfBoundsException: 1. [hail.log.txt](https://github.com/broadinstitute/hail/files/250349/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/367:563,abort,aborted,563,https://hail.is,https://github.com/hail-is/hail/issues/367,1,['abort'],['aborted']
Safety,"Compiled method (c1) 88328 14270 3 is.hail.annotations.Region::storeInt (6 bytes); total in heap [0x00007fbeaec3c810,0x00007fbeaec3cbc0] = 944; relocation [0x00007fbeaec3c938,0x00007fbeaec3c968] = 48; main code [0x00007fbeaec3c980,0x00007fbeaec3caa0] = 288; stub code [0x00007fbeaec3caa0,0x00007fbeaec3cb30] = 144; oops [0x00007fbeaec3cb30,0x00007fbeaec3cb38] = 8; metadata [0x00007fbeaec3cb38,0x00007fbeaec3cb48] = 16; scopes data [0x00007fbeaec3cb48,0x00007fbeaec3cb78] = 48; scopes pcs [0x00007fbeaec3cb78,0x00007fbeaec3cbb8] = 64; dependencies [0x00007fbeaec3cbb8,0x00007fbeaec3cbc0] = 8; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; FATAL: caught signal 6 SIGABRT; /tmp/libhail7224206977949339430.so(+0x1788c)[0x7fbdea5db88c]; /lib/x86_64-linux-gnu/libc.so.6(+0x33060)[0x7fbec2eae060]; /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xcf)[0x7fbec2eadfff]; /lib/x86_64-linux-gnu/libc.so.6(abort+0x16a)[0x7fbec2eaf42a]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8c0259)[0x7fbec27f0259]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0xa744f8)[0x7fbec29a44f8]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(JVM_handle_linux_signal+0x265)[0x7fbec27f9e45]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8bd4c8)[0x7fbec27ed4c8]; /lib/x86_64-linux-gnu/libpthread.so.0(+0x110c0)[0x7fbec38580c0]; [0x7fbeaec3ca22]; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [828e66d5a71741d7ab2c8d6580997da3] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""pyhail.py"", line 132, in <module>; main(args, pass_through_args); File ""pyhail.py"", line 113, in main; subprocess.check_output(job); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 573, in check_output; raise CalledProcessError(retcode, cmd, output=output); subprocess.CalledProcessError: Command '['gcloud', 'datap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4418:3471,abort,abort,3471,https://hail.is,https://github.com/hail-is/hail/issues/4418,1,['abort'],['abort']
Safety,Compute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806:2922,abort,abortStage,2922,https://hail.is,https://github.com/hail-is/hail/issues/1806,1,['abort'],['abortStage']
Safety,Compute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:8788,abort,abortStage,8788,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['abort'],['abortStage']
Safety,"ConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw); 443 try:; --> 444 httplib_response = conn.getresponse(); 445 except BaseException as e:; 446 # Remove the TypeError from the exception chain in; 447 # Python 3 (including for exceptions like SystemExit).; 448 # Otherwise it looks like a bug in the code. File /opt/conda/lib/python3.10/http/client.py:1375, in HTTPConnection.getresponse(self); 1374 try:; -> 1375 response.begin(); 1376 except ConnectionError:. File /opt/conda/lib/python3.10/http/client.py:318, in HTTPResponse.begin(self); 317 while True:; --> 318 version, status, reason = self._read_status(); 319 if status != CONTINUE:. File /opt/conda/lib/python3.10/http/client.py:287, in HTTPResponse._read_status(self); 284 if not line:; 285 # Presumably, the server closed the connection before; 286 # sending a valid response.; --> 287 raise RemoteDisconnected(""Remote end closed connection without""; 288 "" response""); 289 try:. ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last); File <timed exec>:9. File <decorator-gen-1235>:2, in export(self, output, types_file, header, parallel, delimiter). File ~/.local/lib/python3.10/site-packages/hail/typecheck/check.py:587, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 584 @decorator; 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_). File ~/.local/lib/python3.10/site-packages/hail/table.py:1153, in Table.export(self, output, types_file, header, parallel, delimiter); 1150 hl.current_backend().validate_file(output); 1152 parallel = ir.ExportType.default(parallel); -> 1153 Env.backend().execute(; 1154 ir.TableWrite(self._tir, ir.Ta",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:9986,abort,aborted,9986,https://hail.is,https://github.com/hail-is/hail/issues/13960,1,['abort'],['aborted']
Safety,"Current State; ---. When a new PR is created or the source SHA for a PR changes, a build is unconditionally started for that SHA merged with the latest target SHA. When the target SHA changes, all PR builds for that target SHA are killed. When a target SHA changes, the CI heals that target. When healing a target, the CI attempts to avoid n^2 unnecessary builds. It achieves this by serializing the build+merge of approved PRs for a given target. When there are no approved PRs, the CI will build every remaining PR with pending/`Buildable` status. If a PR is unapproved and there are a number of approved PRs, it is likely the PR will spend a significant amount of time as ""pending"" as it waits for the approved PRs to be merged. Desired State; ---. The CI should track if a source SHA has ever been tested (success or failure). If the target SHA changes, a build should only be killed if the source SHA has been successfully tested before. If the source SHA changes, a PR build should be killed regardless of whether the old source SHA has been successfully built before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4438:334,avoid,avoid,334,https://hail.is,https://github.com/hail-is/hail/issues/4438,1,['avoid'],['avoid']
Safety,"Currently the binding structure is redundantly specified in two places: Binds.scala, and the parser. We need the binding structure in the parser to propagate the environment, so we can annotate `Ref` nodes (and a few other things) with their types. But we can't use Binds.scala because we don't yet have an IR. This PR removes environment maintenance from the parser by deferring type annotation to a separate pass (which is simple, because it can use the Binds.scala infrastructure). One consequence is that we can't assign types to nodes like `Ref` during parsing, which means we can't ask for the type of any node during parsing, and by extension we can't ask for types of children in IR node constructors. Instead, all typechecking logic is moved to the `TypeCheck` pass. Some benefits of this change:; * The parser is simpler, as it doesn't have to maintain a typing environment.; * Binds.scala is now the single source of truth on the binding structure of the IR.; * Instead of typechecking being split in an ad-hoc way between IR constructors and the `TypeCheck` pass, all typechecking and type error reporting logic is in one place.; * The parser parses a context-free grammar, no more and no less. If the input is gramatically correct, the parser succeeds.; * We can round trip IR with type errors through the text representation. For instance, if we log an IR that fails TypeCheck, we can copy the IR from the log, parse it, then debug. This change was motivated by my work in progress to convert the parser to use the SSA grammar, which this should greatly simplify. I chose to make the type annotation pass after parsing mutate the IR in place (with the unfortunate exception of `Apply`, which can change into an `ApplyIR` or `ApplySpecial`. Do these really need to be separate nodes?). The type of a `Ref` node was already mutable to allow this sort of deferred annotation, and I've had to make a few other things mutable as well. Alternatively we could rebuild the entire IR to include t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13990:35,redund,redundantly,35,https://hail.is,https://github.com/hail-is/hail/pull/13990,1,['redund'],['redundantly']
Safety,"Currently writing a new block matrix to the same path leaves the old blocks in the folder, making them hard to delete. Adding an overwrite parameter fixes the issue and is safer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4051:172,safe,safer,172,https://hail.is,https://github.com/hail-is/hail/pull/4051,1,['safe'],['safer']
Safety,"Currently, constructing a `hl.Struct` is slower than I'd like, since it requires copying every field from the input `kwargs` into the `Struct` object's `__dict__`. This PR's main goal was to avoid the need to do that, by just using the input `kwargs` dict we were already saving as `_fields`. . When working on this, I also noticed that we allowed users to mutate fields of a struct, and we didn't do so in a consistent way (`s.a` vs `s[""a""]` could return different answers). I avoid that by throwing an error from now on if a user tries to modify one of the ""main"" fields of the struct (i.e. the ones that are in the `_fields` array).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10968:191,avoid,avoid,191,https://hail.is,https://github.com/hail-is/hail/pull/10968,2,['avoid'],['avoid']
Safety,"Currently, dev deploy always eventually returns a timeout in the terminal because it tries to wait for the entire batch deploy / tests to run before returning. Now it will just create the batch and return the batch number instead of waiting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6670:50,timeout,timeout,50,https://hail.is,https://github.com/hail-is/hail/pull/6670,1,['timeout'],['timeout']
Safety,"Currently, if we have a file structure like:. a/; b/; aa/; bb/. A glob pattern like `*/b` will raise FileNotFoundError beacuse; we try to list the file or folder named ""b"" inside `aa`. We should; not error. We should return `['a/b']`. It is insufficient to avoid FileNotFoundError altogether because; the Hadoop API treats paths without globs differently. In particular,; listing the path `aa/b` should raise an error. This change fixes behavior in the first case and treats the second; case explicitly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11905:257,avoid,avoid,257,https://hail.is,https://github.com/hail-is/hail/pull/11905,1,['avoid'],['avoid']
Safety,"Currently, jobs in hail batch can only be run on n1 machines but with the rise of deep learning in bioinformatics, the ability to run jobs on g2 machines, as well as other GPU supported machines, is an important and exciting addition to hail batch. This PR highlights the steps needed to add new machine types into hail batch and could be used as a template for further development support. . The changes in this PR can broadly be divided into additions to the job crun container and insertion of g2 resources (CPU, RAM, L4 Accelerator) into the resources table for billing. This PR uses the NVIDIA Container Toolkit, which allows the creation of GPU accelerated containers. This toolkit is integrated with docker via the parameters runtime=nvidia and the specification of GPUs is made through gpus all. The toolkit is installed in the batch worker VM startup script and the corresponding docker parameters are configured if the machine type is g2, so there is no change to the docker configuration for n1 machines. For the toolkit to work there is a nvidia hook that needs to be injected into the crun config. These modifications are also done based on machine type. On the billing side, the existing pricing setup was expanded to include g2 machines. The g2 instance cores and RAM are inserted into the database, and the SKUs are hard coded. For future machine type incorporation or updates, [https://cloud.google.com/skus/?currency=USD&filter=](https://cloud.google.com/skus/?currency=USD&filter=) may serve as a useful resource to identify relevant SKU ids. A new resource type was also added for the accelerator, including preemptible and non-preemtible. Finally, g2 machines mount the worker data disk under the name nvme0n2 so the code is updated to reflect this. Future work may want to investigate a way to automatically detect what the proper disk name is or make the disk naming logic more robust.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13430:1832,detect,detect,1832,https://hail.is,https://github.com/hail-is/hail/pull/13430,1,['detect'],['detect']
Safety,"Currently, the `mark_job_complete` SQL procedure deadlocks with itself, because the transaction takes a shared lock on the relevant row of the `batches` table, which it proceeds to try to upgrade to an exclusive lock later in the transaction when it calls `UPDATE batches `. The lock upgrade is not an issue in itself, but it will cause a deadlock if we run multiple `mark_job_complete` transactions for the same batch at the same time, which we clearly would like to do. One way to avoid this deadlock is to never issue an UPDATE to the batches table during the `mark_job_complete` procedure. Currently, we conduct the following updates to the batch row:; - increment number of completed jobs; - mark the batch as complete if the number of completed jobs is equal to the total number of jobs; - increment the number of successful/failed/cancelled jobs depending on the completion type of the job. This deadlock is a symptom of the problem that the `batches` table holds both very static information (like the billing project) and very volatile information like the number of completed jobs. It's not an issue to have many transactions holding read-only locks on rows of the batches table so long as it contains mostly static data. Therefore, it seems appropriate to move the job counters out of the batch table and into a new small table that just contains these counters. This eliminates all but one of the UPDATEs to the batch table in the procedure. The one sticky issue is marking the batch as complete. This is a much more popular column and I was hesitant to move it around just yet. I ended up moving the completeness update into a second transaction, which I think is not too bad for now. I also think that it could merge nicely with the following transaction that does the callback. Resolving this deadlock uncovered yet another deadlock underneath, which I want to tackle next. This PR does NOT resolve the issue that all mark job complete transactions are serialized due to the way we inc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352:484,avoid,avoid,484,https://hail.is,https://github.com/hail-is/hail/pull/11352,1,['avoid'],['avoid']
Safety,"Currently, the router-resolver returns 500 if the session id is invalid. Instead,; it should return 401. This collapses two states: not authorized due to not being; a developer and not authorized due to not being logged in. This is unfortunate, but; we should avoid leaking information as to *why* this endpoint is unauthorized to; an attacker. Developers, presumably, are knowledgable enough to figure out why; they cannot log in on their own.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8583:260,avoid,avoid,260,https://hail.is,https://github.com/hail-is/hail/pull/8583,1,['avoid'],['avoid']
Safety,"D=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">; ##INFO=<ID=MQRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities"">; ##INFO=<ID=QD,Number=1,Type=Float,Description=""Variant Confidence/Quality by Depth"">; ##INFO=<ID=RAW_MQ,Number=1,Type=Float,Description=""Raw data for RMS Mapping Quality"">; ##INFO=<ID=ReadPosRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias"">; ##INFO=<ID=SOR,Number=1,Type=Float,Description=""Symmetric Odds Ratio of 2x2 contingency table to detect strand bias"">; ##INFO=<ID=TYPE,Number=A,Type=String,Description=""The type of allele, either snp, mnp, ins, del, or complex."">; ##INFO=<ID=LEN,Number=A,Type=Integer,Description=""allele length"">; ##INFO=<ID=VCFALLELICPRIMITIVE,Number=0,Type=Flag,Description=""The allele was parsed using vcfallelicprimitives."">; ##INFO=<ID=TENX,Number=0,Type=Flag,Description=""called by 10X"">; ##INFO=<ID=POSTHPC,Number=.,Type=Integer,Description=""Postvariant homopolymer count"">; ##INFO=<ID=POSTHPB,Number=.,Type=Character,Description=""Postvariant homopolymer base"">; ##INFO=<ID=MUMAP_REF,Number=1,Type=Float,Description=""Mean mapping score of ref allele"">; ##INFO=<ID=MUMAP_ALT,Number=.,Type=Float,Description=""Mean mapping scores of alt alleles"">; ##INFO=<ID=AO,Number=.,Type=Integer,Description=""Alternate allele observed count"">; ##INFO=<ID=RO,Number=1,Type=Integer,Description=""Reference allele observed count"">; ##INFO=<ID=MMD,Number=.,Type=Float,Description=""Mean molecule divergence from reference per read",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:9311,detect,detect,9311,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['detect'],['detect']
Safety,DD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3790:6541,abort,abortStage,6541,https://hail.is,https://github.com/hail-is/hail/issues/3790,1,['abort'],['abortStage']
Safety,DD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:471); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055:5799,abort,abortStage,5799,https://hail.is,https://github.com/hail-is/hail/issues/4055,2,['abort'],['abortStage']
Safety,DD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3040:7046,abort,abortStage,7046,https://hail.is,https://github.com/hail-is/hail/issues/3040,1,['abort'],['abortStage']
Safety,DD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:7996,abort,abortStage,7996,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['abort'],['abortStage']
Safety,"D`, possibly with some non-empty key. This is consistent with the rule that the `rvd` must always have a stronger/longer key than the `TableType`.; * **small tweaks** - Now I start working through the `TableIR` nodes, rewriting them to remove explicit uses of `UnpartitionedRVD`. The general plan is to sandwich the rvd logic between `toOrderedRVD` and `toOldStyleRVD`. The first takes an `UnpartitionedRVD` to an `OrderedRVD` with empty key (and leaves `OrderedRVD`s alone), and the second takes an `OrderedRVD` to an `UnpartitionedRVD` if its key was empty, and leaves it alone otherwise. Once they're all rewritten this way, I redefine `toOldStyleRVD` to always return `OrderedRVD`, and `UnpartitionedRVD` is no longer used.; * **remove `TableUnkey`** - With `UnpartitionedRVD` going away, `TableUnkey` is no longer necessary, it's equivalent to keying by an empty key.; * **small tweaks** - these next two rewrite more `TableIR` nodes; * **Merge master** - the big one; * **tweak MatrixColsTable** - 1) Optimize `coerce` by checking if the requested key is empty, avoiding a scan in that case. 2) Optimize `sortedColsValue` by checking if the column key is empty, avoiding the sort in that case. 3) Simplify `colsRVD`, removing the case on the type of the `RVD`, just calling `coerce` and letting the previous optimizations avoid unnecessary work.; * **`distinctByKey` fix** - While looking over `TableIR` implementations, I noticed a bug in `distinctByKey`: you need to be sure no key is split across multiple partitions. To be sure the empty key edge case still works, I added a test to check that `strictify` on an empty-key partitioner will always collapse everything to one partition.; * **Flipped switch** - redifines `toOldStyleRVD` to just return the `OrderedRVD` unchanged, and asserts that `TableValue.rvd` is always an `OrderedRVD`.; * **rest of the `TableIR` tweaks** - added a factory method `OrderedRVD.unkeyed` to replace `UnpartitionedRVD.apply`.; * the rest are simple tidying up",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4319:1994,avoid,avoiding,1994,https://hail.is,https://github.com/hail-is/hail/pull/4319,2,['avoid'],"['avoid', 'avoiding']"
Safety,DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 10 more; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at is.hail.sparkextras.ContextRDD.aggregate(ContextRDD.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:3009,abort,abortStage,3009,https://hail.is,https://github.com/hail-is/hail/issues/4215,1,['abort'],['abortStage']
Safety,"Defeat the optimizer, which is unaware of these partitioners and misoptimizes; to operations that require shuffles. The first change is easy. I added `RVDPartitioner.keysIfOneToOne` which looks; for these kinds of partitioners in the special case of keys consisting of 32-; and 64-bit integers. The second change eluded me for a long time. Finally, I discovered; `isSorted=true` and realized the optimizer refuses to modify such; `TableKeyBy`s. I exposed this field in Python as: `Table._key_by_assert_sorted`. With this infrastructure in place, I was able to implement read, write, and; matrix-multiply for DNDArray!. In addition, to the arguable hacks above, a couple pain points remain:; 1. I do not know how to rename keys in Python without triggering shuffles. If I; write `key_by(x=t.y, y=t.x)`, Hail implements this as; `TableKeyBy(TableMapRows(TableKeyBy(Array(), ...)`. The inner key by throws; the keys away so that they can be modified with TableMapRows. Unfortunately,; this completely defeats my attempts to avoid shuffles. I avoid this issue by; not using fixed names for the x and y block coordinates (their names are; stored in `x_field` and `y_field`).; 2. Hail lacks `ndarray_sum`. Instead, I convert from ndarray to array so that I; can use `array_sum`. Unfortunately, this operation seems to completely; dominate all of my time. It takes about 10x as much time as the matrix; multiplies take. I do not understand this. I should be reading the entries in; column-major order. Performance; -----------. ```; In [1]: %%time; ...: import hail as hl; ...: mt = hl.balding_nichols_model(n_populations=2,; ...: n_variants=10000,; ...: n_samples=10000,; ...: n_partitions=100); ...: mt = mt.select_entries(gt = hl.float(mt.GT.n_alt_alleles())); ...: da = hl.experimental.dnd.array(mt, 'gt'); ...: da.write('/tmp/in.da', overwrite=True); In [3]: %%time; ...: bm = hl.linalg.BlockMatrix.from_entry_expr(mt.gt); In [5]: %%time; ...: (bm @ bm.T).write('/tmp/foo.bm', overwrite=True); In [7]: %",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8864:1510,avoid,avoid,1510,https://hail.is,https://github.com/hail-is/hail/pull/8864,1,['avoid'],['avoid']
Safety,"Definitely not ready to go in, but functionally working and safe to take a look. Did not change docs / python at all, so don't look there yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1984:60,safe,safe,60,https://hail.is,https://github.com/hail-is/hail/pull/1984,1,['safe'],['safe']
Safety,"Despite the pretty daunting diff I think this is a pretty ""small"" change and perhaps an easier one start out with in the migration from nginx -> envoy than gateway/internal-gateway (certainly less risky). I've tested these manually by make deploying and they work both in dev namespaces and default. Currently, the grafana and prometheus pods have two containers: the app itself (grafana or prometheus) and an nginx container that sits in front of it. The flow is as follows, and since this works the exact same for both prometheus and grafana I will just talk about grafana as the example and the same thing should apply to both:. 1. User sends request to grafana.hail.is; 2. Gateway sees an HTTP request going to a production service and forwards that request to the grafana k8s Service port 443; 3. The grafana K8s Service forwards that request to the grafana pod port 443; 4. Nginx is listening on port 443 in the grafana pod and receives that request. It makes an authorization check to auth to make sure that the request is coming from a developer; 5. Nginx forwards that request to 127.0.0.1:3000, which is where grafana is listening. This PR does not change any behavior, just replaces Nginx with Envoy. Currently, building the nginx container involves running jinja on its config files and building a docker image. With envoy, we can just use the `envoyproxy/envoy` image from DockerHub (which I have copied into our container registries) and feed it a single configmap. The big mess of yaml that is the new configmap for envoy has a lot of boilerplate, but it comprises of the following sections which hopefully on their own are not too bad. ### Envoy config; 1. The top of the `listeners` section shows that Envoy is listening on port 8443 (which is the port that the k8s `Service` will now forward traffic to); 2. The `virtual_hosts` section shows that Envoy will send all paths (prefix ""/"") to the cluster `grafana`; 3. The `http_filters` section says that Envoy will first send an author",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12364:197,risk,risky,197,https://hail.is,https://github.com/hail-is/hail/pull/12364,1,['risk'],['risky']
Safety,"Disables the test `test_tree_matmul_splits`, which seems to be leaking memory, and causing tests run after it to timeout. Disabling to allow CI to make progress on other PRs, but we do need to diagnose the actual issue here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14550:113,timeout,timeout,113,https://hail.is,https://github.com/hail-is/hail/pull/14550,1,['timeout'],['timeout']
Safety,"Due to bytecode verification rules, an allocated but uninitalized object cannot be stored into a field, so the NEW and INVOKESPECIAL constructor call bytecodes cannot be split across methods. Therefore, I modified newInstance to fuse those operations together. I broke out control simplification and made it a stronger. Added method splitting. Currently, method splitting splits out basic blocks into their own, straight-line methods and all the control flow remains in the original method. All locals are spilled to fields which is terrible, but what we're doing now. I expect two changes in the future: recover the structured control flow (there are standard algorithms for this) so we can split out control flow, and use the dataflow analysis from InitializeLocals to only spill locals split across method boundaries. I will make a stacked PR on this that removes method wrapping from Emit and enables lir method splitting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8958:605,recover,recover,605,https://hail.is,https://github.com/hail-is/hail/pull/8958,1,['recover'],['recover']
Safety,"During a test that created 30,000 pods a number of pods timed out waiting for `gsa-key` or `default-token-8h99c` to mount. Example:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```. All events not containing the string ""Successfully created batch-pods"" [events.log](https://github.com/hail-is/hail/files/3350369/events.log)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6546:279,timeout,timeout,279,https://hail.is,https://github.com/hail-is/hail/issues/6546,1,['timeout'],['timeout']
Safety,ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2861); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2792); 	at org.apache.spark.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:10116,abort,abortStage,10116,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['abort'],['abortStage']
Safety,Example transient error:. ```; E hail.utils.java.FatalError: batch id was 2301842; E IllegalStateException: Timeout on blocking read for 30000000000 NANOSECONDS; E java.lang.IllegalStateException: Timeout on blocking read for 30000000000 NANOSECONDS; E at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:123); E at reactor.core.publisher.Mono.block(Mono.java:1727); E at com.azure.storage.common.implementation.StorageImplUtils.blockWithOptionalTimeout(StorageImplUtils.java:130); E at com.azure.storage.blob.specialized.BlobClientBase.downloadStreamWithResponse(BlobClientBase.java:731); E at is.hail.io.fs.AzureStorageFS$$anon$1.fill(AzureStorageFS.scala:152); E at is.hail.io.fs.FSSeekableInputStream.read(FS.scala:141); E at java.io.DataInputStream.read(DataInputStream.java:149); E at is.hail.utils.richUtils.RichInputStream$.readRepeatedly$extension0(RichInputStream.scala:21); E at is.hail.utils.richUtils.RichInputStream$.readFully$extension1(RichInputStream.scala:12); E at is.hail.io.StreamBlockInputBuffer.readBlock(InputBuffers.scala:546); E at is.hail.io.LZ4SizeBasedCompressingInputBlockBuffer.readBlock(InputBuffers.scala:608); E at is.hail.io.BlockingInputBuffer.readBlock(InputBuffers.scala:382); E at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:388); E at is.hail.io.BlockingInputBuffer.readByte(InputBuffers.scala:405); E at is.hail.io.LEB128InputBuffer.readByte(InputBuffers.scala:217); E at is.hail.io.LEB128InputBuffer.readInt(InputBuffers.scala:223); E at __C173548Compiled.__m174060INPLACE_DECODE_r_binary_TO_r_binary(Emit.scala); E at __C173548Compiled.__m174059DECODE_r_struct_of_r_binaryEND_TO_SBaseStructPointer(Emit.scala); E at __C173548Compiled.__m174058begin_group_0(Emit.scala); E at __C173548Compiled.__m174057begin_group_0(Emit.scala); E at __C173548Compiled.__m173566split_Let(Emit.scala); E at __C173548Compiled.apply(Emit.scala); E at is.hail.expr.ir.lowering.LowerToCDA$.$anonfun$lower$2(LowerToCDA.scala:51,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12261:108,Timeout,Timeout,108,https://hail.is,https://github.com/hail-is/hail/pull/12261,2,['Timeout'],['Timeout']
Safety,ExecuteContext(SparkBackend.scala:229); 			at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); 			at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); 			at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 			at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 			at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 			at java.lang.reflect.Method.invoke(Method.java:498); 			at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 			at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 			at py4j.Gateway.invoke(Gateway.java:282); 			at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 			at py4j.commands.CallCommand.execute(CallCommand.java:79); 			at py4j.GatewayConnection.run(GatewayConnection.java:238); 			at java.lang.Thread.run(Thread.java:748). 	Hail version: 0.2.44-6cfa355a1954; 	Error summary: SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:15673,abort,aborted,15673,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['abort'],['aborted']
Safety,ExtractAggregators uses a `Ref` with a null type (which is set before; returning to its callee) to avoid traversing the IR tree twice because; the type of the aggregation result is not known until all; the aggregations have been seen. A non lazy call to `typ` will see the null type (and blow up with an; NPE) when an `ApplySpecial`/`Apply` IR node is copied to replace a; leaf IR aggregation node with a `Ref` to the aggregation result; struct.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3576:99,avoid,avoid,99,https://hail.is,https://github.com/hail-is/hail/pull/3576,1,['avoid'],['avoid']
Safety,"FO: Found 729 overlapping samples; Left: 729 total samples; Right: 729 total samples; 2018-01-17 18:32:10 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 0:====================================================>(4627 + 1) / 4628]2018-01-17 18:47:04 Hail: INFO: Coerced sorted dataset; 2018-01-17 18:47:04 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:1846,abort,aborted,1846,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['abort'],['aborted']
Safety,"FYI @konradjk . I want to do a bit more testing but this should be ready later today for you to play with. I will ping you when it is ready and send instructions and some potential gotchas. For the most part it is self-explanatory:. ```; from hailtop import pipeline. p = pipeline.Pipeline(; backend=pipeline.GoogleBackend(; service_account='...',; scratch_dir='gs://hail-cseed/pipeline/tmp',; worker_cores=1,; worker_disk_size_gb='20',; pool_size=3,; max_instances=1000),; default_image='ubuntu:18.04'). input = p.read_input('gs://hail-cseed/cs-hack/input.txt'). t1 = p.new_task('concat'); t1.command(f'cp {input} {t1.ofile} && echo ""end"" >> {t1.ofile}'). t2 = p.new_task('sum'); t2.command(f'sum {t1.ofile} > {t2.sum}'). p.write_output(t2.sum, 'gs://hail-cseed/cs-hack/sum.txt'). p.run(); ```. You have to run this in a VM with a custom image. pool_size is the (maximum) number of active workers. max_instances is a cap on running instances to avoid blowing out CPU quota if you're close to the limit and we're creating new instances while others are shutting down. @jigold I'm not 100% sure this should go in. It stores resources in gs://hail-common (worker startup scripts, etc.) and to do this right we'll need to test deployments, version files, etc. It might make sense just to keep this as a reference and steal what you can from it for batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6772:946,avoid,avoid,946,https://hail.is,https://github.com/hail-is/hail/pull/6772,1,['avoid'],['avoid']
Safety,"FatalError: IllegalArgumentException: Zero-length interval cannot be lifted over. Interval: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 21, ap01-sw-wdp1.c.topmed-kathiresan-lipids-wgs.internal, executor 7): java.lang.IllegalArgumentException: Zero-length interval cannot be lifted over. Interval: null; at htsjdk.samtools.liftover.LiftOver.liftOver(LiftOver.java:137); at is.hail.io.reference.LiftOver.queryInterval(LiftOver.scala:61); at is.hail.variant.ReferenceGenome.liftoverLocusInterval(ReferenceGenome.scala:435); at is.hail.codegen.generated.C11.method1(Unknown Source); at is.hail.codegen.generated.C11.apply(Unknown Source); at is.hail.codegen.generated.C11.apply(Unknown Source); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:627); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:626); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1264); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1258); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$JoinIterator.next(Iterator.scala:232); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1138); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5174:154,abort,aborted,154,https://hail.is,https://github.com/hail-is/hail/issues/5174,1,['abort'],['aborted']
Safety,FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: sun.reflect.generics.reflectiveObjects.NotImplementedException; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 10 more; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:1781,Unsafe,UnsafeRow,1781,https://hail.is,https://github.com/hail-is/hail/issues/4215,1,['Unsafe'],['UnsafeRow']
Safety,"File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.Par",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:2550,Unsafe,UnsafeRow,2550,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['Unsafe'],['UnsafeRow']
Safety,"First error was we were not cancelling the batch on TimeoutErrors. I kept the interface to timeout on the client side rather than passing the timeout to the service spec. We can revisit this design at another point. I also added another layer of tasks to make sure we were cancelling batches in the case that a submit failed. Some coroutines may have already succeeded and created a BatchPoolFuture, which we need to cancel. We need to use tasks instead of coroutines as the input to `gather` because on at least one failure we need to know if the submit task was completed successfully to extract the BatchPoolFuture to cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10696:52,Timeout,TimeoutErrors,52,https://hail.is,https://github.com/hail-is/hail/pull/10696,3,"['Timeout', 'timeout']","['TimeoutErrors', 'timeout']"
Safety,"First, I changed import_vcfs to return a MatrixTable only keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5424:143,avoid,avoid,143,https://hail.is,https://github.com/hail-is/hail/pull/5424,1,['avoid'],['avoid']
Safety,Fix AggregateRows to use SafeRow,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3488:25,Safe,SafeRow,25,https://hail.is,https://github.com/hail-is/hail/pull/3488,1,['Safe'],['SafeRow']
Safety,"Fix key detection with ""drop""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3252:8,detect,detection,8,https://hail.is,https://github.com/hail-is/hail/pull/3252,1,['detect'],['detection']
Safety,Fix unsafe ordering,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3256:4,unsafe,unsafe,4,https://hail.is,https://github.com/hail-is/hail/pull/3256,1,['unsafe'],['unsafe']
Safety,Fixed bug in unsafe ordering.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2199:13,unsafe,unsafe,13,https://hail.is,https://github.com/hail-is/hail/pull/2199,1,['unsafe'],['unsafe']
Safety,"Fixes #14262. Ever since starting to control job network namespaces ourselves, we run the worker container with `--network host`. But running with the host's network namespace means there's no need (nor meaning) to use port forwarding rules with `-p`. Docker safely ignores this redundant setting but emit some log messages like:. ```; WARNING: Published ports are discarded when using host network mode; ```. The solution here is to just remove the old port publishing settings.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14252:259,safe,safely,259,https://hail.is,https://github.com/hail-is/hail/pull/14252,2,"['redund', 'safe']","['redundant', 'safely']"
Safety,Fixes #14634. Always prompt for which google account to use during login. Avoids confusion over whether logout succeeded or not (especially considering #14635),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14636:74,Avoid,Avoids,74,https://hail.is,https://github.com/hail-is/hail/pull/14636,1,['Avoid'],['Avoids']
Safety,"Fixes #3920. I'm a bit dubious on `collectPerPartitions` because it can only be safely used if there's a `ctx.region.clear` in the right spot. This should avoid some issues that caitlin was experiencing. The root issue is that `clearingRun` clears once per item, but, after the `cmapPartitions` there is only one item per partition. That item was produced by iterating through every element (with `it.foreach`) and accumulating some state. When `it.foreach` is finished, the entire partition will be in memory. On sufficiently large datasets, YARN will kill the executors for exceeding memory limits. We previously observed these errors coming from Java, but now that the regions are in native code, there are no JVM limits, the memory usage is instead noticed by YARN at the container-level. The fix is to clear after we are finished with each row, i.e. before the lambda passed to `foreach` returns.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3921:80,safe,safely,80,https://hail.is,https://github.com/hail-is/hail/pull/3921,2,"['avoid', 'safe']","['avoid', 'safely']"
Safety,Fixes #6888. Also simplify code to remove redundant checks.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6897:42,redund,redundant,42,https://hail.is,https://github.com/hail-is/hail/pull/6897,1,['redund'],['redundant']
Safety,"Fixes #9640. CHANGELOG: Fixed bug where show output for ndarrays was not correct. `TNDArray` shouldn't store strides on it. It only existed because of the old world where `PTypes` weren't fully fleshed out. Especially since `strides` is a series of offsets in bytes, and at the virtual type level we have no idea what elements of the ndarray will actually be. . The way I've fixed this for now is by making all incoming ndarray literals row major and all Java `Row` representation style things use `UnsafeIndexedSeqRowMajorView` to make them indexable in arow major way. I mostly just did this since row by row is the way you'd want to display data for `show`. My eventual plan is reorganize the ndarray emitter so we know that all ndarrays are being emitted column major all the time, and then we won't have to do this view wrapper thing since we will know what striding to expect. . This change is also good because now that strides are off of `TNDArray`/literals, there should be nothing preventing us from passing ndarrays of arbitrary data types back to Python from the JVM (currently we only support collecting ndarrays of primitives).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9641:499,Unsafe,UnsafeIndexedSeqRowMajorView,499,https://hail.is,https://github.com/hail-is/hail/pull/9641,1,['Unsafe'],['UnsafeIndexedSeqRowMajorView']
Safety,"Fixes connection timeout after 8 hours. . When we transition to aiomysql, will port well to a pooled connection version (`async with self.pool.acquire() as conn:`. Even now however, the time it takes to acquire a connection is not the bottleneck during login. cc @danking assigned you as well in case you're on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5815:17,timeout,timeout,17,https://hail.is,https://github.com/hail-is/hail/pull/5815,1,['timeout'],['timeout']
Safety,"Fixes: #14247. Issues resolved herein:. 1. build.yaml tests must not use `exit 0` as it exits the test early.; 2. Always prefer `orjson` to `json`.; 3. Add `--wait` which waits for the submitted batch to complete and exits success only when the batch is success.; 4. Whenever working with paths, we must use the `realpath` which resolves symlinks. In particular, on Mac OS X, `/tmp` is a symlink to `/private/tmp` and Python's APIs are inconsistent on whether they return a realpath or a path with symlinks. [1]; 5. If the destination looks like a directory (e.g. ""bar:/foo/"", ""bar:/""), the tests all suggest we should copy *into* not *to*. We now check for a trailing slash and copy *into*.; 6. `ln -s src dst` means different things depending on whether dst is an extant folder or not. In this PR, I prefer to always be fully explicit so I never rely on `ln` detecting the destination is a directory and acting differently. Put differently: `file_input_to_src_dest` now never returns a file source and a destination folder.; 7. We need to create the `real_absolute_cwd()` on the job before we `cd` into it.; 8. `test_dir_outside_curdir` suggests that `--file foo/:/` is meant to copy the contents of foo into the root. This cannot be implemented with our symlink strategy (you can't replace the root with a symlink), so I changed the interpretation: a trailing slash on the source is meaningless. If the destination ends in a slash, we ""copy into"", otherwise we ""copy to"".; 9. Add examples of --files usage. [1]:. ```ipython3; In [1]: import tempfile; ...: tempfile.TemporaryDirectory(); Out[1]: <TemporaryDirectory '/var/folders/x1/601098gx0v11qjx2l_7qfw2c0000gq/T/tmp_pmj3lr9'>. In [2]: import os; ...: os.getcwd(); Out[2]: '/private/tmp'. In [3]: !ls -al /tmp; lrwxr-xr-x@ 1 root wheel 11 Aug 2 05:44 /tmp -> private/tmp; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14186:861,detect,detecting,861,https://hail.is,https://github.com/hail-is/hail/pull/14186,1,['detect'],['detecting']
Safety,Fixes: #14559; `hl.nd.array`s constructed from stream pipelines can cause out of memory exceptions owing to a limitation in the python CSE algorithm that does not eliminate partially redundant expressions in if-expressions. Explicitly `let`-binding the input collection prevents it from being evaluated twice: once for the flattened data stream and once for the original shape.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14571:183,redund,redundant,183,https://hail.is,https://github.com/hail-is/hail/pull/14571,1,['redund'],['redundant']
Safety,"Fixes:. ```. async def test_billing_monitoring():; deploy_config = get_deploy_config(); monitoring_deploy_config_url = deploy_config.url('monitoring', '/api/v1alpha/billing'); headers = service_auth_headers(deploy_config, 'monitoring'); async with in_cluster_ssl_client_session(; raise_for_status=True,; timeout=aiohttp.ClientTimeout(total=60)) as session:; ; async def wait_forever():; data = None; while data is None:; resp = await utils.request_retry_transient_errors(; session, 'GET', f'{monitoring_deploy_config_url}', headers=headers); data = await resp.json(); await asyncio.sleep(5); return data; ; data = await asyncio.wait_for(wait_forever(), timeout=30 * 60); > assert data['cost_by_service'], str(data); E AssertionError: {'cost_by_service': [], 'compute_cost_breakdown': [], 'cost_by_sku_label': [], 'time_period_query': '09/2020'}; E assert []; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9389:304,timeout,timeout,304,https://hail.is,https://github.com/hail-is/hail/pull/9389,2,['timeout'],['timeout']
Safety,"For Hail Batch on Terra Azure, the production artifact is Helm chart containing the necessary kubernetes resources to run a Hail Batch deployment in a Terra k8s cluster. This deployment contains slightly modified containers of the batch front-end, batch driver and a mysql database. This chart is currently built manually using the targets in `batch/terra-chart/Makefile`. As this process is not currently automatically tested, it's very prone to bit rot. This PR is an amalgamation of fixes that I needed to make to get `main` to build in the current Terra. A non-exhaustive list of the changes are:. - After changing from gradle to mill, the some Dockerfiles and make targets needed to change to account for the new location of the JAR.; - I removed some redundancy in invocations of `docker build` by relying on the generic targets that we now have in the top level Makefile.; - Terra changed how they handle identity management for the kubernetes deployment, from `aadpodidentity` to `workloadIdentity`. This changes the chart to work with their new inputs they provide. Ultimately, terra should have a CI system that we can push charts to and receive feedback on whether it passed our test suite in their test environment.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14450:757,redund,redundancy,757,https://hail.is,https://github.com/hail-is/hail/pull/14450,1,['redund'],['redundancy']
Safety,Found this while working on unsafe stuff.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1972:28,unsafe,unsafe,28,https://hail.is,https://github.com/hail-is/hail/pull/1972,1,['unsafe'],['unsafe']
Safety,"From @armartin on a pretty simple line of code (ukbb was just loaded from bgen, tgp was just ld_pruned, but `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3447:342,abort,aborted,342,https://hail.is,https://github.com/hail-is/hail/issues/3447,3,['abort'],"['abortStage', 'aborted']"
Safety,"Furthermore, update build.gradle to (finally) not print all the; Unsafe warnings.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9221:65,Unsafe,Unsafe,65,https://hail.is,https://github.com/hail-is/hail/pull/9221,1,['Unsafe'],['Unsafe']
Safety,"GCP Logging attempts to infer the severity of a log entry and defaults to labelling everything written to `stdout` as `INFO` and everything written to `stderr` as `ERROR`. There are some [LogEntry fields](https://cloud.google.com/logging/docs/structured-logging) that can be overwritten by fields in our JSON output, including `severity`. Until now we had been logging the severity level as `levelname`, which is the expectation of `jsonlogger`, but this means GCP's levels and ours do not necessarily match. This adds another `severity` field to the logs so GCP can pick up the level. This should get rid of a swath of non-error log messages written to `stderr` (only JSON though) and lets other levels like `WARNING` get marked as such. GCP does quite literally extract the severity field though, so it does not appear in the `jsonPayload`. I've kept the `levelname` in then as a sanity check but am also happy to try to remove it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9910:882,sanity check,sanity check,882,https://hail.is,https://github.com/hail-is/hail/pull/9910,1,['sanity check'],['sanity check']
Safety,"GES.rst"">importlib-metadata's changelog</a>.</em></p>; <blockquote>; <h1>v4.11.3</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/372"">#372</a>: Removed cast of path items in FastPath, not needed.</li>; </ul>; <h1>v4.11.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/369"">#369</a>: Fixed bug where <code>EntryPoint.extras</code> was returning; match objects and not the extras strings.</li>; </ul>; <h1>v4.11.1</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/367"">#367</a>: In <code>Distribution.requires</code> for egg-info, if <code>requires.txt</code>; is empty, return an empty list.</li>; </ul>; <h1>v4.11.0</h1>; <ul>; <li>bpo-46246: Added <code>__slots__</code> to <code>EntryPoints</code>.</li>; </ul>; <h1>v4.10.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/365"">#365</a> and bpo-46546: Avoid leaking <code>method_name</code> in; <code>DeprecatedList</code>.</li>; </ul>; <h1>v4.10.1</h1>; <h1>v2.1.3</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/361"">#361</a>: Avoid potential REDoS in <code>EntryPoint.pattern</code>.</li>; </ul>; <h1>v4.10.0</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/354"">#354</a>: Removed <code>Distribution._local</code> factory. This; functionality was created as a demonstration of the; possible implementation. Now, the; <code>pep517 &lt;https://pypi.org/project/pep517&gt;</code>_ package; provides this functionality directly through; <code>pep517.meta.load &lt;https://github.com/pypa/pep517/blob/a942316305395f8f757f210e2b16f738af73f8b8/pep517/meta.py#L63-L73&gt;</code>_.</li>; </ul>; <h1>v4.9.0</h1>; <ul>; <li>Require Python 3.7 or later.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11596:1253,Avoid,Avoid,1253,https://hail.is,https://github.com/hail-is/hail/pull/11596,1,['Avoid'],['Avoid']
Safety,"GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527:1816,abort,abortStage,1816,https://hail.is,https://github.com/hail-is/hail/issues/2527,1,['abort'],['abortStage']
Safety,"Getting a sporadic task failure, with the error:; ```; ExecutorLostFailure (executor 99 exited caused by one of the running tasks) Reason: Container marked as failed: container_1519994715701_0003_01_000102 on host: exomes-sw-pxt3.c.broad-mpg-gnomad.internal. Exit status: 134. Diagnostics: Exception from container-launch.; Container id: container_1519994715701_0003_01_000102; Exit code: 134; Exception message: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053:437,Abort,Aborted,437,https://hail.is,https://github.com/hail-is/hail/issues/3053,1,['Abort'],['Aborted']
Safety,"Given we have only gone a hair over 50% lately, 3 should be safe. cc: @daniel-goldstein",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11506:60,safe,safe,60,https://hail.is,https://github.com/hail-is/hail/pull/11506,1,['safe'],['safe']
Safety,"Google Cloud Storage documentation and [best practices] for object names; recommends avoiding sequential names like 'part-0nnnn'. We already use; UUIDs for randomness to avoid two tasks writing to the exact same; object, but by using the UUID as a prefix rather than a suffix we; (to a degree) uniformly distribute part file names over a range,; (hopefully) improving throughput. [best practices]: https://cloud.google.com/storage/docs/best-practices#naming",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10836:85,avoid,avoiding,85,https://hail.is,https://github.com/hail-is/hail/pull/10836,2,['avoid'],"['avoid', 'avoiding']"
Safety,"Had to cast toLong to avoid overflow on multiplication, cast back to Int because number of partitions must be an integer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1358:22,avoid,avoid,22,https://hail.is,https://github.com/hail-is/hail/pull/1358,1,['avoid'],['avoid']
Safety,"Hail fails when trying to filter a MatrixTable based on locus position; -----------------------------------------------------------------------------; To reproduce; ```python; import hail as hl; mt = hl.import_vcf(""http://hgdownload.cse.ucsc.edu/gbdb/hg19/1000Genomes/phase3/ALL.chrY.phase3_integrated_v1a.20130502.genotypes.vcf.gz"", force_bgz=True); ----------------------------------------------------------------------; Initializing Hail with default parameters...; 2022-10-06 15:56:03 WARN Utils:69 - Your hostname, nid resolves to a loopback address: 127.0.1.1; using 192.168.248.80 instead (on interface wlp0s20f3); 2022-10-06 15:56:03 WARN Utils:69 - Set SPARK_LOCAL_IP if you need to bind to another address; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/med/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 2022-10-06 15:56:03 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.1.3; SparkUI available at http://192.168.248.80:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.100-2ea2615a797a; LOGGING: writing to /; --------------------------------------------------------------------------; mt.filter_rows(mt.locus.position==2867101).count_rows(); ```; ### Expected ; Return a count of rows with that condition. ### Error ; ```; FatalError: Assertio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12280:834,unsafe,unsafe,834,https://hail.is,https://github.com/hail-is/hail/issues/12280,1,['unsafe'],['unsafe']
Safety,"Hail should have a ""sanity check"" operation",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5163:20,sanity check,sanity check,20,https://hail.is,https://github.com/hail-is/hail/issues/5163,1,['sanity check'],['sanity check']
Safety,"Happened on VDS with small number of partitions (18) but large number of variants (~150mio). [Stage 0:=============================> (1 + 1) / 2]hail: info: running: vep --force --config /home/users/cseed/vep.properties; [Stage 1:======================================> (12 + 6) / 18]hail: vep: caught exception: Job aborted due to stage failure: Task 17 in stage 1.0 failed 4 times, most recent failure: Lost task 17.3 in stage 1.0 (TID 22, nid00019.urika.com): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:836); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:125); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:113); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:127); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:134); at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:512); at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:429); at org.apache.spark.storage.BlockManager.get(BlockManager.scala:618); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44); at org.apache.spark.rdd.RDD.iterator(RDD.scala:262); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/430:317,abort,aborted,317,https://hail.is,https://github.com/hail-is/hail/issues/430,1,['abort'],['aborted']
Safety,"Here's the error:. ```; 2427:2016-12-07 16:34:33 ERROR TaskSetManager:75 - Task 257 in stage 3.0 failed 4 times; aborting job; 2435:2016-12-07 16:34:33 ERROR Hail:93 - hail: annotatesamples expr: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 257 in stage 3.0 failed 4 times, most recent failure: Lost task 257.3 in stage 3.0 (TID 590, nid00026.urika.com): scala.MatchError: ArrayBuffer(3.549E-4) (of class scala.collection.mutable.ArrayBuffer); ```. Log: /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log. Here's the full pipeline:. ```; /mnt/lustre/tpoterba/bin/hail -l /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log \; 	read -i file:///mnt/lustre/gtiao/PCAWG/data/PCAWG.full_callset.chr_ALL.GQ20_AB.split.updated.WGS_1KG_tissue_annot.promoters.QCed.vds \; 	annotatesamples table -i file:///mnt/lustre/gtiao/PCAWG/germline_callset/housekeeping/Broad_callset.115k_SNP.8PC.ethnicity_inference.txt \; 	-e Sample -c 'sa.annots.Ethnicity = table.Ethnicity' \; 	annotatesamples expr -c 'sa.AF_hist = gs.filter(g => g.isCalledNonRef).map(g => va.info.AF).hist(0, 1, 100)' \; 	annotateglobal expr -c 'global.AF_hist = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	exportsamples -c 'SAMPLE = s.id, AF_hist = sa.AF_hist, Ethnicity = sa.annots.Ethnicity, Tissue = sa.annots.tissue_type' \; 	-o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.txt \; 	filtersamples expr -c '(sa.annots.tissue_type != ""BRCA"") && (sa.annots.Ethnicity == ""EUR"")' --keep \; 	filtersamples expr --keep -c 'samples.collect().sortBy(x => runif(0.0, 1.0))[:250]' \; 	annotateglobal expr -c 'global.AF_hist.iter1 = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	variantqc filtervariants expr -c 'va.qc.AC >= 1' --keep \; 	exportvariants -o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.iter1.promoter_variants.txt \; 	-c 'CHROM = v.contig, POS = v.start, REF = v.ref, ALT = v.alt, TARGET = va.promoter_target, AC = va.qc.AC, AC_To",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1151:113,abort,aborting,113,https://hail.is,https://github.com/hail-is/hail/issues/1151,2,['abort'],"['aborted', 'aborting']"
Safety,"Hi!. Trying to calculate polygenic risk score with code from the [Polygenic Score Calculation](https://hail.is/docs/0.2/guides/genetics.html#polygenic-score-calculation), getting error with stacktrace:. `2022-05-14 12:09:07 Hail: INFO: Running Hail version 0.2.94-f0b38d6c436f; 2022-05-14 12:09:08 SparkContext: WARN: Using an existing SparkContext; some configuration may not take effect.; 2022-05-14 12:09:08 root: INFO: RegionPool: initialized for thread 30: Thread-4; 2022-05-14 12:09:09 MemoryStore: INFO: Block broadcast_0 stored as values in memory (estimated size 34.3 KiB, free 434.4 MiB); 2022-05-14 12:09:09 MemoryStore: INFO: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 434.4 MiB); 2022-05-14 12:09:09 BlockManagerInfo: INFO: Added broadcast_0_piece0 in memory on 10.40.3.21:33951 (size: 3.2 KiB, free: 434.4 MiB); 2022-05-14 12:09:09 SparkContext: INFO: Created broadcast 0 from broadcast at SparkBackend.scala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:35,risk,risk,35,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['risk'],['risk']
Safety,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1003:740,abort,aborted,740,https://hail.is,https://github.com/hail-is/hail/issues/1003,2,['abort'],['aborted']
Safety,"Hi,. Thanks so much for setting up a tutorial for Batch, and for all your work on Hail as a service - it's a really great project and I'm excited to try it out!. I'm following the tutorial and was wondering if this line is there by mistake?. https://github.com/hail-is/hail/blob/8140f17d926235470b1ed1cdefd591c3b41838a5/hail/python/hailtop/batch/docs/cookbook/files/batch_clumping.py#L73. It throws `AttributeError: 'InputResourceFile' object has no attribute 'add_extension'`, and the method is not defined for `InputResourceFile` indeed. However it is defined for `JobResourceFile`, which, if I understand, Batch uses to find the job output? If so, I guess, for the input resource with an explicitly defined name, calling `add_extension` is redundant? . I'm on Hail `0.2.59-63cf625e29e5`. Vlad",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9645:743,redund,redundant,743,https://hail.is,https://github.com/hail-is/hail/issues/9645,1,['redund'],['redundant']
Safety,"Hi,; While loading a plink binary file generated by plink2, I receive the following error in my hail.log: . hail: info: running: importplink --bfile plinktest_chr21 --delimiter ' '; hail: info: Found 152249 samples in fam file.; hail: info: Found 982854 variants in bim file.; ^M[Stage 0:> (0 + 0) / 279]^M[Stage 0:> (0 + 31) / 279]hail: importplink: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 0.0 failed 4 times, most recent failure: Lost task 18.3 in stage 0.0 (TID 60, 10.93.109.80): java.io.EOFException: Cannot seek to a negative offset; at org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:325); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.broadinstitute.hail.io.HadoopFSDataBinaryReader.seek(HadoopFSDataBinaryReader.scala:17); at org.broadinstitute.hail.io.plink.PlinkBlockReader.seekToFirstBlockInSplit(PlinkBlockReader.scala:34); at org.broadinstitute.hail.io.plink.PlinkBlockReader.<init>(PlinkBlockReader.scala:23); at org.broadinstitute.hail.io.plink.PlinkInputFormat.getRecordReader(PlinkInputFormat.scala:11); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:237); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/715:406,abort,aborted,406,https://hail.is,https://github.com/hail-is/hail/issues/715,1,['abort'],['aborted']
Safety,"I added a test to demonstrate the problem. The `InsertFields` is overwriting the type of a field that is not part of the requested type. Previously we would just not insert anything and leave the rebuilt child alone. But when the child is a `Ref` or a `Literal` or something that doesn't actually get rebuilt differently, the old way would lead to a situation where the rebuilt IR is not a supertype of the original IR. By inserting a `SelectFields` to subset away the fields that would have been overwritten, we avoid this problem. . Happy to further elaborate if the above isn't clear.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9633:513,avoid,avoid,513,https://hail.is,https://github.com/hail-is/hail/pull/9633,1,['avoid'],['avoid']
Safety,"I added the columns to the batches table for the real-time cost estimate even though that will be v2 to avoid having to reset the database twice. Although, we may need to do that anyways so the fields are properly interpreted.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7600:104,avoid,avoid,104,https://hail.is,https://github.com/hail-is/hail/pull/7600,1,['avoid'],['avoid']
Safety,"I agree with this comment: https://stackoverflow.com/a/17329465/431282. > Lazy val is *not* free (or even cheap). Use it only if you absolutely need laziness for correctness, not for optimization. I think we should avoid lazy val. This was borne out when profiling `RegionValue` stuff.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2316:215,avoid,avoid,215,https://hail.is,https://github.com/hail-is/hail/pull/2316,1,['avoid'],['avoid']
Safety,"I also included a little change to use `Region.scoped` for writing a local `IndexedSeq[Annotation]`. In general, we cannot move Region off-heap until all the allocations of Regions are matched with `Region.close()`. We don't use the context's region yet in the persist. I'll introduce that in a later pull request. This just avoids serializing regions and region values, which will soon become off-heap, non-serializable values.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3391:325,avoid,avoids,325,https://hail.is,https://github.com/hail-is/hail/pull/3391,1,['avoid'],['avoids']
Safety,"I asked in an SO post how to make SBT work the right way but until then we should disable fatal `-Werror` on Javac (build.sbt didn't even have the right javac option, my bad). Pending SO question asking for help: https://stackoverflow.com/questions/56495453/how-do-i-suppress-warnings-about-the-unsafe-api-when-compiling-with-sbt. Also fix two warnings I saw in SBT but not gradle.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6292:295,unsafe,unsafe-api-when-compiling-with-sbt,295,https://hail.is,https://github.com/hail-is/hail/pull/6292,1,['unsafe'],['unsafe-api-when-compiling-with-sbt']
Safety,I avoid printing the full exception into the body in most cases. Seems prudent to not expose too much about our internals. CI already uses a broad except and prints the full message when building PRs so I adopted that for building the branch (`unwatched_branch.deploy`) in dev deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8828:2,avoid,avoid,2,https://hail.is,https://github.com/hail-is/hail/pull/8828,1,['avoid'],['avoid']
Safety,"I changed the Spark `persist`s to `writeRead` which writes and then reads. I tried to maintain this invariant: a block matrix partition always reads a linear number of partitions in the number of referenced block matrices. In particular, the result of *any* matmul must `writeRead`. I removed the boxing of Doubles to check for NaN. I avoided a bunch of allocation when performing matmul by using a fused multiply and add operation (`dgemm`). I sped up conversation to BlockMatrix somewhat by introducing an iterator that caches the firstelementoffset. I substantially improved `BlockMatrix.checkpoint` by using the fast lz4 codec. *new*: I also added some tasteful cache'ing to PCRelate which substantially reduced the time spent reading data from disk. cc: @johnc1231 @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7962:335,avoid,avoided,335,https://hail.is,https://github.com/hail-is/hail/pull/7962,1,['avoid'],['avoided']
Safety,"I detect no performance difference on blanczos running the benchmark; locally. This pattern appears a lot in the NDArrayEmitter, though,; so we should fix it everywhere and see what happens!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9708:2,detect,detect,2,https://hail.is,https://github.com/hail-is/hail/pull/9708,1,['detect'],['detect']
Safety,"I didn't bust js yet because there are some external libraries (MathJax) that cary their own version strings and I don't want to break them. In testing seems safe, although I admit the regex isn't the most specific. In the worst case I believe we would append an unnecessary version string, which shouldn't break anything (just will cache the browser to reload the css instead of using cache)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6997:158,safe,safe,158,https://hail.is,https://github.com/hail-is/hail/pull/6997,1,['safe'],['safe']
Safety,"I do not think we frequently get errors in `storage.reader`, but I think `storage.writer` was always flaky and we were protected by the `retryTransientErrors` on `createNoCompression`. My change to fix requester pays delayed the error until either the first `write` or the `close` which do not have a `retryTransientErrors` (and it is not obvious to me that it is safe to retry a `flush`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12868:364,safe,safe,364,https://hail.is,https://github.com/hail-is/hail/pull/12868,1,['safe'],['safe']
Safety,I don't have great evidence but 15 seconds might not be very much if we get big preemptions in k8s and our auth pods need to be redeployed on new nodes. I've been seeing more timeouts in Azure CI in this step and thought we should retry a little longer. If this issue persists I'll devote some time to digging deeper on it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14353:175,timeout,timeouts,175,https://hail.is,https://github.com/hail-is/hail/pull/14353,1,['timeout'],['timeouts']
Safety,I find this useful for identifying which local branches I can safely delete.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11028:62,safe,safely,62,https://hail.is,https://github.com/hail-is/hail/pull/11028,1,['safe'],['safely']
Safety,"I found this while on a PR based on:. ```; * 8e61ad87c - (3 days ago) [batch] fix scheduler -- schedule job timeout 1sec (#8022) - jigold (hi/master, master); ```. The error was:; ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 281, in run; await docker_call_retry(self.container.start); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 87, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 188, in start; data=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown'); ```. Unfortunately the batch worker had already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8029:108,timeout,timeout,108,https://hail.is,https://github.com/hail-is/hail/issues/8029,1,['timeout'],['timeout']
Safety,"I haven't ported type checking to everything yet -- just hail context. It'll take an hour or two to write it for everything, and I want to wait until after breakingbad is merged to do that to avoid a hellish rebase (also in case you want me to change the interface).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1727:192,avoid,avoid,192,https://hail.is,https://github.com/hail-is/hail/pull/1727,1,['avoid'],['avoid']
Safety,"I hope this helps. The regular `timeout` parameter on the Python methods like `stage_block` is for the server-side timeout of the operation. We want the client side timeout to be shorter. I picked 5 seconds, but that was arbitrary. I'm not sure I implemented the Scala code correctly, but that's along the right lines of what needs to happen.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13344:32,timeout,timeout,32,https://hail.is,https://github.com/hail-is/hail/pull/13344,3,['timeout'],['timeout']
Safety,"I implemented `cancelled` as a join from the batch table rather than storing redundant information for each job. @akotlar The issue was that all jobs were getting set to cancelled at the same time (and thus notifying children), so always run jobs were all getting run at once neglecting the hierarchy of job dependencies. This is the purpose of having the `cancelled` flag as separate from the `Cancelled` state and is checked in `create_if_ready`. @cseed Can you look and see if this solves the cancel problem we discussed earlier? The faulty PR was this one: https://github.com/hail-is/hail/pull/6128/files",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6341:77,redund,redundant,77,https://hail.is,https://github.com/hail-is/hail/pull/6341,1,['redund'],['redundant']
Safety,"I left the changes to Query and Batch in separate commits for ease of review. I put these in the same PR because we don't really have standalone testing for JVM Jobs outside of Query-on-Batch so the FASTA use-case serves as a test here that cloudfuse is working properly for JVM Jobs. Would be great if Jackie you could review the batch commit and Tim could review the query commit. ## Hail Query; - Added support for the `FROM_FASTA_FILE` rpc and the service backend now passes sequence file information from RGs in every rpc; - Refactored the liftover handling in service_backend to not redundantly store liftover maps and just take them from the ReferenceGenome objects like I did for sequence files. This means that add/remove liftover/sequence functions on the Backend are just intended to sync up the backend with python, which is a no-op for the service backend.; - Don't localize the index file on fromFASTAFile/addSequence before creating the index object. `FastaSequenceIndex` just loads the whole file on construction so might as well stream it in from whatever storage it's in.; - FASTA caching is left alone because those files will be mounted and unmounted from the jvm container over the life of the job. JVM doesn't have to worry about disk usage because that's handled by Batch XFS quotas, so long as the service backend requests enough storage to fit the FASTA file. Batch will make sure that a given bucket (and therefore a given FASTA file) is mounted once per-user on a batch worker. ## Hail Batch; - Added support for read-only cloudfuse mounts for JVM jobs; - These mounts are shared between jobs on the same machine from the same user; - I did not change DockerJobs, but they could be very easily adapted to use this new mount-sharing code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736:589,redund,redundantly,589,https://hail.is,https://github.com/hail-is/hail/pull/12736,1,['redund'],['redundantly']
Safety,"I like the != syntax but it is not supported by the Make shipped; with recent OS X versions, so I think we should avoid it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6891:114,avoid,avoid,114,https://hail.is,https://github.com/hail-is/hail/pull/6891,1,['avoid'],['avoid']
Safety,I made some changes in the second commit that were necessary to avoid the circular imports. Closing #11059 in favor of this PR. FYI: @danking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11063:64,avoid,avoid,64,https://hail.is,https://github.com/hail-is/hail/pull/11063,1,['avoid'],['avoid']
Safety,I merged these branches and resolved one (false) conflict. - [dependabot/pip/docker/async-timeout-4.0.2](/hail-is/hail/tree/dependabot/pip/docker/async-timeout-4.0.2); - [dependabot/pip/docker/pylint-2.12.2](/hail-is/hail/tree/dependabot/pip/docker/pylint-2.12.2); - [dependabot/pip/docker/python-json-logger-2.0.2](/hail-is/hail/tree/dependabot/pip/docker/python-json-logger-2.0.2); - [dependabot/pip/hail/python/avro-gte-1.10-and-lt-1.12](/hail-is/hail/tree/dependabot/pip/hail/python/avro-gte-1.10-and-lt-1.12); - [dependabot/pip/hail/python/dev/nbsphinx-0.8.8](/hail-is/hail/tree/dependabot/pip/hail/python/dev/nbsphinx-0.8.8); - [dependabot/pip/hail/python/dev/pylint-2.12.2](/hail-is/hail/tree/dependabot/pip/hail/python/dev/pylint-2.12.2); - [dependabot/pip/hail/python/dev/sphinxcontrib-katex-0.8.6](/hail-is/hail/tree/dependabot/pip/hail/python/dev/sphinxcontrib-katex-0.8.6); - [dependabot/pip/hail/python/janus-gte-0.6-and-lt-1.1](/hail-is/hail/tree/dependabot/pip/hail/python/janus-gte-0.6-and-lt-1.1); - [dependabot/pip/hail/python/pre-commit-2.17.0](/hail-is/hail/tree/dependabot/pip/hail/python/pre-commit-2.17.0); - [dependabot/pip/hail/python/tabulate-0.8.9](/hail-is/hail/tree/dependabot/pip/hail/python/tabulate-0.8.9),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11481:90,timeout,timeout-,90,https://hail.is,https://github.com/hail-is/hail/pull/11481,2,['timeout'],['timeout-']
Safety,"I might want collecting to pandas for plotting, so let's figure out how to completely avoid spark when going to Pandas.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11106:86,avoid,avoid,86,https://hail.is,https://github.com/hail-is/hail/pull/11106,1,['avoid'],['avoid']
Safety,"I need to use in decoder builder which doesn't currently chunk struct construction, will use there after https://github.com/hail-is/hail/pull/3667 goes in to avoid conflicts. This introduces 1 change in behavior: code with estimated size > 100 will be put out in its own method, even it fits in one block. If I read it correctly, the old code would put any amount of code inline up to the method limit target, so several medium-sized structs would potentially blow out the method bytecode limit. Also fixed a bug in getChunkBounds where items on the boundary weren't counted in the size.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3669:158,avoid,avoid,158,https://hail.is,https://github.com/hail-is/hail/pull/3669,1,['avoid'],['avoid']
Safety,I often observe the JVM getting stuck in a high memory state unable to recover from after an OOM. Best to just kill the JVM.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12805:71,recover,recover,71,https://hail.is,https://github.com/hail-is/hail/pull/12805,1,['recover'],['recover']
Safety,"I plan to move this to `hl.dnd.DNDArray`. I had to make a couple changes to RVD and Table to make this work. They all; revolve around convincing Hail not to elide *critical* `key_by`s. The critical insight is that 1:1 partitioners (partitioners where each range; bound interval contains exactly one key) are special: permuting their keys is; free. I can take advantage of this by combining two changes:; 1. `RVD.enforceKey` is aware of these partitioners and avoids scans in that case; 2. Defeat the optimizer, which is unaware of these partitioners and misoptimizes; to operations that require shuffles. The first change is easy. I added `RVDPartitioner.keysIfOneToOne` which looks; for these kinds of partitioners in the special case of keys consisting of 32-; and 64-bit integers. The second change eluded me for a long time. Finally, I discovered; `isSorted=true` and realized the optimizer refuses to modify such; `TableKeyBy`s. I exposed this field in Python as: `Table._key_by_assert_sorted`. With this infrastructure in place, I was able to implement read, write, and; matrix-multiply for DNDArray!. In addition, to the arguable hacks above, a couple pain points remain:; 1. I do not know how to rename keys in Python without triggering shuffles. If I; write `key_by(x=t.y, y=t.x)`, Hail implements this as; `TableKeyBy(TableMapRows(TableKeyBy(Array(), ...)`. The inner key by throws; the keys away so that they can be modified with TableMapRows. Unfortunately,; this completely defeats my attempts to avoid shuffles. I avoid this issue by; not using fixed names for the x and y block coordinates (their names are; stored in `x_field` and `y_field`).; 2. Hail lacks `ndarray_sum`. Instead, I convert from ndarray to array so that I; can use `array_sum`. Unfortunately, this operation seems to completely; dominate all of my time. It takes about 10x as much time as the matrix; multiplies take. I do not understand this. I should be reading the entries in; column-major order. Performance; ----",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8864:459,avoid,avoids,459,https://hail.is,https://github.com/hail-is/hail/pull/8864,1,['avoid'],['avoids']
Safety,"I really borked our SQL linting. This PR is short but it catches a few critical problems. 1. The point of `check-sql.sh` is to detect modifications or deletions of SQL files in PRs and fail if such a change occurs. Currently on `main` it does not detect modifications. In #13456, I removed the `delete-<service>-tables.sql` files (intentionally), so added the `^D` to the `grep` regex to indicate that it is OK to have a deletion. What I inadvertently did though is change the rule from ""It's ok to have Additions of any file OR Modifications of estimated-current.sql / delete-<service>-tables.sql"" to ""It's ok to have Additions OR Modifications OR Deletions of estimated-current.sql / delete-<service>-tables.sql"". Really this should have been ""It's ok to have Additions OR Modifications of estimated-current.sql OR Deletions of delete-<service>-tables.sql"". I've changed it to reflect that rule. 2. Rules currently silently *pass* in CI with an error message that git is not installed. In #13437 I changed the image used to run the linters and inadvertently didn't include `git` which `check-sql.sh` needs to run. Here's how it failed in a sneaky way:; - Since `git` is not installed, all calls to `git` fail, but the script is not run with `set -e` so every line of the script is executed; - Since `git` lines fail, `modified_sql_file_list` remains empty; - Since `modified_sql_file_list` remains empty, it appears to the check at the end that everything checked out; - The if statement runs successfully and the script returns with error code 0. To fix this I do a few things:; - installed `git` in the linting image; - `set -e` by default and only enable `set +e` later on when necessary (because we don't want a failed `git diff` to immediately exit); - Do away with the file checking and instead check the error code of the grep. If nothing survives the grep filter, which means there were no illegal changes made, grep will return with exit code 1. So we treat that exit code as a success.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13745:127,detect,detect,127,https://hail.is,https://github.com/hail-is/hail/pull/13745,2,['detect'],['detect']
Safety,I tested this in my namespace and everything seems to be working. I tried really hard to keep the order of the parameters the same everywhere to avoid putting values in the wrong places (i.e. 7200 for max_instances instead of for standing worker idle time).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12575:145,avoid,avoid,145,https://hail.is,https://github.com/hail-is/hail/pull/12575,1,['avoid'],['avoid']
Safety,"I think functionality wise this is all ready to go. Added to the docs as well, though those probably need another iteration. Either way, it's safe to start looking this over and give some feedback.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1929:142,safe,safe,142,https://hail.is,https://github.com/hail-is/hail/pull/1929,1,['safe'],['safe']
Safety,I think our default timeout of 5 seconds is why the disk operations are slow. The `wait` endpoint returns within 2 minutes. https://cloud.google.com/compute/docs/api/how-tos/api-requests-responses#handling_api_responses,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10652:20,timeout,timeout,20,https://hail.is,https://github.com/hail-is/hail/pull/10652,1,['timeout'],['timeout']
Safety,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6011:272,timeout,timeout,272,https://hail.is,https://github.com/hail-is/hail/pull/6011,1,['timeout'],['timeout']
Safety,I think there's now 4 compiler warnings (besides the unsafe warnings) that I wasn't sure what to do with.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3601:53,unsafe,unsafe,53,https://hail.is,https://github.com/hail-is/hail/pull/3601,1,['unsafe'],['unsafe']
Safety,"I think this is a race condition with another process trying to pull the same image after the current process has pulled the image. That would mostly be solved by a per user Docker cache, but I think this solution is still needed as you could have a race condition on the cache timeout boundaries. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8193:278,timeout,timeout,278,https://hail.is,https://github.com/hail-is/hail/pull/8193,3,['timeout'],['timeout']
Safety,"I think this is the direction we need to head with Batch. Mapping DB things to objects and then using recursive functions over the graph is not going to scale. This uses one database call to:; - set the state of every non-always-run, incomplete job in the given batch to `Cancelled`; - move the state from `Running` to `Pending` for every; - job whose parents all succeeded, and every; - always-run job whose parents all completed; - get a list of every `Ready` or `Cancelled` job; - update the batch to cancelled and closed. Then uses a loop to delete k8s resources and create pods, as appropriate for the given job. I think we can go further! We should make our k8s requests in parallel (I don't see anyway to delete a *list* of jobs in k8s [only to delete a whole namespace]) and we should avoid retrieving every column from the database. We only need a few things to `create_pod` or `delete_k8s_resources`. cc: @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6578:793,avoid,avoid,793,https://hail.is,https://github.com/hail-is/hail/pull/6578,1,['avoid'],['avoid']
Safety,"I think this should resolve the issues we were seeing with OnlineBoundedGather2. Changes:; - cancelled tasks (those that raise CancelledError) are ignored (we don't propagate cancelled out of background tasks); - Make sure all exceptions are either reraised or logged; - The first exception is raised out of exit, not call; - call raises PoolShutdownError if the pool is shutdown; - _shutdown doesn't signal _done_event until all cancelled tasks are complete; - call clears _done_event (not strictly necessary because exit checks pending, but seems safer); - added copious docstrings",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10342:549,safe,safer,549,https://hail.is,https://github.com/hail-is/hail/pull/10342,1,['safe'],['safer']
Safety,"I think this was a red-herring caused by Kaniko blowing disk; trying to copy special files. If this PR passes, then Kaniko is able to build itself with normal workers; and no extra disk, so there is no risk of another firedrill wherein; Kaniko is broken",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10400:202,risk,risk,202,https://hail.is,https://github.com/hail-is/hail/pull/10400,1,['risk'],['risk']
Safety,"I think we can get up to 6k, but this is a safe limit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6802:43,safe,safe,43,https://hail.is,https://github.com/hail-is/hail/pull/6802,1,['safe'],['safe']
Safety,"I want to annotate a field like this:; ```Gene_Conseq_MAF=(va.annot.gene + ""\n"" + va.annot.most_severe_csq + ""\nMAF:"" + str(va.lmmreg.maf))```; (so a string with \n) such that when I load in R, the top loci will be highlighted as so. However, the exported .gz file actually has new lines at each \n""; is there a way to avoid this?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1132:320,avoid,avoid,320,https://hail.is,https://github.com/hail-is/hail/issues/1132,1,['avoid'],['avoid']
Safety,"I want to have a functionality when import VCF, disable the filter based on `sum(AD) != DP`. Although this is a good sanity check, but since we are not clear if this error will lead to inaccurate genotype calls, there will be an option for analyst to keep those calls.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/427:117,sanity check,sanity check,117,https://hail.is,https://github.com/hail-is/hail/issues/427,1,['sanity check'],['sanity check']
Safety,I want to use this for unsafe testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2001:23,unsafe,unsafe,23,https://hail.is,https://github.com/hail-is/hail/pull/2001,1,['unsafe'],['unsafe']
Safety,I wanted to be safe rather than sorry for #13487 to make sure we have enough memory to fit all of the 100 row chunk endpoints in memory. We can switch it back to 4 cores after that PR is merged.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13822:15,safe,safe,15,https://hail.is,https://github.com/hail-is/hail/pull/13822,1,['safe'],['safe']
Safety,I was seeing Reason: None. This is probably overkill but it seems safest. We should probably do it consistently.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8836:66,safe,safest,66,https://hail.is,https://github.com/hail-is/hail/pull/8836,1,['safe'],['safest']
Safety,"I would think that we should be able to avoid doing this, and thus avoid needing to scan through the dataset in this case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4388:40,avoid,avoid,40,https://hail.is,https://github.com/hail-is/hail/issues/4388,2,['avoid'],['avoid']
Safety,"I'd like to introduce a function to generate UUIDs in our code. The use case I have in mind is currently to append unique identifiers to partition file names for write to mimic our current behavior, but the ability to generate uuids in our IR seems pretty nice, in general. I added it as an IR node, but there's several problems arising from the fact that UUID is non-deterministic, and we can't effectively seed it. . The fact that UUID4 is non-idempotent is actually the feature I'm looking for here---running the exact same IR twice should get me two different results, because if my WritePartition fails with a certain UUID I want to retry the execution, generating a new UUID. . We can put a ""seed"" in the node to effectively treat each UUID4() node as non-equal to avoid any theoretical CSEing that might get done. I don't think we need to worry about other forms of let binding; we treat it as non-constant, and we already don't push lets inside of nested array scopes, so the only time we'd ever forward a binding with UUID4 is if we have one usage of the binding within the same array scoping as its definition, which is perfectly valid. The part that I'm having trouble with is related to the following concept: What happens when I make a stream of uuids, and then try to do a self-join? Let's look at the example of zipping a stream with itself, and just concatenating the two elements into an array.; ```; val stream = mapIR(rangeIR(5)) { _ => UUID4() }; val zipped = StreamZip(Array(stream, stream), Array(""1"", ""2""),; MakeArray(Array(Ref(""1"", TString), Ref(""1"", TString)), TArray(TString)),; ArrayZipBehavior.AssumeSameLength); ```; In the general case, we'd expect the zipped stream to be composed of arrays of duplicated elements---we rely on this being true when lowering, for example, a TableJoin against itself or a slightly transformed version of itself. Neither `zipped` nor `boundAndZipped` will exhibit that behavior here---since we process each stream independently, the UUIDs g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8891:771,avoid,avoid,771,https://hail.is,https://github.com/hail-is/hail/pull/8891,1,['avoid'],['avoid']
Safety,"I'm generalizing the CI's deploy system. https://github.com/hail-is/ci/pull/77. In particular, I no longer assume you need to authorize from to a gcloud account. Instead, I just mount you a volume. Each repo will have some secrets that it can authorize with. This is safe to merge now because before the CI changes go in, this just re-authorizes. When the CI changes merge, we'll go back to authorizing once, except that the command is in `hail-ci-deploy.sh` instead of baked into the CI system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4261:267,safe,safe,267,https://hail.is,https://github.com/hail-is/hail/pull/4261,1,['safe'],['safe']
Safety,"I'm not sure if this is the right change, but I'm pretty sure the Azure deployment was stuck because `_heal` kept aborting early on a GitHub post error. See #13050 for context.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13115:114,abort,aborting,114,https://hail.is,https://github.com/hail-is/hail/pull/13115,1,['abort'],['aborting']
Safety,"I'm seeing deploy failures where the tests start failing part way through because batch becomes unavailable, for example: https://ci2.hail.is/jobs/2886/log. However, this can't be the whole story, because batch has a readiness check and it isn't clear why it should go unavailable. Either way, this seems safer because it makes sure you pick up the intended version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6093:305,safe,safer,305,https://hail.is,https://github.com/hail-is/hail/pull/6093,1,['safe'],['safer']
Safety,"I'm trying grm for the first time, and I ran:. hail-new read -i /user/satterst/DBS_v2.4/temp.vds \; filtervariants --keep -c /user/satterst/purcell5k_nodups.interval_list \; count \; grm -f rel -o /user/satterst/DBS_v2.4/temp_rel_grm.tsv. This is 6247 exomes and 5284 variants. . Log file is here: /humgen/atgu1/fs03/satterst/hail.grm.log. I tried this once and let it go for over 40 minutes, and it stayed stuck at Stage 4: (0 + 25) / 25. I accidentally overwrote that log, so I did it again just now, and I didn't let it go for as long, but I observed the same behavior. . When I look at the job's task status page, I see the error I copied in the issue title. The details say:; org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 6, required: 8; Serialization trace:; data$mcD$sp (breeze.linalg.DenseMatrix$mcD$sp). To avoid this, increase spark.kryoserializer.buffer.max value.; at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:263); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). I'm curious if I'm doing something wrong or if grm is behaving badly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/321:858,avoid,avoid,858,https://hail.is,https://github.com/hail-is/hail/issues/321,1,['avoid'],['avoid']
Safety,"I've changed BlockMatrix.from(lm: BDM[Double]) so that each executor is transmitted only the blocks it needs (~num_blocks/num_executors) rather than all of them: ""[TorrentBroadcast](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-TorrentBroadcast.html) uses a BitTorrent-like protocol for block distribution (that only happens when tasks access broadcast variables on executors)."". In another branch, I've verified on GCP that distributing and then localizing a 10k x 10k matrix is twice as fast (about 15s vs 30s). Distributing and then writing a 25k by 25k matrix (5GB) with 10+2 standard 8-core workers takes about 30s with the new method but fails for every partition with the old method (months ago I believe I sometimes got the old method to work at this scale using high mem. It's needed for LMM). Note the matrix only has 49 partitions at the new default blockSize so I had more cores than needed for the experiment. I've also added a method to write a local matrix as a block matrix. I use ParRange to parallelize writing from master. Writing and then reading should be the safest way to distribute a big local matrix at the beginning of complex pipelines, and I think it avoids some of the memory overhead associated with broadcast.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2848:1099,safe,safest,1099,https://hail.is,https://github.com/hail-is/hail/pull/2848,2,"['avoid', 'safe']","['avoids', 'safest']"
Safety,"I've left all the instances of `region.loadX` untouched (and left the methods on the region object, but this should let us avoid piping through region objects when we just need to read something. (Broken out from #6580)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6644:123,avoid,avoid,123,https://hail.is,https://github.com/hail-is/hail/pull/6644,1,['avoid'],['avoid']
Safety,"I've reformatted the urls in the `gs://hail-datasets-us`, `gs://hail-datasets-eu`, and `s3://hail-datasets-us-east-1` buckets with a new naming scheme to make things more consistent and clean up the buckets. . This updates the `datasets.json` file with these new urls. Currently there are two copies of each dataset in each bucket, one at the old url and one at the new url. I will remove the datasets at the old urls in a few months to avoid disrupting users not running the latest release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10526:437,avoid,avoid,437,https://hail.is,https://github.com/hail-is/hail/pull/10526,1,['avoid'],['avoid']
Safety,"Ideally, a stream would be able to recover from a transient error by; seeking, but until we have that functionality, this avoids having; one failure out of 5000 (which I have now seen twice). Example: https://batch.hail.is/batches/1531518/jobs/2094.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11716:35,recover,recover,35,https://hail.is,https://github.com/hail-is/hail/pull/11716,2,"['avoid', 'recover']","['avoids', 'recover']"
Safety,"Implements existing query service endpoints using websockets instead of long-running http requests. I open a new socket for each request instead of attempting to hold one open for each client to send all its requests; not sure if one is preferable to the other, but this one seemed easier to handle and more in line with what we were doing before. ~~I haven't put any sort of heartbeat on either end for now for simplicity; the `blocking_to_async` wrapper around the jvm execution interferes with the server's ability to send/receive pings and pongs, and I'm not currently handling retries for timeouts anyways; would love suggestions on how to make this more robust.~~. Since nginx's default behavior is to close the websocket after 60s of non-activity (which seems pretty reasonable), I have a 30s heartbeat on the server-side websocket connection. This meant rewriting the flow to split a task off to execute the blocking JVM function and keeping the websocket task open to handle the heartbeat. Currently, we rely on the client to close the connection once the jvm task is completed and the result response is received; if the connection is closed/something errors for some other reason, we check to see if the task is completed and cancel it if it's not. The client side still doesn't poll the server for existence, but if the socket is unexpectedly closed we'll retry the request.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9636:594,timeout,timeouts,594,https://hail.is,https://github.com/hail-is/hail/pull/9636,1,['timeout'],['timeouts']
Safety,Improve Random Generation: Avoid Excessive Collection Nesting,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1903:27,Avoid,Avoid,27,https://hail.is,https://github.com/hail-is/hail/pull/1903,1,['Avoid'],['Avoid']
Safety,In #9176 Dan and I settled on Docker Hub as the preferred name for that service. This PR makes that consistent across batch docs. I left `docker_resources.rst` alone to avoid conflicting with #9176.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9233:169,avoid,avoid,169,https://hail.is,https://github.com/hail-is/hail/pull/9233,1,['avoid'],['avoid']
Safety,"In R, when you load a data table, it auto-detects whether each column is a character vs. numeric type. It would be super if this could be implemented in Hail. I'm guessing it would take the form of ""if none of the fields in the column contain special characters or letters, then it's numeric, else it's character,"" (but maybe it's not so straight forward, not so sure...). . Anyways, when you have over 30 annotations that are numeric, it's a bit of a pain to have to go through writing all the -t flag options in Hail, so if it could be auto-detected, that would be super! . In the case where 'dummy' variables are used (like 1-5 for Batch), then the user should be able to say that that's a string or a ""factor"" as it is in R (or a character/string, which is essentially the same), for the purposes of analysis in linear regression.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/463:42,detect,detects,42,https://hail.is,https://github.com/hail-is/hail/issues/463,2,['detect'],"['detected', 'detects']"
Safety,"In a89d64a, I modified `build.yaml` to release the wheel we had already built and tested. Unbeknownst to me was that we rebuild the wheel with a different version of `hail/python/hailtop/hailctl/deploy.yaml` and releasing the version used for testing borked `hailctl dataproc` commands. To fix this, we'll rebuild the wheel but use the `jar` we've already built and tested. This is safe to do as far as I know because we don't bundle any information into the jar that depends on the make flag `DEPLOY_REMOTE`. Fixes: #14452",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453:382,safe,safe,382,https://hail.is,https://github.com/hail-is/hail/pull/14453,1,['safe'],['safe']
Safety,"In https://github.com/hail-is/hail/pull/9113, I forced the auth driver to use; the modern, TLS-required, SQL config format. I incorrectly forgot to specify the; TLS file paths. Luckily, when I tried to create a user account for Patrick; Cummings, instead of creating a broken secret, the auth driver; error'ed. Moreover, the clean up code was broken. As a result, Patrick's account; was stuck in `creating`. This PR fixes both the clean up code issue (I set `self.namespace` in; `K8sSecretResource`) and specifies the TLS file paths (see driver.py near; line 217). In addition, this PR attempts to avoid future problems with the sql; configuration by codifying the required components as a NamedTuple, `SQLConfig`. I also; co-located all the parsing and transformation logic between JSON, dicts, and CNF; in the `SQLConfig` class. I traced back all the users of `create_secret_data_from_config` to ensure they; all now use SQLConfig. I added lots of type annotations, but those won't do; anything right now because we don't have mypy enabled for hailtop.auth. ---. There's a separate issue of us not getting notified that Patrick's account was; not being created due to an error. The relevant logs are linked below. I'm glad; we're starting work on better monitoring. Hopefully error logs like these will; trigger emails to services team. https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22;timeRange=2020-08-11T15:44:00.000Z%2F2020-08-11T23:55:00.000Z?project=hail-vdc&query=%0A. Moreover, the infinite retry of his account created tens of google service; accounts that were not cleaned up. I do not yet understand why the google; service account clean up code failed. The clean up code bug that I *do* fix in; this PR addresses the GSA secret and the tokens secret.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9259:598,avoid,avoid,598,https://hail.is,https://github.com/hail-is/hail/pull/9259,1,['avoid'],['avoid']
Safety,"In order to start using Google or AAD access tokens instead of hail-minted tokens, we need to be able to identify a service account in the system by its UID at their identity provider. In Google, this UID is the `uniqueId` field of the Service Account. In AAD, it is the Service Principal Object ID. In an upcoming change, we'll update the user creation process to add this ID upon user creation, at which point we will be able to safely remove this code that updates existing records. I've marked this PR as `full-deploy` so the AUS folks can roll this commit out specifically before this column is relied on. This way we can safely remove the loop in a follow-up PR and know they will have run this code to populate the column.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13207:431,safe,safely,431,https://hail.is,https://github.com/hail-is/hail/pull/13207,2,['safe'],['safely']
Safety,"In the Sphinx theme which the Hail docs use, the search page does not show anything unless a query has been provided and a search performed. https://github.com/readthedocs/sphinx_rtd_theme/blob/master/sphinx_rtd_theme/search.html. Thus, the Search link on the [home page of the Hail 0.2 docs](https://hail.is/docs/0.2/index.html) leads to a [blank page](https://hail.is/docs/0.2/search.html). ![image](https://user-images.githubusercontent.com/1156625/74118640-44333c80-4b8a-11ea-9147-7a0d188d44a0.png). To avoid confusion, this change removes the link to the search page from the home page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8065:507,avoid,avoid,507,https://hail.is,https://github.com/hail-is/hail/pull/8065,1,['avoid'],['avoid']
Safety,Includes `unsafeInsert`. @tpoterba: @danking got the random draw but I thought you'd like to take a look at this.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2317:10,unsafe,unsafeInsert,10,https://hail.is,https://github.com/hail-is/hail/pull/2317,1,['unsafe'],['unsafeInsert']
Safety,"Information below. It isn't totally clear what to do here. I think the k8s refresh loop should probably restart pods (mark_unscheduled) that have been scheduled but aren't running after a timeout (few mins). ```; $ kubectl -n batch-pods describe pods batch-3-job-41-39d17b; Name: batch-3-job-41-39d17b; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-1f89/10.128.0.101; Start Time: Fri, 12 Jul 2019 13:17:15 -0400; Labels: app=batch-job; batch_id=3; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=41; task=main; user=ci; uuid=f53f127847864f1cbf7d4bdc911a6646; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; Image ID: ; Port: <none>; Host Port: <none>; Command:; bash; -c; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-3-job-41-39d17b (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /test-gsa-key from test-gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6625:188,timeout,timeout,188,https://hail.is,https://github.com/hail-is/hail/issues/6625,1,['timeout'],['timeout']
Safety,Inspect Relational IRs to detect read/write to same file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3856:26,detect,detect,26,https://hail.is,https://github.com/hail-is/hail/issues/3856,1,['detect'],['detect']
Safety,"Instead of using a SafeRow for globals, use a region that is owned by the caller (CompileAndEvaluate). . This will be easier after Matrix lowering is done.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5715:19,Safe,SafeRow,19,https://hail.is,https://github.com/hail-is/hail/issues/5715,1,['Safe'],['SafeRow']
Safety,"It definitely looks like ""ZONE_RESOURCE_POOL_EXHAUSTED"" is the cause of these GPU test failures. In this case it looks like it took ~4 minutes to successfully get a VM (after two exhaustion errors) & schedule the job. By then, our uniform 6 minute timeout per test left us with just two minutes. It looks like the job actually did succeed in the worker (seems to have taken ~2 minutes, seems long, does testing for CUDA do some kind of initialization work?). Looks like backing that off to 10 minutes might be just enough to eventually get us a GPU. Might be worth pulling that into its own build.yaml test job so that it does not block the queue of other tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13739:248,timeout,timeout,248,https://hail.is,https://github.com/hail-is/hail/pull/13739,1,['timeout'],['timeout']
Safety,"It is impossible to submit large batches without this. What happens? The timeout per request is 60s. We have 50 x 8MB = 400MB worth of requests in flight. That means the client needs a reliable sustained MINIMUM bandwidth of ~7MB/s to not time out. This doesn't seem reasonable. Without this change, Konrad wasn't able to submit a large batch (although it probably would have gone through eventually with enough retry/backoff). With this, 136K jobs took 2-3m to submit. FYI @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7971:73,timeout,timeout,73,https://hail.is,https://github.com/hail-is/hail/pull/7971,1,['timeout'],['timeout']
Safety,"It looks like the configuration files for `xfs_quota` serve mostly to persist mappings from project name -> project id, and project id -> filesystem path. We keep that information in the worker anyway, and `xfs_quota` (way deep in its documentation) allows you to specify a path and project id in the project-creation command and then use a project id in the command that sets limits. This avoids locking on configuration files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10467:390,avoid,avoids,390,https://hail.is,https://github.com/hail-is/hail/pull/10467,1,['avoid'],['avoids']
Safety,"It seems that 83 is potentially too high now. I saw batch-driver get overwhelmed with the current setting. However, there was also a bug related leaking database connections which also contributed to the observed behavior. Batch was able to make forward progress with 45, so at least this is safe and can be adjusted again next time we're running at scale.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9337:292,safe,safe,292,https://hail.is,https://github.com/hail-is/hail/pull/9337,1,['safe'],['safe']
Safety,"It seems that sessions sometimes become inaccessible to auth. Using some; logging, I realized that `/login` will set some session parameters that do not; reappear in `/oauth2callback`. While trying to debug this, I deleted my cookie; and everything started working again. Luckily, my phone was still borked. The; fix is to use `new_session` which I discovered with a big red warning in; aiohttp-session's docs: [Always use new_session() instead of get_session() in; your login views to guard against Session Fixation; attacks!](https://aiohttp-session.readthedocs.io/en/stable/reference.html#aiohttp_session.new_session). If nothing else, we are now safe from session fixation attacks. I do not; understand why this is necessary for correctness.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8052:650,safe,safe,650,https://hail.is,https://github.com/hail-is/hail/pull/8052,1,['safe'],['safe']
Safety,"It turns out that Kryo serialization is extra sneaky and will often just try to serialize the parts of a class if the class itself doesn't implement the KryoSerializable interface. I made a trait, `UnKryoSerializable`, that extends KryoSerializable but throws errors on read and write to try to weed out the rest of the places where UnsafeRows are being serialized. The biggest place this popped up was with colValues. For now, they're just being broadcast as safe Annotations everywhere. This depends on a change in #3258 and I'll rebase when that goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3288:333,Unsafe,UnsafeRows,333,https://hail.is,https://github.com/hail-is/hail/pull/3288,2,"['Unsafe', 'safe']","['UnsafeRows', 'safe']"
Safety,"It's safe here since we're within bounds and don't use negative indexing.; ```; def unsafeValueAt(row: Int, col: Int): V = data(linearIndex(row, col)); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1566:5,safe,safe,5,https://hail.is,https://github.com/hail-is/hail/pull/1566,2,"['safe', 'unsafe']","['safe', 'unsafeValueAt']"
Safety,It's wildly unsafe. It's better to scope the unsafety in; `IEmitCode.handle` for `loadField`. Also add `PNDArrayValue.shapes` to handle the previous use case of; `PBaseStructValue.apply`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9289:12,unsafe,unsafe,12,https://hail.is,https://github.com/hail-is/hail/pull/9289,2,['unsafe'],"['unsafe', 'unsafety']"
Safety,Java NDarray Safety,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10304:13,Safe,Safety,13,https://hail.is,https://github.com/hail-is/hail/pull/10304,1,['Safe'],['Safety']
Safety,"Jobs are now only allowed to have parents in the same batch. This is necessary for a long term goal: inter-job dependencies. We plan to temporarily store the output of a job. We need to know when a job can no longer have children (ergo it is safe to delete the job's output). We will add a `batch.close` which prevents a batch from receiving new children. If batch jobs may only depend on other jobs in the batch, then a `close` means that we can delete any output from a job whose children have already read its output.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5231:242,safe,safe,242,https://hail.is,https://github.com/hail-is/hail/pull/5231,1,['safe'],['safe']
Safety,"Just a refactor, simplifies the interactions between `Worker`, `CloudWorkerAPI` and `CloudUserCredentials` and hopefully makes this code safer and easier to work with. Instead of the following occuring in worker.py:. 1. get credentials string from `CloudUserCredentials`; 2. tell `CloudWorkerAPI` to write credentials string to `path` owned by the job; 3. tell `CloudWorkerAPI` to mount cloudfuse using the credentials stored at `path`. we instead just do. 1. tell `CloudWorkerAPI` to mount cloudfuse using `CloudUserCredentials`. On its own I think this change makes the codepath simpler and easier to think about in terms of where credentials are stored, but this also gets rid of the requirement from `worker.py`'s point of view that credentials must be stored on the filesystem. This will make it easier to transition off of key files and over to metadata server tokens. In order to make the new statement sound in terms of types, we can't have `CloudWorkerAPI.mount_cloudfuse` just accept a `CloudUserCredentials` argument, because that means `GCPWorkerAPI` would need to be able to support an argument of type `AzureUserCredentials`, which would never happen and doesn't make sense. What we can do here is make `CloudWorkerAPI` generic over the subtype of `CloudUserCredentials` that it both produces and consumes. This allows us to use stricter types like `GCPUserCredentials` and `AzureUserCredentials` inside of `GCPWorkerAPI` and `AzureWorkerAPI` respectively and now the type system is happy. It also relaxes the restriction that both of the `GCPUserCredentials` and `AzureUserCredentials` need to conform to the same `cloudfuse_credentials` interface.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12962:137,safe,safer,137,https://hail.is,https://github.com/hail-is/hail/pull/12962,1,['safe'],['safer']
Safety,"L. This will only impact users who build; <code>cryptography</code> from source (i.e., not from a <code>wheel</code>), and specify their; own version of OpenSSL. For those users, the <code>CFLAGS</code>, <code>LDFLAGS</code>,; <code>INCLUDE</code>, <code>LIB</code>, and <code>CRYPTOGRAPHY_SUPPRESS_LINK_FLAGS</code> environment; variables will no longer be respected. Instead, users will need to; configure their builds <code>as documented here</code>_.</li>; <li>Added support for; :ref:<code>disabling the legacy provider in OpenSSL 3.0.x&lt;legacy-provider&gt;</code>.</li>; <li>Added support for disabling RSA key validation checks when loading RSA; keys via; :func:<code>~cryptography.hazmat.primitives.serialization.load_pem_private_key</code>,; :func:<code>~cryptography.hazmat.primitives.serialization.load_der_private_key</code>,; and; :meth:<code>~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateNumbers.private_key</code>.; This speeds up key loading but is :term:<code>unsafe</code> if you are loading potentially; attacker supplied keys.</li>; <li>Significantly improved performance for; :class:<code>~cryptography.hazmat.primitives.ciphers.aead.ChaCha20Poly1305</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyca/cryptography/commit/d6951dca25de45abd52da51b608055371fbcde4e""><code>d6951dc</code></a> changelog + security fix backport (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8231"">#8231</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/138da90c8450446b19619e3faa77b9da54c34be3""><code>138da90</code></a> workaround scapy bug in downstream tests (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8218"">#8218</a>) (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8228"">#8228</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/69527bc7",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12668:3014,unsafe,unsafe,3014,https://hail.is,https://github.com/hail-is/hail/pull/12668,4,['unsafe'],['unsafe']
Safety,"LTRjZjJhNTdhZDkzOCJ9fQ=="" width=""0"" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""d384d00e-b18b-41bc-871f-4cf2a57ad938"",""prPublicId"":""d384d00e-b18b-41bc-871f-4cf2a57ad938"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr);  [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr);  [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr);  [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13718:5384,remediat,remediationStrategy,5384,https://hail.is,https://github.com/hail-is/hail/pull/13718,1,['remediat'],['remediationStrategy']
Safety,"LWJmMWY5Mzc1NTVhYyJ9fQ=="" width=""0"" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""759202b1-ae50-4125-b3a5-bf1f937555ac"",""prPublicId"":""759202b1-ae50-4125-b3a5-bf1f937555ac"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr);  [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr);  [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr);  [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13836:5452,remediat,remediationStrategy,5452,https://hail.is,https://github.com/hail-is/hail/pull/13836,1,['remediat'],['remediationStrategy']
Safety,"LWY3ZGM4YjIwOTVhNiJ9fQ=="" width=""0"" height=""0""/>;  [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr).  [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings).  [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""6e02a47f-633e-4605-b359-f7dc8b2095a6"",""prPublicId"":""6e02a47f-633e-4605-b359-f7dc8b2095a6"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**.  [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr);  [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr);  [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr);  [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13933:5452,remediat,remediationStrategy,5452,https://hail.is,https://github.com/hail-is/hail/pull/13933,1,['remediat'],['remediationStrategy']
Safety,LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:6853,abort,abortStage,6853,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['abort'],['abortStage']
Safety,"Looking at the IR generated by table.flatten, this snippet:; ```; >>> import hail as hl; >>> t = hl.utils.range_table(10); >>> t2 = t.annotate(**{f'f{i}': i for i in range(5)}); >>> t2.flatten().collect(); ```; generates the following IR:; ```; (GetField rows; (TableCollect; (TableMapRows; (TableOrderBy (Aidx); (TableMapRows; (TableRange 10 8); (InsertFields; (SelectFields (idx); (Ref row)); None; (f0; (I32 0)); (f1; (I32 1)); (f2; (I32 2)); (f3; (I32 3)); (f4; (I32 4))))); (Let __uid_3; (Ref row); (InsertFields; (SelectFields (); (SelectFields (idx f0 f1 f2 f3 f4); (Ref row))); None; (idx; (GetField idx; (Ref __uid_3))); (f0; (GetField f0; (Ref __uid_3))); (f1; (GetField f1; (Ref __uid_3))); (f2; (GetField f2; (Ref __uid_3))); (f3; (GetField f3; (Ref __uid_3))); (f4; (GetField f4; (Ref __uid_3)))))))); ```; If we look at the last `TableMapRows` IR, the entire thing `(Let __uid_3 )` is entirely a no-op, but we're still compiling and generating code for the (post-optimization) IR:; ```; (InsertFields; (SelectFields (); (Ref row)); None; (idx; (GetField idx; (Ref row))); (f0; (GetField f0; (Ref row))); (f1; (GetField f1; (Ref row))); (f2; (GetField f2; (Ref row))); (f3; (GetField f3; (Ref row))); (f4; (GetField f4; (Ref row)))); ```. (cc @tpoterba I added a second `ForwardLets` in `Optimize` before the `Simplify`, although I'm not sure that's actually the correct place to put it; in this case, I think it may eventually come out in the wash given how many passes we make through any given pipeline, but I've noticed that currently our python tends to generate IR of the form:; ```; (TableMapRows; (Let __uid_n; (Ref row); <mapped value, sometimes using (Ref __uid_n) and sometimes (Ref row)>; ```; and that redundant binding at the top level means that the first Simplify pass misses quite a few optimizations! I'm not super attached to leaving it there, but I do think we might want to consider forwarding Lets on any IRs from python before optimization.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7719:1729,redund,redundant,1729,https://hail.is,https://github.com/hail-is/hail/pull/7719,1,['redund'],['redundant']
Safety,"MER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe wi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10705:2851,timeout,timeout,2851,https://hail.is,https://github.com/hail-is/hail/pull/10705,1,['timeout'],['timeout']
