quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Availability," as low as possible as long as it fits in my GPU or should I do something smarter?. Yes, essentially. > * try to make bigger train_patch_size : I'm a bit uncomfortable setting batch_size lower than 4 (or even 8) to be honest, do you perform gradient accumulation under the hood? Maybe I'm just freaking out for nothing but don't you feel that a batch_size of 2 is a bit too small to get a nice gradient descent?. Keep in mind that every pixel (that belongs to an object) contributes a gradient signal. We often trained with a batch size of 1 and didn't see a problem. > My problem is that some cells are close to each other, so my initial labels are not very good at differientiating adjacent cells (which creates big objects that don't look like cells at all) and same goes with the resulting UNet segmentation, that's why I decided to switch to StarDist3D.; > First results from StarDist seem already much better at creating nice round cells, but in order to train StarDist I simply separated my binary mask using connex component : so my ground truth for StarDist never really shows adjacent cells (as they are merged into one same label). I don't see how you can overcome this problem without fixing the ground truth (at least in part). What we have seen is that StarDist can potentially be trained with labelling errors â€“ as long as these are not biased, which they seem to be in your case (i.e. touching nuclei are always incorrectly merged). > Because of this I don't really trust the automated nms_thresh search function (labels are wrong so it's going to be hard for the search). (As I said above, I wouldn't trust the results in general.); Note that the automatic threshold finding is in some sense just for convenience to set good thresholds that work well on average. Depending on the application, one would, e.g., increase the probability threshold to prefer more accurate predictions (fewer false positives) at the expense of missing some nuclei (more false negatives). > Does 0.4 means",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-692842138:1053,mask,mask,1053,,https://github.com/stardist/stardist/issues/87#issuecomment-692842138,1,['mask'],['mask']
Availability,"+G ...b3d8bbwe\WinStore.App.exe N/A |; | 0 N/A N/A 20168 C+G ...zilla Firefox\firefox.exe N/A |; | 0 N/A N/A 22232 C ...\envs\stardist\python.exe N/A |; +-----------------------------------------------------------------------------+; ```. . - Type:. `$ nvcc -V`. . Output:. ```; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2020 NVIDIA Corporation; Built on Thu_Jun_11_22:26:48_Pacific_Daylight_Time_2020; Cuda compilation tools, release 11.0, V11.0.194; Build cuda_11.0_bu.relgpu_drvr445TC445_37.28540450_0; ```. . 3. Open the anaconda terminal and create the stardist conda environment. `$ conda create -n stardist pip python==3.8`. . 4. Activate the conda environment. `$ conda activate stardist`. . 5. Install *numpy*. `$ conda install numpy`. 6. Install *tesorflow-gpu* nightly using pip (tf-nightly-gpu 2.5.0.dev20201212). `pip install tf-nightly-gpu`. 7. Follow the installation instructions for stardist. https://github.com/mpicbg-csbd/stardist. . <!--I got a compatibility issue error with h5py library version. I just continue with the installation and everything seems to work correctly-->. 8. Install the NEUBIAS academy Jupyter Notebook for testing stardist installation . `$ git clone https://github.com/maweigert/neubias_academy_stardist.git`. ##### Run stardist-gpu on the NVIDIA RTX 3080. 1. Start the *Jupyter Notebook* . . 2. Comment the following lines:. ```; $ #%tensorflow_version 1.x; ; $ #!pip install stardist; ```. . 3. The direct downloads didn't work for me, I just comment these lines out and downloaded the sample data before. ```; '''; download_and_extract_zip_file(; url = 'https://github.com/mpicbg-csbd/stardist/releases/download/0.1.0/dsb2018.zip',; targetdir = 'data',; verbose = 1,; ); '''; ```. 4. To avoid TF GPU out of memory issues I add the following code:. ```python; import tensorflow as tf; gpus = tf.config.experimental.list_physical_devices('GPU'); tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.Virt",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:3931,error,error,3931,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,1,['error'],['error']
Availability,"2. Using numba frees the gpu, BUT leaves it in a state that is not valid and pytorch then fails with the following error:; ```; RuntimeError: CUDA error: invalid argument ; Traceback (most recent call last): ; File ""run_all_workflows.py"", line 62, in <module> ; run_all_workflows() ; File ""run_all_workflows.py"", line 50, in run_all_workflows ; name, rt = run_instance_analysis2(config) ; File ""/g/kreshuk/pape/Work/covid/batchlib/antibodies/instance_analysis_workflow2.py"", line 123, in run_instance_analysis2 ; ignore_failed_outputs=config.ignore_failed_outputs) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/workflow.py"", line 104, in run_workflow ; raise e ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/workflow.py"", line 99, in run_workflow ; state = job(folder, **run_kwargs) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/base.py"", line 421, in __call__ ; super().__call__(folder, **kwargs) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/base.py"", line 210, in __call__ ; self.run(input_files, output_files, **kwargs) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/analysis/cell_level_analysis.py"", line 116, in run ; _save_all_stats(input_file, output_file) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/analysis/cell_level_analysis.py"", line 101, in save_all_stats ; sample = self.load_sample(in_file, device=device) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/analysis/cell_level_analysis.py"", line 52, in load_sample ; marker = torch.FloatTensor(marker.astype(np.float32)).to(device) ; RuntimeError: CUDA error: invalid argument; ```; (If I run the same task without tensorflow being involved it runs through without any issues). 3. I also tried running in a sub-process as suggested e.g. [here](https://stackoverflow.com/questions/39758094/clearing-tensorflow-gpu-memory-after-model-execution), but it results in the same issue as with 2.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/44#issuecomment-615880829:115,error,error,115,,https://github.com/stardist/stardist/issues/44#issuecomment-615880829,3,['error'],['error']
Availability,"40450_0; ```. . 3. Open the anaconda terminal and create the stardist conda environment. `$ conda create -n stardist pip python==3.8`. . 4. Activate the conda environment. `$ conda activate stardist`. . 5. Install *numpy*. `$ conda install numpy`. 6. Install *tesorflow-gpu* nightly using pip (tf-nightly-gpu 2.5.0.dev20201212). `pip install tf-nightly-gpu`. 7. Follow the installation instructions for stardist. https://github.com/mpicbg-csbd/stardist. . <!--I got a compatibility issue error with h5py library version. I just continue with the installation and everything seems to work correctly-->. 8. Install the NEUBIAS academy Jupyter Notebook for testing stardist installation . `$ git clone https://github.com/maweigert/neubias_academy_stardist.git`. ##### Run stardist-gpu on the NVIDIA RTX 3080. 1. Start the *Jupyter Notebook* . . 2. Comment the following lines:. ```; $ #%tensorflow_version 1.x; ; $ #!pip install stardist; ```. . 3. The direct downloads didn't work for me, I just comment these lines out and downloaded the sample data before. ```; '''; download_and_extract_zip_file(; url = 'https://github.com/mpicbg-csbd/stardist/releases/download/0.1.0/dsb2018.zip',; targetdir = 'data',; verbose = 1,; ); '''; ```. 4. To avoid TF GPU out of memory issues I add the following code:. ```python; import tensorflow as tf; gpus = tf.config.experimental.list_physical_devices('GPU'); tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4092)]); ```. . Output:. ```; 2020-12-13 07:44:01.102879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties:; pciBusID: 0000:68:00.0 name: GeForce RTX 3080 computeCapability: 8.6; coreClock: 1.785GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s; 2020-12-13 07:44:01.103001: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll; 2020-12-13 07:44",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:4400,down,downloads,4400,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,2,['down'],"['downloaded', 'downloads']"
Availability,"> Based on the tqdm estimates, the time will still be quite long, ~8 hours, but using only a single cpu (accessible on my cluster now due to lower RAM requirement) will significantly bring down the cost. Why are you only using a single CPU now? The non-maximum suppression will be much faster with more CPU/cores available. > Is there any interest in running the tiles in a cluster environment with DASK (or similar)?. We first want to roll out this functionality with a simple API for single-machine use. Running this in a distributed environment is certainly appealing, but makes everything _much_ more complicated, hence I can't promise you when/if we'll do it. If you have suggestions on how to do this, please tell us.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/36#issuecomment-597133170:189,down,down,189,,https://github.com/stardist/stardist/issues/36#issuecomment-597133170,2,"['avail', 'down']","['available', 'down']"
Availability,"> First thank you and congratulations for this great work, this repo is truly amazing. Thanks, I'm glad you like it!. > ```; > conf = Config3D (; > ...; > train_patch_size = (48,192,192), # should I make something more cubic?; > train_batch_size = 4, # Tried to set the largest batch size possible without get OOM error; > train_epochs = 50; > ); > ```. Since your object sizes are almost isotropic, you could try to use `(160,192,192)` or similar and decrease the batch size if needed, but I doubt it'll make much of a difference. Of course, if the objects are (often) larger than 48 pixels, it can make a big difference to enlarge the patch size such that objects are fully included.; The number of epochs seems a bit small, but you seem to know what you're doing.; ; > ```; > n,h,w = X_test[2].shape; > labels, details = model.predict_instances_big(X_test[2],; > axes='ZYX',; > block_size=(n, int(h/2), int(w/2)), #conf.train_patch_size; > min_overlap=16,; > context = (30,30,30), #(30,30,30),; > prob_thresh=0.4; > ); > ```. How big are the test stacks, i.e. what is `X_test[2].shape`?; If it has similar size than the training stacks, you can simply use `model.predict_instances`, which doesn't require you to set this many parameters. The only important one would be `n_tiles`, which determines into how many tiles the input stack will be chopped before prediction runs on the GPU. Please try using this function first. The function `model.predict_instances_big` is only intended for *really* big images, which can't even be loaded in RAM or which require excessive computation and RAM requirements during the CPU-based non-maximum suppression step (to prune redundant object predictions) that runs after the CNN prediction on the GPU. > 1- block_size : I understand that we are treating the large volume with smaller blocks, but shouldn't block_size be the same as training size? If I set `block_size=conf.train_patch_size` I end up with issues with context and overlap, probably because I have",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:314,error,error,314,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,1,['error'],['error']
Availability,"> Hello everyone, the fractional offsets issue should be corrected now in the latest DeepImageJ [release](https://github.com/deepimagej/deepimagej-plugin/releases/tag/2.1.11). I'm testing this release right now on Ubuntu 20.04 (without GPU acceleration). I still see the same behavior when adding a model (dialog quickly appears and disappears), but the model seems to be actually installed this time. It was quite confusing to me that there was no user feedback whether a model was successfully installed or not. Unfortunately, I get an error when I open ""DeepImageJ Run"", select my newly-installed model, and then click on ""Test model"". First, nothing happens but one CPU core is used 100%. After a couple of minutes, the Fiji Console pops up with the following error message:. <details>; <summary>Expand to show long error message</summary>. ```; [INFO] No TF library found in /home/uwe/Downloads/fiji-linux64/Fiji.app/lib/.; Exception in thread ""AWT-EventQueue-0"" java.lang.StackOverflowError; 	at java.util.regex.Pattern$Loop.match(Pattern.java:4767); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3779); 	at java.util.regex.Pattern$Branch.match(Pattern.java:4606); 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4660); 	at java.util.regex.Pattern$Loop.match(Pattern.java:4787); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3779); 	at java.util.regex.Pattern$Branch.match(Pattern.java:4606); 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4660); 	at java.util.regex.Pattern$Loop.match(Pattern.java:4787); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1042890741:538,error,error,538,,https://github.com/stardist/stardist/pull/171#issuecomment-1042890741,3,['error'],['error']
Availability,"> Hi @talkhanz,; > ; > It's hard to understand what you mean (the left image above seems to be 8bit, but the normalized image should be 32 bit). Please describe in more detail what you did, what you expected, and what happened instead :). Im sorry if I was not clear. The image was converted to 8 bit while I was using FIJI (i.e not something that was done through code. The code produces 32 bit as you suggested). I basically ran the training notebook from the github example directory but with slight modification to the file reading processing. I have emailed the training and prediction .py files at your epfl email. I used the Allen Institute aicsimageio [https://allencellmodeling.github.io/aicsimageio/index.html](url) The dataset I used was from the Broad Institute. ( train images can be found from [https://data.broadinstitute.org/bbbc/BBBC024/BBBC024_v1_c00_highSNR_images_TIFF.zip](url) while the masks from [https://data.broadinstitute.org/bbbc/BBBC024/BBBC024_v1_c00_highSNR_foreground.zip](url). I was expecting elongated cellular segmentation like the ground truth available from the broad_institute. The snap for ground truth looks like this :; <img width=""486"" alt=""expected_"" src=""https://user-images.githubusercontent.com/63508490/91554922-77cc8d00-e949-11ea-811e-51307bb55ed3.PNG"">. This is what I got instead : ; <img width=""159"" alt=""actual_"" src=""https://user-images.githubusercontent.com/63508490/91554950-8155f500-e949-11ea-8598-ae7e882251d3.PNG"">. I hope I was a bit clearer,; Please let me know if you require anything else,; Best; Edit:; Just to add a little detail. The training data is being modified through normalization but the model seems to be predict the labels correctly given the normalized images it has received. Segmentation is correct based on the training images",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/84#issuecomment-682469217:909,mask,masks,909,,https://github.com/stardist/stardist/issues/84#issuecomment-682469217,2,"['avail', 'mask']","['available', 'masks']"
Availability,"> However DeepImageJ should already be able to run Stardist. I have succesfully run it in my computer. Instead of using the ""Test model"" button, please try opening an image, then selecting the model and finally running the model pressing ""ok"". Ah, I didn't expect that DeepImageJ works like this. Wouldn't it be more intuitive to have separate menu items for model testing and running?. > the ""Test model"" button should be fixed now, feel free to test the new release with the funcionality fixed:; > https://github.com/deepimagej/deepimagej-plugin/releases/tag/2.1.12. I can confirm that model testing and running on a different opened image works for me now. > @uschmidt83 I think we can assume it works now in deepImageJ. Should we go ahead and merge it?. There's one issue I don't understand yet. Why did we bother to put the `stardist_postprocessing.ijm` macro into the exported model zip file if there's no way to run this as *Postprocessing* from DeepImageJ?. When I did run the postprocessing macro from the Script Editor (opened manually from the `Fiji.app/models/<model_name>` folder) after DeepImageJ prediction, it showed me an error `[ERROR] Could not find input image with name/title ""scores"".` in the Console. I then tried the exact same thing again, and it worked as intended without errors ðŸ¤·â€â™‚ï¸. > (Also there are still some failing tests, but this seems to be unrelated to the changes here.). Yes, those are unrelated. The bioimage.io tests are only active in Github action tests with `tensorflow<2` in the name, and those run through just fine.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1047694133:1139,error,error,1139,,https://github.com/stardist/stardist/pull/171#issuecomment-1047694133,2,['error'],"['error', 'errors']"
Availability,"> However, I noticed the absence of a `checkpoint.ckpt` file (instead there is a `variables.index` file). Is there a way to generate this `.ckpt` file?. I have never needed a checkpoint myself, hence I don't know. However, you can try to export one yourself since you have full access to the Keras model via `model.keras_model`. Here's the general documentation for the [SavedModel format](https://www.tensorflow.org/guide/saved_model), and here is a guide specific to [Training checkpoints](https://www.tensorflow.org/guide/checkpoint). Hope that helps. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/61#issuecomment-643508305:39,checkpoint,checkpoint,39,,https://github.com/stardist/stardist/issues/61#issuecomment-643508305,4,['checkpoint'],"['checkpoint', 'checkpoints']"
Availability,"> I am not sure whether the bug with the error message still needs to be fixed, but the underlying problem was in my data. Yes, we should maybe change the error message :). Thanks for the feedback!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/80#issuecomment-672828787:41,error,error,41,,https://github.com/stardist/stardist/issues/80#issuecomment-672828787,2,['error'],['error']
Availability,"> I can confirm that model testing and running on a different opened image works for me now. Great. thanks for checking!. > There's one issue I don't understand yet. Why did we bother to put the `stardist_postprocessing.ijm` macro into the exported model zip file if there's no way to run this as _Postprocessing_ from DeepImageJ?. We want to support this in deepImageJ but are currently a bit out of manpower to implement it. There's probably a relatively simple solution. But also with the given solution, the script is at least packaged with the model, so that it can be applied manually. ; We will work on better post-processing integration, and there is probably a simple solution, but we don't want to block releasing the model zoo for now to incorporate it. We can update the readme to explain this, but I would suggest to do this in a follow-up PR as this one is pretty large already and it's not critical to the functionality. > When I did run the postprocessing macro from the Script Editor (opened manually from the `Fiji.app/models/<model_name>` folder) after DeepImageJ prediction, it showed me an error `[ERROR] Could not find input image with name/title ""scores"".` in the Console. I then tried the exact same thing again, and it worked as intended without errors man_shrugging. @esgomezm @carlosuc3m do you have any idea how to fix this?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1047715559:1111,error,error,1111,,https://github.com/stardist/stardist/pull/171#issuecomment-1047715559,2,['error'],"['error', 'errors']"
Availability,"> Is there any general guidance re: image scale and resolution or are there published statistics for the training sets?. The dsb2018 training data had objects in the range of 10-70 pixels in diameter. So if your objects are considerably larger, I would suggest downscaling them. See as well the [FAQ](https://stardist.net/docs/faq.html#is-there-an-upper-size-limit-for-objects-to-be-well-segmented). > Similarly is there a recommendation to tile images to specific sizes before segmenting?. You can use any image size, no need to tile.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/170#issuecomment-935189051:261,down,downscaling,261,,https://github.com/stardist/stardist/issues/170#issuecomment-935189051,1,['down'],['downscaling']
Availability,"> No, I never do. It's still open. Do I have to close it? I'm really sorry, but I cannot find out, how to run the `nvidia-smi`. I know it was running once, but now I only get the output `nvidia-smi is not recognized as an internal or external command, operable program or batch file.`. Tensorflow will reserve all GPU memory available once its first called, so a training notebook that is left open after being run will ""occupy"" the GPU leaving the prediction notebook with no memory to compute on, which is what you have seen. So shutting down all GPU notebooks before running another one is always recommended. We should add that to the notebook, so thanks for the feedback! :). ; > ,I get an image without any labeling. Can you post the figure that is produced by this cell?. > when I run `example(model, 0)`, I get the following error:. you need to change `model.predict_instances(img, n_tiles=(1,4,4)` inside the `example` function too",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/119#issuecomment-787089461:325,avail,available,325,,https://github.com/stardist/stardist/issues/119#issuecomment-787089461,3,"['avail', 'down', 'error']","['available', 'down', 'error']"
Availability,"> Since the pretrained HE model uses this dataset, did you generate single label masks during training?. Yes, we simply max projected the labels. . > And in doing so wouldn't one drop crucial neighborhood information?. What do you mean by that?. > Would you see hindrances to just cutting labels where they overlap? There would be obviously an information loss, however, one would incorporate neighborhood information during training?. Yes, one could as well just remove the overlapping areas. We however didn't really look into the effects of doing so systematically yet.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/140#issuecomment-834236329:81,mask,masks,81,,https://github.com/stardist/stardist/issues/140#issuecomment-834236329,1,['mask'],['masks']
Availability,"> Sorry, too tired apparently :). @maweigert No problem, I hadn't made it too clear that it wasn't my data :). > Did you try to play around with the grid size (e.g. setting it to (4,4))?. Yeah I could try reducing the resolution, so far I was using (2,2) because that seemed like a reasonable choice considering the data and detail of segmentation I want to recover. I guess I'm also a bit curious regarding how precise the polygon reconstructions could get in principle with optimised training and whether there'd be a clear way of thinking about it. In practical terms for now the segmentation is nicely reliable and of sufficient quality so I'll stick to that for the data of my collaborator!. Thanks a lot for the thoughts everyone and feel free to close the issue :)",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-604961417:358,recover,recover,358,,https://github.com/stardist/stardist/issues/33#issuecomment-604961417,2,"['recover', 'reliab']","['recover', 'reliable']"
Availability,"> This is working locally now. (Needs the changes in [bioimage-io/core-bioimage-io-python#142](https://github.com/bioimage-io/core-bioimage-io-python/pull/142)). Thanks, I updated this locally. > Also, I can only run one of the tests successfully at a time, because tensorflow does not properly clear the gpu. Do you get `tensorflow.python.framework.errors_impl.FailedPreconditionError: 2 root error(s) found.`?. I think that's not the reason, rather TensorFlow is in a bad state after running `model.export_TF` once. This is a known issue, and I currently don't know how to workaround that.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-968756455:394,error,error,394,,https://github.com/stardist/stardist/pull/171#issuecomment-968756455,1,['error'],['error']
Availability,"> We actually found that we don't have GPU available for Tensorflow after our build. What do you mean? Are you referring to [this problem](https://github.com/NVIDIA/nvidia-docker/issues/1034)?. > we suspect this is because we used NVIDIA_DRIVER_VERSION=455 (not listed as supported in the README).; > Will the updated version of tensorflow work with this driver version?. The variable `NVIDIA_DRIVER_VERSION` is simply used to indicate the name of a package to be installed. If the `docker build` command completed successfully, then it was likely not the issue.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/108#issuecomment-754113033:43,avail,available,43,,https://github.com/stardist/stardist/issues/108#issuecomment-754113033,1,['avail'],['available']
Availability,"> a bit confusing. I agree, we should change it a some point (the naming you suggest indeed makes a bit more sense). ; . > Specifically, why do you use `use_gpu = False and ...` which will always evaluate to `False` ?. This is to ensure that `gputools` is available, once the user decides to set `use_gpu`. Note, that this is a bit of a power-user setting (if you want to speed up training), since installing `gputools`/`pyopencl` can be a bit involved on some systems. This is why it is deactivated by default. . Hope that clears it up a bit and thanks for the feedback! . M",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/75#issuecomment-661022071:256,avail,available,256,,https://github.com/stardist/stardist/issues/75#issuecomment-661022071,1,['avail'],['available']
Availability,"> the median size object is [ 37.5 156. 160.5]. Before or after downscaling the input images?. > I increased the grid and the unet to 3 .; > After learning, when I apply to predictions on new images the issues is that the Z is right (because below 64) but the X and Y are way too small because of the FOV of [ 64 128 128] .; > ; > Hence it can only see object below that shape : so many small nucleus detected on 1 single nucleus; > Hence my question is how to use Stardist when trained on downscale object onto bigger images with full scale (50, 1440, 1920) ?. It is difficult to understand what you mean. Ideally show us the code/notebook.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/118#issuecomment-785909768:64,down,downscaling,64,,https://github.com/stardist/stardist/issues/118#issuecomment-785909768,2,['down'],"['downscale', 'downscaling']"
Availability,"Do I understand max projection correctly: for every single nucleus you have a single label mask resulting in many labelmasks Y_1 ... Y_N for only one input image X? If thats right - in order to lower overall cost - the network still will give all segmentations for the image in the best case. However, wouldn't it be helpful to the network to learn the constraining neighbors of each nucleus at a time? Or is this implicitly also captured if one has many label matrices corresponding to one image?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/140#issuecomment-834275557:91,mask,mask,91,,https://github.com/stardist/stardist/issues/140#issuecomment-834275557,1,['mask'],['mask']
Availability,"Hello,; Yes, so i install homebrew then i installed gcc then i ran the command CC=gcc-9 CXX=g++-9 pip install stardist. Then i get this error below. I checked the version of python i have on the terminal and it is; % python3 --version; Python 3.8.4. so im not really sure what to do now. ```; Defaulting to user installation because normal site-packages is not writeable; Collecting stardist; Using cached stardist-0.1.0.tar.gz (46 kB); Collecting csbdeep; Using cached csbdeep-0.6.0-py2.py3-none-any.whl (67 kB); Collecting scikit-image; Using cached scikit_image-0.14.5-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (29.3 MB); Requirement already satisfied: numpy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.8.0rc1); Collecting tifffile; Using cached tifffile-2019.7.26.2-py2.py3-none-any.whl (131 kB); Collecting backports.tempfile; python_version < ""3.4""; Using cached backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB); Requirement already satisfied: six in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.12.0); Collecting h5py; Using cached h5py-2.10.0-cp27-cp27m-macosx_10_6_intel.whl (2.9 MB); Requirement already satisfied: matplotlib in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.3.1); Collecting keras<2.4,>=2.1.2; Using cached Keras-2.3.1-py2.py3-none-any.whl (377 kB); Requirement already satisfied: scipy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (0.13.0b1); Collecting tqdm; Using cached tqdm-4.47.0-py2.py3-none-any.whl (66 kB); Collecting pathlib2; python_version < ""3""; Using cached pathlib2-2.3.5-py2.py3-none-any.whl (18 kB); Collecting imagecodecs-lite<=2020; python_version < ""3.6""; Using cached imagecodecs-lite-2019.12.3.tar.gz (1.1 MB); ERROR: Command errored out with exi",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:136,error,error,136,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['error'],['error']
Availability,"Hey,. Thank you for your feedback. 1. Access to cluster GPUs is handled by IBM LSF (Load Sharing Facility) batch system preventing usage of a GPU by multiple users. The above error is thrown when I initialise the model according to ; ```python; In[11]: model = StarDist2D(conf, name='stardist', basedir='models'); ````; in [notebook 2](https://github.com/mpicbg-csbd/stardist/blob/master/examples/2D/2_training.ipynb) so GPU memory should not be full. 2. `CUDA_VISIBLE_DEVICES` is handled by the LSF batch system. However, I also manually assigned the `CUDA_VISIBLE_DEVICES` and the error persists. If I am not mistaking, the above error is expected if we run >1 ""independent processes"" on a single GPU that is in Exclusive Process mode. Isn't this exactly what `StarDist` does and why we use . ```python; In [10]: if use_gpu:; from csbdeep.utils.tf import limit_gpu_memory; # adjust as necessary: limit GPU memory to be used by TensorFlow to leave some to OpenCL-based computations; limit_gpu_memory(0.8); ```; ?. Best,; D",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/102#issuecomment-728158838:175,error,error,175,,https://github.com/stardist/stardist/issues/102#issuecomment-728158838,3,['error'],['error']
Availability,"Hi @imand500 ,. Sorry for the late reply. . > Since the model is not saved as h5 and it's really dependent on the configuration you made. If a pretrained models is loaded; ```python ; model = StarDist2D.from_pretrained(""2D_versatile_he""); ```; it is actually downloaded and stored as a normal stardist model in your keras cache directory. You can find the location like so:; ```python; print(model.logdir); ```; So you could simply copy that folder somewhere, create a new model from that folder and then continue training with your own data. Hope that helps,. M",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/127#issuecomment-830683343:259,down,downloaded,259,,https://github.com/stardist/stardist/issues/127#issuecomment-830683343,1,['down'],['downloaded']
Availability,"Hi @maweigert,. Right now I'm working on a different strategy from the software standpoint that will solve the extra time cost for using Max and the extra memory from having to store the larger lists of neighbors (potential suppression candidates). My first implementation in the PR was a good demonstration of kdtree, but definitely suboptimal w.r.t. what can be done with the Python C-API, which I'm learning on the fly so things have gone through various stages of kludge. The idea now is that I can call the sklearn kdtree object, and it's query_radius method, from within the C++ code itself - enabling a separate query for each polygon candidate just before the inner loop of NMS begins. That way, points which are ""suppressed early"" will never be queried against the tree - reducing total query time even if the distance threshold is large. The second benefit is that the code now only has to store one neighbor list at a time, thus reducing the total RAM used. This is definitely the best way to built it _algorithmically_ but I still don't have the Python C-API down perfectly so implementing this today has been tricky. I also implemented what I mentioned before - applying the probability threshold for each tile separately during prediction. For the whole image, it cut max RAM down from ~400GB to ~50GB, and that was including the neighbors list from kdtree. So, if I can get the tile based threshold combined with the embedded python approach to the kdtree, I think I can get the whole zebrafish image segmented in like 10-15 minutes for something like 30GB RAM. Of course, the tile based prob threshold breaks a bunch of stuff, so I'll leave it to you guys to decide if you want to move in that direction generally or to just have my commit somewhere accessible on the repo so if others run into RAM issues there's some path to cutting it down without changing so many internal dependencies between functions. This is all WIP but I'm hoping to finish it soon. Thanks,; Greg",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-606357837:1071,down,down,1071,,https://github.com/stardist/stardist/pull/40#issuecomment-606357837,3,['down'],['down']
Availability,"Hi @maweigert,. for some context: we are currently setting up a user-friendly workflow for users to train stardist and we stumbled over this line.; Thanks to your explanation I understand what the `use_gpu` variable does now, but I still find the line. ```; # Use OpenCL-based computations for data generator during training (requires 'gputools'); use_gpu = False and gputools_available(); ```; a bit confusing. Specifically, why do you use `use_gpu = False and ...` which will always evaluate to `False` ? ; Wouldn't `use_gpu = gputools_available()` be sufficient?; I would assume that `gputools_available` returns `True` if the additional dependency is available and `False` otherwise. One more point: the variable name `use_gpu` could maybe be renamed to `use_gpu_for_data_processing` or so; because now it implies that `use_gpu = False` means the gpu is not used at all (which I know is not the case, but doesn't become quite clear from the notebook). Sorry for being rather nit-picky here, but if this confuses us it will likely be confusing for users as well.; In any case, we will probably adapt the notebook a bit for users and just make these changes there, but we were wondering why you were choosing to do it like this in the first case.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/75#issuecomment-661014557:655,avail,available,655,,https://github.com/stardist/stardist/issues/75#issuecomment-661014557,1,['avail'],['available']
Availability,"Hi @seismonastic ,. > Is the model in `3D_demo` a pre-trained model for 3D images?. No, that is simply the model of the 3D demo notebook when trained for a long time. A proper pre-trained model in 3D is not yet available unfortunately (many more things to consider). . Cheers,; M",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/31#issuecomment-622037189:211,avail,available,211,,https://github.com/stardist/stardist/issues/31#issuecomment-622037189,1,['avail'],['available']
Availability,"Hi Martin,. Yes, the 6Mio candidates are for prediction on the whole stack. I did try `predict_big` on this volume. What I noticed was that the memory footprint went way down. Using [Uwe's recommended parameters](https://github.com/mpicbg-csbd/stardist/issues/36#issuecomment-594974089) I was able to run the job with a single cpu (# cores requested and maximum amount of RAM available are mixed on our cluster; each core comes with 15GB RAM). However the remaining runtime predicted by tqdm after a few iterations had accumulated was something like 36 hours - so I cancelled it and gave up on it. Uwe reminded me that with one core, I likely had a maximum of 2 threads, but I did not go back and try `predict_big` with more cores. Do you think `predict_big` with say 32 cores (64 threads) could finish in less than an hour? If so I will give it another try. For different kdtree implementations - I did look through [this article written by the scikit-learn kdtree and balltree developer](https://jakevdp.github.io/blog/2013/04/29/benchmarking-nearest-neighbor-searches-in-python/), but I didn't think there was enough differences in his graphs to justify introducing a new dependency (I don't think scikit-learn was a dependency before?) and I also preferred something with an underlying C++ implementation, as there are more big speed ups available if the kdtree data structure is available for query directly within the NMS algorithm. Unfortunately the only RAM measurements I have are the maximum and average usage (over the full run, prediction and NMS) for prediction on the full volumes both w/ and w/o the kdtree. The command in both cases was:. `pred, det = model.predict_instances(image_norm, verbose=True, n_tiles=(4, 8, 2))`; On a volume with 860x1676x301 voxels and 6.32e6 polygon candidates. w/o kdtree max: 434.439GB; w/ kdtree max: 430.482GB. w/o kdtree avg: 221.672GB; w/ kdtree avg: 297.733GB. I'm thinking that the kdtree and the query results for this many candidates took somethi",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-601381585:170,down,down,170,,https://github.com/stardist/stardist/pull/40#issuecomment-601381585,2,"['avail', 'down']","['available', 'down']"
Availability,"Hi Michael,. > 1. My data (example data provided here: https://drive.google.com/drive/folders/17q11-hAJjCs72YFbsVKoGve3e5nzXyJf?usp=sharing) appears to be the wrong shape. Any ideas for a simple fix?. I can reproduce that one of your images is opened with seemingly wrong shape in Python. I don't know why that happens, but I've found that simply re-saving the image as a Tiff file from Fiji solves the problem, i.e. it then loads correctly in Python. > 2. I tried running prediction anyway, and find that I get an out of memory error.; >; > Is this because my tensorflow/CUDA is not communicating with the GPU?. No, this is also related to the wrong image shape and should go away.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/62#issuecomment-644116825:529,error,error,529,,https://github.com/stardist/stardist/issues/62#issuecomment-644116825,1,['error'],['error']
Availability,"Hi Uwe, . loading directly in a shell does not throw the error. It happens only when I use snakemake which is bizarre. This might be related to another error I get which is that the libiomp5.so is already initialised ( OMP: Error #15: Initializing libiomp5.so, but found libomp.so already initialized). I wonder whether due to this conflict the model is accessed simultaneously by two process leading to the error. . Thanks for looking into this, ; Marc . Dr. Marc Bickle ; Technology Development Studio ; Max Planck Institute of Molecular Cell Biology and Genetics ; Pfotenhauerstrasse 108 ; 01307 Dresden Germany . Phone: +49 (0)172 536 5517 . From: ""Uwe Schmidt"" <notifications@github.com> ; To: ""mpicbg-csbd/stardist"" <stardist@noreply.github.com> ; Cc: ""Marc Bickle"" <bickle@mpi-cbg.de>, ""Author"" <author@noreply.github.com> ; Sent: Sunday, October 25, 2020 11:36:57 PM ; Subject: Re: [mpicbg-csbd/stardist] loading 2D_versatile_fluo error (#93) . Hi, the error message indicates a problem with loading the weights from the HDF5 file. . First, to rule out Snakemake as an issue, did you try to simply load the pre-trained model in a Python shell? For example, does this throw the same error? ; $ python -c ""from stardist.models import StarDist2D; StarDist2D.from_pretrained('2D_versatile_fluo')"" . If that's the case, then I'd try to use a different/newer version of the HDF5 library h5py . . Best, ; Uwe . â€” ; You are receiving this because you authored the thread. ; Reply to this email directly, [ https://github.com/mpicbg-csbd/stardist/issues/93#issuecomment-716223889 | view it on GitHub ] , or [ https://github.com/notifications/unsubscribe-auth/AAU56KJ4GEUFX4BPHFYPRBLSMSSATANCNFSM4SZXDKIQ | unsubscribe ] .",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/93#issuecomment-716329806:57,error,error,57,,https://github.com/stardist/stardist/issues/93#issuecomment-716329806,6,['error'],['error']
Availability,"Hi Uwe,; It's entirely possible it's specific to our implementation then. What I meant by unstable is that I had obtained a few times an unusual loss curve with an additional peak half way through the number of epoch. See below. This does not match any changes in learning rate either. <img width=""910"" alt=""Screenshot 2021-02-24 at 22 12 44"" src=""https://user-images.githubusercontent.com/21193399/109297054-4d408b80-7829-11eb-9aac-f0e9788e5907.png"">. But either way, we've now upgraded it to TF2.x and implemented the default settings as close to yours as possible, as well as the exact ways to do augmentation. And this seems to perform much better and not give the phantom masks or the unusual loss curves. Thanks a lot for your help. I really appreciate it.; Best,. Romain",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/116#issuecomment-786602735:677,mask,masks,677,,https://github.com/stardist/stardist/issues/116#issuecomment-786602735,1,['mask'],['masks']
Availability,"Hi!. Thanks, I made sure now that all the stardist modules are the same versions. Unfortunately, tensorflow is still not configured properly on the computing cluster, crashing with errors that say:; ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; Aborted; ```. with while trying to do very simple print statements, so this is something I need to solve with our sysadmin. I am loading the trained module onto my local machine (OSX, Python3.6) and using it to predict on similar images. I can now load the cluster-trained (stardist 0.41) model onto my machine:; ```; model = StarDist3D(None, name='modelname',; basedir=path.join(trainingdir,'models')); labels, details = model.predict_instances(test_img[); ```. The kernel runs for a minute or so and then silently crashes without any error message, and then restarts silently (no warning messages on the bash shell that spyder is running in either). <img width=""804"" alt=""Screen Shot 2020-02-21 at 3 09 51 PM"" src=""https://user-images.githubusercontent.com/5126258/75041128-3cde2e00-54bc-11ea-9233-cd8212b14a3b.png"">. --. The training/labels are 3D images of clusters of cells. One mid-level slice looks like this, with 1Âµm slices, about 20Âµm:. <img width=""851"" alt=""Screen Shot 2020-02-21 at 3 02 38 PM"" src=""https://user-images.githubusercontent.com/5126258/75040633-4c10ac00-54bb-11ea-8f3a-f853a9fda51b.png"">. ---. On my local machine, I am running: tensorflow 1.15.0, Python 3.6.; On the cluster, I was running: tf 1.14.0, Python 3.6. Thank you!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/35#issuecomment-589670082:181,error,errors,181,,https://github.com/stardist/stardist/issues/35#issuecomment-589670082,2,['error'],"['error', 'errors']"
Availability,"Hi, thanks for using StarDist. > As StarDist need numpy, its installation failed as numpy is not yet available. Yes, thanks for reminding me about that. Since `numpy` is typically installed, most people don't notice. > I fix my issue by installing numpy before my other requirements. Yes, that's currently the only way. > however I suggest to add numpy in the requirements of StarDist, I think it is missing. It's not that simple. Actually, we implicitly require `numpy` through `csbdeep`.; The real issue is that we need `numpy` to even build the wheel (package), i.e. our `setup.py` imports from `numpy`. I've [just added](https://github.com/mpicbg-csbd/stardist/commit/9bd7ff911bca5e87cbe70a8549b9c746d6d1c245) a `pyproject.toml`, which will hopefully fix this issue in the next release. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/71#issuecomment-657642225:101,avail,available,101,,https://github.com/stardist/stardist/issues/71#issuecomment-657642225,1,['avail'],['available']
Availability,"Hi, the error message indicates a problem with loading the weights from the HDF5 file. First, to rule out Snakemake as an issue, did you try to simply load the pre-trained model in a Python shell? For example, does this throw the same error?; ```; $ python -c ""from stardist.models import StarDist2D; StarDist2D.from_pretrained('2D_versatile_fluo')""; ```. If that's the case, then I'd try to use a different/newer version of the HDF5 library `h5py`. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/93#issuecomment-716223889:8,error,error,8,,https://github.com/stardist/stardist/issues/93#issuecomment-716223889,2,['error'],['error']
Availability,"I don't understand why that should be a problem. Please be more descriptive - what are you trying to do exactly and what error do you observe? . Besides (from the issue template):. `If you open a new topic, please provide a clear and concise description to understand and ideally reproduce the issue you're having`",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/168#issuecomment-922922737:121,error,error,121,,https://github.com/stardist/stardist/issues/168#issuecomment-922922737,1,['error'],['error']
Availability,I think the next steps are:; - also generate the config and macro for stardist postprocessing with the exporter so that the model can be run in deepimagej; - create the python functionality to run a stardist bioimageio model + stardist postprocessing (I think I would just add this to bioimageio.core for simplicity); - check that we can use the model in deepimagej and python; - release a new stardist version so that the functionality is available; - update the zero-cost notebook to use this functionality to export stardist bioimageio models. I will start working on the first point later.,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-973972757:440,avail,available,440,,https://github.com/stardist/stardist/pull/171#issuecomment-973972757,1,['avail'],['available']
Availability,"Thank you for your quick reply! . > I suggest you do the prediction in Python to get the label image, which you can then import in Fiji with the [3D Roi Manager](https://imagejdocu.tudor.lu/plugin/stacks/3d_roi_manager/start) (installation instructions [here](https://imagejdocu.tudor.lu/plugin/stacks/3d_ij_suite/start#installation)). This should work well, as long as you don't have (strongly) overlapping predictions or it's not important to you to get the complete shapes for all objects. This sounds really good. I do have quite overlapping nuclei though unfortunately. Apologies if this is a bit of a starter question but would I just save the label image as a tiff file and then open it in FIJI, and then segment it with the 3D ROI viewer? . > It's not that simple and even if it worked, this won't get you 3D ROIs in Fiji. Our [StarDist Fiji plugin](https://imagej.net/StarDist) currently only works in 2D because it's not that easy to support 3D... but we hope to get this done eventually. Ah I thought it wouldn't be simple. The error message I was getting was that the input tensor had to be 5 dimensional, and was wondering whether the mismatch was due to tensor organisation convention of NHWC not including a third spatial dimension. . The images you have from paintera look really beautiful, I think that is my aim here, but perhaps FIJI's 3d viewer of the 3d ROIs would get something similar?. Thanks again!!. Michael Schwimmer",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/37#issuecomment-597304169:1039,error,error,1039,,https://github.com/stardist/stardist/issues/37#issuecomment-597304169,1,['error'],['error']
Availability,"Thank you very much for the detailed answer, it helps me a lot. All right, I'm going to:; - switch to `model.predict_instances` as my test stacks are of similar sizes as my training stacks (~400x1000x1000); - play with `n_tiles` : should I just set this as low as possible as long as it fits in my GPU or should I do something smarter?; - try to make bigger train_patch_size : I'm a bit uncomfortable setting batch_size lower than 4 (or even 8) to be honest, do you perform gradient accumulation under the hood? Maybe I'm just freaking out for nothing but don't you feel that a batch_size of 2 is a bit too small to get a nice gradient descent?. Also I wanted to ask you about something else. ; At the moment I don't have nice labels that have been curated by humans (I might find some time to do this but later), but I have good enough binary masks that allows me to train algorithms like 3DUnets.; My problem is that some cells are close to each other, so my initial labels are not very good at differientiating adjacent cells (which creates big objects that don't look like cells at all) and same goes with the resulting UNet segmentation, that's why I decided to switch to StarDist3D.; First results from StarDist seem already much better at creating nice round cells, but in order to train StarDist I simply separated my binary mask using connex component : so my ground truth for StarDist never really shows adjacent cells (as they are merged into one same label).; Because of this I don't really trust the automated nms_thresh search function (labels are wrong so it's going to be hard for the search). So I played a bit with the nms_thresh to visually inspect the results but I'd like to understand better what 0.4 or 0.05 means for example. Does 0.4 means that I would tolerate that 40% of the volume of two cells (or a proxy of the volume) are actually overlapping? In terms of physics and biology, I don't expect my cells to be able to overlap at all, they can touch each other but that's i",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691334428:844,mask,masks,844,,https://github.com/stardist/stardist/issues/87#issuecomment-691334428,1,['mask'],['masks']
Availability,"Thanks for the replies, @maweigert and @uschmidt83 . > For training, a majority of cells should be fully included in the annotated trainings stacks - I fear there is no real way around that. Sorry, I may not have been as clear as I intended. We have a working model that has been created with thicker images, and the problem occurs when applying the model on thin image-stacks. However, it seems like I am not able to reproduce the error (as in crashing the process) with the data I currently have access to, but I can't get any predictions with thinner images. I performed an example run with a representative image (XY-cropped) and a duplicate image with one z-slice removed, i.e. from 5 to 4 z-slices. I have included my output below with verbose=True. All in all, it seems that for some reason StarDist is not able to create any candidates for the thinner stack. At the thickness of 4 z-layers it is expected that the largest cells do not neatly fit into the stack, but there should be plenty of smaller cells that I would think should still create candidates. So I would presume there has to be a reason for why none are created?. @uschmidt83 ; > I don't know how your data looks like, but have you tried padding the image such that the Z axis is big enough (as a trivial workaround)?. I did try that some months back by adding empty slices but it caused bloating of the labels and was not viable. But now that I think about it there probably is a better way for padding the image, such as duplicating the edge layers to provide continuity in intensities. **OUTPUT**:. Holidic_2018-12-19_180224-2; Model = DAPI20x ; Image dims = (4, 944, 2008). predicting instances with prob_thresh = 0.01 and nms_thresh = 0.4; found 0 candidates; non-maximum suppression...; Non Maximum Suppression (3D) ++++ ; NMS: n_polys = 0 ; NMS: n_rays = 128 ; NMS: n_faces = 252 ; NMS: thresh = 0.400 ; NMS: use_bbox = 1 ; NMS: using OpenMP with 16 thread(s); NMS: precompute volumes, bounding boxes, etc; precompute done",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/133#issuecomment-831993257:432,error,error,432,,https://github.com/stardist/stardist/issues/133#issuecomment-831993257,1,['error'],['error']
Availability,"The code can be found in the ZCDL4M notebook, section 5.3: https://github.com/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/BioImage.io%20notebooks/StarDist_2D_ZeroCostDL4Mic_BioImageModelZoo_export.ipynb. This is the resume:; 1. Get the ""raw model"" by concatenating the outputs (we can skip this one, it would be ok on deepImageJ):. ```python; # Load the model; model = StarDist2D(None, name=QC_model_name, basedir=QC_model_path); thres, nms = model.thresholds. # Check minimum size: it is [8,8] for the 2D XY plane; depth = model.config.unet_n_depth; pool = model.config.unet_pool; MinimumSize = [pool[0]**(depth+1), pool[1]**(depth+1)]. # Concatenate model output to use pydeepimagej and the StarDist macro; input = model.keras_model.inputs[0]; single_output = Concatenate()([model.keras_model.output[0], model.keras_model.output[1]]); new_model = Model(input, single_output). dij_config = BioImageModelZooConfig(new_model, MinimumSize); .; .; .; # Add weights information; dij_config.add_weights_formats(new_model, 'TensorFlow', ; parent=""keras_hdf5"",; tf_version=tf.__version__); dij_config.add_weights_formats(new_model, 'KerasHDF5',; tf_version=tf.__version__); ```; 2. Use `pydeepimagej` library to export the TensorFlow and keras models: https://github.com/deepimagej/pydeepimagej/blob/5aaf0e71f9b04df591d5ca596f0af633a7e024f5/pydeepimagej/yaml/create_config.py#L251. **Concerning the upsampling and downsamplings:**; The model we export does the downsampling inside as deepImageJ deals with inputs-outputs of different sizes. For the concatenation, we can skip concatenating them. The only problem is that doing so, the model won't be compatible with CSBDeep. If we make the model compatible with CSBDeep, then the TF model would be different from the keras one, so here is maybe a potential solution (?): could we use the parent key in the specs @constantinpape @oeway to link the TF and keras models?? architecture is slightly different but the weights are the same.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-968831030:1419,down,downsamplings,1419,,https://github.com/stardist/stardist/pull/171#issuecomment-968831030,2,['down'],"['downsampling', 'downsamplings']"
Availability,"but don't you feel that a batch_size of 2 is a bit too small to get a nice gradient descent?. Keep in mind that every pixel (that belongs to an object) contributes a gradient signal. We often trained with a batch size of 1 and didn't see a problem. > My problem is that some cells are close to each other, so my initial labels are not very good at differientiating adjacent cells (which creates big objects that don't look like cells at all) and same goes with the resulting UNet segmentation, that's why I decided to switch to StarDist3D.; > First results from StarDist seem already much better at creating nice round cells, but in order to train StarDist I simply separated my binary mask using connex component : so my ground truth for StarDist never really shows adjacent cells (as they are merged into one same label). I don't see how you can overcome this problem without fixing the ground truth (at least in part). What we have seen is that StarDist can potentially be trained with labelling errors â€“ as long as these are not biased, which they seem to be in your case (i.e. touching nuclei are always incorrectly merged). > Because of this I don't really trust the automated nms_thresh search function (labels are wrong so it's going to be hard for the search). (As I said above, I wouldn't trust the results in general.); Note that the automatic threshold finding is in some sense just for convenience to set good thresholds that work well on average. Depending on the application, one would, e.g., increase the probability threshold to prefer more accurate predictions (fewer false positives) at the expense of missing some nuclei (more false negatives). > Does 0.4 means that I would tolerate that 40% of the volume of two cells (or a proxy of the volume) are actually overlapping?. Yes. > In terms of physics and biology, I don't expect my cells to be able to overlap at all, they can touch each other but that's it. Does this physics rule mean that setting nms_thresh to 0 makes sense?. ",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-692842138:1366,error,errors,1366,,https://github.com/stardist/stardist/issues/87#issuecomment-692842138,1,['error'],['errors']
Availability,"ften trained with a batch size of 1 and didn't see a problem. > My problem is that some cells are close to each other, so my initial labels are not very good at differientiating adjacent cells (which creates big objects that don't look like cells at all) and same goes with the resulting UNet segmentation, that's why I decided to switch to StarDist3D.; > First results from StarDist seem already much better at creating nice round cells, but in order to train StarDist I simply separated my binary mask using connex component : so my ground truth for StarDist never really shows adjacent cells (as they are merged into one same label). I don't see how you can overcome this problem without fixing the ground truth (at least in part). What we have seen is that StarDist can potentially be trained with labelling errors â€“ as long as these are not biased, which they seem to be in your case (i.e. touching nuclei are always incorrectly merged). > Because of this I don't really trust the automated nms_thresh search function (labels are wrong so it's going to be hard for the search). (As I said above, I wouldn't trust the results in general.); Note that the automatic threshold finding is in some sense just for convenience to set good thresholds that work well on average. Depending on the application, one would, e.g., increase the probability threshold to prefer more accurate predictions (fewer false positives) at the expense of missing some nuclei (more false negatives). > Does 0.4 means that I would tolerate that 40% of the volume of two cells (or a proxy of the volume) are actually overlapping?. Yes. > In terms of physics and biology, I don't expect my cells to be able to overlap at all, they can touch each other but that's it. Does this physics rule mean that setting nms_thresh to 0 makes sense?. Yes, in some sense. But keep in mind that predicted nucleus shapes are not perfect and thus can overlap to some degree. That's why you typically want to allow for some overlap. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-692842138:2062,toler,tolerate,2062,,https://github.com/stardist/stardist/issues/87#issuecomment-692842138,1,['toler'],['tolerate']
Availability,"ges/scipy/ndimage/measurements.py in find_objects(input, max_label); 301 ; 302 if max_label < 1:; --> 303 max_label = input.max(); 304 ; 305 return _nd_image.find_objects(input, max_label). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/numpy/core/_methods.py in _amax(a, axis, out, keepdims, initial, where); 37 def _amax(a, axis=None, out=None, keepdims=False,; 38 initial=_NoValue, where=True):; ---> 39 return umr_maximum(a, axis, None, out, keepdims, initial, where); 40 ; 41 def _amin(a, axis=None, out=None, keepdims=False,. ValueError: operands could not be broadcast together with shapes (80,330,500) (85,400,500); ```; I think maybe somehow my new class is considered as one large numpy array and not as a list of numpy arrays, is that possible? Any idea how to overcome this? . - if I switch to a batch size of 2, training does not work anymore and I end up with this error:; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-21-54ffb4970400> in <module>; 3 augmenter=augmenter_fn(ZOOM_RATIO, ZOOM_PROB),; 4 epochs=200, #100,; ----> 5 seed=42). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/models/model3d.py in train(self, X, Y, validation_data, augmenter, seed, epochs, steps_per_epoch, workers); 480 callbacks=self.callbacks, verbose=1,; 481 # set validation batchsize to training batchsize (only works in tf 2.x); --> 482 **(dict(validation_batch_size = self.config.train_batch_size) if _tf_version_at_least(""2.2.0"") else {})); 483 ; 484 self._training_finished(). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs); 64 def _method_wrapper(self, *args, **kwargs):; 65 if not self._in_multi_worker_mode(): # pylint: disable=protected-access; ---> 66 return method(self, *args, **kwargs); 67 ; 68 # Running ",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:3482,error,error,3482,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['error'],['error']
Availability,"it can make a big difference to enlarge the patch size such that objects are fully included.; The number of epochs seems a bit small, but you seem to know what you're doing.; ; > ```; > n,h,w = X_test[2].shape; > labels, details = model.predict_instances_big(X_test[2],; > axes='ZYX',; > block_size=(n, int(h/2), int(w/2)), #conf.train_patch_size; > min_overlap=16,; > context = (30,30,30), #(30,30,30),; > prob_thresh=0.4; > ); > ```. How big are the test stacks, i.e. what is `X_test[2].shape`?; If it has similar size than the training stacks, you can simply use `model.predict_instances`, which doesn't require you to set this many parameters. The only important one would be `n_tiles`, which determines into how many tiles the input stack will be chopped before prediction runs on the GPU. Please try using this function first. The function `model.predict_instances_big` is only intended for *really* big images, which can't even be loaded in RAM or which require excessive computation and RAM requirements during the CPU-based non-maximum suppression step (to prune redundant object predictions) that runs after the CNN prediction on the GPU. > 1- block_size : I understand that we are treating the large volume with smaller blocks, but shouldn't block_size be the same as training size? If I set `block_size=conf.train_patch_size` I end up with issues with context and overlap, probably because I have one small dimension in training (48). No, `block_size` should be much larger in comparison to `min_overlap` and `context`. It's a bit difficult to explain in words and I should really make a diagram to explain what's happening under the hood. When calling `model.predict_instances`, the block size is essentially the entire input image. > 2-min_overlap : this is the overlap between blocks right? Why should it be larger than the size of a training object as written in the source code? I mean whatever the size here, I could have an object half in my block and half in the next one, so why i",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:1665,redundant,redundant,1665,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,1,['redundant'],['redundant']
Availability,"ivate the conda environment. `$ conda activate stardist`. . 5. Install *numpy*. `$ conda install numpy`. 6. Install *tesorflow-gpu* nightly using pip (tf-nightly-gpu 2.5.0.dev20201212). `pip install tf-nightly-gpu`. 7. Follow the installation instructions for stardist. https://github.com/mpicbg-csbd/stardist. . <!--I got a compatibility issue error with h5py library version. I just continue with the installation and everything seems to work correctly-->. 8. Install the NEUBIAS academy Jupyter Notebook for testing stardist installation . `$ git clone https://github.com/maweigert/neubias_academy_stardist.git`. ##### Run stardist-gpu on the NVIDIA RTX 3080. 1. Start the *Jupyter Notebook* . . 2. Comment the following lines:. ```; $ #%tensorflow_version 1.x; ; $ #!pip install stardist; ```. . 3. The direct downloads didn't work for me, I just comment these lines out and downloaded the sample data before. ```; '''; download_and_extract_zip_file(; url = 'https://github.com/mpicbg-csbd/stardist/releases/download/0.1.0/dsb2018.zip',; targetdir = 'data',; verbose = 1,; ); '''; ```. 4. To avoid TF GPU out of memory issues I add the following code:. ```python; import tensorflow as tf; gpus = tf.config.experimental.list_physical_devices('GPU'); tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4092)]); ```. . Output:. ```; 2020-12-13 07:44:01.102879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties:; pciBusID: 0000:68:00.0 name: GeForce RTX 3080 computeCapability: 8.6; coreClock: 1.785GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s; 2020-12-13 07:44:01.103001: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll; 2020-12-13 07:44:01.103940: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll; 2020-12-13 07:",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:4598,down,download,4598,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,1,['down'],['download']
Availability,"k/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1144, in run_setup; run_setup(setup_script, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 253, in run_setup; raise; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 166, in save_modules; saved_exc.resume(); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 141, in resume; six.reraise(type, exc, self._tb); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 154, in save_modules; yield saved; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 250, in run_setup; _execfile(setup_script, ns); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 45, in _execfile; exec(code, globals, locals); File ""/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/easy_install-d8fAD9/numpy-1.19.0/setup.py"", line 30, in <module>; re.MULTILINE | re.DOTALL).groups()[0]; RuntimeError: Python version >= 3.6 required.; ----------------------------------------; ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.; ```. Thank you",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:6984,error,errored,6984,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['error'],['errored']
Availability,"lib/python/pkg_resources/__init__.py"", line 782, in resolve; replace_conflicting=replace_conflicting; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py"", line 1065, in best_match; return self.obtain(req, installer); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py"", line 1077, in obtain; return installer(requirement); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/dist.py"", line 784, in fetch_build_egg; return cmd.easy_install(req); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 679, in easy_install; return self.install_item(spec, dist.location, tmpdir, deps); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 705, in install_item; dists = self.install_eggs(spec, download, tmpdir); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 890, in install_eggs; return self.build_and_install(setup_script, setup_base); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1158, in build_and_install; self.run_setup(setup_script, setup_base, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1144, in run_setup; run_setup(setup_script, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 253, in run_setup; raise; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yi",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:4628,down,download,4628,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['down'],['download']
Availability,"nd that ` 2*context < block_size` in order not to discard everything, but why does `min_overlap` appears here? Would you have a link to a visual explanation how you define the different blocks? I think this could help me understand. After the context is discarded on all sides, blocks still have to overlap at least `min_overlap` pixels. As you say, a diagram would make this clear. Note that the `model.predict_instances_big` is relatively new and really meant for people who have huge images. It's not a mainstream function. > 5- all my 3D stacks don't have the same sizes, should I change the above parameters according to each sizes or one set should work for all?. The parameters can be the the same and really only depend on the CNN architecture (context) and the object sizes (min_overlap). However, the stack must obviously be bigger than the block size. > 6- Overall would you have some advise to improve my pipeline?. Like I said, try using `model.predict_instances` instead and save yourself the headache ;). > I'm working on a docker container of my own and installed the latest tensorflow version, I was getting this error when instantiating the StarDist3D`MemoryError: clEnqueueWriteBuffer failed: MEM_OBJECT_ALLOCATION_FAILURE` it was taking all my 12Go of GPU Memory (I have an RTX 2080 Ti). Yes, TensorFlow unfortunately grabs all the GPU memory by default. That's why [our training notebook](https://nbviewer.jupyter.org/github/mpicbg-csbd/stardist/blob/master/examples/3D/2_training.ipynb) calls `limit_gpu_memory` to reserve 20% of the GPU memory to be used for OpenCL. Note that 20% may not be enough, it really depends on the patch size and the amount of total GPU memory. > Adding this lines of code solved my problem (just wanted to let you know in case someone has the same problem in the future):. Allowing GPU memory growth is an alternative, but might be less efficient (not sure). As far as I recall, `limit_gpu_memory` also allows you to enable memory growth. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:4421,error,error,4421,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,1,['error'],['error']
Availability,"ong as it fits in my GPU or should I do something smarter?; - try to make bigger train_patch_size : I'm a bit uncomfortable setting batch_size lower than 4 (or even 8) to be honest, do you perform gradient accumulation under the hood? Maybe I'm just freaking out for nothing but don't you feel that a batch_size of 2 is a bit too small to get a nice gradient descent?. Also I wanted to ask you about something else. ; At the moment I don't have nice labels that have been curated by humans (I might find some time to do this but later), but I have good enough binary masks that allows me to train algorithms like 3DUnets.; My problem is that some cells are close to each other, so my initial labels are not very good at differientiating adjacent cells (which creates big objects that don't look like cells at all) and same goes with the resulting UNet segmentation, that's why I decided to switch to StarDist3D.; First results from StarDist seem already much better at creating nice round cells, but in order to train StarDist I simply separated my binary mask using connex component : so my ground truth for StarDist never really shows adjacent cells (as they are merged into one same label).; Because of this I don't really trust the automated nms_thresh search function (labels are wrong so it's going to be hard for the search). So I played a bit with the nms_thresh to visually inspect the results but I'd like to understand better what 0.4 or 0.05 means for example. Does 0.4 means that I would tolerate that 40% of the volume of two cells (or a proxy of the volume) are actually overlapping? In terms of physics and biology, I don't expect my cells to be able to overlap at all, they can touch each other but that's it. Does this physics rule mean that setting nms_thresh to 0 makes sense? Or does the nms_thresh represents something completely different? ( I tried to have a look at the source code but I can't read C++!). Hope my questions make sense. Thanks again for your help,. Best,; Seb",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691334428:1333,mask,mask,1333,,https://github.com/stardist/stardist/issues/87#issuecomment-691334428,2,"['mask', 'toler']","['mask', 'tolerate']"
Availability,"t; else:; self.dtype = np.float; ; def __len__(self):; return len(self.x). def __getitem__(self, idx):; ; filename = self.x[idx]; if self.mode == ""X"":; return imread(filename) / 255; else:; return imread(filename); ```. I'm actually able to launch a training with batch size of 1 without specifying anisotropy and things seem to run ok (note that I had to add `dtype` and `ndim` to make things work and that I have some problems in my pipeline as explained bellow). However some part of my training pipeline are broken:. - when running this to compute anisotropy `extents = calculate_extents(Y)` I first get this warning; ```_asarray.py (83): Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray```; and after a few seconds of computation the code fails with this error message :; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-14-36ac39a79e87> in <module>; ----> 1 extents = calculate_extents(Y); 2 anisotropy = tuple(np.max(extents) / extents); 3 print('empirical anisotropy of labeled objects = %s' % str(anisotropy)). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/utils.py in calculate_extents(lbl, func); 165 n in (2,3) or _raise(ValueError(""label image should be 2- or 3-dimensional (or pass a list of these)"")); 166 ; --> 167 regs = regionprops(lbl); 168 if len(regs) == 0:; 169 return np.zeros(n). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/skimage/measure/_regionprops.py in regionprops(label_image, intensity_image, cache, coordinates); 883 regions = []; 884 ; --> 885 objects = ndi.find_objects(label_image); 886 for i, sl in enumerate(objects):; 887 if sl is None:. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:1508,error,error,1508,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['error'],['error']
Availability,"tempfile-1.0-py2.py3-none-any.whl (4.4 kB); Requirement already satisfied: six in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.12.0); Collecting h5py; Using cached h5py-2.10.0-cp27-cp27m-macosx_10_6_intel.whl (2.9 MB); Requirement already satisfied: matplotlib in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.3.1); Collecting keras<2.4,>=2.1.2; Using cached Keras-2.3.1-py2.py3-none-any.whl (377 kB); Requirement already satisfied: scipy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (0.13.0b1); Collecting tqdm; Using cached tqdm-4.47.0-py2.py3-none-any.whl (66 kB); Collecting pathlib2; python_version < ""3""; Using cached pathlib2-2.3.5-py2.py3-none-any.whl (18 kB); Collecting imagecodecs-lite<=2020; python_version < ""3.6""; Using cached imagecodecs-lite-2019.12.3.tar.gz (1.1 MB); ERROR: Command errored out with exit status 1:; command: /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py'""'""'; __file__='""'""'/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-pip-egg-info-I2_dBJ; cwd: /private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/; Complete output (51 lines):; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py"", line 101, in <module>; ",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:1981,error,errored,1981,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['error'],['errored']
Availability,"ub.com/mpicbg-csbd/stardist/issues/36#issuecomment-594974089) I was able to run the job with a single cpu (# cores requested and maximum amount of RAM available are mixed on our cluster; each core comes with 15GB RAM). However the remaining runtime predicted by tqdm after a few iterations had accumulated was something like 36 hours - so I cancelled it and gave up on it. Uwe reminded me that with one core, I likely had a maximum of 2 threads, but I did not go back and try `predict_big` with more cores. Do you think `predict_big` with say 32 cores (64 threads) could finish in less than an hour? If so I will give it another try. For different kdtree implementations - I did look through [this article written by the scikit-learn kdtree and balltree developer](https://jakevdp.github.io/blog/2013/04/29/benchmarking-nearest-neighbor-searches-in-python/), but I didn't think there was enough differences in his graphs to justify introducing a new dependency (I don't think scikit-learn was a dependency before?) and I also preferred something with an underlying C++ implementation, as there are more big speed ups available if the kdtree data structure is available for query directly within the NMS algorithm. Unfortunately the only RAM measurements I have are the maximum and average usage (over the full run, prediction and NMS) for prediction on the full volumes both w/ and w/o the kdtree. The command in both cases was:. `pred, det = model.predict_instances(image_norm, verbose=True, n_tiles=(4, 8, 2))`; On a volume with 860x1676x301 voxels and 6.32e6 polygon candidates. w/o kdtree max: 434.439GB; w/ kdtree max: 430.482GB. w/o kdtree avg: 221.672GB; w/ kdtree avg: 297.733GB. I'm thinking that the kdtree and the query results for this many candidates took something like 70GB RAM. One thing that I haven't looked at in the code - it seems like the biggest expansion of data in the method is from raw data --> prediction; e.g. in my case I'm using 128 rays, so I need (129 * 2) more RAM re",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-601381585:1342,avail,available,1342,,https://github.com/stardist/stardist/pull/40#issuecomment-601381585,2,['avail'],['available']
Deployability," cached h5py-2.10.0-cp27-cp27m-macosx_10_6_intel.whl (2.9 MB); Requirement already satisfied: matplotlib in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.3.1); Collecting keras<2.4,>=2.1.2; Using cached Keras-2.3.1-py2.py3-none-any.whl (377 kB); Requirement already satisfied: scipy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (0.13.0b1); Collecting tqdm; Using cached tqdm-4.47.0-py2.py3-none-any.whl (66 kB); Collecting pathlib2; python_version < ""3""; Using cached pathlib2-2.3.5-py2.py3-none-any.whl (18 kB); Collecting imagecodecs-lite<=2020; python_version < ""3.6""; Using cached imagecodecs-lite-2019.12.3.tar.gz (1.1 MB); ERROR: Command errored out with exit status 1:; command: /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py'""'""'; __file__='""'""'/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-pip-egg-info-I2_dBJ; cwd: /private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/; Complete output (51 lines):; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py"", line 101, in <module>; 'Programming Language :: Python :: Implementation :: CPython',; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/__init__.py"", line 144, in setup; _install_setup_requires(",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:2241,install,install-FhuQIv,2241,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['install'],['install-FhuQIv']
Deployability," tools, release 11.0, V11.0.194; Build cuda_11.0_bu.relgpu_drvr445TC445_37.28540450_0; ```. . 3. Open the anaconda terminal and create the stardist conda environment. `$ conda create -n stardist pip python==3.8`. . 4. Activate the conda environment. `$ conda activate stardist`. . 5. Install *numpy*. `$ conda install numpy`. 6. Install *tesorflow-gpu* nightly using pip (tf-nightly-gpu 2.5.0.dev20201212). `pip install tf-nightly-gpu`. 7. Follow the installation instructions for stardist. https://github.com/mpicbg-csbd/stardist. . <!--I got a compatibility issue error with h5py library version. I just continue with the installation and everything seems to work correctly-->. 8. Install the NEUBIAS academy Jupyter Notebook for testing stardist installation . `$ git clone https://github.com/maweigert/neubias_academy_stardist.git`. ##### Run stardist-gpu on the NVIDIA RTX 3080. 1. Start the *Jupyter Notebook* . . 2. Comment the following lines:. ```; $ #%tensorflow_version 1.x; ; $ #!pip install stardist; ```. . 3. The direct downloads didn't work for me, I just comment these lines out and downloaded the sample data before. ```; '''; download_and_extract_zip_file(; url = 'https://github.com/mpicbg-csbd/stardist/releases/download/0.1.0/dsb2018.zip',; targetdir = 'data',; verbose = 1,; ); '''; ```. 4. To avoid TF GPU out of memory issues I add the following code:. ```python; import tensorflow as tf; gpus = tf.config.experimental.list_physical_devices('GPU'); tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4092)]); ```. . Output:. ```; 2020-12-13 07:44:01.102879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties:; pciBusID: 0000:68:00.0 name: GeForce RTX 3080 computeCapability: 8.6; coreClock: 1.785GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s; 2020-12-13 07:44:01.103001: I tensorflow/stream_executor/platform/default/dso_loader",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:4361,install,install,4361,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,1,['install'],['install']
Deployability,"--------------------------------------------------------------------------+; ```. . - Type:. `$ nvcc -V`. . Output:. ```; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2020 NVIDIA Corporation; Built on Thu_Jun_11_22:26:48_Pacific_Daylight_Time_2020; Cuda compilation tools, release 11.0, V11.0.194; Build cuda_11.0_bu.relgpu_drvr445TC445_37.28540450_0; ```. . 3. Open the anaconda terminal and create the stardist conda environment. `$ conda create -n stardist pip python==3.8`. . 4. Activate the conda environment. `$ conda activate stardist`. . 5. Install *numpy*. `$ conda install numpy`. 6. Install *tesorflow-gpu* nightly using pip (tf-nightly-gpu 2.5.0.dev20201212). `pip install tf-nightly-gpu`. 7. Follow the installation instructions for stardist. https://github.com/mpicbg-csbd/stardist. . <!--I got a compatibility issue error with h5py library version. I just continue with the installation and everything seems to work correctly-->. 8. Install the NEUBIAS academy Jupyter Notebook for testing stardist installation . `$ git clone https://github.com/maweigert/neubias_academy_stardist.git`. ##### Run stardist-gpu on the NVIDIA RTX 3080. 1. Start the *Jupyter Notebook* . . 2. Comment the following lines:. ```; $ #%tensorflow_version 1.x; ; $ #!pip install stardist; ```. . 3. The direct downloads didn't work for me, I just comment these lines out and downloaded the sample data before. ```; '''; download_and_extract_zip_file(; url = 'https://github.com/mpicbg-csbd/stardist/releases/download/0.1.0/dsb2018.zip',; targetdir = 'data',; verbose = 1,; ); '''; ```. 4. To avoid TF GPU out of memory issues I add the following code:. ```python; import tensorflow as tf; gpus = tf.config.experimental.list_physical_devices('GPU'); tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4092)]); ```. . Output:. ```; 2020-12-13 07:44:01.102879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found ",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:4114,install,installation,4114,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,1,['install'],['installation']
Deployability,"0 C+G ...airLink4\CorsairLink4.exe N/A |; | 0 N/A N/A 12044 C+G ...cw5n1h2txyewy\LockApp.exe N/A |; | 0 N/A N/A 15564 C+G ...qxf38zg5c\Skype\Skype.exe N/A |; | 0 N/A N/A 15808 C+G ...qxf38zg5c\Skype\Skype.exe N/A |; | 0 N/A N/A 16340 C+G ...zilla Firefox\firefox.exe N/A |; | 0 N/A N/A 16596 C+G Insufficient Permissions N/A |; | 0 N/A N/A 16824 C+G ...lPanel\SystemSettings.exe N/A |; | 0 N/A N/A 16956 C+G ...IR iCUE Software\iCUE.exe N/A |; | 0 N/A N/A 17380 C+G ...ropbox\Client\Dropbox.exe N/A |; | 0 N/A N/A 17700 C+G ...nputApp\TextInputHost.exe N/A |; | 0 N/A N/A 17796 C+G ...y\ShellExperienceHost.exe N/A |; | 0 N/A N/A 19544 C+G ...b3d8bbwe\WinStore.App.exe N/A |; | 0 N/A N/A 20168 C+G ...zilla Firefox\firefox.exe N/A |; | 0 N/A N/A 22232 C ...\envs\stardist\python.exe N/A |; +-----------------------------------------------------------------------------+; ```. . - Type:. `$ nvcc -V`. . Output:. ```; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2020 NVIDIA Corporation; Built on Thu_Jun_11_22:26:48_Pacific_Daylight_Time_2020; Cuda compilation tools, release 11.0, V11.0.194; Build cuda_11.0_bu.relgpu_drvr445TC445_37.28540450_0; ```. . 3. Open the anaconda terminal and create the stardist conda environment. `$ conda create -n stardist pip python==3.8`. . 4. Activate the conda environment. `$ conda activate stardist`. . 5. Install *numpy*. `$ conda install numpy`. 6. Install *tesorflow-gpu* nightly using pip (tf-nightly-gpu 2.5.0.dev20201212). `pip install tf-nightly-gpu`. 7. Follow the installation instructions for stardist. https://github.com/mpicbg-csbd/stardist. . <!--I got a compatibility issue error with h5py library version. I just continue with the installation and everything seems to work correctly-->. 8. Install the NEUBIAS academy Jupyter Notebook for testing stardist installation . `$ git clone https://github.com/maweigert/neubias_academy_stardist.git`. ##### Run stardist-gpu on the NVIDIA RTX 3080. 1. Start the *Jupyter Notebook* . . 2. Commen",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:3373,release,release,3373,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,1,['release'],['release']
Deployability,"0000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py"", line 101, in <module>; 'Programming Language :: Python :: Implementation :: CPython',; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/__init__.py"", line 144, in setup; _install_setup_requires(attrs); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/__init__.py"", line 139, in _install_setup_requires; dist.fetch_build_eggs(dist.setup_requires); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/dist.py"", line 717, in fetch_build_eggs; replace_conflicting=True,; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py"", line 782, in resolve; replace_conflicting=replace_conflicting; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py"", line 1065, in best_match; return self.obtain(req, installer); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py"", line 1077, in obtain; return installer(requirement); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/dist.py"", line 784, in fetch_build_egg; return cmd.easy_install(req); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 679, in easy_install; return self.install_item(spec, dist.location, tmpdir, deps); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 705, in install_item; dists = self.install_eggs(spec, download, tmpdir); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 890, in install_eggs; return self.build_and_install(setup_script, setup_base); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:3909,install,installer,3909,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['install'],['installer']
Deployability,"; Collecting pathlib2; python_version < ""3""; Using cached pathlib2-2.3.5-py2.py3-none-any.whl (18 kB); Collecting imagecodecs-lite<=2020; python_version < ""3.6""; Using cached imagecodecs-lite-2019.12.3.tar.gz (1.1 MB); ERROR: Command errored out with exit status 1:; command: /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py'""'""'; __file__='""'""'/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-pip-egg-info-I2_dBJ; cwd: /private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/; Complete output (51 lines):; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py"", line 101, in <module>; 'Programming Language :: Python :: Implementation :: CPython',; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/__init__.py"", line 144, in setup; _install_setup_requires(attrs); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/__init__.py"", line 139, in _install_setup_requires; dist.fetch_build_eggs(dist.setup_requires); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/dist.py"", line 717, in fetch_build_eggs; replace_conflicting=True,; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py"", line 782, in resolve; replace_conflicting=replace_conflicting",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:2738,install,install-FhuQIv,2738,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,2,['install'],['install-FhuQIv']
Deployability,"; | 0 N/A N/A 16956 C+G ...IR iCUE Software\iCUE.exe N/A |; | 0 N/A N/A 17380 C+G ...ropbox\Client\Dropbox.exe N/A |; | 0 N/A N/A 17700 C+G ...nputApp\TextInputHost.exe N/A |; | 0 N/A N/A 17796 C+G ...y\ShellExperienceHost.exe N/A |; | 0 N/A N/A 19544 C+G ...b3d8bbwe\WinStore.App.exe N/A |; | 0 N/A N/A 20168 C+G ...zilla Firefox\firefox.exe N/A |; | 0 N/A N/A 22232 C ...\envs\stardist\python.exe N/A |; +-----------------------------------------------------------------------------+; ```. . - Type:. `$ nvcc -V`. . Output:. ```; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2020 NVIDIA Corporation; Built on Thu_Jun_11_22:26:48_Pacific_Daylight_Time_2020; Cuda compilation tools, release 11.0, V11.0.194; Build cuda_11.0_bu.relgpu_drvr445TC445_37.28540450_0; ```. . 3. Open the anaconda terminal and create the stardist conda environment. `$ conda create -n stardist pip python==3.8`. . 4. Activate the conda environment. `$ conda activate stardist`. . 5. Install *numpy*. `$ conda install numpy`. 6. Install *tesorflow-gpu* nightly using pip (tf-nightly-gpu 2.5.0.dev20201212). `pip install tf-nightly-gpu`. 7. Follow the installation instructions for stardist. https://github.com/mpicbg-csbd/stardist. . <!--I got a compatibility issue error with h5py library version. I just continue with the installation and everything seems to work correctly-->. 8. Install the NEUBIAS academy Jupyter Notebook for testing stardist installation . `$ git clone https://github.com/maweigert/neubias_academy_stardist.git`. ##### Run stardist-gpu on the NVIDIA RTX 3080. 1. Start the *Jupyter Notebook* . . 2. Comment the following lines:. ```; $ #%tensorflow_version 1.x; ; $ #!pip install stardist; ```. . 3. The direct downloads didn't work for me, I just comment these lines out and downloaded the sample data before. ```; '''; download_and_extract_zip_file(; url = 'https://github.com/mpicbg-csbd/stardist/releases/download/0.1.0/dsb2018.zip',; targetdir = 'data',; verbose = 1,; ); '''; ```. 4.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:3675,install,install,3675,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,1,['install'],['install']
Deployability,"> > Is the new Java library for deep learning ready yet? What is the name and where can I find it?; > ; > Yes, here it is: https://github.com/bioimage-io/model-runner-java. Thanks for the link! A few questions:; - This is only going to work for models in the bioimage.io format, right?; - Will this library handle pre- and post-processing?; - Is it going to support tile-based prediction, i.e. chopping the input image into tiles, running the model on each of them, and then re-assembling the individual results?. > > I really need to understand the current situation first... I don't even know how to export the model such that it will work with the new Java library.; > ; > The bioimageio.core library will export it directly in a compatible format (as the model I linked in the previous lines). In the upcoming days, we will update deepImageJ with the java model runner so all the TF2 models can also be deployed in Fiji. I can let you know when the plugin is ready if you want. I'm first interested in using `model-runner-java` to replace the TF1-only CSBDeep library in Fiji, in order to run TF2-based models in StarDist.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-1460949099:828,update,update,828,,https://github.com/stardist/stardist/issues/68#issuecomment-1460949099,2,"['deploy', 'update']","['deployed', 'update']"
Deployability,"> First thank you and congratulations for this great work, this repo is truly amazing. Thanks, I'm glad you like it!. > ```; > conf = Config3D (; > ...; > train_patch_size = (48,192,192), # should I make something more cubic?; > train_batch_size = 4, # Tried to set the largest batch size possible without get OOM error; > train_epochs = 50; > ); > ```. Since your object sizes are almost isotropic, you could try to use `(160,192,192)` or similar and decrease the batch size if needed, but I doubt it'll make much of a difference. Of course, if the objects are (often) larger than 48 pixels, it can make a big difference to enlarge the patch size such that objects are fully included.; The number of epochs seems a bit small, but you seem to know what you're doing.; ; > ```; > n,h,w = X_test[2].shape; > labels, details = model.predict_instances_big(X_test[2],; > axes='ZYX',; > block_size=(n, int(h/2), int(w/2)), #conf.train_patch_size; > min_overlap=16,; > context = (30,30,30), #(30,30,30),; > prob_thresh=0.4; > ); > ```. How big are the test stacks, i.e. what is `X_test[2].shape`?; If it has similar size than the training stacks, you can simply use `model.predict_instances`, which doesn't require you to set this many parameters. The only important one would be `n_tiles`, which determines into how many tiles the input stack will be chopped before prediction runs on the GPU. Please try using this function first. The function `model.predict_instances_big` is only intended for *really* big images, which can't even be loaded in RAM or which require excessive computation and RAM requirements during the CPU-based non-maximum suppression step (to prune redundant object predictions) that runs after the CNN prediction on the GPU. > 1- block_size : I understand that we are treating the large volume with smaller blocks, but shouldn't block_size be the same as training size? If I set `block_size=conf.train_patch_size` I end up with issues with context and overlap, probably because I have",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:637,patch,patch,637,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,1,['patch'],['patch']
Deployability,"> Hello everyone, the fractional offsets issue should be corrected now in the latest DeepImageJ [release](https://github.com/deepimagej/deepimagej-plugin/releases/tag/2.1.11). I'm testing this release right now on Ubuntu 20.04 (without GPU acceleration). I still see the same behavior when adding a model (dialog quickly appears and disappears), but the model seems to be actually installed this time. It was quite confusing to me that there was no user feedback whether a model was successfully installed or not. Unfortunately, I get an error when I open ""DeepImageJ Run"", select my newly-installed model, and then click on ""Test model"". First, nothing happens but one CPU core is used 100%. After a couple of minutes, the Fiji Console pops up with the following error message:. <details>; <summary>Expand to show long error message</summary>. ```; [INFO] No TF library found in /home/uwe/Downloads/fiji-linux64/Fiji.app/lib/.; Exception in thread ""AWT-EventQueue-0"" java.lang.StackOverflowError; 	at java.util.regex.Pattern$Loop.match(Pattern.java:4767); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3779); 	at java.util.regex.Pattern$Branch.match(Pattern.java:4606); 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4660); 	at java.util.regex.Pattern$Loop.match(Pattern.java:4787); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3779); 	at java.util.regex.Pattern$Branch.match(Pattern.java:4606); 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4660); 	at java.util.regex.Pattern$Loop.match(Pattern.java:4787); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1042890741:97,release,release,97,,https://github.com/stardist/stardist/pull/171#issuecomment-1042890741,6,"['install', 'release']","['installed', 'release', 'releases']"
Deployability,"> However DeepImageJ should already be able to run Stardist. I have succesfully run it in my computer. Instead of using the ""Test model"" button, please try opening an image, then selecting the model and finally running the model pressing ""ok"". Ah, I didn't expect that DeepImageJ works like this. Wouldn't it be more intuitive to have separate menu items for model testing and running?. > the ""Test model"" button should be fixed now, feel free to test the new release with the funcionality fixed:; > https://github.com/deepimagej/deepimagej-plugin/releases/tag/2.1.12. I can confirm that model testing and running on a different opened image works for me now. > @uschmidt83 I think we can assume it works now in deepImageJ. Should we go ahead and merge it?. There's one issue I don't understand yet. Why did we bother to put the `stardist_postprocessing.ijm` macro into the exported model zip file if there's no way to run this as *Postprocessing* from DeepImageJ?. When I did run the postprocessing macro from the Script Editor (opened manually from the `Fiji.app/models/<model_name>` folder) after DeepImageJ prediction, it showed me an error `[ERROR] Could not find input image with name/title ""scores"".` in the Console. I then tried the exact same thing again, and it worked as intended without errors ðŸ¤·â€â™‚ï¸. > (Also there are still some failing tests, but this seems to be unrelated to the changes here.). Yes, those are unrelated. The bioimage.io tests are only active in Github action tests with `tensorflow<2` in the name, and those run through just fine.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1047694133:460,release,release,460,,https://github.com/stardist/stardist/pull/171#issuecomment-1047694133,2,['release'],"['release', 'releases']"
Deployability,"> I can confirm that model testing and running on a different opened image works for me now. Great. thanks for checking!. > There's one issue I don't understand yet. Why did we bother to put the `stardist_postprocessing.ijm` macro into the exported model zip file if there's no way to run this as _Postprocessing_ from DeepImageJ?. We want to support this in deepImageJ but are currently a bit out of manpower to implement it. There's probably a relatively simple solution. But also with the given solution, the script is at least packaged with the model, so that it can be applied manually. ; We will work on better post-processing integration, and there is probably a simple solution, but we don't want to block releasing the model zoo for now to incorporate it. We can update the readme to explain this, but I would suggest to do this in a follow-up PR as this one is pretty large already and it's not critical to the functionality. > When I did run the postprocessing macro from the Script Editor (opened manually from the `Fiji.app/models/<model_name>` folder) after DeepImageJ prediction, it showed me an error `[ERROR] Could not find input image with name/title ""scores"".` in the Console. I then tried the exact same thing again, and it worked as intended without errors man_shrugging. @esgomezm @carlosuc3m do you have any idea how to fix this?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1047715559:633,integrat,integration,633,,https://github.com/stardist/stardist/pull/171#issuecomment-1047715559,2,"['integrat', 'update']","['integration', 'update']"
Deployability,"> I'm a complete beginner at Python but find the software really exciting, so have jumped in at the deep end!. Glad that you like it!. > My aim is to be able to export the label maps and visualise them in ImageJ or something similar, and to use the 3 dimensional ROIs to calculate values for nuclear volume and shape and other things. I suggest you do the prediction in Python to get the label image, which you can then import in Fiji with the [3D Roi Manager](https://imagejdocu.tudor.lu/plugin/stacks/3d_roi_manager/start) (installation instructions [here](https://imagejdocu.tudor.lu/plugin/stacks/3d_ij_suite/start#installation)). This should work well, as long as you don't have (strongly) overlapping predictions or it's not important to you to get the complete shapes for all objects. > I can see that you've made a notebook to export the ROIs for the 2D model prediction into FIJI which works great for me. I tried to adapt this for 3D by just having axes set to zyx and having 'dist' instead of 'coord' (in export_imagej_rois('img_rois.zip', details['dist'])). I have found that this doesn't really work, but not sure why. This function only works for 2D polygons and cannot be simply adapted for 3D.; ; > I also tried to export the demo 3d model into deepImageJ, but it runs into trouble when you try to apply it to an image. I can give more details, but as I am quite lost I'm not sure what is the relevant information. It's not that simple and even if it worked, this won't get you 3D ROIs in Fiji. Our [StarDist Fiji plugin](https://imagej.net/StarDist) currently only works in 2D because it's not that easy to support 3D... but we hope to get this done eventually. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/37#issuecomment-597239267:526,install,installation,526,,https://github.com/stardist/stardist/issues/37#issuecomment-597239267,2,['install'],['installation']
Deployability,"> May I ask if you have any idea when the next release on pypi might happen?. Sorry, we don't yet have a date for the next release. In the meantime, you could simply re-define the function that Martin changed. E.g. paste this in your script/notebook (after you imported `calculate_extents` from stardist):. ```python; def calculate_extents(lbl, func=np.median):; """""" Aggregate bounding box sizes of objects in label images. """""". import numpy as np; from collections.abc import Iterable; from csbdeep.utils import _raise; from skimage.measure import regionprops; ; if (isinstance(lbl,np.ndarray) and lbl.ndim==4) or (not isinstance(lbl,np.ndarray) and isinstance(lbl,Iterable)):; return func(np.stack([calculate_extents(_lbl,func) for _lbl in lbl], axis=0), axis=0). n = lbl.ndim; n in (2,3) or _raise(ValueError(""label image should be 2- or 3-dimensional (or pass a list of these)"")). regs = regionprops(lbl); if len(regs) == 0:; return np.zeros(n); else:; extents = np.array([np.array(r.bbox[n:])-np.array(r.bbox[:n]) for r in regs]); return func(extents, axis=0); ```",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-882546652:47,release,release,47,,https://github.com/stardist/stardist/issues/57#issuecomment-882546652,2,['release'],['release']
Deployability,"> Not grid=4; I was referencing this previous discussion:. Ah, I see :). > Qualitatively the results look ok - there are definitely some nuclei missing and very few cells that are just too large. Yep, it's pretty good for that the image have a rather bad SNR and are pretty wild along z! Nice!. > I only manually annotated a 128x128x32 patch. I see. I would try to set `train_patch_size` to the largest possible size (e.g. `(32,128,128)`) to avoid boundary effects. Maybe that will help with the probability. But othe rthan that, your network outputs look reasonable!. > Would you consider manually lowering the learning rate after so many epochs?. That will be done automatically during taring (if the loss saturates), so I wouldn't do that.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/36#issuecomment-593626054:336,patch,patch,336,,https://github.com/stardist/stardist/issues/36#issuecomment-593626054,1,['patch'],['patch']
Deployability,"> Note that [21] (stardist 2d paper) sits somewhere in between object detection and instance segmentation because the predicted shapes are of relatively high fidelity, but are not pixel-accurate. I think this probably refers to the general idea that the Stardist representation is only perfectly pixel accurate - theoretically, and for arbitrarily high resolution - with an infinite number of rays. In the discrete case, it's only pixel accurate if your number of rays equals the number of boundary pixels of the cell (assuming it is unambiguously clear where this boundary is to begin with). You're ultimately representing an object whose boundary is a rough discrete approximation to a smooth continuous surface with a (somewhat sparse, even with 128+ rays) polygon - like an octagon around a circle. Also to clarify - the UNet does not reconstruct the polygons - it just maps the input image to the probability + ray distances feature space. The probability thresholding + non maximum suppression is what ultimately gives the polygons - and then those are reconstructed [in the code you pointed out to me before.](https://github.com/mpicbg-csbd/stardist/issues/33#issuecomment-583421466). One approach to refining the boundary segmentations would be to begin with the Stardist segmentation as an initialization, and then do graph based segmentation to refine boundaries locally [[1]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1542036) [[2]](https://link.springer.com/content/pdf/10.1007%2F3-540-45465-9_88.pdf). I think that would be really cool - but also a whole project unto itself and potentially a lot of work. Depending on how complex the shape you're trying to segment, it's probably not worth it.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-604064572:695,continuous,continuous,695,,https://github.com/stardist/stardist/issues/33#issuecomment-604064572,1,['continuous'],['continuous']
Deployability,"> Thanks for the excellent job with StarDist. I love this tool. Thanks, we appreciate it!. > I recently purchased a RTX 3080 ... Only CUDA 11.1 can run properly on the RTX 3080.; > I have spent my weekend to try to solve the problem but I could not find any solution. It's a known issue, e.g. see this: https://lambdalabs.com/blog/install-tensorflow-and-pytorch-on-rtx-30-series/. Quote from that article:. Right now, getting these libraries to work with 30XX GPUs requires manual compilation or NVIDIA docker containers. It seems that you have several sub-optimal options: You can either go through the trouble of compiling TensorFlow yourself (I wouldn't do it), get one of those solutions to work (e.g. Lambda Stack, Docker container), or simply wait until the latest version of TensorFlow supports RTX 30 series GPUs.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-740151822:331,install,install-tensorflow-and-pytorch-on-rtx-,331,,https://github.com/stardist/stardist/issues/104#issuecomment-740151822,1,['install'],['install-tensorflow-and-pytorch-on-rtx-']
Deployability,"> This is working locally now. (Needs the changes in [bioimage-io/core-bioimage-io-python#142](https://github.com/bioimage-io/core-bioimage-io-python/pull/142)). Thanks, I updated this locally. > Also, I can only run one of the tests successfully at a time, because tensorflow does not properly clear the gpu. Do you get `tensorflow.python.framework.errors_impl.FailedPreconditionError: 2 root error(s) found.`?. I think that's not the reason, rather TensorFlow is in a bad state after running `model.export_TF` once. This is a known issue, and I currently don't know how to workaround that.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-968756455:172,update,updated,172,,https://github.com/stardist/stardist/pull/171#issuecomment-968756455,1,['update'],['updated']
Deployability,"> We actually found that we don't have GPU available for Tensorflow after our build. What do you mean? Are you referring to [this problem](https://github.com/NVIDIA/nvidia-docker/issues/1034)?. > we suspect this is because we used NVIDIA_DRIVER_VERSION=455 (not listed as supported in the README).; > Will the updated version of tensorflow work with this driver version?. The variable `NVIDIA_DRIVER_VERSION` is simply used to indicate the name of a package to be installed. If the `docker build` command completed successfully, then it was likely not the issue.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/108#issuecomment-754113033:310,update,updated,310,,https://github.com/stardist/stardist/issues/108#issuecomment-754113033,2,"['install', 'update']","['installed', 'updated']"
Deployability,"> a bit confusing. I agree, we should change it a some point (the naming you suggest indeed makes a bit more sense). ; . > Specifically, why do you use `use_gpu = False and ...` which will always evaluate to `False` ?. This is to ensure that `gputools` is available, once the user decides to set `use_gpu`. Note, that this is a bit of a power-user setting (if you want to speed up training), since installing `gputools`/`pyopencl` can be a bit involved on some systems. This is why it is deactivated by default. . Hope that clears it up a bit and thanks for the feedback! . M",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/75#issuecomment-661022071:398,install,installing,398,,https://github.com/stardist/stardist/issues/75#issuecomment-661022071,1,['install'],['installing']
Deployability,"@GFleishman True, the python version is extremely slow!. > I'd like to modify this and rebuild - but I'm unsure how to do it, since the entire build was part of the pip install stardist and there is no documentation on building from source. I think the package gets built when installing with pip, on the Mac I had to follow these instructions by @maweigert: https://github.com/mpicbg-csbd/stardist/issues/21. I also have stardist installed on a Ubuntu 18 machine for the training and there was no problem. Otherwise, you could speed up the python loops using `numba.jit` e.g. directly from your jupyter notebook like this:. ```python; from stardist.geometry import geom2d; from numba import jit. def _py_star_dist_modified(a, n_rays=32):; # (np.isscalar(n_rays) and 0 < int(n_rays)) or _raise(ValueError()); n_rays = int(n_rays); a = a.astype(np.uint16,copy=False); dst = np.empty(a.shape+(n_rays,),np.float32). for i in range(a.shape[0]):; for j in range(a.shape[1]):; value = a[i,j]; if value == 0:; dst[i,j] = 0; else:; st_rays = np.float32((2*np.pi) / n_rays); for k in range(n_rays):; phi = np.float32(k*st_rays); dy = np.cos(phi)#/100.; dx = np.sin(phi)#/100.; x, y = np.float32(0), np.float32(0). while True:; x += dx; y += dy; ii = int(round(i+x)); jj = int(round(j+y)); if (ii < 0 or ii >= a.shape[0] or; jj < 0 or jj >= a.shape[1] or; value != a[ii,jj]):; dist = np.sqrt(x*x + y*y); dst[i,j,k] = dist; break; return dst. geom2d._py_star_dist = jit(_py_star_dist_modified); ```. This version performs about the same as the cpp one:; ```; %timeit relabel_image_stardist(test_lbl, n_rays=32, mode='python'); 452 Âµs Â± 7.97 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each); %timeit relabel_image_stardist(test_lbl, n_rays=32, mode='cpp'); 493 Âµs Â± 8.6 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each); ```. > Can you rule out that it's just a phenomenon of the data, i.e. different intensity characteristics at the boundaries of training/testing data - so the model just learne",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-583505792:169,install,install,169,,https://github.com/stardist/stardist/issues/33#issuecomment-583505792,3,['install'],"['install', 'installed', 'installing']"
Deployability,"Hello,. I'm facing the same issue, all my data can not fit in RAM (I have 3D images and total training set > > 100Go). I've been trying `keras.utils.Sequence` as suggested by @maweigert, my idea is to simply replace a list of numpy arrays with a Sequence class that loads images on the fly. ```; from tensorflow.keras.utils import Sequence; from tifffile import imread. class DataLoader(Sequence):; ; def __init__(self, path_list, mode):; self.x, self.mode = path_list, mode; ; #retro compatibility with numpy; self.ndim=3; if mode == ""Y"":; self.dtype = np.int; else:; self.dtype = np.float; ; def __len__(self):; return len(self.x). def __getitem__(self, idx):; ; filename = self.x[idx]; if self.mode == ""X"":; return imread(filename) / 255; else:; return imread(filename); ```. I'm actually able to launch a training with batch size of 1 without specifying anisotropy and things seem to run ok (note that I had to add `dtype` and `ndim` to make things work and that I have some problems in my pipeline as explained bellow). However some part of my training pipeline are broken:. - when running this to compute anisotropy `extents = calculate_extents(Y)` I first get this warning; ```_asarray.py (83): Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray```; and after a few seconds of computation the code fails with this error message :; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-14-36ac39a79e87> in <module>; ----> 1 extents = calculate_extents(Y); 2 anisotropy = tuple(np.max(extents) / extents); 3 print('empirical anisotropy of labeled objects = %s' % str(anisotropy)). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/utils.py in calculate_extents(lbl, func); 165 n in ",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:994,pipeline,pipeline,994,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['pipeline'],['pipeline']
Deployability,"Hello,; Yes, so i install homebrew then i installed gcc then i ran the command CC=gcc-9 CXX=g++-9 pip install stardist. Then i get this error below. I checked the version of python i have on the terminal and it is; % python3 --version; Python 3.8.4. so im not really sure what to do now. ```; Defaulting to user installation because normal site-packages is not writeable; Collecting stardist; Using cached stardist-0.1.0.tar.gz (46 kB); Collecting csbdeep; Using cached csbdeep-0.6.0-py2.py3-none-any.whl (67 kB); Collecting scikit-image; Using cached scikit_image-0.14.5-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (29.3 MB); Requirement already satisfied: numpy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.8.0rc1); Collecting tifffile; Using cached tifffile-2019.7.26.2-py2.py3-none-any.whl (131 kB); Collecting backports.tempfile; python_version < ""3.4""; Using cached backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB); Requirement already satisfied: six in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.12.0); Collecting h5py; Using cached h5py-2.10.0-cp27-cp27m-macosx_10_6_intel.whl (2.9 MB); Requirement already satisfied: matplotlib in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.3.1); Collecting keras<2.4,>=2.1.2; Using cached Keras-2.3.1-py2.py3-none-any.whl (377 kB); Requirement already satisfied: scipy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (0.13.0b1); Collecting tqdm; Using cached tqdm-4.47.0-py2.py3-none-any.whl (66 kB); Collecting pathlib2; python_version < ""3""; Using cached pathlib2-2.3.5-py2.py3-none-any.whl (18 kB); Collecting imagecodecs-lite<=2020; python_version < ""3.6""; Using cached imagecodecs-lite-2019.12.3.tar.gz (1.1 MB); ERROR: Command errored out with exi",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:18,install,install,18,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,4,['install'],"['install', 'installation', 'installed']"
Deployability,"Hi @Cocomolch4000 ,. This is a good point - we've just updated the Readme to reflect this. Thanks for the feedback!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/139#issuecomment-833649844:55,update,updated,55,,https://github.com/stardist/stardist/issues/139#issuecomment-833649844,1,['update'],['updated']
Deployability,"Hi @esgomezm,. > We are actually trying to export the model using TF2.x (the idea is to test it with the new java library in Fiji + deepImageJ). Is the new Java library for deep learning ready yet? What is the name and where can I find it?. > Is it the export being forced inside StarDist to have a model compatible with TF1.15?. Yes, the model export in the CSBDeep and StarDist Python packages was always meant for use in Fiji, which has only ever supported TensorFlow 1.x. > I'm using the bioimageio library (version 0.5.8) with TF2.11 to export TF and Keras models, and everything works perfectly. Using the bioimageio library where to export the model? And ""everything works perfectly"" where?. > A potential solution (probably you have a better one), could be to avoid those imports if TF version > 2.3? Until version 2.3, the compatibility with CSBDeep & StarDist is ensured, but for later versions is not unless the plugins are updated. Also, my feeling is that the export fails with TF>2.3. I really need to understand the current situation first... I don't even know how to export the model such that it will work with the new Java library.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-1453562117:935,update,updated,935,,https://github.com/stardist/stardist/issues/68#issuecomment-1453562117,1,['update'],['updated']
Deployability,"Hi @imand500 ,. Sorry for the late reply. . > Since the model is not saved as h5 and it's really dependent on the configuration you made. If a pretrained models is loaded; ```python ; model = StarDist2D.from_pretrained(""2D_versatile_he""); ```; it is actually downloaded and stored as a normal stardist model in your keras cache directory. You can find the location like so:; ```python; print(model.logdir); ```; So you could simply copy that folder somewhere, create a new model from that folder and then continue training with your own data. Hope that helps,. M",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/127#issuecomment-830683343:114,configurat,configuration,114,,https://github.com/stardist/stardist/issues/127#issuecomment-830683343,1,['configurat'],['configuration']
Deployability,"Hi @m-albert,. > While in the previous example the boundary is simply very smooth, here it seems to be slightly off. Hard to see why it should deviate so much for the images you show, which should be rather easy to segment...would have too look further into it (maybe cells are too large, gridsize too small etc) . > This might be important when dealing with small objects. Not sure decreasing the step size will help with my boundaries but I'll retrain and give it a try, potentially being off at the boundaries could confuse the distance predictions. Thanks (and @GFleishman too) for bringing that up! Indeed the stardist calculations are a bit rough for small objects and we never bothered to refine them correctly. Inspired by this thread I took another look at them: Instead of decreasing the stepsize (which would probably be too slow) one can [directly compute the ""overshoot"" distance after the label switches](https://github.com/mpicbg-csbd/stardist/blob/dev/stardist/lib/stardist2d.cpp#L74). That way, distances for small objects should be now more correct. I've put it in the `dev` branch, so you can try it yourself:. `pip install git+https://github.com/mpicbg-csbd/stardist.git@dev`. Let me know if that helps and thanks for all the feedback and input!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-583762427:1135,install,install,1135,,https://github.com/stardist/stardist/issues/33#issuecomment-583762427,1,['install'],['install']
Deployability,"Hi @pedgomgal1 ,. > first congrats for the complete and accesible pipeline that you provide us, people with very few expertise in machine/deep learning. Thanks! :). > With the IoU graph that I have attached. If you look at the accuracy metrics in the left graph, you see that almost all of them are basically zero, i.e. your model a totally wrong output. This might be the reason for the dying kernel too. Did you have a look at `Y_val_pred `? Does that look reasonable? . So I would first try to see what is going on.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/51#issuecomment-622454317:66,pipeline,pipeline,66,,https://github.com/stardist/stardist/issues/51#issuecomment-622454317,1,['pipeline'],['pipeline']
Deployability,"Hi @qinghongwan, thanks for reporting! It's a simple bug that I've [just fixed](https://github.com/stardist/stardist/commit/0993844dba2bfee55b6b14045798d19e4be1519a) in the `dev` branch. You can install this branch via `pip install git+https://github.com/stardist/stardist@dev` before we make a new proper release (no schedule for that yet). However, you might run into [installation issues](https://github.com/stardist/stardist#installation-1).",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/154#issuecomment-885661470:195,install,install,195,,https://github.com/stardist/stardist/issues/154#issuecomment-885661470,5,"['install', 'release']","['install', 'installation', 'installation-', 'release']"
Deployability,"Hi @romainGuiet,. You should be able to get the original shape by simply linearly upscaling the probability and distance prediction by the grid parameter (i.e. two fold along each axis if `grid=(2,2)`). . Your planning to have the full pipeline (with polygon rendering) as part of a Fiji plugin? . Cheers,; M",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/28#issuecomment-557546384:236,pipeline,pipeline,236,,https://github.com/stardist/stardist/issues/28#issuecomment-557546384,1,['pipeline'],['pipeline']
Deployability,"Hi @uschmidt83 ,. Thanks for clarifying all that! ; Indeed I did not notice this flag was exclusively for the data generation for training step. Just the fact that there was a variable `use_gpu` set to `False` led me to wrong conclusions. Perhaps a more explicit naming can help to avoid confusions like this in the future, something like `use_gpu_for_data_gen`. > No, this is intentional. The and gputools_available() part acts as a [""guard""](https://en.wikipedia.org/wiki/Guard_(computer_science)) to always disable this flag when gputools is not installed. I see, so it is up to the user to actively change it if they want to use it for data generation. Thanks, I believe things are clear for me now.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/222#issuecomment-1445371166:549,install,installed,549,,https://github.com/stardist/stardist/issues/222#issuecomment-1445371166,1,['install'],['installed']
Deployability,"Hi Uwe,; It's entirely possible it's specific to our implementation then. What I meant by unstable is that I had obtained a few times an unusual loss curve with an additional peak half way through the number of epoch. See below. This does not match any changes in learning rate either. <img width=""910"" alt=""Screenshot 2021-02-24 at 22 12 44"" src=""https://user-images.githubusercontent.com/21193399/109297054-4d408b80-7829-11eb-9aac-f0e9788e5907.png"">. But either way, we've now upgraded it to TF2.x and implemented the default settings as close to yours as possible, as well as the exact ways to do augmentation. And this seems to perform much better and not give the phantom masks or the unusual loss curves. Thanks a lot for your help. I really appreciate it.; Best,. Romain",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/116#issuecomment-786602735:479,upgrade,upgraded,479,,https://github.com/stardist/stardist/issues/116#issuecomment-786602735,1,['upgrade'],['upgraded']
Deployability,"Hi guys,. Finally I found time to try out something (see below). It seems to work and I attached the steps and my comments.; Please let me know what you think about. Thanks again for your help. Carlo. ## StarDist-GPU Windows 10 Installation Steps for NVIDIA RTX 3080. Source: https://medium.com/@dun.chwong/the-simple-guide-deep-learning-with-rtx-3090-cuda-cudnn-tensorflow-keras-pytorch-e88a2a8249bc. ##### Used packages: . 1. cuda_11.0.2_win10_network. . 2. cudnn-11.0-windows-x64-v8.0.5.39. . 3. tf-nightly-gpu 2.5.0.dev20201212. ##### Installation steps:. 1. Install CUDA 11 and cudnn-11.0 as described https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html. . 2. Verify CUDA and cudnn installation by doing the following:. â€‹	. - Open Windows *Command Prompt*. . - Type:. `$ nvidia-smi`. . Output:. ```; Sat Dec 12 23:24:40 2020; +-----------------------------------------------------------------------------+; | NVIDIA-SMI 460.79 Driver Version: 460.79 CUDA Version: 11.2 |; |-------------------------------+----------------------+----------------------+; | GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |; | | | MIG M. |; |===============================+======================+======================|; | 0 GeForce RTX 3080 WDDM | 00000000:68:00.0 On | N/A |; | 53% 35C P2 104W / 340W | 6182MiB / 10240MiB | 0% Default |; | | | N/A |; +-------------------------------+----------------------+----------------------+; ; +-----------------------------------------------------------------------------+; | Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | 0 N/A N/A 1464 C+G Insufficient Permissions N/A |; | 0 N/A N/A 4652 C+G ...zilla Firefox\firefox.exe N/A |; | 0 N/A N/A 6760 C+G Insufficient Permissions N/A |; | 0 N/A N/A 7700 C+G ...bbwe\Microsoft.Photos.exe N/A |; | 0 N/A",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:641,install,installation-guide-microsoft-windows,641,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,2,['install'],"['installation', 'installation-guide-microsoft-windows']"
Deployability,"Hi, thanks for using StarDist. > As StarDist need numpy, its installation failed as numpy is not yet available. Yes, thanks for reminding me about that. Since `numpy` is typically installed, most people don't notice. > I fix my issue by installing numpy before my other requirements. Yes, that's currently the only way. > however I suggest to add numpy in the requirements of StarDist, I think it is missing. It's not that simple. Actually, we implicitly require `numpy` through `csbdeep`.; The real issue is that we need `numpy` to even build the wheel (package), i.e. our `setup.py` imports from `numpy`. I've [just added](https://github.com/mpicbg-csbd/stardist/commit/9bd7ff911bca5e87cbe70a8549b9c746d6d1c245) a `pyproject.toml`, which will hopefully fix this issue in the next release. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/71#issuecomment-657642225:61,install,installation,61,,https://github.com/stardist/stardist/issues/71#issuecomment-657642225,4,"['install', 'release']","['installation', 'installed', 'installing', 'release']"
Deployability,"Hi,. > My question is if I can run the whole example script by colab on a better PC and get a model I can export to Fiji?. Yes, basically. We have an [example notebook that runs StarDist 2D on Colab](https://colab.research.google.com/github/mpicbg-csbd/stardist/blob/master/extras/stardist_example_2D_colab.ipynb). Otherwise, you can also try the StarDist notebooks provided by [ZeroCostDL4Mic](https://github.com/HenriquesLab/ZeroCostDL4Mic). > We have one in our imaging facility, but I don't think that we are allowed to install a complete python environment and so on. I would suggest to use your own computer if you're (starting to get) serious about using deep learning in your facility. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/115#issuecomment-782405792:524,install,install,524,,https://github.com/stardist/stardist/issues/115#issuecomment-782405792,1,['install'],['install']
Deployability,I think the next steps are:; - also generate the config and macro for stardist postprocessing with the exporter so that the model can be run in deepimagej; - create the python functionality to run a stardist bioimageio model + stardist postprocessing (I think I would just add this to bioimageio.core for simplicity); - check that we can use the model in deepimagej and python; - release a new stardist version so that the functionality is available; - update the zero-cost notebook to use this functionality to export stardist bioimageio models. I will start working on the first point later.,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-973972757:380,release,release,380,,https://github.com/stardist/stardist/pull/171#issuecomment-973972757,2,"['release', 'update']","['release', 'update']"
Deployability,"Morning @uschmidt83 !. > Is the new Java library for deep learning ready yet? What is the name and where can I find it?. Yes, here it is: https://github.com/bioimage-io/model-runner-java. > Using the bioimageio library where to export the model? And ""everything works perfectly"" where?. Here is an example where we are using it: https://github.com/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/U-Net_2D_Multilabel_ZeroCostDL4Mic.ipynb; And here you can find the model already: https://bioimage.io/#/?type=all&tags=deepimagej,bacillus-subtilis&id=10.5281%2Fzenodo.7261974. > I really need to understand the current situation first... I don't even know how to export the model such that it will work with the new Java library. The bioimageio.core library will export it directly in a compatible format (as the model I linked in the previous lines). In the upcoming days, we will update deepImageJ with the java model runner so all the TF2 models can also be deployed in Fiji. I can let you know when the plugin is ready if you want.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-1455823190:887,update,update,887,,https://github.com/stardist/stardist/issues/68#issuecomment-1455823190,2,"['deploy', 'update']","['deployed', 'update']"
Deployability,"Not grid=4; I was referencing this previous discussion: https://github.com/mpicbg-csbd/stardist/issues/33#issuecomment-583421466; Yeah, 32^3 with grid=4 would be 2^3 patches right?! Definitely too small :). Qualitatively the results look ok - there are definitely some nuclei missing and very few cells that are just too large. The shape of nuclei is fine, they're all roughly elliptical, but exact boundary matching isn't important to me, if I get 0.6 IOU or better for the majority of cells then I will be very happy. I don't really know what magnitude of prob_loss would be considered good, I was just looking at the trend. It's so jumpy, it seemed that I had reached the best I could with the data I have - which is rather noisy and was even quite difficult to segment by hand in many places. Qualitatively things look ok (here's a 128x128x128 patch, z-voxels are 2x bigger than x/y voxels hence z dimension looking larger in viewer):. ![Screen Shot 2020-03-02 at 3 19 11 PM](https://user-images.githubusercontent.com/8507206/75714224-4d9b5a80-5c99-11ea-9b99-5aad450022bf.png); ![Screen Shot 2020-03-02 at 3 19 42 PM](https://user-images.githubusercontent.com/8507206/75714226-4d9b5a80-5c99-11ea-9d26-28ca0ff1a3cd.png). I only manually annotated a 128x128x32 patch in a different part of the brain than what's shown above, so it at least generalizes ok to different ROIs of the acquisition and different z-depths. Would you consider manually lowering the learning rate after so many epochs? Or any other recommendations for getting the prob_loss even lower?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/36#issuecomment-593599163:166,patch,patches,166,,https://github.com/stardist/stardist/issues/36#issuecomment-593599163,3,['patch'],"['patch', 'patches']"
Deployability,"Ok, thanks for letting us know. Btw, I got TensorFlow with GPU support working (also on Windows) by simply installing [this conda environment](https://github.com/CSBDeep/CSBDeep/tree/master/extras#conda-environment). It will automatically install the necessary CUDA and cuDNN libraries. After installing this environment, activate it, and then install StarDist via pip.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/121#issuecomment-789241256:107,install,installing,107,,https://github.com/stardist/stardist/issues/121#issuecomment-789241256,4,['install'],"['install', 'installing']"
Deployability,"Ok, there was indeed an offending non-ascii character. Thanks for the feedback!. If you install the latest version (`stardist 0.3.4`) it should be fixed.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/18#issuecomment-535047059:88,install,install,88,,https://github.com/stardist/stardist/issues/18#issuecomment-535047059,1,['install'],['install']
Deployability,"Perhaps the README just needs to be updated that only macOS 12 is supported on arm64?; I'm not sure how this behaves on x86, but Apple is pretty clear about tensorflow arm64 being macOS 12 and up.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/174#issuecomment-1282938048:36,update,updated,36,,https://github.com/stardist/stardist/issues/174#issuecomment-1282938048,1,['update'],['updated']
Deployability,"Regarding the deepImageJ issues: we will need to wait for feedback from @esgomezm or @carlosuc3m. > Is this something that has to be fixed in `bioimage.io.core`? (We typically use [save_tiff_imagej_compatible](https://github.com/CSBDeep/CSBDeep/blob/b0d2f5f344ebe65a9b4c3007f4567fe74268c813/csbdeep/io/__init__.py#L15-L46) from csbdeep to save tiff files such that ImageJ will read them correctly.). This is fixed already and included in the latest release (0.4.10). > > > As discussed before christmas, the idea would be that you apply this macro automatically in deepimagej if you recognise that you have a stardist model, e.g. by checking if the key `config.stardist` exists.; > ; > Is this already considered in the forthcoming DeepImageJ update?. We decided to not tackle the automatic running for now and just apply the macro by hand for the use-case.; We'll tackle automatically running it later.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1034212624:449,release,release,449,,https://github.com/stardist/stardist/pull/171#issuecomment-1034212624,2,"['release', 'update']","['release', 'update']"
Deployability,"Thank you for your quick reply! . > I suggest you do the prediction in Python to get the label image, which you can then import in Fiji with the [3D Roi Manager](https://imagejdocu.tudor.lu/plugin/stacks/3d_roi_manager/start) (installation instructions [here](https://imagejdocu.tudor.lu/plugin/stacks/3d_ij_suite/start#installation)). This should work well, as long as you don't have (strongly) overlapping predictions or it's not important to you to get the complete shapes for all objects. This sounds really good. I do have quite overlapping nuclei though unfortunately. Apologies if this is a bit of a starter question but would I just save the label image as a tiff file and then open it in FIJI, and then segment it with the 3D ROI viewer? . > It's not that simple and even if it worked, this won't get you 3D ROIs in Fiji. Our [StarDist Fiji plugin](https://imagej.net/StarDist) currently only works in 2D because it's not that easy to support 3D... but we hope to get this done eventually. Ah I thought it wouldn't be simple. The error message I was getting was that the input tensor had to be 5 dimensional, and was wondering whether the mismatch was due to tensor organisation convention of NHWC not including a third spatial dimension. . The images you have from paintera look really beautiful, I think that is my aim here, but perhaps FIJI's 3d viewer of the 3d ROIs would get something similar?. Thanks again!!. Michael Schwimmer",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/37#issuecomment-597304169:227,install,installation,227,,https://github.com/stardist/stardist/issues/37#issuecomment-597304169,2,['install'],['installation']
Deployability,"Thanks Marvin, that's super helpful, I think you're probably right that this is the relevant code to modify for my data. However, this loopy python code is not practical to actually run on my data, where I have several hundred cells per image. It seems the default for my local installation is to run the compiled cpp version:; https://github.com/mpicbg-csbd/stardist/blob/38454feca61804049a3afae34add804e0b91517c/stardist/lib/stardist2d.cpp#L53-L59. I'd like to modify this and rebuild - but I'm unsure how to do it, since the entire build was part of the `pip install stardist` and there is no documentation on building from source. @maweigert @uschmidt83 Can you guys provide any information about how to rebuild the cpp libraries in stardist? I'm using MacOS and gcc-9, but will need to repeat this process later on red hat linux. Edit: This turned out to be easy. I just made my modifications to the cpp code and reran `python setup.py sdist` from the source directory, then `python setup.py install` For MacOSX you need `CC=gcc-9 CXX=g++-9 python setup.py install`. Since you've been so helpful, I'd like to try and help with your issue as well - but since it occurs primarily on reconstructions of the test data polygons, it's probably something different. Can you rule out that it's just a phenomenon of the data, i.e. different intensity characteristics at the boundaries of training/testing data - so the model just learned to produce more smooth/conservative boundaries?. Edit: I forced the python versions to run with a 4 in the denominator for the step size; it is significantly slower than the CPP code and not practically useful, but it does definitely solve the roughness problem for small label areas:. ![Screen Shot 2020-02-07 at 11 17 10 AM](https://user-images.githubusercontent.com/8507206/74045936-a5e18400-499b-11ea-93cf-340bef4e5d6f.png). <img width=""868"" alt=""Screen Shot 2020-02-07 at 11 17 12 AM"" src=""https://user-images.githubusercontent.com/8507206/74045949-aaa63800-499b",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-583469917:278,install,installation,278,,https://github.com/stardist/stardist/issues/33#issuecomment-583469917,2,['install'],"['install', 'installation']"
Deployability,"brary/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.3.1); Collecting keras<2.4,>=2.1.2; Using cached Keras-2.3.1-py2.py3-none-any.whl (377 kB); Requirement already satisfied: scipy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (0.13.0b1); Collecting tqdm; Using cached tqdm-4.47.0-py2.py3-none-any.whl (66 kB); Collecting pathlib2; python_version < ""3""; Using cached pathlib2-2.3.5-py2.py3-none-any.whl (18 kB); Collecting imagecodecs-lite<=2020; python_version < ""3.6""; Using cached imagecodecs-lite-2019.12.3.tar.gz (1.1 MB); ERROR: Command errored out with exit status 1:; command: /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py'""'""'; __file__='""'""'/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-pip-egg-info-I2_dBJ; cwd: /private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/; Complete output (51 lines):; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py"", line 101, in <module>; 'Programming Language :: Python :: Implementation :: CPython',; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/__init__.py"", line 144, in setup; _install_setup_requires(attrs); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/__init__.py"", line",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:2363,install,install-FhuQIv,2363,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['install'],['install-FhuQIv']
Deployability,"e N/A |; | 0 N/A N/A 17700 C+G ...nputApp\TextInputHost.exe N/A |; | 0 N/A N/A 17796 C+G ...y\ShellExperienceHost.exe N/A |; | 0 N/A N/A 19544 C+G ...b3d8bbwe\WinStore.App.exe N/A |; | 0 N/A N/A 20168 C+G ...zilla Firefox\firefox.exe N/A |; | 0 N/A N/A 22232 C ...\envs\stardist\python.exe N/A |; +-----------------------------------------------------------------------------+; ```. . - Type:. `$ nvcc -V`. . Output:. ```; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2020 NVIDIA Corporation; Built on Thu_Jun_11_22:26:48_Pacific_Daylight_Time_2020; Cuda compilation tools, release 11.0, V11.0.194; Build cuda_11.0_bu.relgpu_drvr445TC445_37.28540450_0; ```. . 3. Open the anaconda terminal and create the stardist conda environment. `$ conda create -n stardist pip python==3.8`. . 4. Activate the conda environment. `$ conda activate stardist`. . 5. Install *numpy*. `$ conda install numpy`. 6. Install *tesorflow-gpu* nightly using pip (tf-nightly-gpu 2.5.0.dev20201212). `pip install tf-nightly-gpu`. 7. Follow the installation instructions for stardist. https://github.com/mpicbg-csbd/stardist. . <!--I got a compatibility issue error with h5py library version. I just continue with the installation and everything seems to work correctly-->. 8. Install the NEUBIAS academy Jupyter Notebook for testing stardist installation . `$ git clone https://github.com/maweigert/neubias_academy_stardist.git`. ##### Run stardist-gpu on the NVIDIA RTX 3080. 1. Start the *Jupyter Notebook* . . 2. Comment the following lines:. ```; $ #%tensorflow_version 1.x; ; $ #!pip install stardist; ```. . 3. The direct downloads didn't work for me, I just comment these lines out and downloaded the sample data before. ```; '''; download_and_extract_zip_file(; url = 'https://github.com/mpicbg-csbd/stardist/releases/download/0.1.0/dsb2018.zip',; targetdir = 'data',; verbose = 1,; ); '''; ```. 4. To avoid TF GPU out of memory issues I add the following code:. ```python; import tensorflow as tf; gpus = ",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:3777,install,install,3777,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,1,['install'],['install']
Deployability,"efox\firefox.exe N/A |; | 0 N/A N/A 22232 C ...\envs\stardist\python.exe N/A |; +-----------------------------------------------------------------------------+; ```. . - Type:. `$ nvcc -V`. . Output:. ```; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2020 NVIDIA Corporation; Built on Thu_Jun_11_22:26:48_Pacific_Daylight_Time_2020; Cuda compilation tools, release 11.0, V11.0.194; Build cuda_11.0_bu.relgpu_drvr445TC445_37.28540450_0; ```. . 3. Open the anaconda terminal and create the stardist conda environment. `$ conda create -n stardist pip python==3.8`. . 4. Activate the conda environment. `$ conda activate stardist`. . 5. Install *numpy*. `$ conda install numpy`. 6. Install *tesorflow-gpu* nightly using pip (tf-nightly-gpu 2.5.0.dev20201212). `pip install tf-nightly-gpu`. 7. Follow the installation instructions for stardist. https://github.com/mpicbg-csbd/stardist. . <!--I got a compatibility issue error with h5py library version. I just continue with the installation and everything seems to work correctly-->. 8. Install the NEUBIAS academy Jupyter Notebook for testing stardist installation . `$ git clone https://github.com/maweigert/neubias_academy_stardist.git`. ##### Run stardist-gpu on the NVIDIA RTX 3080. 1. Start the *Jupyter Notebook* . . 2. Comment the following lines:. ```; $ #%tensorflow_version 1.x; ; $ #!pip install stardist; ```. . 3. The direct downloads didn't work for me, I just comment these lines out and downloaded the sample data before. ```; '''; download_and_extract_zip_file(; url = 'https://github.com/mpicbg-csbd/stardist/releases/download/0.1.0/dsb2018.zip',; targetdir = 'data',; verbose = 1,; ); '''; ```. 4. To avoid TF GPU out of memory issues I add the following code:. ```python; import tensorflow as tf; gpus = tf.config.experimental.list_physical_devices('GPU'); tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4092)]); ```. . Output:. ```; 2020-12",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:3989,install,installation,3989,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,1,['install'],['installation']
Deployability,"extInputHost.exe N/A |; | 0 N/A N/A 17796 C+G ...y\ShellExperienceHost.exe N/A |; | 0 N/A N/A 19544 C+G ...b3d8bbwe\WinStore.App.exe N/A |; | 0 N/A N/A 20168 C+G ...zilla Firefox\firefox.exe N/A |; | 0 N/A N/A 22232 C ...\envs\stardist\python.exe N/A |; +-----------------------------------------------------------------------------+; ```. . - Type:. `$ nvcc -V`. . Output:. ```; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2020 NVIDIA Corporation; Built on Thu_Jun_11_22:26:48_Pacific_Daylight_Time_2020; Cuda compilation tools, release 11.0, V11.0.194; Build cuda_11.0_bu.relgpu_drvr445TC445_37.28540450_0; ```. . 3. Open the anaconda terminal and create the stardist conda environment. `$ conda create -n stardist pip python==3.8`. . 4. Activate the conda environment. `$ conda activate stardist`. . 5. Install *numpy*. `$ conda install numpy`. 6. Install *tesorflow-gpu* nightly using pip (tf-nightly-gpu 2.5.0.dev20201212). `pip install tf-nightly-gpu`. 7. Follow the installation instructions for stardist. https://github.com/mpicbg-csbd/stardist. . <!--I got a compatibility issue error with h5py library version. I just continue with the installation and everything seems to work correctly-->. 8. Install the NEUBIAS academy Jupyter Notebook for testing stardist installation . `$ git clone https://github.com/maweigert/neubias_academy_stardist.git`. ##### Run stardist-gpu on the NVIDIA RTX 3080. 1. Start the *Jupyter Notebook* . . 2. Comment the following lines:. ```; $ #%tensorflow_version 1.x; ; $ #!pip install stardist; ```. . 3. The direct downloads didn't work for me, I just comment these lines out and downloaded the sample data before. ```; '''; download_and_extract_zip_file(; url = 'https://github.com/mpicbg-csbd/stardist/releases/download/0.1.0/dsb2018.zip',; targetdir = 'data',; verbose = 1,; ); '''; ```. 4. To avoid TF GPU out of memory issues I add the following code:. ```python; import tensorflow as tf; gpus = tf.config.experimental.list_physical_devices",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:3816,install,installation,3816,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,1,['install'],['installation']
Deployability,"he NMS step. > 4- `Also, it must hold that: min_overlap + 2*context < block_size.` I would easily understand that ` 2*context < block_size` in order not to discard everything, but why does `min_overlap` appears here? Would you have a link to a visual explanation how you define the different blocks? I think this could help me understand. After the context is discarded on all sides, blocks still have to overlap at least `min_overlap` pixels. As you say, a diagram would make this clear. Note that the `model.predict_instances_big` is relatively new and really meant for people who have huge images. It's not a mainstream function. > 5- all my 3D stacks don't have the same sizes, should I change the above parameters according to each sizes or one set should work for all?. The parameters can be the the same and really only depend on the CNN architecture (context) and the object sizes (min_overlap). However, the stack must obviously be bigger than the block size. > 6- Overall would you have some advise to improve my pipeline?. Like I said, try using `model.predict_instances` instead and save yourself the headache ;). > I'm working on a docker container of my own and installed the latest tensorflow version, I was getting this error when instantiating the StarDist3D`MemoryError: clEnqueueWriteBuffer failed: MEM_OBJECT_ALLOCATION_FAILURE` it was taking all my 12Go of GPU Memory (I have an RTX 2080 Ti). Yes, TensorFlow unfortunately grabs all the GPU memory by default. That's why [our training notebook](https://nbviewer.jupyter.org/github/mpicbg-csbd/stardist/blob/master/examples/3D/2_training.ipynb) calls `limit_gpu_memory` to reserve 20% of the GPU memory to be used for OpenCL. Note that 20% may not be enough, it really depends on the patch size and the amount of total GPU memory. > Adding this lines of code solved my problem (just wanted to let you know in case someone has the same problem in the future):. Allowing GPU memory growth is an alternative, but might be less efficie",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:4208,pipeline,pipeline,4208,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,1,['pipeline'],['pipeline']
Deployability,"ivate the conda environment. `$ conda activate stardist`. . 5. Install *numpy*. `$ conda install numpy`. 6. Install *tesorflow-gpu* nightly using pip (tf-nightly-gpu 2.5.0.dev20201212). `pip install tf-nightly-gpu`. 7. Follow the installation instructions for stardist. https://github.com/mpicbg-csbd/stardist. . <!--I got a compatibility issue error with h5py library version. I just continue with the installation and everything seems to work correctly-->. 8. Install the NEUBIAS academy Jupyter Notebook for testing stardist installation . `$ git clone https://github.com/maweigert/neubias_academy_stardist.git`. ##### Run stardist-gpu on the NVIDIA RTX 3080. 1. Start the *Jupyter Notebook* . . 2. Comment the following lines:. ```; $ #%tensorflow_version 1.x; ; $ #!pip install stardist; ```. . 3. The direct downloads didn't work for me, I just comment these lines out and downloaded the sample data before. ```; '''; download_and_extract_zip_file(; url = 'https://github.com/mpicbg-csbd/stardist/releases/download/0.1.0/dsb2018.zip',; targetdir = 'data',; verbose = 1,; ); '''; ```. 4. To avoid TF GPU out of memory issues I add the following code:. ```python; import tensorflow as tf; gpus = tf.config.experimental.list_physical_devices('GPU'); tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4092)]); ```. . Output:. ```; 2020-12-13 07:44:01.102879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties:; pciBusID: 0000:68:00.0 name: GeForce RTX 3080 computeCapability: 8.6; coreClock: 1.785GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s; 2020-12-13 07:44:01.103001: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll; 2020-12-13 07:44:01.103940: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll; 2020-12-13 07:",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:4589,release,releases,4589,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,1,['release'],['releases']
Deployability,"le ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/__init__.py"", line 144, in setup; _install_setup_requires(attrs); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/__init__.py"", line 139, in _install_setup_requires; dist.fetch_build_eggs(dist.setup_requires); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/dist.py"", line 717, in fetch_build_eggs; replace_conflicting=True,; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py"", line 782, in resolve; replace_conflicting=replace_conflicting; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py"", line 1065, in best_match; return self.obtain(req, installer); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py"", line 1077, in obtain; return installer(requirement); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/dist.py"", line 784, in fetch_build_egg; return cmd.easy_install(req); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 679, in easy_install; return self.install_item(spec, dist.location, tmpdir, deps); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 705, in install_item; dists = self.install_eggs(spec, download, tmpdir); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 890, in install_eggs; return self.build_and_install(setup_script, setup_base); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1158, in build_and_install; self.run_setup(setup_script, setup_base, args); File ""/System/Library/Fram",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:4059,install,installer,4059,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['install'],['installer']
Deployability,"lpful, I think you're probably right that this is the relevant code to modify for my data. However, this loopy python code is not practical to actually run on my data, where I have several hundred cells per image. It seems the default for my local installation is to run the compiled cpp version:; https://github.com/mpicbg-csbd/stardist/blob/38454feca61804049a3afae34add804e0b91517c/stardist/lib/stardist2d.cpp#L53-L59. I'd like to modify this and rebuild - but I'm unsure how to do it, since the entire build was part of the `pip install stardist` and there is no documentation on building from source. @maweigert @uschmidt83 Can you guys provide any information about how to rebuild the cpp libraries in stardist? I'm using MacOS and gcc-9, but will need to repeat this process later on red hat linux. Edit: This turned out to be easy. I just made my modifications to the cpp code and reran `python setup.py sdist` from the source directory, then `python setup.py install` For MacOSX you need `CC=gcc-9 CXX=g++-9 python setup.py install`. Since you've been so helpful, I'd like to try and help with your issue as well - but since it occurs primarily on reconstructions of the test data polygons, it's probably something different. Can you rule out that it's just a phenomenon of the data, i.e. different intensity characteristics at the boundaries of training/testing data - so the model just learned to produce more smooth/conservative boundaries?. Edit: I forced the python versions to run with a 4 in the denominator for the step size; it is significantly slower than the CPP code and not practically useful, but it does definitely solve the roughness problem for small label areas:. ![Screen Shot 2020-02-07 at 11 17 10 AM](https://user-images.githubusercontent.com/8507206/74045936-a5e18400-499b-11ea-93cf-340bef4e5d6f.png). <img width=""868"" alt=""Screen Shot 2020-02-07 at 11 17 12 AM"" src=""https://user-images.githubusercontent.com/8507206/74045949-aaa63800-499b-11ea-9c76-3ef96e31b905.png"">",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-583469917:1062,install,install,1062,,https://github.com/stardist/stardist/issues/33#issuecomment-583469917,1,['install'],['install']
Deployability,"nd that ` 2*context < block_size` in order not to discard everything, but why does `min_overlap` appears here? Would you have a link to a visual explanation how you define the different blocks? I think this could help me understand. After the context is discarded on all sides, blocks still have to overlap at least `min_overlap` pixels. As you say, a diagram would make this clear. Note that the `model.predict_instances_big` is relatively new and really meant for people who have huge images. It's not a mainstream function. > 5- all my 3D stacks don't have the same sizes, should I change the above parameters according to each sizes or one set should work for all?. The parameters can be the the same and really only depend on the CNN architecture (context) and the object sizes (min_overlap). However, the stack must obviously be bigger than the block size. > 6- Overall would you have some advise to improve my pipeline?. Like I said, try using `model.predict_instances` instead and save yourself the headache ;). > I'm working on a docker container of my own and installed the latest tensorflow version, I was getting this error when instantiating the StarDist3D`MemoryError: clEnqueueWriteBuffer failed: MEM_OBJECT_ALLOCATION_FAILURE` it was taking all my 12Go of GPU Memory (I have an RTX 2080 Ti). Yes, TensorFlow unfortunately grabs all the GPU memory by default. That's why [our training notebook](https://nbviewer.jupyter.org/github/mpicbg-csbd/stardist/blob/master/examples/3D/2_training.ipynb) calls `limit_gpu_memory` to reserve 20% of the GPU memory to be used for OpenCL. Note that 20% may not be enough, it really depends on the patch size and the amount of total GPU memory. > Adding this lines of code solved my problem (just wanted to let you know in case someone has the same problem in the future):. Allowing GPU memory growth is an alternative, but might be less efficient (not sure). As far as I recall, `limit_gpu_memory` also allows you to enable memory growth. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:4361,install,installed,4361,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,2,"['install', 'patch']","['installed', 'patch']"
Deployability,"not fit in RAM (I have 3D images and total training set > > 100Go). I've been trying `keras.utils.Sequence` as suggested by @maweigert, my idea is to simply replace a list of numpy arrays with a Sequence class that loads images on the fly. ```; from tensorflow.keras.utils import Sequence; from tifffile import imread. class DataLoader(Sequence):; ; def __init__(self, path_list, mode):; self.x, self.mode = path_list, mode; ; #retro compatibility with numpy; self.ndim=3; if mode == ""Y"":; self.dtype = np.int; else:; self.dtype = np.float; ; def __len__(self):; return len(self.x). def __getitem__(self, idx):; ; filename = self.x[idx]; if self.mode == ""X"":; return imread(filename) / 255; else:; return imread(filename); ```. I'm actually able to launch a training with batch size of 1 without specifying anisotropy and things seem to run ok (note that I had to add `dtype` and `ndim` to make things work and that I have some problems in my pipeline as explained bellow). However some part of my training pipeline are broken:. - when running this to compute anisotropy `extents = calculate_extents(Y)` I first get this warning; ```_asarray.py (83): Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray```; and after a few seconds of computation the code fails with this error message :; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-14-36ac39a79e87> in <module>; ----> 1 extents = calculate_extents(Y); 2 anisotropy = tuple(np.max(extents) / extents); 3 print('empirical anisotropy of labeled objects = %s' % str(anisotropy)). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/utils.py in calculate_extents(lbl, func); 165 n in (2,3) or _raise(ValueError(""label image should be 2",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:1058,pipeline,pipeline,1058,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['pipeline'],['pipeline']
Deployability,"r helpful, I think you're probably right that this is the relevant code to modify for my data. However, this loopy python code is not practical to actually run on my data, where I have several hundred cells per image. It seems the default for my local installation is to run the compiled cpp version:; https://github.com/mpicbg-csbd/stardist/blob/38454feca61804049a3afae34add804e0b91517c/stardist/lib/stardist2d.cpp#L53-L59. I'd like to modify this and rebuild - but I'm unsure how to do it, since the entire build was part of the `pip install stardist` and there is no documentation on building from source. @maweigert @uschmidt83 Can you guys provide any information about how to rebuild the cpp libraries in stardist? I'm using MacOS and gcc-9, but will need to repeat this process later on red hat linux. Edit: This turned out to be easy. I just made my modifications to the cpp code and reran `python setup.py sdist` from the source directory, then `python setup.py install` For MacOSX you need `CC=gcc-9 CXX=g++-9 python setup.py install`. Since you've been so helpful, I'd like to try and help with your issue as well - but since it occurs primarily on reconstructions of the test data polygons, it's probably something different. Can you rule out that it's just a phenomenon of the data, i.e. different intensity characteristics at the boundaries of training/testing data - so the model just learned to produce more smooth/conservative boundaries?. Edit: I forced the python versions to run with a 4 in the denominator for the step size; it is significantly slower than the CPP code and not practically useful, but it does definitely solve the roughness problem for small label areas:. ![Screen Shot 2020-02-07 at 11 17 10 AM](https://user-images.githubusercontent.com/8507206/74045936-a5e18400-499b-11ea-93cf-340bef4e5d6f.png). <img width=""868"" alt=""Screen Shot 2020-02-07 at 11 17 12 AM"" src=""https://user-images.githubusercontent.com/8507206/74045949-aaa63800-499b-11ea-9c76-3ef96e31b905.pn",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-583469917:997,install,install,997,,https://github.com/stardist/stardist/issues/33#issuecomment-583469917,1,['install'],['install']
Energy Efficiency,"> I'm a complete beginner at Python but find the software really exciting, so have jumped in at the deep end!. Glad that you like it!. > My aim is to be able to export the label maps and visualise them in ImageJ or something similar, and to use the 3 dimensional ROIs to calculate values for nuclear volume and shape and other things. I suggest you do the prediction in Python to get the label image, which you can then import in Fiji with the [3D Roi Manager](https://imagejdocu.tudor.lu/plugin/stacks/3d_roi_manager/start) (installation instructions [here](https://imagejdocu.tudor.lu/plugin/stacks/3d_ij_suite/start#installation)). This should work well, as long as you don't have (strongly) overlapping predictions or it's not important to you to get the complete shapes for all objects. > I can see that you've made a notebook to export the ROIs for the 2D model prediction into FIJI which works great for me. I tried to adapt this for 3D by just having axes set to zyx and having 'dist' instead of 'coord' (in export_imagej_rois('img_rois.zip', details['dist'])). I have found that this doesn't really work, but not sure why. This function only works for 2D polygons and cannot be simply adapted for 3D.; ; > I also tried to export the demo 3d model into deepImageJ, but it runs into trouble when you try to apply it to an image. I can give more details, but as I am quite lost I'm not sure what is the relevant information. It's not that simple and even if it worked, this won't get you 3D ROIs in Fiji. Our [StarDist Fiji plugin](https://imagej.net/StarDist) currently only works in 2D because it's not that easy to support 3D... but we hope to get this done eventually. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/37#issuecomment-597239267:926,adapt,adapt,926,,https://github.com/stardist/stardist/issues/37#issuecomment-597239267,2,['adapt'],"['adapt', 'adapted']"
Energy Efficiency,"> a bit confusing. I agree, we should change it a some point (the naming you suggest indeed makes a bit more sense). ; . > Specifically, why do you use `use_gpu = False and ...` which will always evaluate to `False` ?. This is to ensure that `gputools` is available, once the user decides to set `use_gpu`. Note, that this is a bit of a power-user setting (if you want to speed up training), since installing `gputools`/`pyopencl` can be a bit involved on some systems. This is why it is deactivated by default. . Hope that clears it up a bit and thanks for the feedback! . M",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/75#issuecomment-661022071:337,power,power-user,337,,https://github.com/stardist/stardist/issues/75#issuecomment-661022071,1,['power'],['power-user']
Energy Efficiency,"Hi @maweigert,. for some context: we are currently setting up a user-friendly workflow for users to train stardist and we stumbled over this line.; Thanks to your explanation I understand what the `use_gpu` variable does now, but I still find the line. ```; # Use OpenCL-based computations for data generator during training (requires 'gputools'); use_gpu = False and gputools_available(); ```; a bit confusing. Specifically, why do you use `use_gpu = False and ...` which will always evaluate to `False` ? ; Wouldn't `use_gpu = gputools_available()` be sufficient?; I would assume that `gputools_available` returns `True` if the additional dependency is available and `False` otherwise. One more point: the variable name `use_gpu` could maybe be renamed to `use_gpu_for_data_processing` or so; because now it implies that `use_gpu = False` means the gpu is not used at all (which I know is not the case, but doesn't become quite clear from the notebook). Sorry for being rather nit-picky here, but if this confuses us it will likely be confusing for users as well.; In any case, we will probably adapt the notebook a bit for users and just make these changes there, but we were wondering why you were choosing to do it like this in the first case.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/75#issuecomment-661014557:1097,adapt,adapt,1097,,https://github.com/stardist/stardist/issues/75#issuecomment-661014557,1,['adapt'],['adapt']
Energy Efficiency,"Hi @qinghongwan, thanks for reporting! It's a simple bug that I've [just fixed](https://github.com/stardist/stardist/commit/0993844dba2bfee55b6b14045798d19e4be1519a) in the `dev` branch. You can install this branch via `pip install git+https://github.com/stardist/stardist@dev` before we make a new proper release (no schedule for that yet). However, you might run into [installation issues](https://github.com/stardist/stardist#installation-1).",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/154#issuecomment-885661470:318,schedul,schedule,318,,https://github.com/stardist/stardist/issues/154#issuecomment-885661470,1,['schedul'],['schedule']
Energy Efficiency,"Hi Greg, . thanks a lot for this contribution - it already looks very promising! . > I evaluated NMS time over a wide number of polygon candidates. Here are the results (note the log_10 scale on the vertical axis):. Thats a nice speed up! The 6Mio candidates are for the whole stack, I assume? So using `predict_big` doesn't have any effect on the prediction time?. I'm slightly worried about the additional memory footprint that a full kdtree of all candidates might bring - did you try measuring the peak memory usage? Additionally I suspect there is quite some heavy differences even for different kdtree implementations e.g. there are `KDTree` and `BallTree` from `scikit-learn`. Here's a short test I did with random points in 3D and those different implementation ; ""ckdtree"" -> `scipy.spatial.ckdtree` ; ""kdtree"" -> `sklearn.neighbors.KDTree`; ""ball"" -> `sklearn.neighbors.BallTree`. <img width=""850"" alt=""Screenshot 2020-03-19 at 18 10 04"" src=""https://user-images.githubusercontent.com/11042162/77096353-85333200-6a0f-11ea-9f97-926c8bd22cd9.png"">; ; shows that i) there are quite some difference esp. for memory, and ii) the overall memory footprint is pretty high (e.g. 18GB for a mere 30k points for ckdtree). So having a similar plot as above but for memory consumption in your example would be nice and instructive. Additionally moving to `BallTree` might make things more efficient...",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-601317246:1270,consumption,consumption,1270,,https://github.com/stardist/stardist/pull/40#issuecomment-601317246,2,"['consumption', 'efficient']","['consumption', 'efficient']"
Energy Efficiency,"Hi Martin,. Here is the training file in HTML,; Please let me know if you need anything else. Thanks. PS : we adapted the code to work form arrays not TIF images. Anthony Lagnado, Ph.D.| Research Fellow | Department of Physiology and Biomedical Engineering | 507-538-1524; Mayo Clinic, 200 First Street SW, Rochester, MN 55905. From: Martin Weigert [mailto:notifications@github.com]; Sent: Monday, July 27, 2020 9:46 AM; To: mpicbg-csbd/stardist; Cc: Lagnado, Anthony B., Ph.D.; Mention; Subject: [EXTERNAL] Re: [mpicbg-csbd/stardist] Grid and model data informations and others (#66). Hi @Nal44<https://github.com/Nal44>,. It's hard to see what your problem is - could you simply share the training notebook with its outputs? (open your 2_training.ipynb and then File > Download as > HTML). â€”; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/mpicbg-csbd/stardist/issues/66#issuecomment-664440851>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/APP3D56PCOPD7VP7SGMASBTR5WHK7ANCNFSM4ODRGWBA>.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/66#issuecomment-664526462:110,adapt,adapted,110,,https://github.com/stardist/stardist/issues/66#issuecomment-664526462,1,['adapt'],['adapted']
Energy Efficiency,"Hi Martin,. I think you are right about scikit-learn kdtree outperforming scipy cKDTree. While working on another change to the package, I was led to trying the scikit-learn kdtree. For the same dataset considered above, the query was a few minutes faster than scipy cKDTree, but more importantly - due to the underlying data types used - the query _results_ consumed significantly less RAM. For both packages I've found that the tree structure itself takes a negligible amount of RAM - I'm not sure exactly how this is implemented under the hood, but I suppose each node in the tree could be as small as a single pointer to where in the array the points are split. If that's the case you really only need something like 3*log_2(N) references (3 axes and N points). However it's implemented, empirically it's quite small. The query results however can be quite large, in the abstract it's a list of lists. The outer list is of length N (for N points) and the ith inner list is of length equal to the number of points within radius distance of the ith point. So this could be on the order of ~1000 * N long ints - that's something like 25GB without considering any extra metadata overhead. Scipy represents this object as a Python list of Python lists of Python ints. Python ints contain a cumbersome amount of metadata, so I don't think they're a great data type here. Scikit-learn on the other hand represents this as a Numpy array with `dtype = 'O'`, that is an array of objects. The objects are 1D Numpy arrays themselves with `dtype = np.int64`. This seems to be overall more space efficient. I'd love to select `np.int32` instead, since that's more than adequate, but it does not seem to be possible without modifying the `Scikit-learn.neighbors.kdtree` code (maybe still worth doing in the long run). So - combining the small query speed up and the better space efficiency, I've added code to use scikit-learn kdtree in a new commit.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-605328480:1586,efficient,efficient,1586,,https://github.com/stardist/stardist/pull/40#issuecomment-605328480,1,['efficient'],['efficient']
Energy Efficiency,"Hi again - I need to revisit this discussion. I've now moved to my cluster and am trying to run on the full image:; `pred, det = model.predict_instances(image_norm, n_tiles=(2, 16, 8))`; The image is not huge: ~900x1600x300 voxels; but there are _many_ cells, something on the order of 10^5. Whichever part of predict_instances that has the tqdm progress bar finishes in a reasonable amount of time, about 30 minutes, but whatever is after is taking several hours. Due to some network connectivity issues I haven't been able to get through a complete run yet. I'm guessing the loop monitored by tqdm is model prediction over the tiles, and what comes after is NMS and ""polyhedron_to_label""? I guess NMS would scale with the number of voxels, but polyhedron_to_label would scale with the number of labels? Do you have any estimate for how long something like my dataset might take?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/36#issuecomment-594086237:582,monitor,monitored,582,,https://github.com/stardist/stardist/issues/36#issuecomment-594086237,1,['monitor'],['monitored']
Energy Efficiency,The `inner` test is simply a sphere intersection so it is negligible and I would have expected that the overall time would be reduced. Maybe all those threads are competing too much as you suspected. You could try running your script with `OMP_NUM_THREADS=8 python script.py` and see whether that helps.,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/277#issuecomment-2236558174:126,reduce,reduced,126,,https://github.com/stardist/stardist/issues/277#issuecomment-2236558174,1,['reduce'],['reduced']
Energy Efficiency,"nd that ` 2*context < block_size` in order not to discard everything, but why does `min_overlap` appears here? Would you have a link to a visual explanation how you define the different blocks? I think this could help me understand. After the context is discarded on all sides, blocks still have to overlap at least `min_overlap` pixels. As you say, a diagram would make this clear. Note that the `model.predict_instances_big` is relatively new and really meant for people who have huge images. It's not a mainstream function. > 5- all my 3D stacks don't have the same sizes, should I change the above parameters according to each sizes or one set should work for all?. The parameters can be the the same and really only depend on the CNN architecture (context) and the object sizes (min_overlap). However, the stack must obviously be bigger than the block size. > 6- Overall would you have some advise to improve my pipeline?. Like I said, try using `model.predict_instances` instead and save yourself the headache ;). > I'm working on a docker container of my own and installed the latest tensorflow version, I was getting this error when instantiating the StarDist3D`MemoryError: clEnqueueWriteBuffer failed: MEM_OBJECT_ALLOCATION_FAILURE` it was taking all my 12Go of GPU Memory (I have an RTX 2080 Ti). Yes, TensorFlow unfortunately grabs all the GPU memory by default. That's why [our training notebook](https://nbviewer.jupyter.org/github/mpicbg-csbd/stardist/blob/master/examples/3D/2_training.ipynb) calls `limit_gpu_memory` to reserve 20% of the GPU memory to be used for OpenCL. Note that 20% may not be enough, it really depends on the patch size and the amount of total GPU memory. > Adding this lines of code solved my problem (just wanted to let you know in case someone has the same problem in the future):. Allowing GPU memory growth is an alternative, but might be less efficient (not sure). As far as I recall, `limit_gpu_memory` also allows you to enable memory growth. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:5179,efficient,efficient,5179,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,1,['efficient'],['efficient']
Integrability," (for the NMS algorithm) or before (like a padding)?. No, this is just because the CNN prediction is less accurate at the image boundary. It is discarded after the NMS step. > 4- `Also, it must hold that: min_overlap + 2*context < block_size.` I would easily understand that ` 2*context < block_size` in order not to discard everything, but why does `min_overlap` appears here? Would you have a link to a visual explanation how you define the different blocks? I think this could help me understand. After the context is discarded on all sides, blocks still have to overlap at least `min_overlap` pixels. As you say, a diagram would make this clear. Note that the `model.predict_instances_big` is relatively new and really meant for people who have huge images. It's not a mainstream function. > 5- all my 3D stacks don't have the same sizes, should I change the above parameters according to each sizes or one set should work for all?. The parameters can be the the same and really only depend on the CNN architecture (context) and the object sizes (min_overlap). However, the stack must obviously be bigger than the block size. > 6- Overall would you have some advise to improve my pipeline?. Like I said, try using `model.predict_instances` instead and save yourself the headache ;). > I'm working on a docker container of my own and installed the latest tensorflow version, I was getting this error when instantiating the StarDist3D`MemoryError: clEnqueueWriteBuffer failed: MEM_OBJECT_ALLOCATION_FAILURE` it was taking all my 12Go of GPU Memory (I have an RTX 2080 Ti). Yes, TensorFlow unfortunately grabs all the GPU memory by default. That's why [our training notebook](https://nbviewer.jupyter.org/github/mpicbg-csbd/stardist/blob/master/examples/3D/2_training.ipynb) calls `limit_gpu_memory` to reserve 20% of the GPU memory to be used for OpenCL. Note that 20% may not be enough, it really depends on the patch size and the amount of total GPU memory. > Adding this lines of code solved my ",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:4012,depend,depend,4012,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,1,['depend'],['depend']
Integrability,"> Hello everyone, the fractional offsets issue should be corrected now in the latest DeepImageJ [release](https://github.com/deepimagej/deepimagej-plugin/releases/tag/2.1.11). I'm testing this release right now on Ubuntu 20.04 (without GPU acceleration). I still see the same behavior when adding a model (dialog quickly appears and disappears), but the model seems to be actually installed this time. It was quite confusing to me that there was no user feedback whether a model was successfully installed or not. Unfortunately, I get an error when I open ""DeepImageJ Run"", select my newly-installed model, and then click on ""Test model"". First, nothing happens but one CPU core is used 100%. After a couple of minutes, the Fiji Console pops up with the following error message:. <details>; <summary>Expand to show long error message</summary>. ```; [INFO] No TF library found in /home/uwe/Downloads/fiji-linux64/Fiji.app/lib/.; Exception in thread ""AWT-EventQueue-0"" java.lang.StackOverflowError; 	at java.util.regex.Pattern$Loop.match(Pattern.java:4767); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3779); 	at java.util.regex.Pattern$Branch.match(Pattern.java:4606); 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4660); 	at java.util.regex.Pattern$Loop.match(Pattern.java:4787); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3779); 	at java.util.regex.Pattern$Branch.match(Pattern.java:4606); 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4660); 	at java.util.regex.Pattern$Loop.match(Pattern.java:4787); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1042890741:770,message,message,770,,https://github.com/stardist/stardist/pull/171#issuecomment-1042890741,2,['message'],['message']
Integrability,"> I am not sure whether the bug with the error message still needs to be fixed, but the underlying problem was in my data. Yes, we should maybe change the error message :). Thanks for the feedback!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/80#issuecomment-672828787:47,message,message,47,,https://github.com/stardist/stardist/issues/80#issuecomment-672828787,2,['message'],['message']
Integrability,"> I can confirm that model testing and running on a different opened image works for me now. Great. thanks for checking!. > There's one issue I don't understand yet. Why did we bother to put the `stardist_postprocessing.ijm` macro into the exported model zip file if there's no way to run this as _Postprocessing_ from DeepImageJ?. We want to support this in deepImageJ but are currently a bit out of manpower to implement it. There's probably a relatively simple solution. But also with the given solution, the script is at least packaged with the model, so that it can be applied manually. ; We will work on better post-processing integration, and there is probably a simple solution, but we don't want to block releasing the model zoo for now to incorporate it. We can update the readme to explain this, but I would suggest to do this in a follow-up PR as this one is pretty large already and it's not critical to the functionality. > When I did run the postprocessing macro from the Script Editor (opened manually from the `Fiji.app/models/<model_name>` folder) after DeepImageJ prediction, it showed me an error `[ERROR] Could not find input image with name/title ""scores"".` in the Console. I then tried the exact same thing again, and it worked as intended without errors man_shrugging. @esgomezm @carlosuc3m do you have any idea how to fix this?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1047715559:633,integrat,integration,633,,https://github.com/stardist/stardist/pull/171#issuecomment-1047715559,1,['integrat'],['integration']
Integrability,"After many tries, best time I can get is 75 seconds (from 97 seconds in my very first message) with `OMP_NUM_THREADS=40`.; What remains concerning is that in this case, I get ; ```python; NMS: Suppressed polyhedra:; NMS: # inner: 191728 / 358813 (53.43 %); NMS: # kernel: 153518 / 358813 (42.78 %); NMS: # render: 6041 / 358813 (1.68 %); NMS: # total: 351287 / 358813 (97.90 %); NMS: # keeping 7526 / 358813 polyhedra (2.10 %); ```; but switching `gravity` off (still with `OMP_NUM_THREADS=40`), I get 85 seconds and almost no `inner` calls (as expected); ```python; NMS: # inner: 1683 / 358813 (0.47 %); NMS: # kernel: 343563 / 358813 (95.75 %); NMS: # render: 6041 / 358813 (1.68 %); NMS: # total: 351287 / 358813 (97.90 %); NMS: # keeping 7526 / 358813 polyhedra (2.10 %); ```; so the difference between `gravity` on and off is small. `inner` calls could simply be very long for my machine for whatever reasons...; I'll gladly take the 20% improvement in time though, thank you very much! :)",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/277#issuecomment-2236669913:86,message,message,86,,https://github.com/stardist/stardist/issues/277#issuecomment-2236669913,1,['message'],['message']
Integrability,"Hi ,. >Sorry for the late reply. no worries ðŸ˜Š. >You are using the scripts branch?. Actually, I'm not... I simply created a conda env with a yml file (see above) , specifying version `stardist==0.8.2` (not specifying a the script branch). I tried the `stardist-predict3d -h` and I saw it was there :; ```; usage: stardist-predict3d [-h] -i INPUT [INPUT ...] [-o OUTDIR]; [--outname OUTNAME [OUTNAME ...]] -m MODEL; [--axes AXES] [--n_tiles N_TILES N_TILES]; [--pnorm PNORM PNORM] [--prob_thresh PROB_THRESH]; [--nms_thresh NMS_THRESH] [-v]. Prediction script for a 3D stardist model, usage: stardist-predict -i; input.tif -m model_folder_or_pretrained_name -o output_folder. optional arguments:; -h, --help show this help message and exit; -i INPUT [INPUT ...], --input INPUT [INPUT ...]; input file (tiff) (default: None); -o OUTDIR, --outdir OUTDIR; output directory (default: .); --outname OUTNAME [OUTNAME ...]; output file name (tiff) (default: {img}.stardist.tif); -m MODEL, --model MODEL; model folder / pretrained model to use (default: None); --axes AXES axes to use for the input, e.g. 'XYC' (default: None); --n_tiles N_TILES N_TILES; number of tiles to use for prediction (default: None); --pnorm PNORM PNORM pmin/pmax to use for normalization (default: [3,; 99.8]); --prob_thresh PROB_THRESH; prob_thresh for model (if not given use model default); (default: None); --nms_thresh NMS_THRESH; nms_thresh for model (if not given use model default); (default: None); -v, --verbose; ```; but I can't make it work ðŸ˜‘",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/191#issuecomment-1141976465:721,message,message,721,,https://github.com/stardist/stardist/issues/191#issuecomment-1141976465,1,['message'],['message']
Integrability,"Hi @imand500 ,. Sorry for the late reply. . > Since the model is not saved as h5 and it's really dependent on the configuration you made. If a pretrained models is loaded; ```python ; model = StarDist2D.from_pretrained(""2D_versatile_he""); ```; it is actually downloaded and stored as a normal stardist model in your keras cache directory. You can find the location like so:; ```python; print(model.logdir); ```; So you could simply copy that folder somewhere, create a new model from that folder and then continue training with your own data. Hope that helps,. M",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/127#issuecomment-830683343:97,depend,dependent,97,,https://github.com/stardist/stardist/issues/127#issuecomment-830683343,1,['depend'],['dependent']
Integrability,"Hi @maweigert,. Right now I'm working on a different strategy from the software standpoint that will solve the extra time cost for using Max and the extra memory from having to store the larger lists of neighbors (potential suppression candidates). My first implementation in the PR was a good demonstration of kdtree, but definitely suboptimal w.r.t. what can be done with the Python C-API, which I'm learning on the fly so things have gone through various stages of kludge. The idea now is that I can call the sklearn kdtree object, and it's query_radius method, from within the C++ code itself - enabling a separate query for each polygon candidate just before the inner loop of NMS begins. That way, points which are ""suppressed early"" will never be queried against the tree - reducing total query time even if the distance threshold is large. The second benefit is that the code now only has to store one neighbor list at a time, thus reducing the total RAM used. This is definitely the best way to built it _algorithmically_ but I still don't have the Python C-API down perfectly so implementing this today has been tricky. I also implemented what I mentioned before - applying the probability threshold for each tile separately during prediction. For the whole image, it cut max RAM down from ~400GB to ~50GB, and that was including the neighbors list from kdtree. So, if I can get the tile based threshold combined with the embedded python approach to the kdtree, I think I can get the whole zebrafish image segmented in like 10-15 minutes for something like 30GB RAM. Of course, the tile based prob threshold breaks a bunch of stuff, so I'll leave it to you guys to decide if you want to move in that direction generally or to just have my commit somewhere accessible on the repo so if others run into RAM issues there's some path to cutting it down without changing so many internal dependencies between functions. This is all WIP but I'm hoping to finish it soon. Thanks,; Greg",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-606357837:1893,depend,dependencies,1893,,https://github.com/stardist/stardist/pull/40#issuecomment-606357837,1,['depend'],['dependencies']
Integrability,"Hi @maweigert,. for some context: we are currently setting up a user-friendly workflow for users to train stardist and we stumbled over this line.; Thanks to your explanation I understand what the `use_gpu` variable does now, but I still find the line. ```; # Use OpenCL-based computations for data generator during training (requires 'gputools'); use_gpu = False and gputools_available(); ```; a bit confusing. Specifically, why do you use `use_gpu = False and ...` which will always evaluate to `False` ? ; Wouldn't `use_gpu = gputools_available()` be sufficient?; I would assume that `gputools_available` returns `True` if the additional dependency is available and `False` otherwise. One more point: the variable name `use_gpu` could maybe be renamed to `use_gpu_for_data_processing` or so; because now it implies that `use_gpu = False` means the gpu is not used at all (which I know is not the case, but doesn't become quite clear from the notebook). Sorry for being rather nit-picky here, but if this confuses us it will likely be confusing for users as well.; In any case, we will probably adapt the notebook a bit for users and just make these changes there, but we were wondering why you were choosing to do it like this in the first case.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/75#issuecomment-661014557:641,depend,dependency,641,,https://github.com/stardist/stardist/issues/75#issuecomment-661014557,1,['depend'],['dependency']
Integrability,"Hi Uwe, . loading directly in a shell does not throw the error. It happens only when I use snakemake which is bizarre. This might be related to another error I get which is that the libiomp5.so is already initialised ( OMP: Error #15: Initializing libiomp5.so, but found libomp.so already initialized). I wonder whether due to this conflict the model is accessed simultaneously by two process leading to the error. . Thanks for looking into this, ; Marc . Dr. Marc Bickle ; Technology Development Studio ; Max Planck Institute of Molecular Cell Biology and Genetics ; Pfotenhauerstrasse 108 ; 01307 Dresden Germany . Phone: +49 (0)172 536 5517 . From: ""Uwe Schmidt"" <notifications@github.com> ; To: ""mpicbg-csbd/stardist"" <stardist@noreply.github.com> ; Cc: ""Marc Bickle"" <bickle@mpi-cbg.de>, ""Author"" <author@noreply.github.com> ; Sent: Sunday, October 25, 2020 11:36:57 PM ; Subject: Re: [mpicbg-csbd/stardist] loading 2D_versatile_fluo error (#93) . Hi, the error message indicates a problem with loading the weights from the HDF5 file. . First, to rule out Snakemake as an issue, did you try to simply load the pre-trained model in a Python shell? For example, does this throw the same error? ; $ python -c ""from stardist.models import StarDist2D; StarDist2D.from_pretrained('2D_versatile_fluo')"" . If that's the case, then I'd try to use a different/newer version of the HDF5 library h5py . . Best, ; Uwe . â€” ; You are receiving this because you authored the thread. ; Reply to this email directly, [ https://github.com/mpicbg-csbd/stardist/issues/93#issuecomment-716223889 | view it on GitHub ] , or [ https://github.com/notifications/unsubscribe-auth/AAU56KJ4GEUFX4BPHFYPRBLSMSSATANCNFSM4SZXDKIQ | unsubscribe ] .",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/93#issuecomment-716329806:967,message,message,967,,https://github.com/stardist/stardist/issues/93#issuecomment-716329806,1,['message'],['message']
Integrability,"Hi!. Thanks, I made sure now that all the stardist modules are the same versions. Unfortunately, tensorflow is still not configured properly on the computing cluster, crashing with errors that say:; ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; Aborted; ```. with while trying to do very simple print statements, so this is something I need to solve with our sysadmin. I am loading the trained module onto my local machine (OSX, Python3.6) and using it to predict on similar images. I can now load the cluster-trained (stardist 0.41) model onto my machine:; ```; model = StarDist3D(None, name='modelname',; basedir=path.join(trainingdir,'models')); labels, details = model.predict_instances(test_img[); ```. The kernel runs for a minute or so and then silently crashes without any error message, and then restarts silently (no warning messages on the bash shell that spyder is running in either). <img width=""804"" alt=""Screen Shot 2020-02-21 at 3 09 51 PM"" src=""https://user-images.githubusercontent.com/5126258/75041128-3cde2e00-54bc-11ea-9233-cd8212b14a3b.png"">. --. The training/labels are 3D images of clusters of cells. One mid-level slice looks like this, with 1Âµm slices, about 20Âµm:. <img width=""851"" alt=""Screen Shot 2020-02-21 at 3 02 38 PM"" src=""https://user-images.githubusercontent.com/5126258/75040633-4c10ac00-54bb-11ea-8f3a-f853a9fda51b.png"">. ---. On my local machine, I am running: tensorflow 1.15.0, Python 3.6.; On the cluster, I was running: tf 1.14.0, Python 3.6. Thank you!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/35#issuecomment-589670082:856,message,message,856,,https://github.com/stardist/stardist/issues/35#issuecomment-589670082,2,['message'],"['message', 'messages']"
Integrability,"Hi, . Thanks for using stardist! . We haven't yet implemented multi-gpu training. In theory it should not be too hard to incorporate it though (something along the lines of this [discussion](https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66458)). You could try it by simply checking out the code and wrapping the model accordingly at the respective place [here](https://github.com/mpicbg-csbd/stardist/blob/master/stardist/models/model3d.py#L301). Let me know how it goes!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/67#issuecomment-650382889:323,wrap,wrapping,323,,https://github.com/stardist/stardist/issues/67#issuecomment-650382889,1,['wrap'],['wrapping']
Integrability,"Hi, the error message indicates a problem with loading the weights from the HDF5 file. First, to rule out Snakemake as an issue, did you try to simply load the pre-trained model in a Python shell? For example, does this throw the same error?; ```; $ python -c ""from stardist.models import StarDist2D; StarDist2D.from_pretrained('2D_versatile_fluo')""; ```. If that's the case, then I'd try to use a different/newer version of the HDF5 library `h5py`. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/93#issuecomment-716223889:14,message,message,14,,https://github.com/stardist/stardist/issues/93#issuecomment-716223889,1,['message'],['message']
Integrability,"Thank you for your quick reply! . > I suggest you do the prediction in Python to get the label image, which you can then import in Fiji with the [3D Roi Manager](https://imagejdocu.tudor.lu/plugin/stacks/3d_roi_manager/start) (installation instructions [here](https://imagejdocu.tudor.lu/plugin/stacks/3d_ij_suite/start#installation)). This should work well, as long as you don't have (strongly) overlapping predictions or it's not important to you to get the complete shapes for all objects. This sounds really good. I do have quite overlapping nuclei though unfortunately. Apologies if this is a bit of a starter question but would I just save the label image as a tiff file and then open it in FIJI, and then segment it with the 3D ROI viewer? . > It's not that simple and even if it worked, this won't get you 3D ROIs in Fiji. Our [StarDist Fiji plugin](https://imagej.net/StarDist) currently only works in 2D because it's not that easy to support 3D... but we hope to get this done eventually. Ah I thought it wouldn't be simple. The error message I was getting was that the input tensor had to be 5 dimensional, and was wondering whether the mismatch was due to tensor organisation convention of NHWC not including a third spatial dimension. . The images you have from paintera look really beautiful, I think that is my aim here, but perhaps FIJI's 3d viewer of the 3d ROIs would get something similar?. Thanks again!!. Michael Schwimmer",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/37#issuecomment-597304169:1045,message,message,1045,,https://github.com/stardist/stardist/issues/37#issuecomment-597304169,1,['message'],['message']
Integrability,"nd that ` 2*context < block_size` in order not to discard everything, but why does `min_overlap` appears here? Would you have a link to a visual explanation how you define the different blocks? I think this could help me understand. After the context is discarded on all sides, blocks still have to overlap at least `min_overlap` pixels. As you say, a diagram would make this clear. Note that the `model.predict_instances_big` is relatively new and really meant for people who have huge images. It's not a mainstream function. > 5- all my 3D stacks don't have the same sizes, should I change the above parameters according to each sizes or one set should work for all?. The parameters can be the the same and really only depend on the CNN architecture (context) and the object sizes (min_overlap). However, the stack must obviously be bigger than the block size. > 6- Overall would you have some advise to improve my pipeline?. Like I said, try using `model.predict_instances` instead and save yourself the headache ;). > I'm working on a docker container of my own and installed the latest tensorflow version, I was getting this error when instantiating the StarDist3D`MemoryError: clEnqueueWriteBuffer failed: MEM_OBJECT_ALLOCATION_FAILURE` it was taking all my 12Go of GPU Memory (I have an RTX 2080 Ti). Yes, TensorFlow unfortunately grabs all the GPU memory by default. That's why [our training notebook](https://nbviewer.jupyter.org/github/mpicbg-csbd/stardist/blob/master/examples/3D/2_training.ipynb) calls `limit_gpu_memory` to reserve 20% of the GPU memory to be used for OpenCL. Note that 20% may not be enough, it really depends on the patch size and the amount of total GPU memory. > Adding this lines of code solved my problem (just wanted to let you know in case someone has the same problem in the future):. Allowing GPU memory growth is an alternative, but might be less efficient (not sure). As far as I recall, `limit_gpu_memory` also allows you to enable memory growth. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:4924,depend,depends,4924,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,1,['depend'],['depends']
Integrability,"t; else:; self.dtype = np.float; ; def __len__(self):; return len(self.x). def __getitem__(self, idx):; ; filename = self.x[idx]; if self.mode == ""X"":; return imread(filename) / 255; else:; return imread(filename); ```. I'm actually able to launch a training with batch size of 1 without specifying anisotropy and things seem to run ok (note that I had to add `dtype` and `ndim` to make things work and that I have some problems in my pipeline as explained bellow). However some part of my training pipeline are broken:. - when running this to compute anisotropy `extents = calculate_extents(Y)` I first get this warning; ```_asarray.py (83): Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray```; and after a few seconds of computation the code fails with this error message :; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-14-36ac39a79e87> in <module>; ----> 1 extents = calculate_extents(Y); 2 anisotropy = tuple(np.max(extents) / extents); 3 print('empirical anisotropy of labeled objects = %s' % str(anisotropy)). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/utils.py in calculate_extents(lbl, func); 165 n in (2,3) or _raise(ValueError(""label image should be 2- or 3-dimensional (or pass a list of these)"")); 166 ; --> 167 regs = regionprops(lbl); 168 if len(regs) == 0:; 169 return np.zeros(n). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/skimage/measure/_regionprops.py in regionprops(label_image, intensity_image, cache, coordinates); 883 regions = []; 884 ; --> 885 objects = ndi.find_objects(label_image); 886 for i, sl in enumerate(objects):; 887 if sl is None:. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:1514,message,message,1514,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['message'],['message']
Integrability,"ub.com/mpicbg-csbd/stardist/issues/36#issuecomment-594974089) I was able to run the job with a single cpu (# cores requested and maximum amount of RAM available are mixed on our cluster; each core comes with 15GB RAM). However the remaining runtime predicted by tqdm after a few iterations had accumulated was something like 36 hours - so I cancelled it and gave up on it. Uwe reminded me that with one core, I likely had a maximum of 2 threads, but I did not go back and try `predict_big` with more cores. Do you think `predict_big` with say 32 cores (64 threads) could finish in less than an hour? If so I will give it another try. For different kdtree implementations - I did look through [this article written by the scikit-learn kdtree and balltree developer](https://jakevdp.github.io/blog/2013/04/29/benchmarking-nearest-neighbor-searches-in-python/), but I didn't think there was enough differences in his graphs to justify introducing a new dependency (I don't think scikit-learn was a dependency before?) and I also preferred something with an underlying C++ implementation, as there are more big speed ups available if the kdtree data structure is available for query directly within the NMS algorithm. Unfortunately the only RAM measurements I have are the maximum and average usage (over the full run, prediction and NMS) for prediction on the full volumes both w/ and w/o the kdtree. The command in both cases was:. `pred, det = model.predict_instances(image_norm, verbose=True, n_tiles=(4, 8, 2))`; On a volume with 860x1676x301 voxels and 6.32e6 polygon candidates. w/o kdtree max: 434.439GB; w/ kdtree max: 430.482GB. w/o kdtree avg: 221.672GB; w/ kdtree avg: 297.733GB. I'm thinking that the kdtree and the query results for this many candidates took something like 70GB RAM. One thing that I haven't looked at in the code - it seems like the biggest expansion of data in the method is from raw data --> prediction; e.g. in my case I'm using 128 rays, so I need (129 * 2) more RAM re",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-601381585:1175,depend,dependency,1175,,https://github.com/stardist/stardist/pull/40#issuecomment-601381585,2,['depend'],['dependency']
Modifiability," args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1144, in run_setup; run_setup(setup_script, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 253, in run_setup; raise; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 166, in save_modules; saved_exc.resume(); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 141, in resume; six.reraise(type, exc, self._tb); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 154, in save_modules; yield saved; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 250, in run_setup; _execfile(setup_script, ns); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 45, in _execfile; exec(code, globals, locals); File ""/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/easy_install-d8fAD9/numpy-1.19.0/setup.py"", line 30, in <module>; re.MULTILINE | re.DOTALL).groups()[0]; RuntimeError: Python version >= 3.6 required.; ----------------------------------------; ERROR: Command errored out with exit status 1: python setup.py egg_info ",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:6052,sandbox,sandbox,6052,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['sandbox'],['sandbox']
Modifiability," compatibility issue error with h5py library version. I just continue with the installation and everything seems to work correctly-->. 8. Install the NEUBIAS academy Jupyter Notebook for testing stardist installation . `$ git clone https://github.com/maweigert/neubias_academy_stardist.git`. ##### Run stardist-gpu on the NVIDIA RTX 3080. 1. Start the *Jupyter Notebook* . . 2. Comment the following lines:. ```; $ #%tensorflow_version 1.x; ; $ #!pip install stardist; ```. . 3. The direct downloads didn't work for me, I just comment these lines out and downloaded the sample data before. ```; '''; download_and_extract_zip_file(; url = 'https://github.com/mpicbg-csbd/stardist/releases/download/0.1.0/dsb2018.zip',; targetdir = 'data',; verbose = 1,; ); '''; ```. 4. To avoid TF GPU out of memory issues I add the following code:. ```python; import tensorflow as tf; gpus = tf.config.experimental.list_physical_devices('GPU'); tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4092)]); ```. . Output:. ```; 2020-12-13 07:44:01.102879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties:; pciBusID: 0000:68:00.0 name: GeForce RTX 3080 computeCapability: 8.6; coreClock: 1.785GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s; 2020-12-13 07:44:01.103001: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll; 2020-12-13 07:44:01.103940: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll; 2020-12-13 07:44:01.103985: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll; 2020-12-13 07:44:01.104016: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll; 2020-12-13 07:44:01.104049: I tensorflow/stream",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:4907,config,config,4907,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,1,['config'],['config']
Modifiability," stardist. https://github.com/mpicbg-csbd/stardist. . <!--I got a compatibility issue error with h5py library version. I just continue with the installation and everything seems to work correctly-->. 8. Install the NEUBIAS academy Jupyter Notebook for testing stardist installation . `$ git clone https://github.com/maweigert/neubias_academy_stardist.git`. ##### Run stardist-gpu on the NVIDIA RTX 3080. 1. Start the *Jupyter Notebook* . . 2. Comment the following lines:. ```; $ #%tensorflow_version 1.x; ; $ #!pip install stardist; ```. . 3. The direct downloads didn't work for me, I just comment these lines out and downloaded the sample data before. ```; '''; download_and_extract_zip_file(; url = 'https://github.com/mpicbg-csbd/stardist/releases/download/0.1.0/dsb2018.zip',; targetdir = 'data',; verbose = 1,; ); '''; ```. 4. To avoid TF GPU out of memory issues I add the following code:. ```python; import tensorflow as tf; gpus = tf.config.experimental.list_physical_devices('GPU'); tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4092)]); ```. . Output:. ```; 2020-12-13 07:44:01.102879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties:; pciBusID: 0000:68:00.0 name: GeForce RTX 3080 computeCapability: 8.6; coreClock: 1.785GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s; 2020-12-13 07:44:01.103001: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll; 2020-12-13 07:44:01.103940: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll; 2020-12-13 07:44:01.103985: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll; 2020-12-13 07:44:01.104016: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic librar",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:4842,config,config,4842,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,1,['config'],['config']
Modifiability,".framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1158, in build_and_install; self.run_setup(setup_script, setup_base, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1144, in run_setup; run_setup(setup_script, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 253, in run_setup; raise; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 166, in save_modules; saved_exc.resume(); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 141, in resume; six.reraise(type, exc, self._tb); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 154, in save_modules; yield saved; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 250, in run_setup; _execfile(setup_script, ns); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 45, in _execfile; exec(code, globals, locals); File ""/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/easy_install-d8fAD9/numpy-1.19.0/setup.py"", line 30, in <module>; re.MULTILINE | re.DOTALL).groups()[0]; RuntimeEr",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:5900,sandbox,sandbox,5900,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['sandbox'],['sandbox']
Modifiability,"2. Using numba frees the gpu, BUT leaves it in a state that is not valid and pytorch then fails with the following error:; ```; RuntimeError: CUDA error: invalid argument ; Traceback (most recent call last): ; File ""run_all_workflows.py"", line 62, in <module> ; run_all_workflows() ; File ""run_all_workflows.py"", line 50, in run_all_workflows ; name, rt = run_instance_analysis2(config) ; File ""/g/kreshuk/pape/Work/covid/batchlib/antibodies/instance_analysis_workflow2.py"", line 123, in run_instance_analysis2 ; ignore_failed_outputs=config.ignore_failed_outputs) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/workflow.py"", line 104, in run_workflow ; raise e ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/workflow.py"", line 99, in run_workflow ; state = job(folder, **run_kwargs) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/base.py"", line 421, in __call__ ; super().__call__(folder, **kwargs) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/base.py"", line 210, in __call__ ; self.run(input_files, output_files, **kwargs) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/analysis/cell_level_analysis.py"", line 116, in run ; _save_all_stats(input_file, output_file) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/analysis/cell_level_analysis.py"", line 101, in save_all_stats ; sample = self.load_sample(in_file, device=device) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/analysis/cell_level_analysis.py"", line 52, in load_sample ; marker = torch.FloatTensor(marker.astype(np.float32)).to(device) ; RuntimeError: CUDA error: invalid argument; ```; (If I run the same task without tensorflow being involved it runs through without any issues). 3. I also tried running in a sub-process as suggested e.g. [here](https://stackoverflow.com/questions/39758094/clearing-tensorflow-gpu-memory-after-model-execution), but it results in the same issue as with 2.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/44#issuecomment-615880829:379,config,config,379,,https://github.com/stardist/stardist/issues/44#issuecomment-615880829,2,['config'],['config']
Modifiability,"> > Is the new Java library for deep learning ready yet? What is the name and where can I find it?; > ; > Yes, here it is: https://github.com/bioimage-io/model-runner-java. Thanks for the link! A few questions:; - This is only going to work for models in the bioimage.io format, right?; - Will this library handle pre- and post-processing?; - Is it going to support tile-based prediction, i.e. chopping the input image into tiles, running the model on each of them, and then re-assembling the individual results?. > > I really need to understand the current situation first... I don't even know how to export the model such that it will work with the new Java library.; > ; > The bioimageio.core library will export it directly in a compatible format (as the model I linked in the previous lines). In the upcoming days, we will update deepImageJ with the java model runner so all the TF2 models can also be deployed in Fiji. I can let you know when the plugin is ready if you want. I'm first interested in using `model-runner-java` to replace the TF1-only CSBDeep library in Fiji, in order to run TF2-based models in StarDist.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-1460949099:953,plugin,plugin,953,,https://github.com/stardist/stardist/issues/68#issuecomment-1460949099,1,['plugin'],['plugin']
Modifiability,"> All in all, it seems that for some reason StarDist is not able to create any candidates for the thinner stack. At the thickness of 4 z-layers it is expected that the largest cells do not neatly fit into the stack, but there should be plenty of smaller cells that I would think should still create candidates. So I would presume there has to be a reason for why none are created?. I don't understand. Are cells really only 4 pixels (or even less for ""smaller cells"") in Z? Was the model trained with such data?. > I did try that some months back by adding empty slices but it caused bloating of the labels and was not viable. But now that I think about it there probably is a better way for padding the image, such as duplicating the edge layers to provide continuity in intensities. You should pad the image with ""background"" values (whatever that means for your image) to not ""bloat"" the segmented objects sizes. > predicting instances with prob_thresh = 0.01 and nms_thresh = 0.4; > found 0 candidates. Something is clearly wrong here. There shouldn't be a need to use such a very small `prob_thresh` value, unless the model is badly trained or not suitable for the given image. Can you share a/some representative training images and also a/some problematic test images?. Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/133#issuecomment-832682530:137,layers,layers,137,,https://github.com/stardist/stardist/issues/133#issuecomment-832682530,2,['layers'],['layers']
Modifiability,"> Hello everyone, the fractional offsets issue should be corrected now in the latest DeepImageJ [release](https://github.com/deepimagej/deepimagej-plugin/releases/tag/2.1.11). I'm testing this release right now on Ubuntu 20.04 (without GPU acceleration). I still see the same behavior when adding a model (dialog quickly appears and disappears), but the model seems to be actually installed this time. It was quite confusing to me that there was no user feedback whether a model was successfully installed or not. Unfortunately, I get an error when I open ""DeepImageJ Run"", select my newly-installed model, and then click on ""Test model"". First, nothing happens but one CPU core is used 100%. After a couple of minutes, the Fiji Console pops up with the following error message:. <details>; <summary>Expand to show long error message</summary>. ```; [INFO] No TF library found in /home/uwe/Downloads/fiji-linux64/Fiji.app/lib/.; Exception in thread ""AWT-EventQueue-0"" java.lang.StackOverflowError; 	at java.util.regex.Pattern$Loop.match(Pattern.java:4767); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3779); 	at java.util.regex.Pattern$Branch.match(Pattern.java:4606); 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4660); 	at java.util.regex.Pattern$Loop.match(Pattern.java:4787); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3779); 	at java.util.regex.Pattern$Branch.match(Pattern.java:4606); 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4660); 	at java.util.regex.Pattern$Loop.match(Pattern.java:4787); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1042890741:147,plugin,plugin,147,,https://github.com/stardist/stardist/pull/171#issuecomment-1042890741,1,['plugin'],['plugin']
Modifiability,"> Hi @uschmidt83, So if I am using a custom config should I write the whole config used instead of just the n_rays but also kernel size etc?. I don't understand what you mean. > But it is still counter intuitive to me because you have the config.json file in the same folder as the model weights so for the prediction step it should know the hyperparameters I used for the training right?. All I'm saying is that supplying a new config to an already trained model is not a good idea or the solution to your problem. As you said, it _should_ work by loading the config from disk with passing `config=None` to the model constructor.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-659566701:44,config,config,44,,https://github.com/stardist/stardist/issues/68#issuecomment-659566701,6,['config'],['config']
Modifiability,"> How does one reload a model after training in python ?; > If it simply calling the model with the same name and basedir ?. Yes. If you set `config=None` in the constructor, the model (and weights) will be loaded from the name/basedir given:; ```python; model = StarDist2D(config=None, name= ..., basedir = ...); ```",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/95#issuecomment-716120226:142,config,config,142,,https://github.com/stardist/stardist/issues/95#issuecomment-716120226,2,['config'],['config']
Modifiability,"> However DeepImageJ should already be able to run Stardist. I have succesfully run it in my computer. Instead of using the ""Test model"" button, please try opening an image, then selecting the model and finally running the model pressing ""ok"". Ah, I didn't expect that DeepImageJ works like this. Wouldn't it be more intuitive to have separate menu items for model testing and running?. > the ""Test model"" button should be fixed now, feel free to test the new release with the funcionality fixed:; > https://github.com/deepimagej/deepimagej-plugin/releases/tag/2.1.12. I can confirm that model testing and running on a different opened image works for me now. > @uschmidt83 I think we can assume it works now in deepImageJ. Should we go ahead and merge it?. There's one issue I don't understand yet. Why did we bother to put the `stardist_postprocessing.ijm` macro into the exported model zip file if there's no way to run this as *Postprocessing* from DeepImageJ?. When I did run the postprocessing macro from the Script Editor (opened manually from the `Fiji.app/models/<model_name>` folder) after DeepImageJ prediction, it showed me an error `[ERROR] Could not find input image with name/title ""scores"".` in the Console. I then tried the exact same thing again, and it worked as intended without errors ðŸ¤·â€â™‚ï¸. > (Also there are still some failing tests, but this seems to be unrelated to the changes here.). Yes, those are unrelated. The bioimage.io tests are only active in Github action tests with `tensorflow<2` in the name, and those run through just fine.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1047694133:541,plugin,plugin,541,,https://github.com/stardist/stardist/pull/171#issuecomment-1047694133,1,['plugin'],['plugin']
Modifiability,"> However, I noticed the absence of a `checkpoint.ckpt` file (instead there is a `variables.index` file). Is there a way to generate this `.ckpt` file?. I have never needed a checkpoint myself, hence I don't know. However, you can try to export one yourself since you have full access to the Keras model via `model.keras_model`. Here's the general documentation for the [SavedModel format](https://www.tensorflow.org/guide/saved_model), and here is a guide specific to [Training checkpoints](https://www.tensorflow.org/guide/checkpoint). Hope that helps. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/61#issuecomment-643508305:82,variab,variables,82,,https://github.com/stardist/stardist/issues/61#issuecomment-643508305,1,['variab'],['variables']
Modifiability,"> I'm a complete beginner at Python but find the software really exciting, so have jumped in at the deep end!. Glad that you like it!. > My aim is to be able to export the label maps and visualise them in ImageJ or something similar, and to use the 3 dimensional ROIs to calculate values for nuclear volume and shape and other things. I suggest you do the prediction in Python to get the label image, which you can then import in Fiji with the [3D Roi Manager](https://imagejdocu.tudor.lu/plugin/stacks/3d_roi_manager/start) (installation instructions [here](https://imagejdocu.tudor.lu/plugin/stacks/3d_ij_suite/start#installation)). This should work well, as long as you don't have (strongly) overlapping predictions or it's not important to you to get the complete shapes for all objects. > I can see that you've made a notebook to export the ROIs for the 2D model prediction into FIJI which works great for me. I tried to adapt this for 3D by just having axes set to zyx and having 'dist' instead of 'coord' (in export_imagej_rois('img_rois.zip', details['dist'])). I have found that this doesn't really work, but not sure why. This function only works for 2D polygons and cannot be simply adapted for 3D.; ; > I also tried to export the demo 3d model into deepImageJ, but it runs into trouble when you try to apply it to an image. I can give more details, but as I am quite lost I'm not sure what is the relevant information. It's not that simple and even if it worked, this won't get you 3D ROIs in Fiji. Our [StarDist Fiji plugin](https://imagej.net/StarDist) currently only works in 2D because it's not that easy to support 3D... but we hope to get this done eventually. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/37#issuecomment-597239267:489,plugin,plugin,489,,https://github.com/stardist/stardist/issues/37#issuecomment-597239267,5,"['adapt', 'plugin']","['adapt', 'adapted', 'plugin']"
Modifiability,"> We actually found that we don't have GPU available for Tensorflow after our build. What do you mean? Are you referring to [this problem](https://github.com/NVIDIA/nvidia-docker/issues/1034)?. > we suspect this is because we used NVIDIA_DRIVER_VERSION=455 (not listed as supported in the README).; > Will the updated version of tensorflow work with this driver version?. The variable `NVIDIA_DRIVER_VERSION` is simply used to indicate the name of a package to be installed. If the `docker build` command completed successfully, then it was likely not the issue.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/108#issuecomment-754113033:376,variab,variable,376,,https://github.com/stardist/stardist/issues/108#issuecomment-754113033,1,['variab'],['variable']
Modifiability,But it is still counter intuitive to me because you have the config.json file in the same folder as the model weights so for the prediction step it should know the hyperparameters I used for the training right?,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-659548054:61,config,config,61,,https://github.com/stardist/stardist/issues/68#issuecomment-659548054,1,['config'],['config']
Modifiability,"Hi @esgomezm,. > We are actually trying to export the model using TF2.x (the idea is to test it with the new java library in Fiji + deepImageJ). Is the new Java library for deep learning ready yet? What is the name and where can I find it?. > Is it the export being forced inside StarDist to have a model compatible with TF1.15?. Yes, the model export in the CSBDeep and StarDist Python packages was always meant for use in Fiji, which has only ever supported TensorFlow 1.x. > I'm using the bioimageio library (version 0.5.8) with TF2.11 to export TF and Keras models, and everything works perfectly. Using the bioimageio library where to export the model? And ""everything works perfectly"" where?. > A potential solution (probably you have a better one), could be to avoid those imports if TF version > 2.3? Until version 2.3, the compatibility with CSBDeep & StarDist is ensured, but for later versions is not unless the plugins are updated. Also, my feeling is that the export fails with TF>2.3. I really need to understand the current situation first... I don't even know how to export the model such that it will work with the new Java library.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-1453562117:923,plugin,plugins,923,,https://github.com/stardist/stardist/issues/68#issuecomment-1453562117,1,['plugin'],['plugins']
Modifiability,"Hi @imand500 ,. Sorry for the late reply. . > Since the model is not saved as h5 and it's really dependent on the configuration you made. If a pretrained models is loaded; ```python ; model = StarDist2D.from_pretrained(""2D_versatile_he""); ```; it is actually downloaded and stored as a normal stardist model in your keras cache directory. You can find the location like so:; ```python; print(model.logdir); ```; So you could simply copy that folder somewhere, create a new model from that folder and then continue training with your own data. Hope that helps,. M",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/127#issuecomment-830683343:114,config,configuration,114,,https://github.com/stardist/stardist/issues/127#issuecomment-830683343,1,['config'],['configuration']
Modifiability,"Hi @maweigert,. for some context: we are currently setting up a user-friendly workflow for users to train stardist and we stumbled over this line.; Thanks to your explanation I understand what the `use_gpu` variable does now, but I still find the line. ```; # Use OpenCL-based computations for data generator during training (requires 'gputools'); use_gpu = False and gputools_available(); ```; a bit confusing. Specifically, why do you use `use_gpu = False and ...` which will always evaluate to `False` ? ; Wouldn't `use_gpu = gputools_available()` be sufficient?; I would assume that `gputools_available` returns `True` if the additional dependency is available and `False` otherwise. One more point: the variable name `use_gpu` could maybe be renamed to `use_gpu_for_data_processing` or so; because now it implies that `use_gpu = False` means the gpu is not used at all (which I know is not the case, but doesn't become quite clear from the notebook). Sorry for being rather nit-picky here, but if this confuses us it will likely be confusing for users as well.; In any case, we will probably adapt the notebook a bit for users and just make these changes there, but we were wondering why you were choosing to do it like this in the first case.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/75#issuecomment-661014557:207,variab,variable,207,,https://github.com/stardist/stardist/issues/75#issuecomment-661014557,3,"['adapt', 'variab']","['adapt', 'variable']"
Modifiability,"Hi @romainGuiet,. You should be able to get the original shape by simply linearly upscaling the probability and distance prediction by the grid parameter (i.e. two fold along each axis if `grid=(2,2)`). . Your planning to have the full pipeline (with polygon rendering) as part of a Fiji plugin? . Cheers,; M",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/28#issuecomment-557546384:288,plugin,plugin,288,,https://github.com/stardist/stardist/issues/28#issuecomment-557546384,1,['plugin'],['plugin']
Modifiability,"Hi @tboo,. > Just had a first climpse at StarDist 2D and it's performance is outstanding. Really happy with it :). Glad you like it! :). > I simply zipped the respective model folder I generated with the notebooks. You need to call `model.export_TF()`, which will create a file `TF_SavedModel.zip` in the respective model directory. You then have to point the Fiji plugin to `TF_SavedModel.zip` (make sure that tensorflow version of the exporting code and of Fiji are the same). Let us know if that works! . Cheers,; Martin",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/42#issuecomment-606856098:365,plugin,plugin,365,,https://github.com/stardist/stardist/issues/42#issuecomment-606856098,1,['plugin'],['plugin']
Modifiability,"Hi @uschmidt83 ,. Thanks for clarifying all that! ; Indeed I did not notice this flag was exclusively for the data generation for training step. Just the fact that there was a variable `use_gpu` set to `False` led me to wrong conclusions. Perhaps a more explicit naming can help to avoid confusions like this in the future, something like `use_gpu_for_data_gen`. > No, this is intentional. The and gputools_available() part acts as a [""guard""](https://en.wikipedia.org/wiki/Guard_(computer_science)) to always disable this flag when gputools is not installed. I see, so it is up to the user to actively change it if they want to use it for data generation. Thanks, I believe things are clear for me now.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/222#issuecomment-1445371166:176,variab,variable,176,,https://github.com/stardist/stardist/issues/222#issuecomment-1445371166,1,['variab'],['variable']
Modifiability,"Hi Martin,. Here is the training file in HTML,; Please let me know if you need anything else. Thanks. PS : we adapted the code to work form arrays not TIF images. Anthony Lagnado, Ph.D.| Research Fellow | Department of Physiology and Biomedical Engineering | 507-538-1524; Mayo Clinic, 200 First Street SW, Rochester, MN 55905. From: Martin Weigert [mailto:notifications@github.com]; Sent: Monday, July 27, 2020 9:46 AM; To: mpicbg-csbd/stardist; Cc: Lagnado, Anthony B., Ph.D.; Mention; Subject: [EXTERNAL] Re: [mpicbg-csbd/stardist] Grid and model data informations and others (#66). Hi @Nal44<https://github.com/Nal44>,. It's hard to see what your problem is - could you simply share the training notebook with its outputs? (open your 2_training.ipynb and then File > Download as > HTML). â€”; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/mpicbg-csbd/stardist/issues/66#issuecomment-664440851>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/APP3D56PCOPD7VP7SGMASBTR5WHK7ANCNFSM4ODRGWBA>.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/66#issuecomment-664526462:110,adapt,adapted,110,,https://github.com/stardist/stardist/issues/66#issuecomment-664526462,1,['adapt'],['adapted']
Modifiability,"Hi!. Thanks, I made sure now that all the stardist modules are the same versions. Unfortunately, tensorflow is still not configured properly on the computing cluster, crashing with errors that say:; ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; Aborted; ```. with while trying to do very simple print statements, so this is something I need to solve with our sysadmin. I am loading the trained module onto my local machine (OSX, Python3.6) and using it to predict on similar images. I can now load the cluster-trained (stardist 0.41) model onto my machine:; ```; model = StarDist3D(None, name='modelname',; basedir=path.join(trainingdir,'models')); labels, details = model.predict_instances(test_img[); ```. The kernel runs for a minute or so and then silently crashes without any error message, and then restarts silently (no warning messages on the bash shell that spyder is running in either). <img width=""804"" alt=""Screen Shot 2020-02-21 at 3 09 51 PM"" src=""https://user-images.githubusercontent.com/5126258/75041128-3cde2e00-54bc-11ea-9233-cd8212b14a3b.png"">. --. The training/labels are 3D images of clusters of cells. One mid-level slice looks like this, with 1Âµm slices, about 20Âµm:. <img width=""851"" alt=""Screen Shot 2020-02-21 at 3 02 38 PM"" src=""https://user-images.githubusercontent.com/5126258/75040633-4c10ac00-54bb-11ea-8f3a-f853a9fda51b.png"">. ---. On my local machine, I am running: tensorflow 1.15.0, Python 3.6.; On the cluster, I was running: tf 1.14.0, Python 3.6. Thank you!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/35#issuecomment-589670082:121,config,configured,121,,https://github.com/stardist/stardist/issues/35#issuecomment-589670082,1,['config'],['configured']
Modifiability,"Hi, we're glad that it works for you!. You can convert the Keras model to TensorFlow's [SavedModel format](https://www.tensorflow.org/guide/saved_model) like this:. ```python; model = StarDist2D(None, 'stardist', basedir='models'); model.export_TF(); ```. The created Zip file will contain the `.pb` file and the weights. Please also see the options of `model.export_TF()`, whose defaults are chosen such that an exported model will work in our Fiji plugin (e.g., it will upsample the output if any `grid` > 1).",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/61#issuecomment-643163493:450,plugin,plugin,450,,https://github.com/stardist/stardist/issues/61#issuecomment-643163493,1,['plugin'],['plugin']
Modifiability,"I currently have access to, but I can't get any predictions with thinner images. I performed an example run with a representative image (XY-cropped) and a duplicate image with one z-slice removed, i.e. from 5 to 4 z-slices. I have included my output below with verbose=True. All in all, it seems that for some reason StarDist is not able to create any candidates for the thinner stack. At the thickness of 4 z-layers it is expected that the largest cells do not neatly fit into the stack, but there should be plenty of smaller cells that I would think should still create candidates. So I would presume there has to be a reason for why none are created?. @uschmidt83 ; > I don't know how your data looks like, but have you tried padding the image such that the Z axis is big enough (as a trivial workaround)?. I did try that some months back by adding empty slices but it caused bloating of the labels and was not viable. But now that I think about it there probably is a better way for padding the image, such as duplicating the edge layers to provide continuity in intensities. **OUTPUT**:. Holidic_2018-12-19_180224-2; Model = DAPI20x ; Image dims = (4, 944, 2008). predicting instances with prob_thresh = 0.01 and nms_thresh = 0.4; found 0 candidates; non-maximum suppression...; Non Maximum Suppression (3D) ++++ ; NMS: n_polys = 0 ; NMS: n_rays = 128 ; NMS: n_faces = 252 ; NMS: thresh = 0.400 ; NMS: use_bbox = 1 ; NMS: using OpenMP with 16 thread(s); NMS: precompute volumes, bounding boxes, etc; precompute done; NMS: calculated anisotropy: -nan(ind) 	 -nan(ind) 	 -nan(ind) ; NMS: starting suppression loop; NMS: Function calls:; NMS: ~ bbox+out: 0; NMS: ~ inner: 0; NMS: ~ kernel: 0; NMS: ~ convex: 0; NMS: ~ render: 0; NMS: Excluded intersection:; NMS: + pretest: 0; NMS: + convex: 0; NMS: Function calls timing:; NMS: / kernel: 0.00 s (0.00 ms per call); NMS: / convex: 0.00 s (0.00 ms per call); NMS: / render: 0.00 s (0.00 ms per call); NMS: Suppressed polyhedra:; NMS: # inner: 0 / 0 ",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/133#issuecomment-831993257:1516,layers,layers,1516,,https://github.com/stardist/stardist/issues/133#issuecomment-831993257,1,['layers'],['layers']
Modifiability,I think the next steps are:; - also generate the config and macro for stardist postprocessing with the exporter so that the model can be run in deepimagej; - create the python functionality to run a stardist bioimageio model + stardist postprocessing (I think I would just add this to bioimageio.core for simplicity); - check that we can use the model in deepimagej and python; - release a new stardist version so that the functionality is available; - update the zero-cost notebook to use this functionality to export stardist bioimageio models. I will start working on the first point later.,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-973972757:49,config,config,49,,https://github.com/stardist/stardist/pull/171#issuecomment-973972757,1,['config'],['config']
Modifiability,"Morning @uschmidt83 !. > Is the new Java library for deep learning ready yet? What is the name and where can I find it?. Yes, here it is: https://github.com/bioimage-io/model-runner-java. > Using the bioimageio library where to export the model? And ""everything works perfectly"" where?. Here is an example where we are using it: https://github.com/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/U-Net_2D_Multilabel_ZeroCostDL4Mic.ipynb; And here you can find the model already: https://bioimage.io/#/?type=all&tags=deepimagej,bacillus-subtilis&id=10.5281%2Fzenodo.7261974. > I really need to understand the current situation first... I don't even know how to export the model such that it will work with the new Java library. The bioimageio.core library will export it directly in a compatible format (as the model I linked in the previous lines). In the upcoming days, we will update deepImageJ with the java model runner so all the TF2 models can also be deployed in Fiji. I can let you know when the plugin is ready if you want.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-1455823190:1012,plugin,plugin,1012,,https://github.com/stardist/stardist/issues/68#issuecomment-1455823190,1,['plugin'],['plugin']
Modifiability,"Regarding the deepImageJ issues: we will need to wait for feedback from @esgomezm or @carlosuc3m. > Is this something that has to be fixed in `bioimage.io.core`? (We typically use [save_tiff_imagej_compatible](https://github.com/CSBDeep/CSBDeep/blob/b0d2f5f344ebe65a9b4c3007f4567fe74268c813/csbdeep/io/__init__.py#L15-L46) from csbdeep to save tiff files such that ImageJ will read them correctly.). This is fixed already and included in the latest release (0.4.10). > > > As discussed before christmas, the idea would be that you apply this macro automatically in deepimagej if you recognise that you have a stardist model, e.g. by checking if the key `config.stardist` exists.; > ; > Is this already considered in the forthcoming DeepImageJ update?. We decided to not tackle the automatic running for now and just apply the macro by hand for the use-case.; We'll tackle automatically running it later.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1034212624:654,config,config,654,,https://github.com/stardist/stardist/pull/171#issuecomment-1034212624,1,['config'],['config']
Modifiability,"Thank you for your quick reply! . > I suggest you do the prediction in Python to get the label image, which you can then import in Fiji with the [3D Roi Manager](https://imagejdocu.tudor.lu/plugin/stacks/3d_roi_manager/start) (installation instructions [here](https://imagejdocu.tudor.lu/plugin/stacks/3d_ij_suite/start#installation)). This should work well, as long as you don't have (strongly) overlapping predictions or it's not important to you to get the complete shapes for all objects. This sounds really good. I do have quite overlapping nuclei though unfortunately. Apologies if this is a bit of a starter question but would I just save the label image as a tiff file and then open it in FIJI, and then segment it with the 3D ROI viewer? . > It's not that simple and even if it worked, this won't get you 3D ROIs in Fiji. Our [StarDist Fiji plugin](https://imagej.net/StarDist) currently only works in 2D because it's not that easy to support 3D... but we hope to get this done eventually. Ah I thought it wouldn't be simple. The error message I was getting was that the input tensor had to be 5 dimensional, and was wondering whether the mismatch was due to tensor organisation convention of NHWC not including a third spatial dimension. . The images you have from paintera look really beautiful, I think that is my aim here, but perhaps FIJI's 3d viewer of the 3d ROIs would get something similar?. Thanks again!!. Michael Schwimmer",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/37#issuecomment-597304169:190,plugin,plugin,190,,https://github.com/stardist/stardist/issues/37#issuecomment-597304169,3,['plugin'],['plugin']
Modifiability,"Thanks for the replies, @maweigert and @uschmidt83 . > For training, a majority of cells should be fully included in the annotated trainings stacks - I fear there is no real way around that. Sorry, I may not have been as clear as I intended. We have a working model that has been created with thicker images, and the problem occurs when applying the model on thin image-stacks. However, it seems like I am not able to reproduce the error (as in crashing the process) with the data I currently have access to, but I can't get any predictions with thinner images. I performed an example run with a representative image (XY-cropped) and a duplicate image with one z-slice removed, i.e. from 5 to 4 z-slices. I have included my output below with verbose=True. All in all, it seems that for some reason StarDist is not able to create any candidates for the thinner stack. At the thickness of 4 z-layers it is expected that the largest cells do not neatly fit into the stack, but there should be plenty of smaller cells that I would think should still create candidates. So I would presume there has to be a reason for why none are created?. @uschmidt83 ; > I don't know how your data looks like, but have you tried padding the image such that the Z axis is big enough (as a trivial workaround)?. I did try that some months back by adding empty slices but it caused bloating of the labels and was not viable. But now that I think about it there probably is a better way for padding the image, such as duplicating the edge layers to provide continuity in intensities. **OUTPUT**:. Holidic_2018-12-19_180224-2; Model = DAPI20x ; Image dims = (4, 944, 2008). predicting instances with prob_thresh = 0.01 and nms_thresh = 0.4; found 0 candidates; non-maximum suppression...; Non Maximum Suppression (3D) ++++ ; NMS: n_polys = 0 ; NMS: n_rays = 128 ; NMS: n_faces = 252 ; NMS: thresh = 0.400 ; NMS: use_bbox = 1 ; NMS: using OpenMP with 16 thread(s); NMS: precompute volumes, bounding boxes, etc; precompute done",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/133#issuecomment-831993257:891,layers,layers,891,,https://github.com/stardist/stardist/issues/133#issuecomment-831993257,1,['layers'],['layers']
Modifiability,"The code can be found in the ZCDL4M notebook, section 5.3: https://github.com/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/BioImage.io%20notebooks/StarDist_2D_ZeroCostDL4Mic_BioImageModelZoo_export.ipynb. This is the resume:; 1. Get the ""raw model"" by concatenating the outputs (we can skip this one, it would be ok on deepImageJ):. ```python; # Load the model; model = StarDist2D(None, name=QC_model_name, basedir=QC_model_path); thres, nms = model.thresholds. # Check minimum size: it is [8,8] for the 2D XY plane; depth = model.config.unet_n_depth; pool = model.config.unet_pool; MinimumSize = [pool[0]**(depth+1), pool[1]**(depth+1)]. # Concatenate model output to use pydeepimagej and the StarDist macro; input = model.keras_model.inputs[0]; single_output = Concatenate()([model.keras_model.output[0], model.keras_model.output[1]]); new_model = Model(input, single_output). dij_config = BioImageModelZooConfig(new_model, MinimumSize); .; .; .; # Add weights information; dij_config.add_weights_formats(new_model, 'TensorFlow', ; parent=""keras_hdf5"",; tf_version=tf.__version__); dij_config.add_weights_formats(new_model, 'KerasHDF5',; tf_version=tf.__version__); ```; 2. Use `pydeepimagej` library to export the TensorFlow and keras models: https://github.com/deepimagej/pydeepimagej/blob/5aaf0e71f9b04df591d5ca596f0af633a7e024f5/pydeepimagej/yaml/create_config.py#L251. **Concerning the upsampling and downsamplings:**; The model we export does the downsampling inside as deepImageJ deals with inputs-outputs of different sizes. For the concatenation, we can skip concatenating them. The only problem is that doing so, the model won't be compatible with CSBDeep. If we make the model compatible with CSBDeep, then the TF model would be different from the keras one, so here is maybe a potential solution (?): could we use the parent key in the specs @constantinpape @oeway to link the TF and keras models?? architecture is slightly different but the weights are the same.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-968831030:542,config,config,542,,https://github.com/stardist/stardist/pull/171#issuecomment-968831030,2,['config'],['config']
Modifiability,"e, keepdims=False,. ValueError: operands could not be broadcast together with shapes (80,330,500) (85,400,500); ```; I think maybe somehow my new class is considered as one large numpy array and not as a list of numpy arrays, is that possible? Any idea how to overcome this? . - if I switch to a batch size of 2, training does not work anymore and I end up with this error:; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-21-54ffb4970400> in <module>; 3 augmenter=augmenter_fn(ZOOM_RATIO, ZOOM_PROB),; 4 epochs=200, #100,; ----> 5 seed=42). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/models/model3d.py in train(self, X, Y, validation_data, augmenter, seed, epochs, steps_per_epoch, workers); 480 callbacks=self.callbacks, verbose=1,; 481 # set validation batchsize to training batchsize (only works in tf 2.x); --> 482 **(dict(validation_batch_size = self.config.train_batch_size) if _tf_version_at_least(""2.2.0"") else {})); 483 ; 484 self._training_finished(). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs); 64 def _method_wrapper(self, *args, **kwargs):; 65 if not self._in_multi_worker_mode(): # pylint: disable=protected-access; ---> 66 return method(self, *args, **kwargs); 67 ; 68 # Running inside `run_distribute_coordinator` already. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing); 813 workers=workers,; 814 use_multiprocessing=use_multiprocessing,; --> 815 model=self); 816 ; 817 # Containe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:4112,config,config,4112,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['config'],['config']
Modifiability,"k/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1144, in run_setup; run_setup(setup_script, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 253, in run_setup; raise; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 166, in save_modules; saved_exc.resume(); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 141, in resume; six.reraise(type, exc, self._tb); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 154, in save_modules; yield saved; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 250, in run_setup; _execfile(setup_script, ns); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 45, in _execfile; exec(code, globals, locals); File ""/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/easy_install-d8fAD9/numpy-1.19.0/setup.py"", line 30, in <module>; re.MULTILINE | re.DOTALL).groups()[0]; RuntimeError: Python version >= 3.6 required.; ----------------------------------------; ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.; ```. Thank you",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:6212,sandbox,sandbox,6212,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,4,['sandbox'],['sandbox']
Modifiability,"kers); 480 callbacks=self.callbacks, verbose=1,; 481 # set validation batchsize to training batchsize (only works in tf 2.x); --> 482 **(dict(validation_batch_size = self.config.train_batch_size) if _tf_version_at_least(""2.2.0"") else {})); 483 ; 484 self._training_finished(). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs); 64 def _method_wrapper(self, *args, **kwargs):; 65 if not self._in_multi_worker_mode(): # pylint: disable=protected-access; ---> 66 return method(self, *args, **kwargs); 67 ; 68 # Running inside `run_distribute_coordinator` already. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing); 813 workers=workers,; 814 use_multiprocessing=use_multiprocessing,; --> 815 model=self); 816 ; 817 # Container that configures and calls `tf.keras.Callback`s. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model); 1110 use_multiprocessing=use_multiprocessing,; 1111 distribution_strategy=ds_context.get_strategy(),; -> 1112 model=model); 1113 ; 1114 strategy = ds_context.get_strategy(). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs); 770 # Since we have to know the dtype of the python generator when we build th",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:5123,config,configures,5123,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['config'],['configures']
Modifiability,"loss: 0.0760 - dist_loss: 0.9107 - prob_kld: 0.0060 - dist_relevant_mae: 0.9104 - dist_relevant_mse: 2.7607 - val_loss: 0.3113 - val_prob_loss: 0.0758 - val_dist_loss: 1.1773 - val_prob_kld: 0.0079 - val_dist_relevant_mae: 1.1770 - val_dist_relevant_mse: 4.0487A: 1:05 - loss: 0.2553 - prob_loss: 0.0752 - dist_loss: 0.9005 - prob_kld: 0.0059 - dist_relevant_mae: 0.9001 - ; Epoch 399/400; 100/100 [==============================] - 356s 4s/step - loss: 0.2914 - prob_loss: 0.0890 - dist_loss: 1.0124 - prob_kld: 0.0068 - dist_relevant_mae: 1.0120 - dist_relevant_mse: 3.4916 - val_loss: 0.3241 - val_prob_loss: 0.0762 - val_dist_loss: 1.2396 - val_prob_kld: 0.0083 - val_dist_relevant_mae: 1.2392 - val_dist_relevant_mse: 4.3051; Epoch 400/400; 100/100 [==============================] - 359s 4s/step - loss: 0.2693 - prob_loss: 0.0773 - dist_loss: 0.9598 - prob_kld: 0.0063 - dist_relevant_mae: 0.9595 - dist_relevant_mse: 3.0423 - val_loss: 0.3100 - val_prob_loss: 0.0759 - val_dist_loss: 1.1705 - val_prob_kld: 0.0080 - val_dist_relevant_mae: 1.1702 - val_dist_relevant_mse: 3.98779096 - prob_kld: 0.0060 - dist_rele - ETA: 2:21 - loss: 0.2635 - prob_loss: 0.0749 - dist_loss: 0.9427 - prob; ```. . Any idea about this problem? I am not sure but it seems everything runs on the CPU after sometime. Maybe the memory limits I setup could be a problem:. ```; tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4092)]); ```. . 2. Overall the result looks good:. ```; DatasetMatching(criterion='iou', thresh=0.5, fp=91, tp=2253, fn=290, precision=0.9611774744027304, recall=0.8859614628391663, accuracy=0.8553530751708428, f1=0.9220380601596071, n_true=2543, n_pred=2344, mean_true_score=0.769742232723271, mean_matched_score=0.8688213483423339, panoptic_quality=0.8010863506508198, by_image=False); ```. ![image-20201213162031217](C:\Users\CarloBeretta\AppData\Roaming\Typora\typora-user-images\image-20201213162031217.png)",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:12187,config,config,12187,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,2,['config'],['config']
Modifiability,"m; dists = self.install_eggs(spec, download, tmpdir); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 890, in install_eggs; return self.build_and_install(setup_script, setup_base); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1158, in build_and_install; self.run_setup(setup_script, setup_base, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1144, in run_setup; run_setup(setup_script, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 253, in run_setup; raise; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 166, in save_modules; saved_exc.resume(); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 141, in resume; six.reraise(type, exc, self._tb); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 154, in save_modules; yield saved; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 250, in run_setup; _execfile(setup_script, ns); File ""/System/Library/Framework",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:5605,sandbox,sandbox,5605,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['sandbox'],['sandbox']
Modifiability,"s/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 679, in easy_install; return self.install_item(spec, dist.location, tmpdir, deps); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 705, in install_item; dists = self.install_eggs(spec, download, tmpdir); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 890, in install_eggs; return self.build_and_install(setup_script, setup_base); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1158, in build_and_install; self.run_setup(setup_script, setup_base, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1144, in run_setup; run_setup(setup_script, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 253, in run_setup; raise; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 166, in save_modules; saved_exc.resume(); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 141, in resume; six.reraise(type, exc, self._tb); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 154, in save_modules; yield saved; File ""/System/Library/Frameworks/Pytho",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:5314,sandbox,sandbox,5314,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['sandbox'],['sandbox']
Modifiability,"tly-gpu`. 7. Follow the installation instructions for stardist. https://github.com/mpicbg-csbd/stardist. . <!--I got a compatibility issue error with h5py library version. I just continue with the installation and everything seems to work correctly-->. 8. Install the NEUBIAS academy Jupyter Notebook for testing stardist installation . `$ git clone https://github.com/maweigert/neubias_academy_stardist.git`. ##### Run stardist-gpu on the NVIDIA RTX 3080. 1. Start the *Jupyter Notebook* . . 2. Comment the following lines:. ```; $ #%tensorflow_version 1.x; ; $ #!pip install stardist; ```. . 3. The direct downloads didn't work for me, I just comment these lines out and downloaded the sample data before. ```; '''; download_and_extract_zip_file(; url = 'https://github.com/mpicbg-csbd/stardist/releases/download/0.1.0/dsb2018.zip',; targetdir = 'data',; verbose = 1,; ); '''; ```. 4. To avoid TF GPU out of memory issues I add the following code:. ```python; import tensorflow as tf; gpus = tf.config.experimental.list_physical_devices('GPU'); tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4092)]); ```. . Output:. ```; 2020-12-13 07:44:01.102879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties:; pciBusID: 0000:68:00.0 name: GeForce RTX 3080 computeCapability: 8.6; coreClock: 1.785GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s; 2020-12-13 07:44:01.103001: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll; 2020-12-13 07:44:01.103940: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll; 2020-12-13 07:44:01.103985: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll; 2020-12-13 07:44:01.104016: I tensorflow/stream_executor/platform/default",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:4789,config,config,4789,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,1,['config'],['config']
Performance," ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-14-36ac39a79e87> in <module>; ----> 1 extents = calculate_extents(Y); 2 anisotropy = tuple(np.max(extents) / extents); 3 print('empirical anisotropy of labeled objects = %s' % str(anisotropy)). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/utils.py in calculate_extents(lbl, func); 165 n in (2,3) or _raise(ValueError(""label image should be 2- or 3-dimensional (or pass a list of these)"")); 166 ; --> 167 regs = regionprops(lbl); 168 if len(regs) == 0:; 169 return np.zeros(n). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/skimage/measure/_regionprops.py in regionprops(label_image, intensity_image, cache, coordinates); 883 regions = []; 884 ; --> 885 objects = ndi.find_objects(label_image); 886 for i, sl in enumerate(objects):; 887 if sl is None:. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/scipy/ndimage/measurements.py in find_objects(input, max_label); 301 ; 302 if max_label < 1:; --> 303 max_label = input.max(); 304 ; 305 return _nd_image.find_objects(input, max_label). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/numpy/core/_methods.py in _amax(a, axis, out, keepdims, initial, where); 37 def _amax(a, axis=None, out=None, keepdims=False,; 38 initial=_NoValue, where=True):; ---> 39 return umr_maximum(a, axis, None, out, keepdims, initial, where); 40 ; 41 def _amin(a, axis=None, out=None, keepdims=False,. ValueError: operands could not be broadcast together with shapes (80,330,500) (85,400,500); ```; I think maybe somehow my new class is considered as one large numpy array and not as a list of numpy arrays, is that possible? Any idea how to overcome this? . - if I switch to a batch size of 2, training does not work anymore and I end up with this error:; ```; -----------------------------",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:2503,cache,cache,2503,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['cache'],['cache']
Performance," kB); Collecting backports.tempfile; python_version < ""3.4""; Using cached backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB); Requirement already satisfied: six in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.12.0); Collecting h5py; Using cached h5py-2.10.0-cp27-cp27m-macosx_10_6_intel.whl (2.9 MB); Requirement already satisfied: matplotlib in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.3.1); Collecting keras<2.4,>=2.1.2; Using cached Keras-2.3.1-py2.py3-none-any.whl (377 kB); Requirement already satisfied: scipy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (0.13.0b1); Collecting tqdm; Using cached tqdm-4.47.0-py2.py3-none-any.whl (66 kB); Collecting pathlib2; python_version < ""3""; Using cached pathlib2-2.3.5-py2.py3-none-any.whl (18 kB); Collecting imagecodecs-lite<=2020; python_version < ""3.6""; Using cached imagecodecs-lite-2019.12.3.tar.gz (1.1 MB); ERROR: Command errored out with exit status 1:; command: /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py'""'""'; __file__='""'""'/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-pip-egg-info-I2_dBJ; cwd: /private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/; Complete output (51 lines):; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/private/var/folders/lz/h4nxm8jd64qdhlr57n1y",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:1915,cache,cached,1915,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['cache'],['cached']
Performance," python3 --version; Python 3.8.4. so im not really sure what to do now. ```; Defaulting to user installation because normal site-packages is not writeable; Collecting stardist; Using cached stardist-0.1.0.tar.gz (46 kB); Collecting csbdeep; Using cached csbdeep-0.6.0-py2.py3-none-any.whl (67 kB); Collecting scikit-image; Using cached scikit_image-0.14.5-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (29.3 MB); Requirement already satisfied: numpy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.8.0rc1); Collecting tifffile; Using cached tifffile-2019.7.26.2-py2.py3-none-any.whl (131 kB); Collecting backports.tempfile; python_version < ""3.4""; Using cached backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB); Requirement already satisfied: six in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.12.0); Collecting h5py; Using cached h5py-2.10.0-cp27-cp27m-macosx_10_6_intel.whl (2.9 MB); Requirement already satisfied: matplotlib in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.3.1); Collecting keras<2.4,>=2.1.2; Using cached Keras-2.3.1-py2.py3-none-any.whl (377 kB); Requirement already satisfied: scipy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (0.13.0b1); Collecting tqdm; Using cached tqdm-4.47.0-py2.py3-none-any.whl (66 kB); Collecting pathlib2; python_version < ""3""; Using cached pathlib2-2.3.5-py2.py3-none-any.whl (18 kB); Collecting imagecodecs-lite<=2020; python_version < ""3.6""; Using cached imagecodecs-lite-2019.12.3.tar.gz (1.1 MB); ERROR: Command errored out with exit status 1:; command: /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/lz/h4nxm8jd64qdh",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:1223,cache,cached,1223,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['cache'],['cached']
Performance,"/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/numpy/core/_methods.py in _amax(a, axis, out, keepdims, initial, where); 37 def _amax(a, axis=None, out=None, keepdims=False,; 38 initial=_NoValue, where=True):; ---> 39 return umr_maximum(a, axis, None, out, keepdims, initial, where); 40 ; 41 def _amin(a, axis=None, out=None, keepdims=False,. ValueError: operands could not be broadcast together with shapes (80,330,500) (85,400,500); ```; I think maybe somehow my new class is considered as one large numpy array and not as a list of numpy arrays, is that possible? Any idea how to overcome this? . - if I switch to a batch size of 2, training does not work anymore and I end up with this error:; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-21-54ffb4970400> in <module>; 3 augmenter=augmenter_fn(ZOOM_RATIO, ZOOM_PROB),; 4 epochs=200, #100,; ----> 5 seed=42). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/models/model3d.py in train(self, X, Y, validation_data, augmenter, seed, epochs, steps_per_epoch, workers); 480 callbacks=self.callbacks, verbose=1,; 481 # set validation batchsize to training batchsize (only works in tf 2.x); --> 482 **(dict(validation_batch_size = self.config.train_batch_size) if _tf_version_at_least(""2.2.0"") else {})); 483 ; 484 self._training_finished(). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs); 64 def _method_wrapper(self, *args, **kwargs):; 65 if not self._in_multi_worker_mode(): # pylint: disable=protected-access; ---> 66 return method(self, *args, **kwargs); 67 ; 68 # Running inside `run_distribute_coordinator` already. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, ver",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:3759,cache,cache,3759,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['cache'],['cache']
Performance,"0.6.0-py2.py3-none-any.whl (67 kB); Collecting scikit-image; Using cached scikit_image-0.14.5-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (29.3 MB); Requirement already satisfied: numpy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.8.0rc1); Collecting tifffile; Using cached tifffile-2019.7.26.2-py2.py3-none-any.whl (131 kB); Collecting backports.tempfile; python_version < ""3.4""; Using cached backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB); Requirement already satisfied: six in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.12.0); Collecting h5py; Using cached h5py-2.10.0-cp27-cp27m-macosx_10_6_intel.whl (2.9 MB); Requirement already satisfied: matplotlib in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.3.1); Collecting keras<2.4,>=2.1.2; Using cached Keras-2.3.1-py2.py3-none-any.whl (377 kB); Requirement already satisfied: scipy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (0.13.0b1); Collecting tqdm; Using cached tqdm-4.47.0-py2.py3-none-any.whl (66 kB); Collecting pathlib2; python_version < ""3""; Using cached pathlib2-2.3.5-py2.py3-none-any.whl (18 kB); Collecting imagecodecs-lite<=2020; python_version < ""3.6""; Using cached imagecodecs-lite-2019.12.3.tar.gz (1.1 MB); ERROR: Command errored out with exit status 1:; command: /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py'""'""'; __file__='""'""'/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().rep",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:1475,cache,cached,1475,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['cache'],['cached']
Performance,"0400> in <module>; 3 augmenter=augmenter_fn(ZOOM_RATIO, ZOOM_PROB),; 4 epochs=200, #100,; ----> 5 seed=42). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/models/model3d.py in train(self, X, Y, validation_data, augmenter, seed, epochs, steps_per_epoch, workers); 480 callbacks=self.callbacks, verbose=1,; 481 # set validation batchsize to training batchsize (only works in tf 2.x); --> 482 **(dict(validation_batch_size = self.config.train_batch_size) if _tf_version_at_least(""2.2.0"") else {})); 483 ; 484 self._training_finished(). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs); 64 def _method_wrapper(self, *args, **kwargs):; 65 if not self._in_multi_worker_mode(): # pylint: disable=protected-access; ---> 66 return method(self, *args, **kwargs); 67 ; 68 # Running inside `run_distribute_coordinator` already. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing); 813 workers=workers,; 814 use_multiprocessing=use_multiprocessing,; --> 815 model=self); 816 ; 817 # Container that configures and calls `tf.keras.Callback`s. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model); 1110 use_multiprocessing=use_multiprocessing,; 1111 distribution_strategy=ds_context.get_strategy(),; -> 1112 model=model); 1113 ; 1114 strategy = ds_context.get_strategy(). /work/.cac",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:4623,cache,cache,4623,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['cache'],['cache']
Performance,"> * play with `n_tiles` : should I just set this as low as possible as long as it fits in my GPU or should I do something smarter?. Yes, essentially. > * try to make bigger train_patch_size : I'm a bit uncomfortable setting batch_size lower than 4 (or even 8) to be honest, do you perform gradient accumulation under the hood? Maybe I'm just freaking out for nothing but don't you feel that a batch_size of 2 is a bit too small to get a nice gradient descent?. Keep in mind that every pixel (that belongs to an object) contributes a gradient signal. We often trained with a batch size of 1 and didn't see a problem. > My problem is that some cells are close to each other, so my initial labels are not very good at differientiating adjacent cells (which creates big objects that don't look like cells at all) and same goes with the resulting UNet segmentation, that's why I decided to switch to StarDist3D.; > First results from StarDist seem already much better at creating nice round cells, but in order to train StarDist I simply separated my binary mask using connex component : so my ground truth for StarDist never really shows adjacent cells (as they are merged into one same label). I don't see how you can overcome this problem without fixing the ground truth (at least in part). What we have seen is that StarDist can potentially be trained with labelling errors â€“ as long as these are not biased, which they seem to be in your case (i.e. touching nuclei are always incorrectly merged). > Because of this I don't really trust the automated nms_thresh search function (labels are wrong so it's going to be hard for the search). (As I said above, I wouldn't trust the results in general.); Note that the automatic threshold finding is in some sense just for convenience to set good thresholds that work well on average. Depending on the application, one would, e.g., increase the probability threshold to prefer more accurate predictions (fewer false positives) at the expense of missing some ",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-692842138:281,perform,perform,281,,https://github.com/stardist/stardist/issues/87#issuecomment-692842138,1,['perform'],['perform']
Performance,"> Can you give me some numbers to put this statement in perspective?. To give you some context:; We run stardist prediction on a DAPI channel to segment nuclei for a high throughput screening setup.; The individual images are approximately 1000x1000 and we have approximately a 1000 images per experiment (= 1 plate). . Running prediction / segmentation with stardist takes ~ 40 minutes for 1 experiment and ; it's clearly cpu bound (judging from the GPU utilization which I think is maybe between 5 and 10 percent on average). (I just call `predict_instances` for now, so I don't really know what the ratio between time spent prediction and time spent on NMS is). We also run two other processing steps: predicting foreground/background and boundaries with a pytorch network (on a different channel) and watersheds (using nuclei as seeds and boundary predictions as height map). Both of these steps can be done in under 5 - 10 minutes each, so stardist is the clear bottleneck right now. (For the pytorch prediction I stack images across the batch axis and the watershed can just be parallelized over multiple CPUs, so this comparison is not really fair!). I can run some experiments next week to really see how long prediction vs NMS takes.; What are the functions I have to call on the model if I want to do this in two separate steps?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/39#issuecomment-612622104:171,throughput,throughput,171,,https://github.com/stardist/stardist/issues/39#issuecomment-612622104,2,"['bottleneck', 'throughput']","['bottleneck', 'throughput']"
Performance,"> Hi @uschmidt83, So if I am using a custom config should I write the whole config used instead of just the n_rays but also kernel size etc?. I don't understand what you mean. > But it is still counter intuitive to me because you have the config.json file in the same folder as the model weights so for the prediction step it should know the hyperparameters I used for the training right?. All I'm saying is that supplying a new config to an already trained model is not a good idea or the solution to your problem. As you said, it _should_ work by loading the config from disk with passing `config=None` to the model constructor.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-659566701:549,load,loading,549,,https://github.com/stardist/stardist/issues/68#issuecomment-659566701,1,['load'],['loading']
Performance,"> How does one reload a model after training in python ?; > If it simply calling the model with the same name and basedir ?. Yes. If you set `config=None` in the constructor, the model (and weights) will be loaded from the name/basedir given:; ```python; model = StarDist2D(config=None, name= ..., basedir = ...); ```",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/95#issuecomment-716120226:207,load,loaded,207,,https://github.com/stardist/stardist/issues/95#issuecomment-716120226,1,['load'],['loaded']
Performance,"> I was wondering if there is an easy way to train the network using an **ImageDataGenerator**. You could use a `Sequence` to do that, e.g. see https://github.com/stardist/stardist/issues/107. > I would like to perform transfer learning using your model **_'2D_versatile_fluo'_**. Any idea how I can do this?. This is how you can duplicate a pretrained model and prepare it for transfer learning:. ```python; # imports; from stardist.models import StarDist2D, keras_import; import shutil; from pathlib import Path; keras = keras_import(). # name of your model; my_name = '2D_versatile_fluo_FINETUNED'. # load pretrained model and make a copy to local folder; model_pretrained = StarDist2D.from_pretrained('2D_versatile_fluo'); if not Path(my_name).exists():; shutil.copytree(model_pretrained.logdir, my_name). # load your duplicate of the pretrained model; my_model = StarDist2D(None, my_name). # change optimizer, especially to use smaller learning rate for finetuning; # (e.g. 1e-5, you have to try what works best); my_model.prepare_for_training(keras.optimizers.Adam(1e-5)). # finetune model with your data...; # my_model.train(...); ```",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/172#issuecomment-964010767:211,perform,perform,211,,https://github.com/stardist/stardist/issues/172#issuecomment-964010767,5,"['load', 'optimiz', 'perform']","['load', 'optimizer', 'optimizers', 'perform']"
Performance,"Hello,. I'm facing the same issue, all my data can not fit in RAM (I have 3D images and total training set > > 100Go). I've been trying `keras.utils.Sequence` as suggested by @maweigert, my idea is to simply replace a list of numpy arrays with a Sequence class that loads images on the fly. ```; from tensorflow.keras.utils import Sequence; from tifffile import imread. class DataLoader(Sequence):; ; def __init__(self, path_list, mode):; self.x, self.mode = path_list, mode; ; #retro compatibility with numpy; self.ndim=3; if mode == ""Y"":; self.dtype = np.int; else:; self.dtype = np.float; ; def __len__(self):; return len(self.x). def __getitem__(self, idx):; ; filename = self.x[idx]; if self.mode == ""X"":; return imread(filename) / 255; else:; return imread(filename); ```. I'm actually able to launch a training with batch size of 1 without specifying anisotropy and things seem to run ok (note that I had to add `dtype` and `ndim` to make things work and that I have some problems in my pipeline as explained bellow). However some part of my training pipeline are broken:. - when running this to compute anisotropy `extents = calculate_extents(Y)` I first get this warning; ```_asarray.py (83): Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray```; and after a few seconds of computation the code fails with this error message :; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-14-36ac39a79e87> in <module>; ----> 1 extents = calculate_extents(Y); 2 anisotropy = tuple(np.max(extents) / extents); 3 print('empirical anisotropy of labeled objects = %s' % str(anisotropy)). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/utils.py in calculate_extents(lbl, func); 165 n in ",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:266,load,loads,266,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['load'],['loads']
Performance,"Hello,; Yes, so i install homebrew then i installed gcc then i ran the command CC=gcc-9 CXX=g++-9 pip install stardist. Then i get this error below. I checked the version of python i have on the terminal and it is; % python3 --version; Python 3.8.4. so im not really sure what to do now. ```; Defaulting to user installation because normal site-packages is not writeable; Collecting stardist; Using cached stardist-0.1.0.tar.gz (46 kB); Collecting csbdeep; Using cached csbdeep-0.6.0-py2.py3-none-any.whl (67 kB); Collecting scikit-image; Using cached scikit_image-0.14.5-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (29.3 MB); Requirement already satisfied: numpy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.8.0rc1); Collecting tifffile; Using cached tifffile-2019.7.26.2-py2.py3-none-any.whl (131 kB); Collecting backports.tempfile; python_version < ""3.4""; Using cached backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB); Requirement already satisfied: six in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.12.0); Collecting h5py; Using cached h5py-2.10.0-cp27-cp27m-macosx_10_6_intel.whl (2.9 MB); Requirement already satisfied: matplotlib in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.3.1); Collecting keras<2.4,>=2.1.2; Using cached Keras-2.3.1-py2.py3-none-any.whl (377 kB); Requirement already satisfied: scipy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (0.13.0b1); Collecting tqdm; Using cached tqdm-4.47.0-py2.py3-none-any.whl (66 kB); Collecting pathlib2; python_version < ""3""; Using cached pathlib2-2.3.5-py2.py3-none-any.whl (18 kB); Collecting imagecodecs-lite<=2020; python_version < ""3.6""; Using cached imagecodecs-lite-2019.12.3.tar.gz (1.1 MB); ERROR: Command errored out with exi",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:399,cache,cached,399,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,5,['cache'],['cached']
Performance,"Hi @ajinkya-kulkarni, I agree that `tqdm.auto` loads a nicer progress bar in Jupyter notebooks, but it also has the disadvantage that the progress bar isn't preserved in the notebook output. I.e. one cannot see how many iterations / long it took when the notebook isn't ""live"" anymore. Or has this recently changed?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/237#issuecomment-1637921675:47,load,loads,47,,https://github.com/stardist/stardist/pull/237#issuecomment-1637921675,1,['load'],['loads']
Performance,"Hi @esgomezm, I don't think it's possible to do this is as a simple post-processing script.; We need a separate Java library and ideally multi-threading to make this fast. PS: I don't think the pre-processing script for StarDist in DeepImageJ is correct, at least when you use it with our pre-trained model. (We do a percentile-based normalization and don't clip values to the 0..1 range.). Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/29#issuecomment-561167747:137,multi-thread,multi-threading,137,,https://github.com/stardist/stardist/issues/29#issuecomment-561167747,1,['multi-thread'],['multi-threading']
Performance,"Hi @imand500 ,. Sorry for the late reply. . > Since the model is not saved as h5 and it's really dependent on the configuration you made. If a pretrained models is loaded; ```python ; model = StarDist2D.from_pretrained(""2D_versatile_he""); ```; it is actually downloaded and stored as a normal stardist model in your keras cache directory. You can find the location like so:; ```python; print(model.logdir); ```; So you could simply copy that folder somewhere, create a new model from that folder and then continue training with your own data. Hope that helps,. M",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/127#issuecomment-830683343:164,load,loaded,164,,https://github.com/stardist/stardist/issues/127#issuecomment-830683343,2,"['cache', 'load']","['cache', 'loaded']"
Performance,"Hi @tboo,. > Just had a first climpse at StarDist 2D and it's performance is outstanding. Really happy with it :). Glad you like it! :). > I simply zipped the respective model folder I generated with the notebooks. You need to call `model.export_TF()`, which will create a file `TF_SavedModel.zip` in the respective model directory. You then have to point the Fiji plugin to `TF_SavedModel.zip` (make sure that tensorflow version of the exporting code and of Fiji are the same). Let us know if that works! . Cheers,; Martin",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/42#issuecomment-606856098:62,perform,performance,62,,https://github.com/stardist/stardist/issues/42#issuecomment-606856098,1,['perform'],['performance']
Performance,"Hi Michael,. > 1. My data (example data provided here: https://drive.google.com/drive/folders/17q11-hAJjCs72YFbsVKoGve3e5nzXyJf?usp=sharing) appears to be the wrong shape. Any ideas for a simple fix?. I can reproduce that one of your images is opened with seemingly wrong shape in Python. I don't know why that happens, but I've found that simply re-saving the image as a Tiff file from Fiji solves the problem, i.e. it then loads correctly in Python. > 2. I tried running prediction anyway, and find that I get an out of memory error.; >; > Is this because my tensorflow/CUDA is not communicating with the GPU?. No, this is also related to the wrong image shape and should go away.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/62#issuecomment-644116825:425,load,loads,425,,https://github.com/stardist/stardist/issues/62#issuecomment-644116825,1,['load'],['loads']
Performance,"Hi Uwe, . loading directly in a shell does not throw the error. It happens only when I use snakemake which is bizarre. This might be related to another error I get which is that the libiomp5.so is already initialised ( OMP: Error #15: Initializing libiomp5.so, but found libomp.so already initialized). I wonder whether due to this conflict the model is accessed simultaneously by two process leading to the error. . Thanks for looking into this, ; Marc . Dr. Marc Bickle ; Technology Development Studio ; Max Planck Institute of Molecular Cell Biology and Genetics ; Pfotenhauerstrasse 108 ; 01307 Dresden Germany . Phone: +49 (0)172 536 5517 . From: ""Uwe Schmidt"" <notifications@github.com> ; To: ""mpicbg-csbd/stardist"" <stardist@noreply.github.com> ; Cc: ""Marc Bickle"" <bickle@mpi-cbg.de>, ""Author"" <author@noreply.github.com> ; Sent: Sunday, October 25, 2020 11:36:57 PM ; Subject: Re: [mpicbg-csbd/stardist] loading 2D_versatile_fluo error (#93) . Hi, the error message indicates a problem with loading the weights from the HDF5 file. . First, to rule out Snakemake as an issue, did you try to simply load the pre-trained model in a Python shell? For example, does this throw the same error? ; $ python -c ""from stardist.models import StarDist2D; StarDist2D.from_pretrained('2D_versatile_fluo')"" . If that's the case, then I'd try to use a different/newer version of the HDF5 library h5py . . Best, ; Uwe . â€” ; You are receiving this because you authored the thread. ; Reply to this email directly, [ https://github.com/mpicbg-csbd/stardist/issues/93#issuecomment-716223889 | view it on GitHub ] , or [ https://github.com/notifications/unsubscribe-auth/AAU56KJ4GEUFX4BPHFYPRBLSMSSATANCNFSM4SZXDKIQ | unsubscribe ] .",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/93#issuecomment-716329806:10,load,loading,10,,https://github.com/stardist/stardist/issues/93#issuecomment-716329806,4,['load'],"['load', 'loading']"
Performance,"Hi Uwe,; It's entirely possible it's specific to our implementation then. What I meant by unstable is that I had obtained a few times an unusual loss curve with an additional peak half way through the number of epoch. See below. This does not match any changes in learning rate either. <img width=""910"" alt=""Screenshot 2021-02-24 at 22 12 44"" src=""https://user-images.githubusercontent.com/21193399/109297054-4d408b80-7829-11eb-9aac-f0e9788e5907.png"">. But either way, we've now upgraded it to TF2.x and implemented the default settings as close to yours as possible, as well as the exact ways to do augmentation. And this seems to perform much better and not give the phantom masks or the unusual loss curves. Thanks a lot for your help. I really appreciate it.; Best,. Romain",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/116#issuecomment-786602735:632,perform,perform,632,,https://github.com/stardist/stardist/issues/116#issuecomment-786602735,1,['perform'],['perform']
Performance,"Hi!. Thanks, I made sure now that all the stardist modules are the same versions. Unfortunately, tensorflow is still not configured properly on the computing cluster, crashing with errors that say:; ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; Aborted; ```. with while trying to do very simple print statements, so this is something I need to solve with our sysadmin. I am loading the trained module onto my local machine (OSX, Python3.6) and using it to predict on similar images. I can now load the cluster-trained (stardist 0.41) model onto my machine:; ```; model = StarDist3D(None, name='modelname',; basedir=path.join(trainingdir,'models')); labels, details = model.predict_instances(test_img[); ```. The kernel runs for a minute or so and then silently crashes without any error message, and then restarts silently (no warning messages on the bash shell that spyder is running in either). <img width=""804"" alt=""Screen Shot 2020-02-21 at 3 09 51 PM"" src=""https://user-images.githubusercontent.com/5126258/75041128-3cde2e00-54bc-11ea-9233-cd8212b14a3b.png"">. --. The training/labels are 3D images of clusters of cells. One mid-level slice looks like this, with 1Âµm slices, about 20Âµm:. <img width=""851"" alt=""Screen Shot 2020-02-21 at 3 02 38 PM"" src=""https://user-images.githubusercontent.com/5126258/75040633-4c10ac00-54bb-11ea-8f3a-f853a9fda51b.png"">. ---. On my local machine, I am running: tensorflow 1.15.0, Python 3.6.; On the cluster, I was running: tf 1.14.0, Python 3.6. Thank you!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/35#issuecomment-589670082:443,load,loading,443,,https://github.com/stardist/stardist/issues/35#issuecomment-589670082,2,['load'],"['load', 'loading']"
Performance,"Hi, the error message indicates a problem with loading the weights from the HDF5 file. First, to rule out Snakemake as an issue, did you try to simply load the pre-trained model in a Python shell? For example, does this throw the same error?; ```; $ python -c ""from stardist.models import StarDist2D; StarDist2D.from_pretrained('2D_versatile_fluo')""; ```. If that's the case, then I'd try to use a different/newer version of the HDF5 library `h5py`. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/93#issuecomment-716223889:47,load,loading,47,,https://github.com/stardist/stardist/issues/93#issuecomment-716223889,2,['load'],"['load', 'loading']"
Performance,"IMHO, I didn't find it explicit in the DocString and went all the way to csbdeep to finally realize that this might be the case. At which point I tried to load it and had erased the previous model so it bugged XD . Training a model is very time consuming and simply running a single command might destroy it so IMHO it is a valuable addition to the readme ^^ But I am on the repeat things over school ^^",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/95#issuecomment-716247005:155,load,load,155,,https://github.com/stardist/stardist/issues/95#issuecomment-716247005,1,['load'],['load']
Performance,"Requirement already satisfied: numpy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.8.0rc1); Collecting tifffile; Using cached tifffile-2019.7.26.2-py2.py3-none-any.whl (131 kB); Collecting backports.tempfile; python_version < ""3.4""; Using cached backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB); Requirement already satisfied: six in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.12.0); Collecting h5py; Using cached h5py-2.10.0-cp27-cp27m-macosx_10_6_intel.whl (2.9 MB); Requirement already satisfied: matplotlib in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.3.1); Collecting keras<2.4,>=2.1.2; Using cached Keras-2.3.1-py2.py3-none-any.whl (377 kB); Requirement already satisfied: scipy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (0.13.0b1); Collecting tqdm; Using cached tqdm-4.47.0-py2.py3-none-any.whl (66 kB); Collecting pathlib2; python_version < ""3""; Using cached pathlib2-2.3.5-py2.py3-none-any.whl (18 kB); Collecting imagecodecs-lite<=2020; python_version < ""3.6""; Using cached imagecodecs-lite-2019.12.3.tar.gz (1.1 MB); ERROR: Command errored out with exit status 1:; command: /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py'""'""'; __file__='""'""'/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-pip-egg-info-I2_dBJ; cwd: /private/var/fol",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:1700,cache,cached,1700,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['cache'],['cached']
Performance,"Thank you very much for the detailed answer, it helps me a lot. All right, I'm going to:; - switch to `model.predict_instances` as my test stacks are of similar sizes as my training stacks (~400x1000x1000); - play with `n_tiles` : should I just set this as low as possible as long as it fits in my GPU or should I do something smarter?; - try to make bigger train_patch_size : I'm a bit uncomfortable setting batch_size lower than 4 (or even 8) to be honest, do you perform gradient accumulation under the hood? Maybe I'm just freaking out for nothing but don't you feel that a batch_size of 2 is a bit too small to get a nice gradient descent?. Also I wanted to ask you about something else. ; At the moment I don't have nice labels that have been curated by humans (I might find some time to do this but later), but I have good enough binary masks that allows me to train algorithms like 3DUnets.; My problem is that some cells are close to each other, so my initial labels are not very good at differientiating adjacent cells (which creates big objects that don't look like cells at all) and same goes with the resulting UNet segmentation, that's why I decided to switch to StarDist3D.; First results from StarDist seem already much better at creating nice round cells, but in order to train StarDist I simply separated my binary mask using connex component : so my ground truth for StarDist never really shows adjacent cells (as they are merged into one same label).; Because of this I don't really trust the automated nms_thresh search function (labels are wrong so it's going to be hard for the search). So I played a bit with the nms_thresh to visually inspect the results but I'd like to understand better what 0.4 or 0.05 means for example. Does 0.4 means that I would tolerate that 40% of the volume of two cells (or a proxy of the volume) are actually overlapping? In terms of physics and biology, I don't expect my cells to be able to overlap at all, they can touch each other but that's i",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691334428:466,perform,perform,466,,https://github.com/stardist/stardist/issues/87#issuecomment-691334428,1,['perform'],['perform']
Performance,"Thanks for the replies, @maweigert and @uschmidt83 . > For training, a majority of cells should be fully included in the annotated trainings stacks - I fear there is no real way around that. Sorry, I may not have been as clear as I intended. We have a working model that has been created with thicker images, and the problem occurs when applying the model on thin image-stacks. However, it seems like I am not able to reproduce the error (as in crashing the process) with the data I currently have access to, but I can't get any predictions with thinner images. I performed an example run with a representative image (XY-cropped) and a duplicate image with one z-slice removed, i.e. from 5 to 4 z-slices. I have included my output below with verbose=True. All in all, it seems that for some reason StarDist is not able to create any candidates for the thinner stack. At the thickness of 4 z-layers it is expected that the largest cells do not neatly fit into the stack, but there should be plenty of smaller cells that I would think should still create candidates. So I would presume there has to be a reason for why none are created?. @uschmidt83 ; > I don't know how your data looks like, but have you tried padding the image such that the Z axis is big enough (as a trivial workaround)?. I did try that some months back by adding empty slices but it caused bloating of the labels and was not viable. But now that I think about it there probably is a better way for padding the image, such as duplicating the edge layers to provide continuity in intensities. **OUTPUT**:. Holidic_2018-12-19_180224-2; Model = DAPI20x ; Image dims = (4, 944, 2008). predicting instances with prob_thresh = 0.01 and nms_thresh = 0.4; found 0 candidates; non-maximum suppression...; Non Maximum Suppression (3D) ++++ ; NMS: n_polys = 0 ; NMS: n_rays = 128 ; NMS: n_faces = 252 ; NMS: thresh = 0.400 ; NMS: use_bbox = 1 ; NMS: using OpenMP with 16 thread(s); NMS: precompute volumes, bounding boxes, etc; precompute done",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/133#issuecomment-831993257:564,perform,performed,564,,https://github.com/stardist/stardist/issues/133#issuecomment-831993257,1,['perform'],['performed']
Performance,"Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.8.0rc1); Collecting tifffile; Using cached tifffile-2019.7.26.2-py2.py3-none-any.whl (131 kB); Collecting backports.tempfile; python_version < ""3.4""; Using cached backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB); Requirement already satisfied: six in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.12.0); Collecting h5py; Using cached h5py-2.10.0-cp27-cp27m-macosx_10_6_intel.whl (2.9 MB); Requirement already satisfied: matplotlib in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (1.3.1); Collecting keras<2.4,>=2.1.2; Using cached Keras-2.3.1-py2.py3-none-any.whl (377 kB); Requirement already satisfied: scipy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from csbdeep->stardist) (0.13.0b1); Collecting tqdm; Using cached tqdm-4.47.0-py2.py3-none-any.whl (66 kB); Collecting pathlib2; python_version < ""3""; Using cached pathlib2-2.3.5-py2.py3-none-any.whl (18 kB); Collecting imagecodecs-lite<=2020; python_version < ""3.6""; Using cached imagecodecs-lite-2019.12.3.tar.gz (1.1 MB); ERROR: Command errored out with exit status 1:; command: /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py'""'""'; __file__='""'""'/private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-pip-egg-info-I2_dBJ; cwd: /private/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/pip-install-FhuQIv/imagecodecs-lite/; Comp",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:1798,cache,cached,1798,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['cache'],['cached']
Performance,"array from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray```; and after a few seconds of computation the code fails with this error message :; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-14-36ac39a79e87> in <module>; ----> 1 extents = calculate_extents(Y); 2 anisotropy = tuple(np.max(extents) / extents); 3 print('empirical anisotropy of labeled objects = %s' % str(anisotropy)). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/utils.py in calculate_extents(lbl, func); 165 n in (2,3) or _raise(ValueError(""label image should be 2- or 3-dimensional (or pass a list of these)"")); 166 ; --> 167 regs = regionprops(lbl); 168 if len(regs) == 0:; 169 return np.zeros(n). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/skimage/measure/_regionprops.py in regionprops(label_image, intensity_image, cache, coordinates); 883 regions = []; 884 ; --> 885 objects = ndi.find_objects(label_image); 886 for i, sl in enumerate(objects):; 887 if sl is None:. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/scipy/ndimage/measurements.py in find_objects(input, max_label); 301 ; 302 if max_label < 1:; --> 303 max_label = input.max(); 304 ; 305 return _nd_image.find_objects(input, max_label). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/numpy/core/_methods.py in _amax(a, axis, out, keepdims, initial, where); 37 def _amax(a, axis=None, out=None, keepdims=False,; 38 initial=_NoValue, where=True):; ---> 39 return umr_maximum(a, axis, None, out, keepdims, initial, where); 40 ; 41 def _amin(a, axis=None, out=None, keepdims=False,. ValueError: operands could not be broadcast together with shapes (80,330,500) (85",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:2195,cache,cache,2195,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['cache'],['cache']
Performance,"ck`s. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model); 1110 use_multiprocessing=use_multiprocessing,; 1111 distribution_strategy=ds_context.get_strategy(),; -> 1112 model=model); 1113 ; 1114 strategy = ds_context.get_strategy(). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs); 770 # Since we have to know the dtype of the python generator when we build the; 771 # dataset, we have to look at a batch to infer the structure.; --> 772 peek, x = self._peek_and_restore(x); 773 assert_not_namedtuple(peek); 774 peek = self._standardize_batch(peek). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in _peek_and_restore(x); 828 @staticmethod; 829 def _peek_and_restore(x):; --> 830 peek = next(x); 831 return peek, itertools.chain([peek], x); 832 . /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/csbdeep/internals/train.py in __iter__(self); 112 # print(f""### __iter__"", flush=True); 113 for i in range(len(self)):; --> 114 yield self[i]; 115 ; 116 def batch(self, i):. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/models/model3d.py in __getitem__(self, i); 64 X = X[0][np.newaxis]; 65 else:; ---> 66 X = np.stack(X, out=self.out_X[:len(Y)]); 67 if X.ndim == 4: # input image has no channel axis; 68 X = np.expand_dims(X,-1). <__array_function__ internals> in stack(*args, **kwargs). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/numpy/core/shape_base.py in stack(arrays, axis, out); 425 shape",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:6139,cache,cache,6139,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['cache'],['cache']
Performance,"ed. If you meant to do this, you must specify 'dtype=object' when creating the ndarray```; and after a few seconds of computation the code fails with this error message :; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-14-36ac39a79e87> in <module>; ----> 1 extents = calculate_extents(Y); 2 anisotropy = tuple(np.max(extents) / extents); 3 print('empirical anisotropy of labeled objects = %s' % str(anisotropy)). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/utils.py in calculate_extents(lbl, func); 165 n in (2,3) or _raise(ValueError(""label image should be 2- or 3-dimensional (or pass a list of these)"")); 166 ; --> 167 regs = regionprops(lbl); 168 if len(regs) == 0:; 169 return np.zeros(n). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/skimage/measure/_regionprops.py in regionprops(label_image, intensity_image, cache, coordinates); 883 regions = []; 884 ; --> 885 objects = ndi.find_objects(label_image); 886 for i, sl in enumerate(objects):; 887 if sl is None:. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/scipy/ndimage/measurements.py in find_objects(input, max_label); 301 ; 302 if max_label < 1:; --> 303 max_label = input.max(); 304 ; 305 return _nd_image.find_objects(input, max_label). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/numpy/core/_methods.py in _amax(a, axis, out, keepdims, initial, where); 37 def _amax(a, axis=None, out=None, keepdims=False,; 38 initial=_NoValue, where=True):; ---> 39 return umr_maximum(a, axis, None, out, keepdims, initial, where); 40 ; 41 def _amin(a, axis=None, out=None, keepdims=False,. ValueError: operands could not be broadcast together with shapes (80,330,500) (85,400,500); ```; I think maybe somehow my new class is considered as one large numpy array and not as a list of numpy arrays, is that possi",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:2344,cache,cache,2344,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['cache'],['cache']
Performance,"f._training_finished(). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs); 64 def _method_wrapper(self, *args, **kwargs):; 65 if not self._in_multi_worker_mode(): # pylint: disable=protected-access; ---> 66 return method(self, *args, **kwargs); 67 ; 68 # Running inside `run_distribute_coordinator` already. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing); 813 workers=workers,; 814 use_multiprocessing=use_multiprocessing,; --> 815 model=self); 816 ; 817 # Container that configures and calls `tf.keras.Callback`s. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model); 1110 use_multiprocessing=use_multiprocessing,; 1111 distribution_strategy=ds_context.get_strategy(),; -> 1112 model=model); 1113 ; 1114 strategy = ds_context.get_strategy(). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs); 770 # Since we have to know the dtype of the python generator when we build the; 771 # dataset, we have to look at a batch to infer the structure.; --> 772 peek, x = self._peek_and_restore(x); 773 assert_not_namedtuple(peek); 774 peek = self._standardize_batch(peek). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:5173,cache,cache,5173,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['cache'],['cache']
Performance,"it can make a big difference to enlarge the patch size such that objects are fully included.; The number of epochs seems a bit small, but you seem to know what you're doing.; ; > ```; > n,h,w = X_test[2].shape; > labels, details = model.predict_instances_big(X_test[2],; > axes='ZYX',; > block_size=(n, int(h/2), int(w/2)), #conf.train_patch_size; > min_overlap=16,; > context = (30,30,30), #(30,30,30),; > prob_thresh=0.4; > ); > ```. How big are the test stacks, i.e. what is `X_test[2].shape`?; If it has similar size than the training stacks, you can simply use `model.predict_instances`, which doesn't require you to set this many parameters. The only important one would be `n_tiles`, which determines into how many tiles the input stack will be chopped before prediction runs on the GPU. Please try using this function first. The function `model.predict_instances_big` is only intended for *really* big images, which can't even be loaded in RAM or which require excessive computation and RAM requirements during the CPU-based non-maximum suppression step (to prune redundant object predictions) that runs after the CNN prediction on the GPU. > 1- block_size : I understand that we are treating the large volume with smaller blocks, but shouldn't block_size be the same as training size? If I set `block_size=conf.train_patch_size` I end up with issues with context and overlap, probably because I have one small dimension in training (48). No, `block_size` should be much larger in comparison to `min_overlap` and `context`. It's a bit difficult to explain in words and I should really make a diagram to explain what's happening under the hood. When calling `model.predict_instances`, the block size is essentially the entire input image. > 2-min_overlap : this is the overlap between blocks right? Why should it be larger than the size of a training object as written in the source code? I mean whatever the size here, I could have an object half in my block and half in the next one, so why i",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:1531,load,loaded,1531,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,1,['load'],['loaded']
Performance,"ll; 2020-12-13 07:44:01.104140: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll; 2020-12-13 07:44:01.104217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0; 2020-12-13 07:44:01.104287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:; 2020-12-13 07:44:01.104317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273] 0; 2020-12-13 07:44:01.104339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 0: N; 2020-12-13 07:44:01.104415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4092 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3080, pci bus id: 0000:68:00.0, compute capability: 8.6); 2020-12-13 07:44:02.066463: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:258] None of the MLIR optimization passes are enabled (registered 0 passes); ```. . ##### All right but I would like to add some more comments:. 1. Something I do not really get:. ```; ...; 2020-12-13 07:34:30.293682: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295; ; 2020-12-13 07:34:30.348179: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295; ; 2020-12-13 07:34:30.399592: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295; ; 2020-12-13 07:34:30.455934: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295; ; 2020-12-13 07:34:30.512528: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295; ; 2020-12-13 07:34:30.576156: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295; ; 2020-12-13 07:34:30.631200: I tensorflow/core/platform/windows/",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:7282,optimiz,optimization,7282,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,1,['optimiz'],['optimization']
Performance,"n ok (note that I had to add `dtype` and `ndim` to make things work and that I have some problems in my pipeline as explained bellow). However some part of my training pipeline are broken:. - when running this to compute anisotropy `extents = calculate_extents(Y)` I first get this warning; ```_asarray.py (83): Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray```; and after a few seconds of computation the code fails with this error message :; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-14-36ac39a79e87> in <module>; ----> 1 extents = calculate_extents(Y); 2 anisotropy = tuple(np.max(extents) / extents); 3 print('empirical anisotropy of labeled objects = %s' % str(anisotropy)). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/utils.py in calculate_extents(lbl, func); 165 n in (2,3) or _raise(ValueError(""label image should be 2- or 3-dimensional (or pass a list of these)"")); 166 ; --> 167 regs = regionprops(lbl); 168 if len(regs) == 0:; 169 return np.zeros(n). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/skimage/measure/_regionprops.py in regionprops(label_image, intensity_image, cache, coordinates); 883 regions = []; 884 ; --> 885 objects = ndi.find_objects(label_image); 886 for i, sl in enumerate(objects):; 887 if sl is None:. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/scipy/ndimage/measurements.py in find_objects(input, max_label); 301 ; 302 if max_label < 1:; --> 303 max_label = input.max(); 304 ; 305 return _nd_image.find_objects(input, max_label). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/numpy/core/_methods.py in _amax(a, axis, out, keep",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:1869,cache,cache,1869,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['cache'],['cache']
Performance,"odel); 1113 ; 1114 strategy = ds_context.get_strategy(). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs); 770 # Since we have to know the dtype of the python generator when we build the; 771 # dataset, we have to look at a batch to infer the structure.; --> 772 peek, x = self._peek_and_restore(x); 773 assert_not_namedtuple(peek); 774 peek = self._standardize_batch(peek). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in _peek_and_restore(x); 828 @staticmethod; 829 def _peek_and_restore(x):; --> 830 peek = next(x); 831 return peek, itertools.chain([peek], x); 832 . /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/csbdeep/internals/train.py in __iter__(self); 112 # print(f""### __iter__"", flush=True); 113 for i in range(len(self)):; --> 114 yield self[i]; 115 ; 116 def batch(self, i):. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/models/model3d.py in __getitem__(self, i); 64 X = X[0][np.newaxis]; 65 else:; ---> 66 X = np.stack(X, out=self.out_X[:len(Y)]); 67 if X.ndim == 4: # input image has no channel axis; 68 X = np.expand_dims(X,-1). <__array_function__ internals> in stack(*args, **kwargs). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/numpy/core/shape_base.py in stack(arrays, axis, out); 425 shapes = {arr.shape for arr in arrays}; 426 if len(shapes) != 1:; --> 427 raise ValueError('all input arrays must have the same shape'); 428 ; 429 result_ndim = arrays[0].ndim + 1. ValueError: all input arrays must have the same shape; ```; So it looks like again there is a problem with shapes being different, as if the `train_path_size` did not work. Do you have any idea how to solve this?. Thank you very much,. Bests",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:6668,cache,cache,6668,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,2,['cache'],['cache']
Performance,"print('empirical anisotropy of labeled objects = %s' % str(anisotropy)). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/utils.py in calculate_extents(lbl, func); 165 n in (2,3) or _raise(ValueError(""label image should be 2- or 3-dimensional (or pass a list of these)"")); 166 ; --> 167 regs = regionprops(lbl); 168 if len(regs) == 0:; 169 return np.zeros(n). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/skimage/measure/_regionprops.py in regionprops(label_image, intensity_image, cache, coordinates); 883 regions = []; 884 ; --> 885 objects = ndi.find_objects(label_image); 886 for i, sl in enumerate(objects):; 887 if sl is None:. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/scipy/ndimage/measurements.py in find_objects(input, max_label); 301 ; 302 if max_label < 1:; --> 303 max_label = input.max(); 304 ; 305 return _nd_image.find_objects(input, max_label). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/numpy/core/_methods.py in _amax(a, axis, out, keepdims, initial, where); 37 def _amax(a, axis=None, out=None, keepdims=False,; 38 initial=_NoValue, where=True):; ---> 39 return umr_maximum(a, axis, None, out, keepdims, initial, where); 40 ; 41 def _amin(a, axis=None, out=None, keepdims=False,. ValueError: operands could not be broadcast together with shapes (80,330,500) (85,400,500); ```; I think maybe somehow my new class is considered as one large numpy array and not as a list of numpy arrays, is that possible? Any idea how to overcome this? . - if I switch to a batch size of 2, training does not work anymore and I end up with this error:; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-21-54ffb4970400> in <module>; 3 augmenter=augmenter_fn(ZOOM_RATIO, ZOOM_PROB),; 4 epochs=200, #100,; ----> 5 seed=42). /work/.cache/poetry/kc-segmentation-D",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:2768,cache,cache,2768,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['cache'],['cache']
Performance,"somehow my new class is considered as one large numpy array and not as a list of numpy arrays, is that possible? Any idea how to overcome this? . - if I switch to a batch size of 2, training does not work anymore and I end up with this error:; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-21-54ffb4970400> in <module>; 3 augmenter=augmenter_fn(ZOOM_RATIO, ZOOM_PROB),; 4 epochs=200, #100,; ----> 5 seed=42). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/models/model3d.py in train(self, X, Y, validation_data, augmenter, seed, epochs, steps_per_epoch, workers); 480 callbacks=self.callbacks, verbose=1,; 481 # set validation batchsize to training batchsize (only works in tf 2.x); --> 482 **(dict(validation_batch_size = self.config.train_batch_size) if _tf_version_at_least(""2.2.0"") else {})); 483 ; 484 self._training_finished(). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs); 64 def _method_wrapper(self, *args, **kwargs):; 65 if not self._in_multi_worker_mode(): # pylint: disable=protected-access; ---> 66 return method(self, *args, **kwargs); 67 ; 68 # Running inside `run_distribute_coordinator` already. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing); 813 workers=workers,; 814 use_multiprocessing=use_multiprocessing,; --> 815 model=self); 816 ; 817 # Container that configures and calls `tf.keras.Callback`s. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/t",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:4225,cache,cache,4225,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['cache'],['cache']
Performance,"tion on building from source. I think the package gets built when installing with pip, on the Mac I had to follow these instructions by @maweigert: https://github.com/mpicbg-csbd/stardist/issues/21. I also have stardist installed on a Ubuntu 18 machine for the training and there was no problem. Otherwise, you could speed up the python loops using `numba.jit` e.g. directly from your jupyter notebook like this:. ```python; from stardist.geometry import geom2d; from numba import jit. def _py_star_dist_modified(a, n_rays=32):; # (np.isscalar(n_rays) and 0 < int(n_rays)) or _raise(ValueError()); n_rays = int(n_rays); a = a.astype(np.uint16,copy=False); dst = np.empty(a.shape+(n_rays,),np.float32). for i in range(a.shape[0]):; for j in range(a.shape[1]):; value = a[i,j]; if value == 0:; dst[i,j] = 0; else:; st_rays = np.float32((2*np.pi) / n_rays); for k in range(n_rays):; phi = np.float32(k*st_rays); dy = np.cos(phi)#/100.; dx = np.sin(phi)#/100.; x, y = np.float32(0), np.float32(0). while True:; x += dx; y += dy; ii = int(round(i+x)); jj = int(round(j+y)); if (ii < 0 or ii >= a.shape[0] or; jj < 0 or jj >= a.shape[1] or; value != a[ii,jj]):; dist = np.sqrt(x*x + y*y); dst[i,j,k] = dist; break; return dst. geom2d._py_star_dist = jit(_py_star_dist_modified); ```. This version performs about the same as the cpp one:; ```; %timeit relabel_image_stardist(test_lbl, n_rays=32, mode='python'); 452 Âµs Â± 7.97 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each); %timeit relabel_image_stardist(test_lbl, n_rays=32, mode='cpp'); 493 Âµs Â± 8.6 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each); ```. > Can you rule out that it's just a phenomenon of the data, i.e. different intensity characteristics at the boundaries of training/testing data - so the model just learned to produce more smooth/conservative boundaries?. Hmm you're right that I cannot rule out that it's a training thing. Will definitely train again with a dataset in which I have many shapes represented. Thanks!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-583505792:1502,perform,performs,1502,,https://github.com/stardist/stardist/issues/33#issuecomment-583505792,1,['perform'],['performs']
Performance,"ultiprocessing, model); 1110 use_multiprocessing=use_multiprocessing,; 1111 distribution_strategy=ds_context.get_strategy(),; -> 1112 model=model); 1113 ; 1114 strategy = ds_context.get_strategy(). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs); 770 # Since we have to know the dtype of the python generator when we build the; 771 # dataset, we have to look at a batch to infer the structure.; --> 772 peek, x = self._peek_and_restore(x); 773 assert_not_namedtuple(peek); 774 peek = self._standardize_batch(peek). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in _peek_and_restore(x); 828 @staticmethod; 829 def _peek_and_restore(x):; --> 830 peek = next(x); 831 return peek, itertools.chain([peek], x); 832 . /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/csbdeep/internals/train.py in __iter__(self); 112 # print(f""### __iter__"", flush=True); 113 for i in range(len(self)):; --> 114 yield self[i]; 115 ; 116 def batch(self, i):. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/models/model3d.py in __getitem__(self, i); 64 X = X[0][np.newaxis]; 65 else:; ---> 66 X = np.stack(X, out=self.out_X[:len(Y)]); 67 if X.ndim == 4: # input image has no channel axis; 68 X = np.expand_dims(X,-1). <__array_function__ internals> in stack(*args, **kwargs). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/numpy/core/shape_base.py in stack(arrays, axis, out); 425 shapes = {arr.shape for arr in arrays}; 426 if len(shapes) != 1:; --> 427 raise ValueError('all input arrays must have the same shape'); 428 ; 429 result_ndim = arrays[0].ndim + 1. ValueError: all input arrays must have the same shape; ```; So it looks like again there is a proble",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:6415,cache,cache,6415,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['cache'],['cache']
Performance,"y3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing); 813 workers=workers,; 814 use_multiprocessing=use_multiprocessing,; --> 815 model=self); 816 ; 817 # Container that configures and calls `tf.keras.Callback`s. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model); 1110 use_multiprocessing=use_multiprocessing,; 1111 distribution_strategy=ds_context.get_strategy(),; -> 1112 model=model); 1113 ; 1114 strategy = ds_context.get_strategy(). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs); 770 # Since we have to know the dtype of the python generator when we build the; 771 # dataset, we have to look at a batch to infer the structure.; --> 772 peek, x = self._peek_and_restore(x); 773 assert_not_namedtuple(peek); 774 peek = self._standardize_batch(peek). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in _peek_and_restore(x); 828 @staticmethod; 829 def _peek_and_restore(x):; --> 830 peek = next(x); 831 return peek, itertools.chain([peek], x); 832 . /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/csbdeep/internals/train.py in __iter__(self); 112 # print(f""### __iter__"", flush=True); 113 for i in range(len(self)):; --> 114 yield self[i]; 115 ; 116 def batch(self, i):. /",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:5641,cache,cache,5641,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['cache'],['cache']
Safety," : I understand that we are treating the large volume with smaller blocks, but shouldn't block_size be the same as training size? If I set `block_size=conf.train_patch_size` I end up with issues with context and overlap, probably because I have one small dimension in training (48). No, `block_size` should be much larger in comparison to `min_overlap` and `context`. It's a bit difficult to explain in words and I should really make a diagram to explain what's happening under the hood. When calling `model.predict_instances`, the block size is essentially the entire input image. > 2-min_overlap : this is the overlap between blocks right? Why should it be larger than the size of a training object as written in the source code? I mean whatever the size here, I could have an object half in my block and half in the next one, so why is this so important?. The main assumption is that each predicted object is fully contained in at least one of the blocks. This is not important for the CNN prediction, but the non-maximum suppression step. > 3- context : I don't really understand the doc here `Amount of image context on all sides of a block, which is discarded.` What is the point of discarding some pixels on all sides? Are they discarded after the forward pass (for the NMS algorithm) or before (like a padding)?. No, this is just because the CNN prediction is less accurate at the image boundary. It is discarded after the NMS step. > 4- `Also, it must hold that: min_overlap + 2*context < block_size.` I would easily understand that ` 2*context < block_size` in order not to discard everything, but why does `min_overlap` appears here? Would you have a link to a visual explanation how you define the different blocks? I think this could help me understand. After the context is discarded on all sides, blocks still have to overlap at least `min_overlap` pixels. As you say, a diagram would make this clear. Note that the `model.predict_instances_big` is relatively new and really meant for ",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:2750,predict,prediction,2750,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,1,['predict'],['prediction']
Safety," error; > train_epochs = 50; > ); > ```. Since your object sizes are almost isotropic, you could try to use `(160,192,192)` or similar and decrease the batch size if needed, but I doubt it'll make much of a difference. Of course, if the objects are (often) larger than 48 pixels, it can make a big difference to enlarge the patch size such that objects are fully included.; The number of epochs seems a bit small, but you seem to know what you're doing.; ; > ```; > n,h,w = X_test[2].shape; > labels, details = model.predict_instances_big(X_test[2],; > axes='ZYX',; > block_size=(n, int(h/2), int(w/2)), #conf.train_patch_size; > min_overlap=16,; > context = (30,30,30), #(30,30,30),; > prob_thresh=0.4; > ); > ```. How big are the test stacks, i.e. what is `X_test[2].shape`?; If it has similar size than the training stacks, you can simply use `model.predict_instances`, which doesn't require you to set this many parameters. The only important one would be `n_tiles`, which determines into how many tiles the input stack will be chopped before prediction runs on the GPU. Please try using this function first. The function `model.predict_instances_big` is only intended for *really* big images, which can't even be loaded in RAM or which require excessive computation and RAM requirements during the CPU-based non-maximum suppression step (to prune redundant object predictions) that runs after the CNN prediction on the GPU. > 1- block_size : I understand that we are treating the large volume with smaller blocks, but shouldn't block_size be the same as training size? If I set `block_size=conf.train_patch_size` I end up with issues with context and overlap, probably because I have one small dimension in training (48). No, `block_size` should be much larger in comparison to `min_overlap` and `context`. It's a bit difficult to explain in words and I should really make a diagram to explain what's happening under the hood. When calling `model.predict_instances`, the block size is essentially",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:1360,predict,prediction,1360,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,1,['predict'],['prediction']
Safety," was something like 36 hours - so I cancelled it and gave up on it. Uwe reminded me that with one core, I likely had a maximum of 2 threads, but I did not go back and try `predict_big` with more cores. Do you think `predict_big` with say 32 cores (64 threads) could finish in less than an hour? If so I will give it another try. For different kdtree implementations - I did look through [this article written by the scikit-learn kdtree and balltree developer](https://jakevdp.github.io/blog/2013/04/29/benchmarking-nearest-neighbor-searches-in-python/), but I didn't think there was enough differences in his graphs to justify introducing a new dependency (I don't think scikit-learn was a dependency before?) and I also preferred something with an underlying C++ implementation, as there are more big speed ups available if the kdtree data structure is available for query directly within the NMS algorithm. Unfortunately the only RAM measurements I have are the maximum and average usage (over the full run, prediction and NMS) for prediction on the full volumes both w/ and w/o the kdtree. The command in both cases was:. `pred, det = model.predict_instances(image_norm, verbose=True, n_tiles=(4, 8, 2))`; On a volume with 860x1676x301 voxels and 6.32e6 polygon candidates. w/o kdtree max: 434.439GB; w/ kdtree max: 430.482GB. w/o kdtree avg: 221.672GB; w/ kdtree avg: 297.733GB. I'm thinking that the kdtree and the query results for this many candidates took something like 70GB RAM. One thing that I haven't looked at in the code - it seems like the biggest expansion of data in the method is from raw data --> prediction; e.g. in my case I'm using 128 rays, so I need (129 * 2) more RAM relative to my input image (assuming 32 bit floats for probs and dists and 16 bit int for image). The n_tiles parameter cuts the image up into overlapping tiles for prediction. Does each tile also get separately prob thresholded before stitching back up for NMS? Based on the way RAM usage appears to me, i",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-601381585:1540,predict,prediction,1540,,https://github.com/stardist/stardist/pull/40#issuecomment-601381585,2,['predict'],['prediction']
Safety,"> > Is the new Java library for deep learning ready yet? What is the name and where can I find it?; > ; > Yes, here it is: https://github.com/bioimage-io/model-runner-java. Thanks for the link! A few questions:; - This is only going to work for models in the bioimage.io format, right?; - Will this library handle pre- and post-processing?; - Is it going to support tile-based prediction, i.e. chopping the input image into tiles, running the model on each of them, and then re-assembling the individual results?. > > I really need to understand the current situation first... I don't even know how to export the model such that it will work with the new Java library.; > ; > The bioimageio.core library will export it directly in a compatible format (as the model I linked in the previous lines). In the upcoming days, we will update deepImageJ with the java model runner so all the TF2 models can also be deployed in Fiji. I can let you know when the plugin is ready if you want. I'm first interested in using `model-runner-java` to replace the TF1-only CSBDeep library in Fiji, in order to run TF2-based models in StarDist.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-1460949099:377,predict,prediction,377,,https://github.com/stardist/stardist/issues/68#issuecomment-1460949099,1,['predict'],['prediction']
Safety,"> All in all, it seems that for some reason StarDist is not able to create any candidates for the thinner stack. At the thickness of 4 z-layers it is expected that the largest cells do not neatly fit into the stack, but there should be plenty of smaller cells that I would think should still create candidates. So I would presume there has to be a reason for why none are created?. I don't understand. Are cells really only 4 pixels (or even less for ""smaller cells"") in Z? Was the model trained with such data?. > I did try that some months back by adding empty slices but it caused bloating of the labels and was not viable. But now that I think about it there probably is a better way for padding the image, such as duplicating the edge layers to provide continuity in intensities. You should pad the image with ""background"" values (whatever that means for your image) to not ""bloat"" the segmented objects sizes. > predicting instances with prob_thresh = 0.01 and nms_thresh = 0.4; > found 0 candidates. Something is clearly wrong here. There shouldn't be a need to use such a very small `prob_thresh` value, unless the model is badly trained or not suitable for the given image. Can you share a/some representative training images and also a/some problematic test images?. Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/133#issuecomment-832682530:918,predict,predicting,918,,https://github.com/stardist/stardist/issues/133#issuecomment-832682530,1,['predict'],['predicting']
Safety,"> Can you give me some numbers to put this statement in perspective?. To give you some context:; We run stardist prediction on a DAPI channel to segment nuclei for a high throughput screening setup.; The individual images are approximately 1000x1000 and we have approximately a 1000 images per experiment (= 1 plate). . Running prediction / segmentation with stardist takes ~ 40 minutes for 1 experiment and ; it's clearly cpu bound (judging from the GPU utilization which I think is maybe between 5 and 10 percent on average). (I just call `predict_instances` for now, so I don't really know what the ratio between time spent prediction and time spent on NMS is). We also run two other processing steps: predicting foreground/background and boundaries with a pytorch network (on a different channel) and watersheds (using nuclei as seeds and boundary predictions as height map). Both of these steps can be done in under 5 - 10 minutes each, so stardist is the clear bottleneck right now. (For the pytorch prediction I stack images across the batch axis and the watershed can just be parallelized over multiple CPUs, so this comparison is not really fair!). I can run some experiments next week to really see how long prediction vs NMS takes.; What are the functions I have to call on the model if I want to do this in two separate steps?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/39#issuecomment-612622104:113,predict,prediction,113,,https://github.com/stardist/stardist/issues/39#issuecomment-612622104,7,['predict'],"['predicting', 'prediction', 'predictions']"
Safety,"> Hi @talkhanz,; > ; > It's hard to understand what you mean (the left image above seems to be 8bit, but the normalized image should be 32 bit). Please describe in more detail what you did, what you expected, and what happened instead :). Im sorry if I was not clear. The image was converted to 8 bit while I was using FIJI (i.e not something that was done through code. The code produces 32 bit as you suggested). I basically ran the training notebook from the github example directory but with slight modification to the file reading processing. I have emailed the training and prediction .py files at your epfl email. I used the Allen Institute aicsimageio [https://allencellmodeling.github.io/aicsimageio/index.html](url) The dataset I used was from the Broad Institute. ( train images can be found from [https://data.broadinstitute.org/bbbc/BBBC024/BBBC024_v1_c00_highSNR_images_TIFF.zip](url) while the masks from [https://data.broadinstitute.org/bbbc/BBBC024/BBBC024_v1_c00_highSNR_foreground.zip](url). I was expecting elongated cellular segmentation like the ground truth available from the broad_institute. The snap for ground truth looks like this :; <img width=""486"" alt=""expected_"" src=""https://user-images.githubusercontent.com/63508490/91554922-77cc8d00-e949-11ea-811e-51307bb55ed3.PNG"">. This is what I got instead : ; <img width=""159"" alt=""actual_"" src=""https://user-images.githubusercontent.com/63508490/91554950-8155f500-e949-11ea-8598-ae7e882251d3.PNG"">. I hope I was a bit clearer,; Please let me know if you require anything else,; Best; Edit:; Just to add a little detail. The training data is being modified through normalization but the model seems to be predict the labels correctly given the normalized images it has received. Segmentation is correct based on the training images",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/84#issuecomment-682469217:580,predict,prediction,580,,https://github.com/stardist/stardist/issues/84#issuecomment-682469217,2,['predict'],"['predict', 'prediction']"
Safety,"> Hi @uschmidt83, So if I am using a custom config should I write the whole config used instead of just the n_rays but also kernel size etc?. I don't understand what you mean. > But it is still counter intuitive to me because you have the config.json file in the same folder as the model weights so for the prediction step it should know the hyperparameters I used for the training right?. All I'm saying is that supplying a new config to an already trained model is not a good idea or the solution to your problem. As you said, it _should_ work by loading the config from disk with passing `config=None` to the model constructor.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-659566701:307,predict,prediction,307,,https://github.com/stardist/stardist/issues/68#issuecomment-659566701,1,['predict'],['prediction']
Safety,"> However DeepImageJ should already be able to run Stardist. I have succesfully run it in my computer. Instead of using the ""Test model"" button, please try opening an image, then selecting the model and finally running the model pressing ""ok"". Ah, I didn't expect that DeepImageJ works like this. Wouldn't it be more intuitive to have separate menu items for model testing and running?. > the ""Test model"" button should be fixed now, feel free to test the new release with the funcionality fixed:; > https://github.com/deepimagej/deepimagej-plugin/releases/tag/2.1.12. I can confirm that model testing and running on a different opened image works for me now. > @uschmidt83 I think we can assume it works now in deepImageJ. Should we go ahead and merge it?. There's one issue I don't understand yet. Why did we bother to put the `stardist_postprocessing.ijm` macro into the exported model zip file if there's no way to run this as *Postprocessing* from DeepImageJ?. When I did run the postprocessing macro from the Script Editor (opened manually from the `Fiji.app/models/<model_name>` folder) after DeepImageJ prediction, it showed me an error `[ERROR] Could not find input image with name/title ""scores"".` in the Console. I then tried the exact same thing again, and it worked as intended without errors ðŸ¤·â€â™‚ï¸. > (Also there are still some failing tests, but this seems to be unrelated to the changes here.). Yes, those are unrelated. The bioimage.io tests are only active in Github action tests with `tensorflow<2` in the name, and those run through just fine.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1047694133:1111,predict,prediction,1111,,https://github.com/stardist/stardist/pull/171#issuecomment-1047694133,1,['predict'],['prediction']
Safety,"> I can confirm that model testing and running on a different opened image works for me now. Great. thanks for checking!. > There's one issue I don't understand yet. Why did we bother to put the `stardist_postprocessing.ijm` macro into the exported model zip file if there's no way to run this as _Postprocessing_ from DeepImageJ?. We want to support this in deepImageJ but are currently a bit out of manpower to implement it. There's probably a relatively simple solution. But also with the given solution, the script is at least packaged with the model, so that it can be applied manually. ; We will work on better post-processing integration, and there is probably a simple solution, but we don't want to block releasing the model zoo for now to incorporate it. We can update the readme to explain this, but I would suggest to do this in a follow-up PR as this one is pretty large already and it's not critical to the functionality. > When I did run the postprocessing macro from the Script Editor (opened manually from the `Fiji.app/models/<model_name>` folder) after DeepImageJ prediction, it showed me an error `[ERROR] Could not find input image with name/title ""scores"".` in the Console. I then tried the exact same thing again, and it worked as intended without errors man_shrugging. @esgomezm @carlosuc3m do you have any idea how to fix this?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1047715559:1083,predict,prediction,1083,,https://github.com/stardist/stardist/pull/171#issuecomment-1047715559,1,['predict'],['prediction']
Safety,"> I would however suggest not doing this via global seed setting but by creating and using a local `rng=np.random.RandomState(seed)` such to avoid the side effect of non-transparently manipulating the global numpy rng state. I would've suggested the same, but I still don't understand why this is necessary. Why not simply do this?; ```python; # np.random.seed(seed) # optional; my_cmap = random_color_cmap(); ...; plt.imshow(..., cmap=my_cmap); ...; plt.imshow(..., cmap=my_cmap); ```",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/273#issuecomment-2187329538:141,avoid,avoid,141,,https://github.com/stardist/stardist/pull/273#issuecomment-2187329538,1,['avoid'],['avoid']
Safety,"> I'm a complete beginner at Python but find the software really exciting, so have jumped in at the deep end!. Glad that you like it!. > My aim is to be able to export the label maps and visualise them in ImageJ or something similar, and to use the 3 dimensional ROIs to calculate values for nuclear volume and shape and other things. I suggest you do the prediction in Python to get the label image, which you can then import in Fiji with the [3D Roi Manager](https://imagejdocu.tudor.lu/plugin/stacks/3d_roi_manager/start) (installation instructions [here](https://imagejdocu.tudor.lu/plugin/stacks/3d_ij_suite/start#installation)). This should work well, as long as you don't have (strongly) overlapping predictions or it's not important to you to get the complete shapes for all objects. > I can see that you've made a notebook to export the ROIs for the 2D model prediction into FIJI which works great for me. I tried to adapt this for 3D by just having axes set to zyx and having 'dist' instead of 'coord' (in export_imagej_rois('img_rois.zip', details['dist'])). I have found that this doesn't really work, but not sure why. This function only works for 2D polygons and cannot be simply adapted for 3D.; ; > I also tried to export the demo 3d model into deepImageJ, but it runs into trouble when you try to apply it to an image. I can give more details, but as I am quite lost I'm not sure what is the relevant information. It's not that simple and even if it worked, this won't get you 3D ROIs in Fiji. Our [StarDist Fiji plugin](https://imagej.net/StarDist) currently only works in 2D because it's not that easy to support 3D... but we hope to get this done eventually. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/37#issuecomment-597239267:356,predict,prediction,356,,https://github.com/stardist/stardist/issues/37#issuecomment-597239267,3,['predict'],"['prediction', 'predictions']"
Safety,"> No, I never do. It's still open. Do I have to close it? I'm really sorry, but I cannot find out, how to run the `nvidia-smi`. I know it was running once, but now I only get the output `nvidia-smi is not recognized as an internal or external command, operable program or batch file.`. Tensorflow will reserve all GPU memory available once its first called, so a training notebook that is left open after being run will ""occupy"" the GPU leaving the prediction notebook with no memory to compute on, which is what you have seen. So shutting down all GPU notebooks before running another one is always recommended. We should add that to the notebook, so thanks for the feedback! :). ; > ,I get an image without any labeling. Can you post the figure that is produced by this cell?. > when I run `example(model, 0)`, I get the following error:. you need to change `model.predict_instances(img, n_tiles=(1,4,4)` inside the `example` function too",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/119#issuecomment-787089461:449,predict,prediction,449,,https://github.com/stardist/stardist/issues/119#issuecomment-787089461,1,['predict'],['prediction']
Safety,"> Not grid=4; I was referencing this previous discussion:. Ah, I see :). > Qualitatively the results look ok - there are definitely some nuclei missing and very few cells that are just too large. Yep, it's pretty good for that the image have a rather bad SNR and are pretty wild along z! Nice!. > I only manually annotated a 128x128x32 patch. I see. I would try to set `train_patch_size` to the largest possible size (e.g. `(32,128,128)`) to avoid boundary effects. Maybe that will help with the probability. But othe rthan that, your network outputs look reasonable!. > Would you consider manually lowering the learning rate after so many epochs?. That will be done automatically during taring (if the loss saturates), so I wouldn't do that.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/36#issuecomment-593626054:442,avoid,avoid,442,,https://github.com/stardist/stardist/issues/36#issuecomment-593626054,1,['avoid'],['avoid']
Safety,"> Note that [21] (stardist 2d paper) sits somewhere in between object detection and instance segmentation because the predicted shapes are of relatively high fidelity, but are not pixel-accurate. I think this probably refers to the general idea that the Stardist representation is only perfectly pixel accurate - theoretically, and for arbitrarily high resolution - with an infinite number of rays. In the discrete case, it's only pixel accurate if your number of rays equals the number of boundary pixels of the cell (assuming it is unambiguously clear where this boundary is to begin with). You're ultimately representing an object whose boundary is a rough discrete approximation to a smooth continuous surface with a (somewhat sparse, even with 128+ rays) polygon - like an octagon around a circle. Also to clarify - the UNet does not reconstruct the polygons - it just maps the input image to the probability + ray distances feature space. The probability thresholding + non maximum suppression is what ultimately gives the polygons - and then those are reconstructed [in the code you pointed out to me before.](https://github.com/mpicbg-csbd/stardist/issues/33#issuecomment-583421466). One approach to refining the boundary segmentations would be to begin with the Stardist segmentation as an initialization, and then do graph based segmentation to refine boundaries locally [[1]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1542036) [[2]](https://link.springer.com/content/pdf/10.1007%2F3-540-45465-9_88.pdf). I think that would be really cool - but also a whole project unto itself and potentially a lot of work. Depending on how complex the shape you're trying to segment, it's probably not worth it.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-604064572:70,detect,detection,70,,https://github.com/stardist/stardist/issues/33#issuecomment-604064572,2,"['detect', 'predict']","['detection', 'predicted']"
Safety,"> Sorry, too tired apparently :). @maweigert No problem, I hadn't made it too clear that it wasn't my data :). > Did you try to play around with the grid size (e.g. setting it to (4,4))?. Yeah I could try reducing the resolution, so far I was using (2,2) because that seemed like a reasonable choice considering the data and detail of segmentation I want to recover. I guess I'm also a bit curious regarding how precise the polygon reconstructions could get in principle with optimised training and whether there'd be a clear way of thinking about it. In practical terms for now the segmentation is nicely reliable and of sufficient quality so I'll stick to that for the data of my collaborator!. Thanks a lot for the thoughts everyone and feel free to close the issue :)",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-604961417:358,recover,recover,358,,https://github.com/stardist/stardist/issues/33#issuecomment-604961417,1,['recover'],['recover']
Safety,"> `n_tiles` looks like exactly what I need. My image isn't that big ~1024x1014x512 voxels. Ok, for that size that indeed should be the way to go!. > (running on the CPU on my laptop - I'm happy to say though that with like 10 minutes of training on my CPU I already get pretty darn good results!. Happy to hear that! Although 10mins on a CPU is like 1 epoch? I would never expect that to work :). > I may stick with the 256 rays but use lots of tiles at prediction time?. Yes, that'd be an option. Alternatively you could use less rays (I practically never use more than ~96). And again, in the future the RAM limitations should be taken care of by the label tiling (there is already a branch `big` for that). . Cheers and thanks for the feedback!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/36#issuecomment-593572037:454,predict,prediction,454,,https://github.com/stardist/stardist/issues/36#issuecomment-593572037,1,['predict'],['prediction']
Safety,"> the median size object is [ 37.5 156. 160.5]. Before or after downscaling the input images?. > I increased the grid and the unet to 3 .; > After learning, when I apply to predictions on new images the issues is that the Z is right (because below 64) but the X and Y are way too small because of the FOV of [ 64 128 128] .; > ; > Hence it can only see object below that shape : so many small nucleus detected on 1 single nucleus; > Hence my question is how to use Stardist when trained on downscale object onto bigger images with full scale (50, 1440, 1920) ?. It is difficult to understand what you mean. Ideally show us the code/notebook.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/118#issuecomment-785909768:173,predict,predictions,173,,https://github.com/stardist/stardist/issues/118#issuecomment-785909768,2,"['detect', 'predict']","['detected', 'predictions']"
Safety,But it is still counter intuitive to me because you have the config.json file in the same folder as the model weights so for the prediction step it should know the hyperparameters I used for the training right?,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-659548054:129,predict,prediction,129,,https://github.com/stardist/stardist/issues/68#issuecomment-659548054,1,['predict'],['prediction']
Safety,"Coming back to this after a while. I was going through the stardist 3d paper (as someone in the lab was planning to use it!) and read this statement:. > Note that [21] (stardist 2d paper) sits somewhere in between object detection and instance segmentation because the predicted shapes are of relatively high fidelity, but are not pixel-accurate. What would be your explanation for why segmentations are not necessarily pixel accurate? I guess for me it's not super clear how the output of a u-net behaves when needing to precisely reconstruct the ray lengths. > Hard to see why it should deviate so much for the images you show, which should be rather easy to segment...would have too look further into it (maybe cells are too large, gridsize too small etc). At the end the segmentations I'm getting are not bad at all, but since that project involves cell shape characterisations I was wondering whether there would be some nice trick to improve the accuracies of the boundary. For the images I showed I used the default settings from the notebook, i.e. the shown image resolution (objects of around 70 pixels in diameter) and `grid=(2,2)`. . Cheers!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-604001939:221,detect,detection,221,,https://github.com/stardist/stardist/issues/33#issuecomment-604001939,2,"['detect', 'predict']","['detection', 'predicted']"
Safety,"Hi ,. >Sorry for the late reply. no worries ðŸ˜Š. >You are using the scripts branch?. Actually, I'm not... I simply created a conda env with a yml file (see above) , specifying version `stardist==0.8.2` (not specifying a the script branch). I tried the `stardist-predict3d -h` and I saw it was there :; ```; usage: stardist-predict3d [-h] -i INPUT [INPUT ...] [-o OUTDIR]; [--outname OUTNAME [OUTNAME ...]] -m MODEL; [--axes AXES] [--n_tiles N_TILES N_TILES]; [--pnorm PNORM PNORM] [--prob_thresh PROB_THRESH]; [--nms_thresh NMS_THRESH] [-v]. Prediction script for a 3D stardist model, usage: stardist-predict -i; input.tif -m model_folder_or_pretrained_name -o output_folder. optional arguments:; -h, --help show this help message and exit; -i INPUT [INPUT ...], --input INPUT [INPUT ...]; input file (tiff) (default: None); -o OUTDIR, --outdir OUTDIR; output directory (default: .); --outname OUTNAME [OUTNAME ...]; output file name (tiff) (default: {img}.stardist.tif); -m MODEL, --model MODEL; model folder / pretrained model to use (default: None); --axes AXES axes to use for the input, e.g. 'XYC' (default: None); --n_tiles N_TILES N_TILES; number of tiles to use for prediction (default: None); --pnorm PNORM PNORM pmin/pmax to use for normalization (default: [3,; 99.8]); --prob_thresh PROB_THRESH; prob_thresh for model (if not given use model default); (default: None); --nms_thresh NMS_THRESH; nms_thresh for model (if not given use model default); (default: None); -v, --verbose; ```; but I can't make it work ðŸ˜‘",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/191#issuecomment-1141976465:599,predict,predict,599,,https://github.com/stardist/stardist/issues/191#issuecomment-1141976465,2,['predict'],"['predict', 'prediction']"
Safety,"Hi @GFleishman,. Thanks for all the work! Two things that I think are still worth pondering:. 1) The `max_distance` cutoff for the spatial data structure should ideally be chosen such that no potential intersecting polygon is missed. Some options are ; * (Mean) `max_distance= 2*(np.mean(dist)+np.std(dist))` what is used in the PR, but might miss some pairs ; * (Max) `max_distance= 2*np.max(dist)`, preferable as it won't miss any pairs but which leads to order of magnitude more pairs. . There likely won't be any difference between (Mean) and (Max) in practice (as your initial comparison showed), but I am still a bit wary of not having that guarantee. . 2) Memory: I tried prediction on a small crop (with ~250k candidates) of the test data above:; * without kdtree: 15.4 GB, 97s ; * with kdtree (Mean): 15.5 GB, 98s ; * with kdtree (Max): 39.2 GB, 121s . So using (Max) will clearly take a hit on memory...",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-606352621:679,predict,prediction,679,,https://github.com/stardist/stardist/pull/40#issuecomment-606352621,1,['predict'],['prediction']
Safety,"Hi @esgomezm,. > We are actually trying to export the model using TF2.x (the idea is to test it with the new java library in Fiji + deepImageJ). Is the new Java library for deep learning ready yet? What is the name and where can I find it?. > Is it the export being forced inside StarDist to have a model compatible with TF1.15?. Yes, the model export in the CSBDeep and StarDist Python packages was always meant for use in Fiji, which has only ever supported TensorFlow 1.x. > I'm using the bioimageio library (version 0.5.8) with TF2.11 to export TF and Keras models, and everything works perfectly. Using the bioimageio library where to export the model? And ""everything works perfectly"" where?. > A potential solution (probably you have a better one), could be to avoid those imports if TF version > 2.3? Until version 2.3, the compatibility with CSBDeep & StarDist is ensured, but for later versions is not unless the plugins are updated. Also, my feeling is that the export fails with TF>2.3. I really need to understand the current situation first... I don't even know how to export the model such that it will work with the new Java library.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-1453562117:768,avoid,avoid,768,,https://github.com/stardist/stardist/issues/68#issuecomment-1453562117,1,['avoid'],['avoid']
Safety,"Hi @m-albert,. > While in the previous example the boundary is simply very smooth, here it seems to be slightly off. Hard to see why it should deviate so much for the images you show, which should be rather easy to segment...would have too look further into it (maybe cells are too large, gridsize too small etc) . > This might be important when dealing with small objects. Not sure decreasing the step size will help with my boundaries but I'll retrain and give it a try, potentially being off at the boundaries could confuse the distance predictions. Thanks (and @GFleishman too) for bringing that up! Indeed the stardist calculations are a bit rough for small objects and we never bothered to refine them correctly. Inspired by this thread I took another look at them: Instead of decreasing the stepsize (which would probably be too slow) one can [directly compute the ""overshoot"" distance after the label switches](https://github.com/mpicbg-csbd/stardist/blob/dev/stardist/lib/stardist2d.cpp#L74). That way, distances for small objects should be now more correct. I've put it in the `dev` branch, so you can try it yourself:. `pip install git+https://github.com/mpicbg-csbd/stardist.git@dev`. Let me know if that helps and thanks for all the feedback and input!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-583762427:540,predict,predictions,540,,https://github.com/stardist/stardist/issues/33#issuecomment-583762427,1,['predict'],['predictions']
Safety,"Hi @maweigert,. Right now I'm working on a different strategy from the software standpoint that will solve the extra time cost for using Max and the extra memory from having to store the larger lists of neighbors (potential suppression candidates). My first implementation in the PR was a good demonstration of kdtree, but definitely suboptimal w.r.t. what can be done with the Python C-API, which I'm learning on the fly so things have gone through various stages of kludge. The idea now is that I can call the sklearn kdtree object, and it's query_radius method, from within the C++ code itself - enabling a separate query for each polygon candidate just before the inner loop of NMS begins. That way, points which are ""suppressed early"" will never be queried against the tree - reducing total query time even if the distance threshold is large. The second benefit is that the code now only has to store one neighbor list at a time, thus reducing the total RAM used. This is definitely the best way to built it _algorithmically_ but I still don't have the Python C-API down perfectly so implementing this today has been tricky. I also implemented what I mentioned before - applying the probability threshold for each tile separately during prediction. For the whole image, it cut max RAM down from ~400GB to ~50GB, and that was including the neighbors list from kdtree. So, if I can get the tile based threshold combined with the embedded python approach to the kdtree, I think I can get the whole zebrafish image segmented in like 10-15 minutes for something like 30GB RAM. Of course, the tile based prob threshold breaks a bunch of stuff, so I'll leave it to you guys to decide if you want to move in that direction generally or to just have my commit somewhere accessible on the repo so if others run into RAM issues there's some path to cutting it down without changing so many internal dependencies between functions. This is all WIP but I'm hoping to finish it soon. Thanks,; Greg",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-606357837:1242,predict,prediction,1242,,https://github.com/stardist/stardist/pull/40#issuecomment-606357837,1,['predict'],['prediction']
Safety,"Hi @romainGuiet,. You should be able to get the original shape by simply linearly upscaling the probability and distance prediction by the grid parameter (i.e. two fold along each axis if `grid=(2,2)`). . Your planning to have the full pipeline (with polygon rendering) as part of a Fiji plugin? . Cheers,; M",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/28#issuecomment-557546384:121,predict,prediction,121,,https://github.com/stardist/stardist/issues/28#issuecomment-557546384,1,['predict'],['prediction']
Safety,"Hi @uschmidt83 ,. Thanks for clarifying all that! ; Indeed I did not notice this flag was exclusively for the data generation for training step. Just the fact that there was a variable `use_gpu` set to `False` led me to wrong conclusions. Perhaps a more explicit naming can help to avoid confusions like this in the future, something like `use_gpu_for_data_gen`. > No, this is intentional. The and gputools_available() part acts as a [""guard""](https://en.wikipedia.org/wiki/Guard_(computer_science)) to always disable this flag when gputools is not installed. I see, so it is up to the user to actively change it if they want to use it for data generation. Thanks, I believe things are clear for me now.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/222#issuecomment-1445371166:282,avoid,avoid,282,,https://github.com/stardist/stardist/issues/222#issuecomment-1445371166,1,['avoid'],['avoid']
Safety,"Hi Greg, . thanks a lot for this contribution - it already looks very promising! . > I evaluated NMS time over a wide number of polygon candidates. Here are the results (note the log_10 scale on the vertical axis):. Thats a nice speed up! The 6Mio candidates are for the whole stack, I assume? So using `predict_big` doesn't have any effect on the prediction time?. I'm slightly worried about the additional memory footprint that a full kdtree of all candidates might bring - did you try measuring the peak memory usage? Additionally I suspect there is quite some heavy differences even for different kdtree implementations e.g. there are `KDTree` and `BallTree` from `scikit-learn`. Here's a short test I did with random points in 3D and those different implementation ; ""ckdtree"" -> `scipy.spatial.ckdtree` ; ""kdtree"" -> `sklearn.neighbors.KDTree`; ""ball"" -> `sklearn.neighbors.BallTree`. <img width=""850"" alt=""Screenshot 2020-03-19 at 18 10 04"" src=""https://user-images.githubusercontent.com/11042162/77096353-85333200-6a0f-11ea-9f97-926c8bd22cd9.png"">; ; shows that i) there are quite some difference esp. for memory, and ii) the overall memory footprint is pretty high (e.g. 18GB for a mere 30k points for ckdtree). So having a similar plot as above but for memory consumption in your example would be nice and instructive. Additionally moving to `BallTree` might make things more efficient...",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-601317246:348,predict,prediction,348,,https://github.com/stardist/stardist/pull/40#issuecomment-601317246,1,['predict'],['prediction']
Safety,"Hi Martin,. Yes, the 6Mio candidates are for prediction on the whole stack. I did try `predict_big` on this volume. What I noticed was that the memory footprint went way down. Using [Uwe's recommended parameters](https://github.com/mpicbg-csbd/stardist/issues/36#issuecomment-594974089) I was able to run the job with a single cpu (# cores requested and maximum amount of RAM available are mixed on our cluster; each core comes with 15GB RAM). However the remaining runtime predicted by tqdm after a few iterations had accumulated was something like 36 hours - so I cancelled it and gave up on it. Uwe reminded me that with one core, I likely had a maximum of 2 threads, but I did not go back and try `predict_big` with more cores. Do you think `predict_big` with say 32 cores (64 threads) could finish in less than an hour? If so I will give it another try. For different kdtree implementations - I did look through [this article written by the scikit-learn kdtree and balltree developer](https://jakevdp.github.io/blog/2013/04/29/benchmarking-nearest-neighbor-searches-in-python/), but I didn't think there was enough differences in his graphs to justify introducing a new dependency (I don't think scikit-learn was a dependency before?) and I also preferred something with an underlying C++ implementation, as there are more big speed ups available if the kdtree data structure is available for query directly within the NMS algorithm. Unfortunately the only RAM measurements I have are the maximum and average usage (over the full run, prediction and NMS) for prediction on the full volumes both w/ and w/o the kdtree. The command in both cases was:. `pred, det = model.predict_instances(image_norm, verbose=True, n_tiles=(4, 8, 2))`; On a volume with 860x1676x301 voxels and 6.32e6 polygon candidates. w/o kdtree max: 434.439GB; w/ kdtree max: 430.482GB. w/o kdtree avg: 221.672GB; w/ kdtree avg: 297.733GB. I'm thinking that the kdtree and the query results for this many candidates took somethi",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-601381585:45,predict,prediction,45,,https://github.com/stardist/stardist/pull/40#issuecomment-601381585,2,['predict'],"['predicted', 'prediction']"
Safety,"Hi Michael,. > 1. My data (example data provided here: https://drive.google.com/drive/folders/17q11-hAJjCs72YFbsVKoGve3e5nzXyJf?usp=sharing) appears to be the wrong shape. Any ideas for a simple fix?. I can reproduce that one of your images is opened with seemingly wrong shape in Python. I don't know why that happens, but I've found that simply re-saving the image as a Tiff file from Fiji solves the problem, i.e. it then loads correctly in Python. > 2. I tried running prediction anyway, and find that I get an out of memory error.; >; > Is this because my tensorflow/CUDA is not communicating with the GPU?. No, this is also related to the wrong image shape and should go away.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/62#issuecomment-644116825:473,predict,prediction,473,,https://github.com/stardist/stardist/issues/62#issuecomment-644116825,1,['predict'],['prediction']
Safety,"Hi again - I need to revisit this discussion. I've now moved to my cluster and am trying to run on the full image:; `pred, det = model.predict_instances(image_norm, n_tiles=(2, 16, 8))`; The image is not huge: ~900x1600x300 voxels; but there are _many_ cells, something on the order of 10^5. Whichever part of predict_instances that has the tqdm progress bar finishes in a reasonable amount of time, about 30 minutes, but whatever is after is taking several hours. Due to some network connectivity issues I haven't been able to get through a complete run yet. I'm guessing the loop monitored by tqdm is model prediction over the tiles, and what comes after is NMS and ""polyhedron_to_label""? I guess NMS would scale with the number of voxels, but polyhedron_to_label would scale with the number of labels? Do you have any estimate for how long something like my dataset might take?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/36#issuecomment-594086237:609,predict,prediction,609,,https://github.com/stardist/stardist/issues/36#issuecomment-594086237,1,['predict'],['prediction']
Safety,"Hi!. Thanks, I made sure now that all the stardist modules are the same versions. Unfortunately, tensorflow is still not configured properly on the computing cluster, crashing with errors that say:; ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; Aborted; ```. with while trying to do very simple print statements, so this is something I need to solve with our sysadmin. I am loading the trained module onto my local machine (OSX, Python3.6) and using it to predict on similar images. I can now load the cluster-trained (stardist 0.41) model onto my machine:; ```; model = StarDist3D(None, name='modelname',; basedir=path.join(trainingdir,'models')); labels, details = model.predict_instances(test_img[); ```. The kernel runs for a minute or so and then silently crashes without any error message, and then restarts silently (no warning messages on the bash shell that spyder is running in either). <img width=""804"" alt=""Screen Shot 2020-02-21 at 3 09 51 PM"" src=""https://user-images.githubusercontent.com/5126258/75041128-3cde2e00-54bc-11ea-9233-cd8212b14a3b.png"">. --. The training/labels are 3D images of clusters of cells. One mid-level slice looks like this, with 1Âµm slices, about 20Âµm:. <img width=""851"" alt=""Screen Shot 2020-02-21 at 3 02 38 PM"" src=""https://user-images.githubusercontent.com/5126258/75040633-4c10ac00-54bb-11ea-8f3a-f853a9fda51b.png"">. ---. On my local machine, I am running: tensorflow 1.15.0, Python 3.6.; On the cluster, I was running: tf 1.14.0, Python 3.6. Thank you!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/35#issuecomment-589670082:525,predict,predict,525,,https://github.com/stardist/stardist/issues/35#issuecomment-589670082,1,['predict'],['predict']
Safety,"Thank you for your quick reply! . > I suggest you do the prediction in Python to get the label image, which you can then import in Fiji with the [3D Roi Manager](https://imagejdocu.tudor.lu/plugin/stacks/3d_roi_manager/start) (installation instructions [here](https://imagejdocu.tudor.lu/plugin/stacks/3d_ij_suite/start#installation)). This should work well, as long as you don't have (strongly) overlapping predictions or it's not important to you to get the complete shapes for all objects. This sounds really good. I do have quite overlapping nuclei though unfortunately. Apologies if this is a bit of a starter question but would I just save the label image as a tiff file and then open it in FIJI, and then segment it with the 3D ROI viewer? . > It's not that simple and even if it worked, this won't get you 3D ROIs in Fiji. Our [StarDist Fiji plugin](https://imagej.net/StarDist) currently only works in 2D because it's not that easy to support 3D... but we hope to get this done eventually. Ah I thought it wouldn't be simple. The error message I was getting was that the input tensor had to be 5 dimensional, and was wondering whether the mismatch was due to tensor organisation convention of NHWC not including a third spatial dimension. . The images you have from paintera look really beautiful, I think that is my aim here, but perhaps FIJI's 3d viewer of the 3d ROIs would get something similar?. Thanks again!!. Michael Schwimmer",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/37#issuecomment-597304169:57,predict,prediction,57,,https://github.com/stardist/stardist/issues/37#issuecomment-597304169,2,['predict'],"['prediction', 'predictions']"
Safety,"Thanks for the replies, @maweigert and @uschmidt83 . > For training, a majority of cells should be fully included in the annotated trainings stacks - I fear there is no real way around that. Sorry, I may not have been as clear as I intended. We have a working model that has been created with thicker images, and the problem occurs when applying the model on thin image-stacks. However, it seems like I am not able to reproduce the error (as in crashing the process) with the data I currently have access to, but I can't get any predictions with thinner images. I performed an example run with a representative image (XY-cropped) and a duplicate image with one z-slice removed, i.e. from 5 to 4 z-slices. I have included my output below with verbose=True. All in all, it seems that for some reason StarDist is not able to create any candidates for the thinner stack. At the thickness of 4 z-layers it is expected that the largest cells do not neatly fit into the stack, but there should be plenty of smaller cells that I would think should still create candidates. So I would presume there has to be a reason for why none are created?. @uschmidt83 ; > I don't know how your data looks like, but have you tried padding the image such that the Z axis is big enough (as a trivial workaround)?. I did try that some months back by adding empty slices but it caused bloating of the labels and was not viable. But now that I think about it there probably is a better way for padding the image, such as duplicating the edge layers to provide continuity in intensities. **OUTPUT**:. Holidic_2018-12-19_180224-2; Model = DAPI20x ; Image dims = (4, 944, 2008). predicting instances with prob_thresh = 0.01 and nms_thresh = 0.4; found 0 candidates; non-maximum suppression...; Non Maximum Suppression (3D) ++++ ; NMS: n_polys = 0 ; NMS: n_rays = 128 ; NMS: n_faces = 252 ; NMS: thresh = 0.400 ; NMS: use_bbox = 1 ; NMS: using OpenMP with 16 thread(s); NMS: precompute volumes, bounding boxes, etc; precompute done",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/133#issuecomment-831993257:529,predict,predictions,529,,https://github.com/stardist/stardist/issues/133#issuecomment-831993257,1,['predict'],['predictions']
Safety,"emoved, i.e. from 5 to 4 z-slices. I have included my output below with verbose=True. All in all, it seems that for some reason StarDist is not able to create any candidates for the thinner stack. At the thickness of 4 z-layers it is expected that the largest cells do not neatly fit into the stack, but there should be plenty of smaller cells that I would think should still create candidates. So I would presume there has to be a reason for why none are created?. @uschmidt83 ; > I don't know how your data looks like, but have you tried padding the image such that the Z axis is big enough (as a trivial workaround)?. I did try that some months back by adding empty slices but it caused bloating of the labels and was not viable. But now that I think about it there probably is a better way for padding the image, such as duplicating the edge layers to provide continuity in intensities. **OUTPUT**:. Holidic_2018-12-19_180224-2; Model = DAPI20x ; Image dims = (4, 944, 2008). predicting instances with prob_thresh = 0.01 and nms_thresh = 0.4; found 0 candidates; non-maximum suppression...; Non Maximum Suppression (3D) ++++ ; NMS: n_polys = 0 ; NMS: n_rays = 128 ; NMS: n_faces = 252 ; NMS: thresh = 0.400 ; NMS: use_bbox = 1 ; NMS: using OpenMP with 16 thread(s); NMS: precompute volumes, bounding boxes, etc; precompute done; NMS: calculated anisotropy: -nan(ind) 	 -nan(ind) 	 -nan(ind) ; NMS: starting suppression loop; NMS: Function calls:; NMS: ~ bbox+out: 0; NMS: ~ inner: 0; NMS: ~ kernel: 0; NMS: ~ convex: 0; NMS: ~ render: 0; NMS: Excluded intersection:; NMS: + pretest: 0; NMS: + convex: 0; NMS: Function calls timing:; NMS: / kernel: 0.00 s (0.00 ms per call); NMS: / convex: 0.00 s (0.00 ms per call); NMS: / render: 0.00 s (0.00 ms per call); NMS: Suppressed polyhedra:; NMS: # inner: 0 / 0 (-nan(ind) %); NMS: # kernel: 0 / 0 (-nan(ind) %); NMS: # render: 0 / 0 (-nan(ind) %); NMS: # total: 0 / 0 (-nan(ind) %); NMS: # keeping 0 / 0 polyhedra (-nan(ind) %); NMS took 0.0060 s; kee",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/133#issuecomment-831993257:1650,predict,predicting,1650,,https://github.com/stardist/stardist/issues/133#issuecomment-831993257,1,['predict'],['predicting']
Safety,"ender: 0; NMS: Excluded intersection:; NMS: + pretest: 0; NMS: + convex: 0; NMS: Function calls timing:; NMS: / kernel: 0.00 s (0.00 ms per call); NMS: / convex: 0.00 s (0.00 ms per call); NMS: / render: 0.00 s (0.00 ms per call); NMS: Suppressed polyhedra:; NMS: # inner: 0 / 0 (-nan(ind) %); NMS: # kernel: 0 / 0 (-nan(ind) %); NMS: # render: 0 / 0 (-nan(ind) %); NMS: # total: 0 / 0 (-nan(ind) %); NMS: # keeping 0 / 0 polyhedra (-nan(ind) %); NMS took 0.0060 s; keeping 0/0 polyhedra; render polygons...; warning: empty list of points (returning background-only image). Description of 'Holidic_2018-12-19_180224-2_Ch=1.labels.tif':; Z Y X Intensity Volume Area_Max; count 0.0 0.0 0.0 0.0 0.0 0.0; mean NaN NaN NaN NaN NaN NaN; std NaN NaN NaN NaN NaN NaN; min NaN NaN NaN NaN NaN NaN; 25% NaN NaN NaN NaN NaN NaN; 50% NaN NaN NaN NaN NaN NaN; 75% NaN NaN NaN NaN NaN NaN; max NaN NaN NaN NaN NaN NaN . Holidic_2018-12-19_180224; Model = DAPI20x ; Image dims = (5, 944, 2008). predicting instances with prob_thresh = 0.01 and nms_thresh = 0.4; found 36356 candidates; non-maximum suppression...; Non Maximum Suppression (3D) ++++ ; NMS: n_polys = 36356 ; NMS: n_rays = 128 ; NMS: n_faces = 252 ; NMS: thresh = 0.400 ; NMS: use_bbox = 1 ; NMS: using OpenMP with 16 thread(s); NMS: precompute volumes, bounding boxes, etc; precompute done; NMS: calculated anisotropy: 6.94 	 1.00 	 1.01 ; NMS: starting suppression loop; |################################################# | [98 % suppressed]; NMS: Function calls:; NMS: ~ bbox+out: 5667969; NMS: ~ inner: 37360; NMS: ~ kernel: 37326; NMS: ~ convex: 32949; NMS: ~ render: 32114; NMS: Excluded intersection:; NMS: + pretest: 5630609; NMS: + convex: 835; NMS: Function calls timing:; NMS: / kernel: 22.67 s (0.61 ms per call); NMS: / convex: 94.72 s (2.87 ms per call); NMS: / render: 177.13 s (5.52 ms per call); NMS: Suppressed polyhedra:; NMS: # inner: 34 / 36356 (0.09 %); NMS: # kernel: 4377 / 36356 (12.04 %); NMS: # render: 31495 / 36356 (86.63 %",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/133#issuecomment-831993257:3182,predict,predicting,3182,,https://github.com/stardist/stardist/issues/133#issuecomment-831993257,1,['predict'],['predicting']
Safety,"ext`. It's a bit difficult to explain in words and I should really make a diagram to explain what's happening under the hood. When calling `model.predict_instances`, the block size is essentially the entire input image. > 2-min_overlap : this is the overlap between blocks right? Why should it be larger than the size of a training object as written in the source code? I mean whatever the size here, I could have an object half in my block and half in the next one, so why is this so important?. The main assumption is that each predicted object is fully contained in at least one of the blocks. This is not important for the CNN prediction, but the non-maximum suppression step. > 3- context : I don't really understand the doc here `Amount of image context on all sides of a block, which is discarded.` What is the point of discarding some pixels on all sides? Are they discarded after the forward pass (for the NMS algorithm) or before (like a padding)?. No, this is just because the CNN prediction is less accurate at the image boundary. It is discarded after the NMS step. > 4- `Also, it must hold that: min_overlap + 2*context < block_size.` I would easily understand that ` 2*context < block_size` in order not to discard everything, but why does `min_overlap` appears here? Would you have a link to a visual explanation how you define the different blocks? I think this could help me understand. After the context is discarded on all sides, blocks still have to overlap at least `min_overlap` pixels. As you say, a diagram would make this clear. Note that the `model.predict_instances_big` is relatively new and really meant for people who have huge images. It's not a mainstream function. > 5- all my 3D stacks don't have the same sizes, should I change the above parameters according to each sizes or one set should work for all?. The parameters can be the the same and really only depend on the CNN architecture (context) and the object sizes (min_overlap). However, the stack must obviou",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:3111,predict,prediction,3111,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,1,['predict'],['prediction']
Safety,"ften trained with a batch size of 1 and didn't see a problem. > My problem is that some cells are close to each other, so my initial labels are not very good at differientiating adjacent cells (which creates big objects that don't look like cells at all) and same goes with the resulting UNet segmentation, that's why I decided to switch to StarDist3D.; > First results from StarDist seem already much better at creating nice round cells, but in order to train StarDist I simply separated my binary mask using connex component : so my ground truth for StarDist never really shows adjacent cells (as they are merged into one same label). I don't see how you can overcome this problem without fixing the ground truth (at least in part). What we have seen is that StarDist can potentially be trained with labelling errors â€“ as long as these are not biased, which they seem to be in your case (i.e. touching nuclei are always incorrectly merged). > Because of this I don't really trust the automated nms_thresh search function (labels are wrong so it's going to be hard for the search). (As I said above, I wouldn't trust the results in general.); Note that the automatic threshold finding is in some sense just for convenience to set good thresholds that work well on average. Depending on the application, one would, e.g., increase the probability threshold to prefer more accurate predictions (fewer false positives) at the expense of missing some nuclei (more false negatives). > Does 0.4 means that I would tolerate that 40% of the volume of two cells (or a proxy of the volume) are actually overlapping?. Yes. > In terms of physics and biology, I don't expect my cells to be able to overlap at all, they can touch each other but that's it. Does this physics rule mean that setting nms_thresh to 0 makes sense?. Yes, in some sense. But keep in mind that predicted nucleus shapes are not perfect and thus can overlap to some degree. That's why you typically want to allow for some overlap. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-692842138:1934,predict,predictions,1934,,https://github.com/stardist/stardist/issues/87#issuecomment-692842138,2,['predict'],"['predicted', 'predictions']"
Safety,"it can make a big difference to enlarge the patch size such that objects are fully included.; The number of epochs seems a bit small, but you seem to know what you're doing.; ; > ```; > n,h,w = X_test[2].shape; > labels, details = model.predict_instances_big(X_test[2],; > axes='ZYX',; > block_size=(n, int(h/2), int(w/2)), #conf.train_patch_size; > min_overlap=16,; > context = (30,30,30), #(30,30,30),; > prob_thresh=0.4; > ); > ```. How big are the test stacks, i.e. what is `X_test[2].shape`?; If it has similar size than the training stacks, you can simply use `model.predict_instances`, which doesn't require you to set this many parameters. The only important one would be `n_tiles`, which determines into how many tiles the input stack will be chopped before prediction runs on the GPU. Please try using this function first. The function `model.predict_instances_big` is only intended for *really* big images, which can't even be loaded in RAM or which require excessive computation and RAM requirements during the CPU-based non-maximum suppression step (to prune redundant object predictions) that runs after the CNN prediction on the GPU. > 1- block_size : I understand that we are treating the large volume with smaller blocks, but shouldn't block_size be the same as training size? If I set `block_size=conf.train_patch_size` I end up with issues with context and overlap, probably because I have one small dimension in training (48). No, `block_size` should be much larger in comparison to `min_overlap` and `context`. It's a bit difficult to explain in words and I should really make a diagram to explain what's happening under the hood. When calling `model.predict_instances`, the block size is essentially the entire input image. > 2-min_overlap : this is the overlap between blocks right? Why should it be larger than the size of a training object as written in the source code? I mean whatever the size here, I could have an object half in my block and half in the next one, so why i",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:1665,redund,redundant,1665,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,3,"['predict', 'redund']","['prediction', 'predictions', 'redundant']"
Safety,"ld finish in less than an hour? If so I will give it another try. For different kdtree implementations - I did look through [this article written by the scikit-learn kdtree and balltree developer](https://jakevdp.github.io/blog/2013/04/29/benchmarking-nearest-neighbor-searches-in-python/), but I didn't think there was enough differences in his graphs to justify introducing a new dependency (I don't think scikit-learn was a dependency before?) and I also preferred something with an underlying C++ implementation, as there are more big speed ups available if the kdtree data structure is available for query directly within the NMS algorithm. Unfortunately the only RAM measurements I have are the maximum and average usage (over the full run, prediction and NMS) for prediction on the full volumes both w/ and w/o the kdtree. The command in both cases was:. `pred, det = model.predict_instances(image_norm, verbose=True, n_tiles=(4, 8, 2))`; On a volume with 860x1676x301 voxels and 6.32e6 polygon candidates. w/o kdtree max: 434.439GB; w/ kdtree max: 430.482GB. w/o kdtree avg: 221.672GB; w/ kdtree avg: 297.733GB. I'm thinking that the kdtree and the query results for this many candidates took something like 70GB RAM. One thing that I haven't looked at in the code - it seems like the biggest expansion of data in the method is from raw data --> prediction; e.g. in my case I'm using 128 rays, so I need (129 * 2) more RAM relative to my input image (assuming 32 bit floats for probs and dists and 16 bit int for image). The n_tiles parameter cuts the image up into overlapping tiles for prediction. Does each tile also get separately prob thresholded before stitching back up for NMS? Based on the way RAM usage appears to me, it seems like no; but that would save a lot of space, since the dists of most voxels could be thrown out before the dists of new voxels were predicted/stored. I think if this were implemented, then the additional RAM of the kdtree would become an important factor.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-601381585:2147,predict,prediction,2147,,https://github.com/stardist/stardist/pull/40#issuecomment-601381585,3,['predict'],"['predicted', 'prediction']"
Safety,"ow-gpu* nightly using pip (tf-nightly-gpu 2.5.0.dev20201212). `pip install tf-nightly-gpu`. 7. Follow the installation instructions for stardist. https://github.com/mpicbg-csbd/stardist. . <!--I got a compatibility issue error with h5py library version. I just continue with the installation and everything seems to work correctly-->. 8. Install the NEUBIAS academy Jupyter Notebook for testing stardist installation . `$ git clone https://github.com/maweigert/neubias_academy_stardist.git`. ##### Run stardist-gpu on the NVIDIA RTX 3080. 1. Start the *Jupyter Notebook* . . 2. Comment the following lines:. ```; $ #%tensorflow_version 1.x; ; $ #!pip install stardist; ```. . 3. The direct downloads didn't work for me, I just comment these lines out and downloaded the sample data before. ```; '''; download_and_extract_zip_file(; url = 'https://github.com/mpicbg-csbd/stardist/releases/download/0.1.0/dsb2018.zip',; targetdir = 'data',; verbose = 1,; ); '''; ```. 4. To avoid TF GPU out of memory issues I add the following code:. ```python; import tensorflow as tf; gpus = tf.config.experimental.list_physical_devices('GPU'); tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4092)]); ```. . Output:. ```; 2020-12-13 07:44:01.102879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties:; pciBusID: 0000:68:00.0 name: GeForce RTX 3080 computeCapability: 8.6; coreClock: 1.785GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s; 2020-12-13 07:44:01.103001: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll; 2020-12-13 07:44:01.103940: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll; 2020-12-13 07:44:01.103985: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt6",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:4682,avoid,avoid,4682,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,1,['avoid'],['avoid']
Safety,"redundant object predictions) that runs after the CNN prediction on the GPU. > 1- block_size : I understand that we are treating the large volume with smaller blocks, but shouldn't block_size be the same as training size? If I set `block_size=conf.train_patch_size` I end up with issues with context and overlap, probably because I have one small dimension in training (48). No, `block_size` should be much larger in comparison to `min_overlap` and `context`. It's a bit difficult to explain in words and I should really make a diagram to explain what's happening under the hood. When calling `model.predict_instances`, the block size is essentially the entire input image. > 2-min_overlap : this is the overlap between blocks right? Why should it be larger than the size of a training object as written in the source code? I mean whatever the size here, I could have an object half in my block and half in the next one, so why is this so important?. The main assumption is that each predicted object is fully contained in at least one of the blocks. This is not important for the CNN prediction, but the non-maximum suppression step. > 3- context : I don't really understand the doc here `Amount of image context on all sides of a block, which is discarded.` What is the point of discarding some pixels on all sides? Are they discarded after the forward pass (for the NMS algorithm) or before (like a padding)?. No, this is just because the CNN prediction is less accurate at the image boundary. It is discarded after the NMS step. > 4- `Also, it must hold that: min_overlap + 2*context < block_size.` I would easily understand that ` 2*context < block_size` in order not to discard everything, but why does `min_overlap` appears here? Would you have a link to a visual explanation how you define the different blocks? I think this could help me understand. After the context is discarded on all sides, blocks still have to overlap at least `min_overlap` pixels. As you say, a diagram would make thi",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:2649,predict,predicted,2649,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,1,['predict'],['predicted']
Security,"----------------------------------------; ValueError Traceback (most recent call last); <ipython-input-21-54ffb4970400> in <module>; 3 augmenter=augmenter_fn(ZOOM_RATIO, ZOOM_PROB),; 4 epochs=200, #100,; ----> 5 seed=42). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/models/model3d.py in train(self, X, Y, validation_data, augmenter, seed, epochs, steps_per_epoch, workers); 480 callbacks=self.callbacks, verbose=1,; 481 # set validation batchsize to training batchsize (only works in tf 2.x); --> 482 **(dict(validation_batch_size = self.config.train_batch_size) if _tf_version_at_least(""2.2.0"") else {})); 483 ; 484 self._training_finished(). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs); 64 def _method_wrapper(self, *args, **kwargs):; 65 if not self._in_multi_worker_mode(): # pylint: disable=protected-access; ---> 66 return method(self, *args, **kwargs); 67 ; 68 # Running inside `run_distribute_coordinator` already. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing); 813 workers=workers,; 814 use_multiprocessing=use_multiprocessing,; --> 815 model=self); 816 ; 817 # Container that configures and calls `tf.keras.Callback`s. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model); 1110 use_multiprocessing=use_multiprocessing,; 1111 distribution_strate",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:4499,access,access,4499,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['access'],['access']
Security,"> Based on the tqdm estimates, the time will still be quite long, ~8 hours, but using only a single cpu (accessible on my cluster now due to lower RAM requirement) will significantly bring down the cost. Why are you only using a single CPU now? The non-maximum suppression will be much faster with more CPU/cores available. > Is there any interest in running the tiles in a cluster environment with DASK (or similar)?. We first want to roll out this functionality with a simple API for single-machine use. Running this in a distributed environment is certainly appealing, but makes everything _much_ more complicated, hence I can't promise you when/if we'll do it. If you have suggestions on how to do this, please tell us.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/36#issuecomment-597133170:105,access,accessible,105,,https://github.com/stardist/stardist/issues/36#issuecomment-597133170,1,['access'],['accessible']
Security,"> However, I noticed the absence of a `checkpoint.ckpt` file (instead there is a `variables.index` file). Is there a way to generate this `.ckpt` file?. I have never needed a checkpoint myself, hence I don't know. However, you can try to export one yourself since you have full access to the Keras model via `model.keras_model`. Here's the general documentation for the [SavedModel format](https://www.tensorflow.org/guide/saved_model), and here is a guide specific to [Training checkpoints](https://www.tensorflow.org/guide/checkpoint). Hope that helps. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/61#issuecomment-643508305:278,access,access,278,,https://github.com/stardist/stardist/issues/61#issuecomment-643508305,1,['access'],['access']
Security,"Check, whether the validation stacks `X_val, Y_val` are somehow look different than the training data (e.g. normalisation). How did you generate the validation data, via the notebook? There seems to be clearly something wrong with them...",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/53#issuecomment-625132058:19,validat,validation,19,,https://github.com/stardist/stardist/issues/53#issuecomment-625132058,2,['validat'],['validation']
Security,"Hi @maweigert,. Right now I'm working on a different strategy from the software standpoint that will solve the extra time cost for using Max and the extra memory from having to store the larger lists of neighbors (potential suppression candidates). My first implementation in the PR was a good demonstration of kdtree, but definitely suboptimal w.r.t. what can be done with the Python C-API, which I'm learning on the fly so things have gone through various stages of kludge. The idea now is that I can call the sklearn kdtree object, and it's query_radius method, from within the C++ code itself - enabling a separate query for each polygon candidate just before the inner loop of NMS begins. That way, points which are ""suppressed early"" will never be queried against the tree - reducing total query time even if the distance threshold is large. The second benefit is that the code now only has to store one neighbor list at a time, thus reducing the total RAM used. This is definitely the best way to built it _algorithmically_ but I still don't have the Python C-API down perfectly so implementing this today has been tricky. I also implemented what I mentioned before - applying the probability threshold for each tile separately during prediction. For the whole image, it cut max RAM down from ~400GB to ~50GB, and that was including the neighbors list from kdtree. So, if I can get the tile based threshold combined with the embedded python approach to the kdtree, I think I can get the whole zebrafish image segmented in like 10-15 minutes for something like 30GB RAM. Of course, the tile based prob threshold breaks a bunch of stuff, so I'll leave it to you guys to decide if you want to move in that direction generally or to just have my commit somewhere accessible on the repo so if others run into RAM issues there's some path to cutting it down without changing so many internal dependencies between functions. This is all WIP but I'm hoping to finish it soon. Thanks,; Greg",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-606357837:1766,access,accessible,1766,,https://github.com/stardist/stardist/pull/40#issuecomment-606357837,1,['access'],['accessible']
Security,"Hi Uwe, . loading directly in a shell does not throw the error. It happens only when I use snakemake which is bizarre. This might be related to another error I get which is that the libiomp5.so is already initialised ( OMP: Error #15: Initializing libiomp5.so, but found libomp.so already initialized). I wonder whether due to this conflict the model is accessed simultaneously by two process leading to the error. . Thanks for looking into this, ; Marc . Dr. Marc Bickle ; Technology Development Studio ; Max Planck Institute of Molecular Cell Biology and Genetics ; Pfotenhauerstrasse 108 ; 01307 Dresden Germany . Phone: +49 (0)172 536 5517 . From: ""Uwe Schmidt"" <notifications@github.com> ; To: ""mpicbg-csbd/stardist"" <stardist@noreply.github.com> ; Cc: ""Marc Bickle"" <bickle@mpi-cbg.de>, ""Author"" <author@noreply.github.com> ; Sent: Sunday, October 25, 2020 11:36:57 PM ; Subject: Re: [mpicbg-csbd/stardist] loading 2D_versatile_fluo error (#93) . Hi, the error message indicates a problem with loading the weights from the HDF5 file. . First, to rule out Snakemake as an issue, did you try to simply load the pre-trained model in a Python shell? For example, does this throw the same error? ; $ python -c ""from stardist.models import StarDist2D; StarDist2D.from_pretrained('2D_versatile_fluo')"" . If that's the case, then I'd try to use a different/newer version of the HDF5 library h5py . . Best, ; Uwe . â€” ; You are receiving this because you authored the thread. ; Reply to this email directly, [ https://github.com/mpicbg-csbd/stardist/issues/93#issuecomment-716223889 | view it on GitHub ] , or [ https://github.com/notifications/unsubscribe-auth/AAU56KJ4GEUFX4BPHFYPRBLSMSSATANCNFSM4SZXDKIQ | unsubscribe ] .",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/93#issuecomment-716329806:354,access,accessed,354,,https://github.com/stardist/stardist/issues/93#issuecomment-716329806,1,['access'],['accessed']
Security,"Thanks for the replies, @maweigert and @uschmidt83 . > For training, a majority of cells should be fully included in the annotated trainings stacks - I fear there is no real way around that. Sorry, I may not have been as clear as I intended. We have a working model that has been created with thicker images, and the problem occurs when applying the model on thin image-stacks. However, it seems like I am not able to reproduce the error (as in crashing the process) with the data I currently have access to, but I can't get any predictions with thinner images. I performed an example run with a representative image (XY-cropped) and a duplicate image with one z-slice removed, i.e. from 5 to 4 z-slices. I have included my output below with verbose=True. All in all, it seems that for some reason StarDist is not able to create any candidates for the thinner stack. At the thickness of 4 z-layers it is expected that the largest cells do not neatly fit into the stack, but there should be plenty of smaller cells that I would think should still create candidates. So I would presume there has to be a reason for why none are created?. @uschmidt83 ; > I don't know how your data looks like, but have you tried padding the image such that the Z axis is big enough (as a trivial workaround)?. I did try that some months back by adding empty slices but it caused bloating of the labels and was not viable. But now that I think about it there probably is a better way for padding the image, such as duplicating the edge layers to provide continuity in intensities. **OUTPUT**:. Holidic_2018-12-19_180224-2; Model = DAPI20x ; Image dims = (4, 944, 2008). predicting instances with prob_thresh = 0.01 and nms_thresh = 0.4; found 0 candidates; non-maximum suppression...; Non Maximum Suppression (3D) ++++ ; NMS: n_polys = 0 ; NMS: n_rays = 128 ; NMS: n_faces = 252 ; NMS: thresh = 0.400 ; NMS: use_bbox = 1 ; NMS: using OpenMP with 16 thread(s); NMS: precompute volumes, bounding boxes, etc; precompute done",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/133#issuecomment-831993257:498,access,access,498,,https://github.com/stardist/stardist/issues/133#issuecomment-831993257,1,['access'],['access']
Security,"n umr_maximum(a, axis, None, out, keepdims, initial, where); 40 ; 41 def _amin(a, axis=None, out=None, keepdims=False,. ValueError: operands could not be broadcast together with shapes (80,330,500) (85,400,500); ```; I think maybe somehow my new class is considered as one large numpy array and not as a list of numpy arrays, is that possible? Any idea how to overcome this? . - if I switch to a batch size of 2, training does not work anymore and I end up with this error:; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-21-54ffb4970400> in <module>; 3 augmenter=augmenter_fn(ZOOM_RATIO, ZOOM_PROB),; 4 epochs=200, #100,; ----> 5 seed=42). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/models/model3d.py in train(self, X, Y, validation_data, augmenter, seed, epochs, steps_per_epoch, workers); 480 callbacks=self.callbacks, verbose=1,; 481 # set validation batchsize to training batchsize (only works in tf 2.x); --> 482 **(dict(validation_batch_size = self.config.train_batch_size) if _tf_version_at_least(""2.2.0"") else {})); 483 ; 484 self._training_finished(). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs); 64 def _method_wrapper(self, *args, **kwargs):; 65 if not self._in_multi_worker_mode(): # pylint: disable=protected-access; ---> 66 return method(self, *args, **kwargs); 67 ; 68 # Running inside `run_distribute_coordinator` already. /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing); 813 work",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:4000,validat,validation,4000,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['validat'],['validation']
Testability," args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1144, in run_setup; run_setup(setup_script, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 253, in run_setup; raise; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 166, in save_modules; saved_exc.resume(); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 141, in resume; six.reraise(type, exc, self._tb); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 154, in save_modules; yield saved; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 250, in run_setup; _execfile(setup_script, ns); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 45, in _execfile; exec(code, globals, locals); File ""/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/easy_install-d8fAD9/numpy-1.19.0/setup.py"", line 30, in <module>; re.MULTILINE | re.DOTALL).groups()[0]; RuntimeError: Python version >= 3.6 required.; ----------------------------------------; ERROR: Command errored out with exit status 1: python setup.py egg_info ",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:6052,sandbox,sandbox,6052,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['sandbox'],['sandbox']
Testability,"--------------------------------------------------------------------------+; ```. . - Type:. `$ nvcc -V`. . Output:. ```; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2020 NVIDIA Corporation; Built on Thu_Jun_11_22:26:48_Pacific_Daylight_Time_2020; Cuda compilation tools, release 11.0, V11.0.194; Build cuda_11.0_bu.relgpu_drvr445TC445_37.28540450_0; ```. . 3. Open the anaconda terminal and create the stardist conda environment. `$ conda create -n stardist pip python==3.8`. . 4. Activate the conda environment. `$ conda activate stardist`. . 5. Install *numpy*. `$ conda install numpy`. 6. Install *tesorflow-gpu* nightly using pip (tf-nightly-gpu 2.5.0.dev20201212). `pip install tf-nightly-gpu`. 7. Follow the installation instructions for stardist. https://github.com/mpicbg-csbd/stardist. . <!--I got a compatibility issue error with h5py library version. I just continue with the installation and everything seems to work correctly-->. 8. Install the NEUBIAS academy Jupyter Notebook for testing stardist installation . `$ git clone https://github.com/maweigert/neubias_academy_stardist.git`. ##### Run stardist-gpu on the NVIDIA RTX 3080. 1. Start the *Jupyter Notebook* . . 2. Comment the following lines:. ```; $ #%tensorflow_version 1.x; ; $ #!pip install stardist; ```. . 3. The direct downloads didn't work for me, I just comment these lines out and downloaded the sample data before. ```; '''; download_and_extract_zip_file(; url = 'https://github.com/mpicbg-csbd/stardist/releases/download/0.1.0/dsb2018.zip',; targetdir = 'data',; verbose = 1,; ); '''; ```. 4. To avoid TF GPU out of memory issues I add the following code:. ```python; import tensorflow as tf; gpus = tf.config.experimental.list_physical_devices('GPU'); tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4092)]); ```. . Output:. ```; 2020-12-13 07:44:01.102879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found ",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:4097,test,testing,4097,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,1,['test'],['testing']
Testability,".framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1158, in build_and_install; self.run_setup(setup_script, setup_base, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1144, in run_setup; run_setup(setup_script, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 253, in run_setup; raise; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 166, in save_modules; saved_exc.resume(); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 141, in resume; six.reraise(type, exc, self._tb); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 154, in save_modules; yield saved; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 250, in run_setup; _execfile(setup_script, ns); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 45, in _execfile; exec(code, globals, locals); File ""/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/easy_install-d8fAD9/numpy-1.19.0/setup.py"", line 30, in <module>; re.MULTILINE | re.DOTALL).groups()[0]; RuntimeEr",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:5900,sandbox,sandbox,5900,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['sandbox'],['sandbox']
Testability,"> All in all, it seems that for some reason StarDist is not able to create any candidates for the thinner stack. At the thickness of 4 z-layers it is expected that the largest cells do not neatly fit into the stack, but there should be plenty of smaller cells that I would think should still create candidates. So I would presume there has to be a reason for why none are created?. I don't understand. Are cells really only 4 pixels (or even less for ""smaller cells"") in Z? Was the model trained with such data?. > I did try that some months back by adding empty slices but it caused bloating of the labels and was not viable. But now that I think about it there probably is a better way for padding the image, such as duplicating the edge layers to provide continuity in intensities. You should pad the image with ""background"" values (whatever that means for your image) to not ""bloat"" the segmented objects sizes. > predicting instances with prob_thresh = 0.01 and nms_thresh = 0.4; > found 0 candidates. Something is clearly wrong here. There shouldn't be a need to use such a very small `prob_thresh` value, unless the model is badly trained or not suitable for the given image. Can you share a/some representative training images and also a/some problematic test images?. Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/133#issuecomment-832682530:1263,test,test,1263,,https://github.com/stardist/stardist/issues/133#issuecomment-832682530,1,['test'],['test']
Testability,"> Hello everyone, the fractional offsets issue should be corrected now in the latest DeepImageJ [release](https://github.com/deepimagej/deepimagej-plugin/releases/tag/2.1.11). I'm testing this release right now on Ubuntu 20.04 (without GPU acceleration). I still see the same behavior when adding a model (dialog quickly appears and disappears), but the model seems to be actually installed this time. It was quite confusing to me that there was no user feedback whether a model was successfully installed or not. Unfortunately, I get an error when I open ""DeepImageJ Run"", select my newly-installed model, and then click on ""Test model"". First, nothing happens but one CPU core is used 100%. After a couple of minutes, the Fiji Console pops up with the following error message:. <details>; <summary>Expand to show long error message</summary>. ```; [INFO] No TF library found in /home/uwe/Downloads/fiji-linux64/Fiji.app/lib/.; Exception in thread ""AWT-EventQueue-0"" java.lang.StackOverflowError; 	at java.util.regex.Pattern$Loop.match(Pattern.java:4767); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3779); 	at java.util.regex.Pattern$Branch.match(Pattern.java:4606); 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4660); 	at java.util.regex.Pattern$Loop.match(Pattern.java:4787); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3779); 	at java.util.regex.Pattern$Branch.match(Pattern.java:4606); 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4660); 	at java.util.regex.Pattern$Loop.match(Pattern.java:4787); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1042890741:180,test,testing,180,,https://github.com/stardist/stardist/pull/171#issuecomment-1042890741,1,['test'],['testing']
Testability,"> However DeepImageJ should already be able to run Stardist. I have succesfully run it in my computer. Instead of using the ""Test model"" button, please try opening an image, then selecting the model and finally running the model pressing ""ok"". Ah, I didn't expect that DeepImageJ works like this. Wouldn't it be more intuitive to have separate menu items for model testing and running?. > the ""Test model"" button should be fixed now, feel free to test the new release with the funcionality fixed:; > https://github.com/deepimagej/deepimagej-plugin/releases/tag/2.1.12. I can confirm that model testing and running on a different opened image works for me now. > @uschmidt83 I think we can assume it works now in deepImageJ. Should we go ahead and merge it?. There's one issue I don't understand yet. Why did we bother to put the `stardist_postprocessing.ijm` macro into the exported model zip file if there's no way to run this as *Postprocessing* from DeepImageJ?. When I did run the postprocessing macro from the Script Editor (opened manually from the `Fiji.app/models/<model_name>` folder) after DeepImageJ prediction, it showed me an error `[ERROR] Could not find input image with name/title ""scores"".` in the Console. I then tried the exact same thing again, and it worked as intended without errors ðŸ¤·â€â™‚ï¸. > (Also there are still some failing tests, but this seems to be unrelated to the changes here.). Yes, those are unrelated. The bioimage.io tests are only active in Github action tests with `tensorflow<2` in the name, and those run through just fine.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1047694133:365,test,testing,365,,https://github.com/stardist/stardist/pull/171#issuecomment-1047694133,6,['test'],"['test', 'testing', 'tests']"
Testability,"> I can confirm that model testing and running on a different opened image works for me now. Great. thanks for checking!. > There's one issue I don't understand yet. Why did we bother to put the `stardist_postprocessing.ijm` macro into the exported model zip file if there's no way to run this as _Postprocessing_ from DeepImageJ?. We want to support this in deepImageJ but are currently a bit out of manpower to implement it. There's probably a relatively simple solution. But also with the given solution, the script is at least packaged with the model, so that it can be applied manually. ; We will work on better post-processing integration, and there is probably a simple solution, but we don't want to block releasing the model zoo for now to incorporate it. We can update the readme to explain this, but I would suggest to do this in a follow-up PR as this one is pretty large already and it's not critical to the functionality. > When I did run the postprocessing macro from the Script Editor (opened manually from the `Fiji.app/models/<model_name>` folder) after DeepImageJ prediction, it showed me an error `[ERROR] Could not find input image with name/title ""scores"".` in the Console. I then tried the exact same thing again, and it worked as intended without errors man_shrugging. @esgomezm @carlosuc3m do you have any idea how to fix this?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1047715559:27,test,testing,27,,https://github.com/stardist/stardist/pull/171#issuecomment-1047715559,1,['test'],['testing']
Testability,"> I was wondering if there is an easy way to train the network using an **ImageDataGenerator**. You could use a `Sequence` to do that, e.g. see https://github.com/stardist/stardist/issues/107. > I would like to perform transfer learning using your model **_'2D_versatile_fluo'_**. Any idea how I can do this?. This is how you can duplicate a pretrained model and prepare it for transfer learning:. ```python; # imports; from stardist.models import StarDist2D, keras_import; import shutil; from pathlib import Path; keras = keras_import(). # name of your model; my_name = '2D_versatile_fluo_FINETUNED'. # load pretrained model and make a copy to local folder; model_pretrained = StarDist2D.from_pretrained('2D_versatile_fluo'); if not Path(my_name).exists():; shutil.copytree(model_pretrained.logdir, my_name). # load your duplicate of the pretrained model; my_model = StarDist2D(None, my_name). # change optimizer, especially to use smaller learning rate for finetuning; # (e.g. 1e-5, you have to try what works best); my_model.prepare_for_training(keras.optimizers.Adam(1e-5)). # finetune model with your data...; # my_model.train(...); ```",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/172#issuecomment-964010767:792,log,logdir,792,,https://github.com/stardist/stardist/issues/172#issuecomment-964010767,1,['log'],['logdir']
Testability,"> This is working locally now. (Needs the changes in [bioimage-io/core-bioimage-io-python#142](https://github.com/bioimage-io/core-bioimage-io-python/pull/142)). Thanks, I updated this locally. > Also, I can only run one of the tests successfully at a time, because tensorflow does not properly clear the gpu. Do you get `tensorflow.python.framework.errors_impl.FailedPreconditionError: 2 root error(s) found.`?. I think that's not the reason, rather TensorFlow is in a bad state after running `model.export_TF` once. This is a known issue, and I currently don't know how to workaround that.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-968756455:228,test,tests,228,,https://github.com/stardist/stardist/pull/171#issuecomment-968756455,1,['test'],['tests']
Testability,"> `If you open a new topic, please provide a clear and concise description to understand and ideally reproduce the issue you're having (e.g. including a code snippet, Python script, or Jupyter notebook).`. [code.zip](https://github.com/stardist/stardist/files/6434116/code.zip); We cut out the training code and test code according to the training steps provided, and the rest of the parameters are kept as defaults. We directly use this training file to train and test, and the result seems to be wrong.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/138#issuecomment-833459146:312,test,test,312,,https://github.com/stardist/stardist/issues/138#issuecomment-833459146,2,['test'],['test']
Testability,"Hi @GFleishman,. Thanks for all the work! Two things that I think are still worth pondering:. 1) The `max_distance` cutoff for the spatial data structure should ideally be chosen such that no potential intersecting polygon is missed. Some options are ; * (Mean) `max_distance= 2*(np.mean(dist)+np.std(dist))` what is used in the PR, but might miss some pairs ; * (Max) `max_distance= 2*np.max(dist)`, preferable as it won't miss any pairs but which leads to order of magnitude more pairs. . There likely won't be any difference between (Mean) and (Max) in practice (as your initial comparison showed), but I am still a bit wary of not having that guarantee. . 2) Memory: I tried prediction on a small crop (with ~250k candidates) of the test data above:; * without kdtree: 15.4 GB, 97s ; * with kdtree (Mean): 15.5 GB, 98s ; * with kdtree (Max): 39.2 GB, 121s . So using (Max) will clearly take a hit on memory...",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-606352621:737,test,test,737,,https://github.com/stardist/stardist/pull/40#issuecomment-606352621,1,['test'],['test']
Testability,"Hi @esgomezm,. > We are actually trying to export the model using TF2.x (the idea is to test it with the new java library in Fiji + deepImageJ). Is the new Java library for deep learning ready yet? What is the name and where can I find it?. > Is it the export being forced inside StarDist to have a model compatible with TF1.15?. Yes, the model export in the CSBDeep and StarDist Python packages was always meant for use in Fiji, which has only ever supported TensorFlow 1.x. > I'm using the bioimageio library (version 0.5.8) with TF2.11 to export TF and Keras models, and everything works perfectly. Using the bioimageio library where to export the model? And ""everything works perfectly"" where?. > A potential solution (probably you have a better one), could be to avoid those imports if TF version > 2.3? Until version 2.3, the compatibility with CSBDeep & StarDist is ensured, but for later versions is not unless the plugins are updated. Also, my feeling is that the export fails with TF>2.3. I really need to understand the current situation first... I don't even know how to export the model such that it will work with the new Java library.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-1453562117:88,test,test,88,,https://github.com/stardist/stardist/issues/68#issuecomment-1453562117,1,['test'],['test']
Testability,"Hi @imand500 ,. Sorry for the late reply. . > Since the model is not saved as h5 and it's really dependent on the configuration you made. If a pretrained models is loaded; ```python ; model = StarDist2D.from_pretrained(""2D_versatile_he""); ```; it is actually downloaded and stored as a normal stardist model in your keras cache directory. You can find the location like so:; ```python; print(model.logdir); ```; So you could simply copy that folder somewhere, create a new model from that folder and then continue training with your own data. Hope that helps,. M",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/127#issuecomment-830683343:398,log,logdir,398,,https://github.com/stardist/stardist/issues/127#issuecomment-830683343,1,['log'],['logdir']
Testability,"Hi Greg, . thanks a lot for this contribution - it already looks very promising! . > I evaluated NMS time over a wide number of polygon candidates. Here are the results (note the log_10 scale on the vertical axis):. Thats a nice speed up! The 6Mio candidates are for the whole stack, I assume? So using `predict_big` doesn't have any effect on the prediction time?. I'm slightly worried about the additional memory footprint that a full kdtree of all candidates might bring - did you try measuring the peak memory usage? Additionally I suspect there is quite some heavy differences even for different kdtree implementations e.g. there are `KDTree` and `BallTree` from `scikit-learn`. Here's a short test I did with random points in 3D and those different implementation ; ""ckdtree"" -> `scipy.spatial.ckdtree` ; ""kdtree"" -> `sklearn.neighbors.KDTree`; ""ball"" -> `sklearn.neighbors.BallTree`. <img width=""850"" alt=""Screenshot 2020-03-19 at 18 10 04"" src=""https://user-images.githubusercontent.com/11042162/77096353-85333200-6a0f-11ea-9f97-926c8bd22cd9.png"">; ; shows that i) there are quite some difference esp. for memory, and ii) the overall memory footprint is pretty high (e.g. 18GB for a mere 30k points for ckdtree). So having a similar plot as above but for memory consumption in your example would be nice and instructive. Additionally moving to `BallTree` might make things more efficient...",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-601317246:699,test,test,699,,https://github.com/stardist/stardist/pull/40#issuecomment-601317246,1,['test'],['test']
Testability,Thank you for the reply @maweigert. ; I was using the function to test some morphological operations (not deep learning) based on distances. For my operations I needed the exact l2-distance to the pixel and I was confused why I always got integer distances.,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/12#issuecomment-512886685:66,test,test,66,,https://github.com/stardist/stardist/issues/12#issuecomment-512886685,1,['test'],['test']
Testability,"Thank you very much for the detailed answer, it helps me a lot. All right, I'm going to:; - switch to `model.predict_instances` as my test stacks are of similar sizes as my training stacks (~400x1000x1000); - play with `n_tiles` : should I just set this as low as possible as long as it fits in my GPU or should I do something smarter?; - try to make bigger train_patch_size : I'm a bit uncomfortable setting batch_size lower than 4 (or even 8) to be honest, do you perform gradient accumulation under the hood? Maybe I'm just freaking out for nothing but don't you feel that a batch_size of 2 is a bit too small to get a nice gradient descent?. Also I wanted to ask you about something else. ; At the moment I don't have nice labels that have been curated by humans (I might find some time to do this but later), but I have good enough binary masks that allows me to train algorithms like 3DUnets.; My problem is that some cells are close to each other, so my initial labels are not very good at differientiating adjacent cells (which creates big objects that don't look like cells at all) and same goes with the resulting UNet segmentation, that's why I decided to switch to StarDist3D.; First results from StarDist seem already much better at creating nice round cells, but in order to train StarDist I simply separated my binary mask using connex component : so my ground truth for StarDist never really shows adjacent cells (as they are merged into one same label).; Because of this I don't really trust the automated nms_thresh search function (labels are wrong so it's going to be hard for the search). So I played a bit with the nms_thresh to visually inspect the results but I'd like to understand better what 0.4 or 0.05 means for example. Does 0.4 means that I would tolerate that 40% of the volume of two cells (or a proxy of the volume) are actually overlapping? In terms of physics and biology, I don't expect my cells to be able to overlap at all, they can touch each other but that's i",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691334428:134,test,test,134,,https://github.com/stardist/stardist/issues/87#issuecomment-691334428,1,['test'],['test']
Testability,The `inner` test is simply a sphere intersection so it is negligible and I would have expected that the overall time would be reduced. Maybe all those threads are competing too much as you suspected. You could try running your script with `OMP_NUM_THREADS=8 python script.py` and see whether that helps.,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/277#issuecomment-2236558174:12,test,test,12,,https://github.com/stardist/stardist/issues/277#issuecomment-2236558174,1,['test'],['test']
Testability,"This is working locally now. (Needs the changes in https://github.com/bioimage-io/core-bioimage-io-python/pull/142); Also, I can only run one of the tests successfully at a time, because tensorflow does not properly clear the gpu.; Do you maybe have a helper function for that in CSBDeep @maweigert, @uschmidt83?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-966429985:149,test,tests,149,,https://github.com/stardist/stardist/pull/171#issuecomment-966429985,1,['test'],['tests']
Testability,"is great work, this repo is truly amazing. Thanks, I'm glad you like it!. > ```; > conf = Config3D (; > ...; > train_patch_size = (48,192,192), # should I make something more cubic?; > train_batch_size = 4, # Tried to set the largest batch size possible without get OOM error; > train_epochs = 50; > ); > ```. Since your object sizes are almost isotropic, you could try to use `(160,192,192)` or similar and decrease the batch size if needed, but I doubt it'll make much of a difference. Of course, if the objects are (often) larger than 48 pixels, it can make a big difference to enlarge the patch size such that objects are fully included.; The number of epochs seems a bit small, but you seem to know what you're doing.; ; > ```; > n,h,w = X_test[2].shape; > labels, details = model.predict_instances_big(X_test[2],; > axes='ZYX',; > block_size=(n, int(h/2), int(w/2)), #conf.train_patch_size; > min_overlap=16,; > context = (30,30,30), #(30,30,30),; > prob_thresh=0.4; > ); > ```. How big are the test stacks, i.e. what is `X_test[2].shape`?; If it has similar size than the training stacks, you can simply use `model.predict_instances`, which doesn't require you to set this many parameters. The only important one would be `n_tiles`, which determines into how many tiles the input stack will be chopped before prediction runs on the GPU. Please try using this function first. The function `model.predict_instances_big` is only intended for *really* big images, which can't even be loaded in RAM or which require excessive computation and RAM requirements during the CPU-based non-maximum suppression step (to prune redundant object predictions) that runs after the CNN prediction on the GPU. > 1- block_size : I understand that we are treating the large volume with smaller blocks, but shouldn't block_size be the same as training size? If I set `block_size=conf.train_patch_size` I end up with issues with context and overlap, probably because I have one small dimension in training (48). No, ",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:1045,test,test,1045,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,1,['test'],['test']
Testability,"k/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1144, in run_setup; run_setup(setup_script, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 253, in run_setup; raise; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 166, in save_modules; saved_exc.resume(); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 141, in resume; six.reraise(type, exc, self._tb); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 154, in save_modules; yield saved; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 250, in run_setup; _execfile(setup_script, ns); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 45, in _execfile; exec(code, globals, locals); File ""/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/easy_install-d8fAD9/numpy-1.19.0/setup.py"", line 30, in <module>; re.MULTILINE | re.DOTALL).groups()[0]; RuntimeError: Python version >= 3.6 required.; ----------------------------------------; ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.; ```. Thank you",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:6212,sandbox,sandbox,6212,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,5,"['log', 'sandbox']","['logs', 'sandbox']"
Testability,"lpful, I think you're probably right that this is the relevant code to modify for my data. However, this loopy python code is not practical to actually run on my data, where I have several hundred cells per image. It seems the default for my local installation is to run the compiled cpp version:; https://github.com/mpicbg-csbd/stardist/blob/38454feca61804049a3afae34add804e0b91517c/stardist/lib/stardist2d.cpp#L53-L59. I'd like to modify this and rebuild - but I'm unsure how to do it, since the entire build was part of the `pip install stardist` and there is no documentation on building from source. @maweigert @uschmidt83 Can you guys provide any information about how to rebuild the cpp libraries in stardist? I'm using MacOS and gcc-9, but will need to repeat this process later on red hat linux. Edit: This turned out to be easy. I just made my modifications to the cpp code and reran `python setup.py sdist` from the source directory, then `python setup.py install` For MacOSX you need `CC=gcc-9 CXX=g++-9 python setup.py install`. Since you've been so helpful, I'd like to try and help with your issue as well - but since it occurs primarily on reconstructions of the test data polygons, it's probably something different. Can you rule out that it's just a phenomenon of the data, i.e. different intensity characteristics at the boundaries of training/testing data - so the model just learned to produce more smooth/conservative boundaries?. Edit: I forced the python versions to run with a 4 in the denominator for the step size; it is significantly slower than the CPP code and not practically useful, but it does definitely solve the roughness problem for small label areas:. ![Screen Shot 2020-02-07 at 11 17 10 AM](https://user-images.githubusercontent.com/8507206/74045936-a5e18400-499b-11ea-93cf-340bef4e5d6f.png). <img width=""868"" alt=""Screen Shot 2020-02-07 at 11 17 12 AM"" src=""https://user-images.githubusercontent.com/8507206/74045949-aaa63800-499b-11ea-9c76-3ef96e31b905.png"">",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-583469917:1209,test,test,1209,,https://github.com/stardist/stardist/issues/33#issuecomment-583469917,2,['test'],"['test', 'testing']"
Testability,"m; dists = self.install_eggs(spec, download, tmpdir); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 890, in install_eggs; return self.build_and_install(setup_script, setup_base); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1158, in build_and_install; self.run_setup(setup_script, setup_base, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1144, in run_setup; run_setup(setup_script, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 253, in run_setup; raise; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 166, in save_modules; saved_exc.resume(); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 141, in resume; six.reraise(type, exc, self._tb); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 154, in save_modules; yield saved; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 250, in run_setup; _execfile(setup_script, ns); File ""/System/Library/Framework",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:5605,sandbox,sandbox,5605,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['sandbox'],['sandbox']
Testability,"s/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 679, in easy_install; return self.install_item(spec, dist.location, tmpdir, deps); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 705, in install_item; dists = self.install_eggs(spec, download, tmpdir); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 890, in install_eggs; return self.build_and_install(setup_script, setup_base); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1158, in build_and_install; self.run_setup(setup_script, setup_base, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1144, in run_setup; run_setup(setup_script, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 253, in run_setup; raise; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 166, in save_modules; saved_exc.resume(); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 141, in resume; six.reraise(type, exc, self._tb); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 154, in save_modules; yield saved; File ""/System/Library/Frameworks/Pytho",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:5314,sandbox,sandbox,5314,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['sandbox'],['sandbox']
Testability,"tion on building from source. I think the package gets built when installing with pip, on the Mac I had to follow these instructions by @maweigert: https://github.com/mpicbg-csbd/stardist/issues/21. I also have stardist installed on a Ubuntu 18 machine for the training and there was no problem. Otherwise, you could speed up the python loops using `numba.jit` e.g. directly from your jupyter notebook like this:. ```python; from stardist.geometry import geom2d; from numba import jit. def _py_star_dist_modified(a, n_rays=32):; # (np.isscalar(n_rays) and 0 < int(n_rays)) or _raise(ValueError()); n_rays = int(n_rays); a = a.astype(np.uint16,copy=False); dst = np.empty(a.shape+(n_rays,),np.float32). for i in range(a.shape[0]):; for j in range(a.shape[1]):; value = a[i,j]; if value == 0:; dst[i,j] = 0; else:; st_rays = np.float32((2*np.pi) / n_rays); for k in range(n_rays):; phi = np.float32(k*st_rays); dy = np.cos(phi)#/100.; dx = np.sin(phi)#/100.; x, y = np.float32(0), np.float32(0). while True:; x += dx; y += dy; ii = int(round(i+x)); jj = int(round(j+y)); if (ii < 0 or ii >= a.shape[0] or; jj < 0 or jj >= a.shape[1] or; value != a[ii,jj]):; dist = np.sqrt(x*x + y*y); dst[i,j,k] = dist; break; return dst. geom2d._py_star_dist = jit(_py_star_dist_modified); ```. This version performs about the same as the cpp one:; ```; %timeit relabel_image_stardist(test_lbl, n_rays=32, mode='python'); 452 Âµs Â± 7.97 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each); %timeit relabel_image_stardist(test_lbl, n_rays=32, mode='cpp'); 493 Âµs Â± 8.6 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each); ```. > Can you rule out that it's just a phenomenon of the data, i.e. different intensity characteristics at the boundaries of training/testing data - so the model just learned to produce more smooth/conservative boundaries?. Hmm you're right that I cannot rule out that it's a training thing. Will definitely train again with a dataset in which I have many shapes represented. Thanks!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-583505792:1962,test,testing,1962,,https://github.com/stardist/stardist/issues/33#issuecomment-583505792,1,['test'],['testing']
Testability,"ub.com/mpicbg-csbd/stardist/issues/36#issuecomment-594974089) I was able to run the job with a single cpu (# cores requested and maximum amount of RAM available are mixed on our cluster; each core comes with 15GB RAM). However the remaining runtime predicted by tqdm after a few iterations had accumulated was something like 36 hours - so I cancelled it and gave up on it. Uwe reminded me that with one core, I likely had a maximum of 2 threads, but I did not go back and try `predict_big` with more cores. Do you think `predict_big` with say 32 cores (64 threads) could finish in less than an hour? If so I will give it another try. For different kdtree implementations - I did look through [this article written by the scikit-learn kdtree and balltree developer](https://jakevdp.github.io/blog/2013/04/29/benchmarking-nearest-neighbor-searches-in-python/), but I didn't think there was enough differences in his graphs to justify introducing a new dependency (I don't think scikit-learn was a dependency before?) and I also preferred something with an underlying C++ implementation, as there are more big speed ups available if the kdtree data structure is available for query directly within the NMS algorithm. Unfortunately the only RAM measurements I have are the maximum and average usage (over the full run, prediction and NMS) for prediction on the full volumes both w/ and w/o the kdtree. The command in both cases was:. `pred, det = model.predict_instances(image_norm, verbose=True, n_tiles=(4, 8, 2))`; On a volume with 860x1676x301 voxels and 6.32e6 polygon candidates. w/o kdtree max: 434.439GB; w/ kdtree max: 430.482GB. w/o kdtree avg: 221.672GB; w/ kdtree avg: 297.733GB. I'm thinking that the kdtree and the query results for this many candidates took something like 70GB RAM. One thing that I haven't looked at in the code - it seems like the biggest expansion of data in the method is from raw data --> prediction; e.g. in my case I'm using 128 rays, so I need (129 * 2) more RAM re",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-601381585:1032,benchmark,benchmarking-nearest-neighbor-searches-in-python,1032,,https://github.com/stardist/stardist/pull/40#issuecomment-601381585,1,['benchmark'],['benchmarking-nearest-neighbor-searches-in-python']
Testability,va:4660); 	at java.util.regex.Pattern$Loop.match(Pattern.java:4787); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3779); 	at java.util.regex.Pattern$Branch.match(Pattern.java:4606); 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4660); 	at java.util.regex.Pattern$Loop.match(Pattern.java:4787); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3779); 	at java.util.regex.Pattern$Branch.match(Pattern.java:4606); 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4660); 	at java.util.regex.Pattern$Loop.match(Pattern.java:4787); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3779); 	at java.util.regex.Pattern$Branch.match(Pattern.java:4606); 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4660); 	at java.util.regex.Pattern$Loop.match(Pattern.java:4787); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3779); 	at java.util.regex.Pattern$Branch.match(Pattern.java:4606); 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4660); 	at java.util.regex.Pattern$Loop.match(Pattern.java:4787); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3779); [DEBUG] Close Tensorflow services; [DEBUG] End execution; ```; </details>. Am I doing something wrong? Can someone else test this? I've uploaded the model [here](https://we.tl/t-3kTliJvgoK) (link expires in 7 days).,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1042890741:65973,test,test,65973,,https://github.com/stardist/stardist/pull/171#issuecomment-1042890741,1,['test'],['test']
Usability," > conf = Config3D (; > ...; > train_patch_size = (48,192,192), # should I make something more cubic?; > train_batch_size = 4, # Tried to set the largest batch size possible without get OOM error; > train_epochs = 50; > ); > ```. Since your object sizes are almost isotropic, you could try to use `(160,192,192)` or similar and decrease the batch size if needed, but I doubt it'll make much of a difference. Of course, if the objects are (often) larger than 48 pixels, it can make a big difference to enlarge the patch size such that objects are fully included.; The number of epochs seems a bit small, but you seem to know what you're doing.; ; > ```; > n,h,w = X_test[2].shape; > labels, details = model.predict_instances_big(X_test[2],; > axes='ZYX',; > block_size=(n, int(h/2), int(w/2)), #conf.train_patch_size; > min_overlap=16,; > context = (30,30,30), #(30,30,30),; > prob_thresh=0.4; > ); > ```. How big are the test stacks, i.e. what is `X_test[2].shape`?; If it has similar size than the training stacks, you can simply use `model.predict_instances`, which doesn't require you to set this many parameters. The only important one would be `n_tiles`, which determines into how many tiles the input stack will be chopped before prediction runs on the GPU. Please try using this function first. The function `model.predict_instances_big` is only intended for *really* big images, which can't even be loaded in RAM or which require excessive computation and RAM requirements during the CPU-based non-maximum suppression step (to prune redundant object predictions) that runs after the CNN prediction on the GPU. > 1- block_size : I understand that we are treating the large volume with smaller blocks, but shouldn't block_size be the same as training size? If I set `block_size=conf.train_patch_size` I end up with issues with context and overlap, probably because I have one small dimension in training (48). No, `block_size` should be much larger in comparison to `min_overlap` and `context`.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:1148,simpl,simply,1148,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,1,['simpl'],['simply']
Usability," as low as possible as long as it fits in my GPU or should I do something smarter?. Yes, essentially. > * try to make bigger train_patch_size : I'm a bit uncomfortable setting batch_size lower than 4 (or even 8) to be honest, do you perform gradient accumulation under the hood? Maybe I'm just freaking out for nothing but don't you feel that a batch_size of 2 is a bit too small to get a nice gradient descent?. Keep in mind that every pixel (that belongs to an object) contributes a gradient signal. We often trained with a batch size of 1 and didn't see a problem. > My problem is that some cells are close to each other, so my initial labels are not very good at differientiating adjacent cells (which creates big objects that don't look like cells at all) and same goes with the resulting UNet segmentation, that's why I decided to switch to StarDist3D.; > First results from StarDist seem already much better at creating nice round cells, but in order to train StarDist I simply separated my binary mask using connex component : so my ground truth for StarDist never really shows adjacent cells (as they are merged into one same label). I don't see how you can overcome this problem without fixing the ground truth (at least in part). What we have seen is that StarDist can potentially be trained with labelling errors â€“ as long as these are not biased, which they seem to be in your case (i.e. touching nuclei are always incorrectly merged). > Because of this I don't really trust the automated nms_thresh search function (labels are wrong so it's going to be hard for the search). (As I said above, I wouldn't trust the results in general.); Note that the automatic threshold finding is in some sense just for convenience to set good thresholds that work well on average. Depending on the application, one would, e.g., increase the probability threshold to prefer more accurate predictions (fewer false positives) at the expense of missing some nuclei (more false negatives). > Does 0.4 means",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-692842138:1026,simpl,simply,1026,,https://github.com/stardist/stardist/issues/87#issuecomment-692842138,1,['simpl'],['simply']
Usability,"2. Using numba frees the gpu, BUT leaves it in a state that is not valid and pytorch then fails with the following error:; ```; RuntimeError: CUDA error: invalid argument ; Traceback (most recent call last): ; File ""run_all_workflows.py"", line 62, in <module> ; run_all_workflows() ; File ""run_all_workflows.py"", line 50, in run_all_workflows ; name, rt = run_instance_analysis2(config) ; File ""/g/kreshuk/pape/Work/covid/batchlib/antibodies/instance_analysis_workflow2.py"", line 123, in run_instance_analysis2 ; ignore_failed_outputs=config.ignore_failed_outputs) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/workflow.py"", line 104, in run_workflow ; raise e ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/workflow.py"", line 99, in run_workflow ; state = job(folder, **run_kwargs) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/base.py"", line 421, in __call__ ; super().__call__(folder, **kwargs) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/base.py"", line 210, in __call__ ; self.run(input_files, output_files, **kwargs) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/analysis/cell_level_analysis.py"", line 116, in run ; _save_all_stats(input_file, output_file) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/analysis/cell_level_analysis.py"", line 101, in save_all_stats ; sample = self.load_sample(in_file, device=device) ; File ""/g/kreshuk/pape/Work/covid/batchlib/batchlib/analysis/cell_level_analysis.py"", line 52, in load_sample ; marker = torch.FloatTensor(marker.astype(np.float32)).to(device) ; RuntimeError: CUDA error: invalid argument; ```; (If I run the same task without tensorflow being involved it runs through without any issues). 3. I also tried running in a sub-process as suggested e.g. [here](https://stackoverflow.com/questions/39758094/clearing-tensorflow-gpu-memory-after-model-execution), but it results in the same issue as with 2.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/44#issuecomment-615880829:1800,clear,clearing-tensorflow-gpu-memory-after-model-execution,1800,,https://github.com/stardist/stardist/issues/44#issuecomment-615880829,1,['clear'],['clearing-tensorflow-gpu-memory-after-model-execution']
Usability,"8, in build_and_install; self.run_setup(setup_script, setup_base, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1144, in run_setup; run_setup(setup_script, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 253, in run_setup; raise; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 166, in save_modules; saved_exc.resume(); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 141, in resume; six.reraise(type, exc, self._tb); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 154, in save_modules; yield saved; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 250, in run_setup; _execfile(setup_script, ns); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 45, in _execfile; exec(code, globals, locals); File ""/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/easy_install-d8fAD9/numpy-1.19.0/setup.py"", line 30, in <module>; re.MULTILINE | re.DOTALL).groups()[0]; RuntimeError: Python version >= 3.6 required.; ----------------------------------------; ERROR:",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:5950,resume,resume,5950,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['resume'],['resume']
Usability,> * create the python functionality to run a stardist bioimageio model + stardist postprocessing (I think I would just add this to bioimageio.core for simplicity). I can prototype that.,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-974111475:151,simpl,simplicity,151,,https://github.com/stardist/stardist/pull/171#issuecomment-974111475,1,['simpl'],['simplicity']
Usability,"> > Is the new Java library for deep learning ready yet? What is the name and where can I find it?; > ; > Yes, here it is: https://github.com/bioimage-io/model-runner-java. Thanks for the link! A few questions:; - This is only going to work for models in the bioimage.io format, right?; - Will this library handle pre- and post-processing?; - Is it going to support tile-based prediction, i.e. chopping the input image into tiles, running the model on each of them, and then re-assembling the individual results?. > > I really need to understand the current situation first... I don't even know how to export the model such that it will work with the new Java library.; > ; > The bioimageio.core library will export it directly in a compatible format (as the model I linked in the previous lines). In the upcoming days, we will update deepImageJ with the java model runner so all the TF2 models can also be deployed in Fiji. I can let you know when the plugin is ready if you want. I'm first interested in using `model-runner-java` to replace the TF1-only CSBDeep library in Fiji, in order to run TF2-based models in StarDist.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-1460949099:37,learn,learning,37,,https://github.com/stardist/stardist/issues/68#issuecomment-1460949099,1,['learn'],['learning']
Usability,"> > The image generated by ""img = test_image_nuclei_2d()"" is not a HE slice image.; > ; > I don't really understand what you mean by that. Can you please post a [minimal example](https://en.wikipedia.org/wiki/Minimal_reproducible_example), otherwise its impossible for me to help you. Hi there! I am interested in training a new StarDist model using my own dataset. Can you please point me to the relevant documentation or resources that I can refer to for guidance? Thank you!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/226#issuecomment-1490364221:457,guid,guidance,457,,https://github.com/stardist/stardist/issues/226#issuecomment-1490364221,1,['guid'],['guidance']
Usability,"> All in all, it seems that for some reason StarDist is not able to create any candidates for the thinner stack. At the thickness of 4 z-layers it is expected that the largest cells do not neatly fit into the stack, but there should be plenty of smaller cells that I would think should still create candidates. So I would presume there has to be a reason for why none are created?. I don't understand. Are cells really only 4 pixels (or even less for ""smaller cells"") in Z? Was the model trained with such data?. > I did try that some months back by adding empty slices but it caused bloating of the labels and was not viable. But now that I think about it there probably is a better way for padding the image, such as duplicating the edge layers to provide continuity in intensities. You should pad the image with ""background"" values (whatever that means for your image) to not ""bloat"" the segmented objects sizes. > predicting instances with prob_thresh = 0.01 and nms_thresh = 0.4; > found 0 candidates. Something is clearly wrong here. There shouldn't be a need to use such a very small `prob_thresh` value, unless the model is badly trained or not suitable for the given image. Can you share a/some representative training images and also a/some problematic test images?. Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/133#issuecomment-832682530:1020,clear,clearly,1020,,https://github.com/stardist/stardist/issues/133#issuecomment-832682530,1,['clear'],['clearly']
Usability,"> Based on the tqdm estimates, the time will still be quite long, ~8 hours, but using only a single cpu (accessible on my cluster now due to lower RAM requirement) will significantly bring down the cost. Why are you only using a single CPU now? The non-maximum suppression will be much faster with more CPU/cores available. > Is there any interest in running the tiles in a cluster environment with DASK (or similar)?. We first want to roll out this functionality with a simple API for single-machine use. Running this in a distributed environment is certainly appealing, but makes everything _much_ more complicated, hence I can't promise you when/if we'll do it. If you have suggestions on how to do this, please tell us.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/36#issuecomment-597133170:471,simpl,simple,471,,https://github.com/stardist/stardist/issues/36#issuecomment-597133170,1,['simpl'],['simple']
Usability,"> Can you give me some numbers to put this statement in perspective?. To give you some context:; We run stardist prediction on a DAPI channel to segment nuclei for a high throughput screening setup.; The individual images are approximately 1000x1000 and we have approximately a 1000 images per experiment (= 1 plate). . Running prediction / segmentation with stardist takes ~ 40 minutes for 1 experiment and ; it's clearly cpu bound (judging from the GPU utilization which I think is maybe between 5 and 10 percent on average). (I just call `predict_instances` for now, so I don't really know what the ratio between time spent prediction and time spent on NMS is). We also run two other processing steps: predicting foreground/background and boundaries with a pytorch network (on a different channel) and watersheds (using nuclei as seeds and boundary predictions as height map). Both of these steps can be done in under 5 - 10 minutes each, so stardist is the clear bottleneck right now. (For the pytorch prediction I stack images across the batch axis and the watershed can just be parallelized over multiple CPUs, so this comparison is not really fair!). I can run some experiments next week to really see how long prediction vs NMS takes.; What are the functions I have to call on the model if I want to do this in two separate steps?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/39#issuecomment-612622104:415,clear,clearly,415,,https://github.com/stardist/stardist/issues/39#issuecomment-612622104,2,['clear'],"['clear', 'clearly']"
Usability,"> Have you tried finding a compatible Docker container?. Not yet, I am doing some reading.:-); I got this GPU to speed up some work from home but it seems not the case for now. I will look at the Docker container and post back my feedback. ; I also found something interesting here: . https://medium.com/@dun.chwong/the-simple-guide-deep-learning-with-rtx-3090-cuda-cudnn-tensorflow-keras-pytorch-e88a2a8249bc. At least could be an other option.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-740572700:230,feedback,feedback,230,,https://github.com/stardist/stardist/issues/104#issuecomment-740572700,2,"['feedback', 'simpl']","['feedback', 'simple-guide-deep-learning-with-rtx-']"
Usability,"> Hello everyone, the fractional offsets issue should be corrected now in the latest DeepImageJ [release](https://github.com/deepimagej/deepimagej-plugin/releases/tag/2.1.11). I'm testing this release right now on Ubuntu 20.04 (without GPU acceleration). I still see the same behavior when adding a model (dialog quickly appears and disappears), but the model seems to be actually installed this time. It was quite confusing to me that there was no user feedback whether a model was successfully installed or not. Unfortunately, I get an error when I open ""DeepImageJ Run"", select my newly-installed model, and then click on ""Test model"". First, nothing happens but one CPU core is used 100%. After a couple of minutes, the Fiji Console pops up with the following error message:. <details>; <summary>Expand to show long error message</summary>. ```; [INFO] No TF library found in /home/uwe/Downloads/fiji-linux64/Fiji.app/lib/.; Exception in thread ""AWT-EventQueue-0"" java.lang.StackOverflowError; 	at java.util.regex.Pattern$Loop.match(Pattern.java:4767); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3779); 	at java.util.regex.Pattern$Branch.match(Pattern.java:4606); 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4660); 	at java.util.regex.Pattern$Loop.match(Pattern.java:4787); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3779); 	at java.util.regex.Pattern$Branch.match(Pattern.java:4606); 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4660); 	at java.util.regex.Pattern$Loop.match(Pattern.java:4787); 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4719); 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4570); 	at java.util.regex.Pattern$CharProperty.match(Pattern.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1042890741:454,feedback,feedback,454,,https://github.com/stardist/stardist/pull/171#issuecomment-1042890741,1,['feedback'],['feedback']
Usability,"> Hi @talkhanz,; > ; > It's hard to understand what you mean (the left image above seems to be 8bit, but the normalized image should be 32 bit). Please describe in more detail what you did, what you expected, and what happened instead :). Im sorry if I was not clear. The image was converted to 8 bit while I was using FIJI (i.e not something that was done through code. The code produces 32 bit as you suggested). I basically ran the training notebook from the github example directory but with slight modification to the file reading processing. I have emailed the training and prediction .py files at your epfl email. I used the Allen Institute aicsimageio [https://allencellmodeling.github.io/aicsimageio/index.html](url) The dataset I used was from the Broad Institute. ( train images can be found from [https://data.broadinstitute.org/bbbc/BBBC024/BBBC024_v1_c00_highSNR_images_TIFF.zip](url) while the masks from [https://data.broadinstitute.org/bbbc/BBBC024/BBBC024_v1_c00_highSNR_foreground.zip](url). I was expecting elongated cellular segmentation like the ground truth available from the broad_institute. The snap for ground truth looks like this :; <img width=""486"" alt=""expected_"" src=""https://user-images.githubusercontent.com/63508490/91554922-77cc8d00-e949-11ea-811e-51307bb55ed3.PNG"">. This is what I got instead : ; <img width=""159"" alt=""actual_"" src=""https://user-images.githubusercontent.com/63508490/91554950-8155f500-e949-11ea-8598-ae7e882251d3.PNG"">. I hope I was a bit clearer,; Please let me know if you require anything else,; Best; Edit:; Just to add a little detail. The training data is being modified through normalization but the model seems to be predict the labels correctly given the normalized images it has received. Segmentation is correct based on the training images",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/84#issuecomment-682469217:261,clear,clear,261,,https://github.com/stardist/stardist/issues/84#issuecomment-682469217,2,['clear'],"['clear', 'clearer']"
Usability,"> Hi @uschmidt83, So if I am using a custom config should I write the whole config used instead of just the n_rays but also kernel size etc?. I don't understand what you mean. > But it is still counter intuitive to me because you have the config.json file in the same folder as the model weights so for the prediction step it should know the hyperparameters I used for the training right?. All I'm saying is that supplying a new config to an already trained model is not a good idea or the solution to your problem. As you said, it _should_ work by loading the config from disk with passing `config=None` to the model constructor.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-659566701:202,intuit,intuitive,202,,https://github.com/stardist/stardist/issues/68#issuecomment-659566701,1,['intuit'],['intuitive']
Usability,"> How does one reload a model after training in python ?; > If it simply calling the model with the same name and basedir ?. Yes. If you set `config=None` in the constructor, the model (and weights) will be loaded from the name/basedir given:; ```python; model = StarDist2D(config=None, name= ..., basedir = ...); ```",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/95#issuecomment-716120226:66,simpl,simply,66,,https://github.com/stardist/stardist/issues/95#issuecomment-716120226,1,['simpl'],['simply']
Usability,"> However DeepImageJ should already be able to run Stardist. I have succesfully run it in my computer. Instead of using the ""Test model"" button, please try opening an image, then selecting the model and finally running the model pressing ""ok"". Ah, I didn't expect that DeepImageJ works like this. Wouldn't it be more intuitive to have separate menu items for model testing and running?. > the ""Test model"" button should be fixed now, feel free to test the new release with the funcionality fixed:; > https://github.com/deepimagej/deepimagej-plugin/releases/tag/2.1.12. I can confirm that model testing and running on a different opened image works for me now. > @uschmidt83 I think we can assume it works now in deepImageJ. Should we go ahead and merge it?. There's one issue I don't understand yet. Why did we bother to put the `stardist_postprocessing.ijm` macro into the exported model zip file if there's no way to run this as *Postprocessing* from DeepImageJ?. When I did run the postprocessing macro from the Script Editor (opened manually from the `Fiji.app/models/<model_name>` folder) after DeepImageJ prediction, it showed me an error `[ERROR] Could not find input image with name/title ""scores"".` in the Console. I then tried the exact same thing again, and it worked as intended without errors ðŸ¤·â€â™‚ï¸. > (Also there are still some failing tests, but this seems to be unrelated to the changes here.). Yes, those are unrelated. The bioimage.io tests are only active in Github action tests with `tensorflow<2` in the name, and those run through just fine.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1047694133:317,intuit,intuitive,317,,https://github.com/stardist/stardist/pull/171#issuecomment-1047694133,1,['intuit'],['intuitive']
Usability,"> However, I noticed the absence of a `checkpoint.ckpt` file (instead there is a `variables.index` file). Is there a way to generate this `.ckpt` file?. I have never needed a checkpoint myself, hence I don't know. However, you can try to export one yourself since you have full access to the Keras model via `model.keras_model`. Here's the general documentation for the [SavedModel format](https://www.tensorflow.org/guide/saved_model), and here is a guide specific to [Training checkpoints](https://www.tensorflow.org/guide/checkpoint). Hope that helps. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/61#issuecomment-643508305:417,guid,guide,417,,https://github.com/stardist/stardist/issues/61#issuecomment-643508305,3,['guid'],['guide']
Usability,"> I am not sure whether the bug with the error message still needs to be fixed, but the underlying problem was in my data. Yes, we should maybe change the error message :). Thanks for the feedback!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/80#issuecomment-672828787:188,feedback,feedback,188,,https://github.com/stardist/stardist/issues/80#issuecomment-672828787,1,['feedback'],['feedback']
Usability,"> I can confirm that model testing and running on a different opened image works for me now. Great. thanks for checking!. > There's one issue I don't understand yet. Why did we bother to put the `stardist_postprocessing.ijm` macro into the exported model zip file if there's no way to run this as _Postprocessing_ from DeepImageJ?. We want to support this in deepImageJ but are currently a bit out of manpower to implement it. There's probably a relatively simple solution. But also with the given solution, the script is at least packaged with the model, so that it can be applied manually. ; We will work on better post-processing integration, and there is probably a simple solution, but we don't want to block releasing the model zoo for now to incorporate it. We can update the readme to explain this, but I would suggest to do this in a follow-up PR as this one is pretty large already and it's not critical to the functionality. > When I did run the postprocessing macro from the Script Editor (opened manually from the `Fiji.app/models/<model_name>` folder) after DeepImageJ prediction, it showed me an error `[ERROR] Could not find input image with name/title ""scores"".` in the Console. I then tried the exact same thing again, and it worked as intended without errors man_shrugging. @esgomezm @carlosuc3m do you have any idea how to fix this?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1047715559:457,simpl,simple,457,,https://github.com/stardist/stardist/pull/171#issuecomment-1047715559,2,['simpl'],['simple']
Usability,"> I read online some people set `steps_per_epoch= training set size / batch size`, as one epoch is defined whenever the model is trained along the whole dataset. Yes, that's a common definition. > I am worried I did something wrong. For all my trainings I kept steps_per_epoch parameter unchanged to its default value=100. No, you didn't. We don't recommend to change the `steps_per_epoch` parameter as you mentioned above. Some people have really small datasets, i.e. would only do a few steps per epoch. However, the `train_reduce_lr` parameter (with `patience ` given in epochs) will cause the learning rate to be automatically adjusted when training plateaus. Hence, these parameters are intertwined and we chose default parameters that would lead to well trained models regardless of dataset size. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/160#issuecomment-896793257:597,learn,learning,597,,https://github.com/stardist/stardist/issues/160#issuecomment-896793257,1,['learn'],['learning']
Usability,"> I was wondering if there is an easy way to train the network using an **ImageDataGenerator**. You could use a `Sequence` to do that, e.g. see https://github.com/stardist/stardist/issues/107. > I would like to perform transfer learning using your model **_'2D_versatile_fluo'_**. Any idea how I can do this?. This is how you can duplicate a pretrained model and prepare it for transfer learning:. ```python; # imports; from stardist.models import StarDist2D, keras_import; import shutil; from pathlib import Path; keras = keras_import(). # name of your model; my_name = '2D_versatile_fluo_FINETUNED'. # load pretrained model and make a copy to local folder; model_pretrained = StarDist2D.from_pretrained('2D_versatile_fluo'); if not Path(my_name).exists():; shutil.copytree(model_pretrained.logdir, my_name). # load your duplicate of the pretrained model; my_model = StarDist2D(None, my_name). # change optimizer, especially to use smaller learning rate for finetuning; # (e.g. 1e-5, you have to try what works best); my_model.prepare_for_training(keras.optimizers.Adam(1e-5)). # finetune model with your data...; # my_model.train(...); ```",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/172#issuecomment-964010767:228,learn,learning,228,,https://github.com/stardist/stardist/issues/172#issuecomment-964010767,3,['learn'],['learning']
Usability,"> I would however suggest not doing this via global seed setting but by creating and using a local `rng=np.random.RandomState(seed)` such to avoid the side effect of non-transparently manipulating the global numpy rng state. I would've suggested the same, but I still don't understand why this is necessary. Why not simply do this?; ```python; # np.random.seed(seed) # optional; my_cmap = random_color_cmap(); ...; plt.imshow(..., cmap=my_cmap); ...; plt.imshow(..., cmap=my_cmap); ```",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/273#issuecomment-2187329538:316,simpl,simply,316,,https://github.com/stardist/stardist/pull/273#issuecomment-2187329538,1,['simpl'],['simply']
Usability,> I'll let you know how your method works for our data. That'd be great! Any feedback is appreciated,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/19#issuecomment-535879055:77,feedback,feedback,77,,https://github.com/stardist/stardist/issues/19#issuecomment-535879055,1,['feedback'],['feedback']
Usability,"> I'm a complete beginner at Python but find the software really exciting, so have jumped in at the deep end!. Glad that you like it!. > My aim is to be able to export the label maps and visualise them in ImageJ or something similar, and to use the 3 dimensional ROIs to calculate values for nuclear volume and shape and other things. I suggest you do the prediction in Python to get the label image, which you can then import in Fiji with the [3D Roi Manager](https://imagejdocu.tudor.lu/plugin/stacks/3d_roi_manager/start) (installation instructions [here](https://imagejdocu.tudor.lu/plugin/stacks/3d_ij_suite/start#installation)). This should work well, as long as you don't have (strongly) overlapping predictions or it's not important to you to get the complete shapes for all objects. > I can see that you've made a notebook to export the ROIs for the 2D model prediction into FIJI which works great for me. I tried to adapt this for 3D by just having axes set to zyx and having 'dist' instead of 'coord' (in export_imagej_rois('img_rois.zip', details['dist'])). I have found that this doesn't really work, but not sure why. This function only works for 2D polygons and cannot be simply adapted for 3D.; ; > I also tried to export the demo 3d model into deepImageJ, but it runs into trouble when you try to apply it to an image. I can give more details, but as I am quite lost I'm not sure what is the relevant information. It's not that simple and even if it worked, this won't get you 3D ROIs in Fiji. Our [StarDist Fiji plugin](https://imagej.net/StarDist) currently only works in 2D because it's not that easy to support 3D... but we hope to get this done eventually. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/37#issuecomment-597239267:1187,simpl,simply,1187,,https://github.com/stardist/stardist/issues/37#issuecomment-597239267,2,['simpl'],"['simple', 'simply']"
Usability,"> I've changed the code again. Ok like that?. The result of your changes is good, i.e. having a subfolder `bioimageio` with the rdf.yaml and other files.; I think there is a simpler way of achieving this without using the tmpfile, see my comment in the code.; But maybe I am missing something there.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/187#issuecomment-1075281704:174,simpl,simpler,174,,https://github.com/stardist/stardist/pull/187#issuecomment-1075281704,1,['simpl'],['simpler']
Usability,"> Is there any general guidance re: image scale and resolution or are there published statistics for the training sets?. The dsb2018 training data had objects in the range of 10-70 pixels in diameter. So if your objects are considerably larger, I would suggest downscaling them. See as well the [FAQ](https://stardist.net/docs/faq.html#is-there-an-upper-size-limit-for-objects-to-be-well-segmented). > Similarly is there a recommendation to tile images to specific sizes before segmenting?. You can use any image size, no need to tile.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/170#issuecomment-935189051:23,guid,guidance,23,,https://github.com/stardist/stardist/issues/170#issuecomment-935189051,1,['guid'],['guidance']
Usability,"> May I ask if you have any idea when the next release on pypi might happen?. Sorry, we don't yet have a date for the next release. In the meantime, you could simply re-define the function that Martin changed. E.g. paste this in your script/notebook (after you imported `calculate_extents` from stardist):. ```python; def calculate_extents(lbl, func=np.median):; """""" Aggregate bounding box sizes of objects in label images. """""". import numpy as np; from collections.abc import Iterable; from csbdeep.utils import _raise; from skimage.measure import regionprops; ; if (isinstance(lbl,np.ndarray) and lbl.ndim==4) or (not isinstance(lbl,np.ndarray) and isinstance(lbl,Iterable)):; return func(np.stack([calculate_extents(_lbl,func) for _lbl in lbl], axis=0), axis=0). n = lbl.ndim; n in (2,3) or _raise(ValueError(""label image should be 2- or 3-dimensional (or pass a list of these)"")). regs = regionprops(lbl); if len(regs) == 0:; return np.zeros(n); else:; extents = np.array([np.array(r.bbox[n:])-np.array(r.bbox[:n]) for r in regs]); return func(extents, axis=0); ```",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-882546652:159,simpl,simply,159,,https://github.com/stardist/stardist/issues/57#issuecomment-882546652,1,['simpl'],['simply']
Usability,"> No, I never do. It's still open. Do I have to close it? I'm really sorry, but I cannot find out, how to run the `nvidia-smi`. I know it was running once, but now I only get the output `nvidia-smi is not recognized as an internal or external command, operable program or batch file.`. Tensorflow will reserve all GPU memory available once its first called, so a training notebook that is left open after being run will ""occupy"" the GPU leaving the prediction notebook with no memory to compute on, which is what you have seen. So shutting down all GPU notebooks before running another one is always recommended. We should add that to the notebook, so thanks for the feedback! :). ; > ,I get an image without any labeling. Can you post the figure that is produced by this cell?. > when I run `example(model, 0)`, I get the following error:. you need to change `model.predict_instances(img, n_tiles=(1,4,4)` inside the `example` function too",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/119#issuecomment-787089461:667,feedback,feedback,667,,https://github.com/stardist/stardist/issues/119#issuecomment-787089461,1,['feedback'],['feedback']
Usability,"> Not grid=4; I was referencing this previous discussion:. Ah, I see :). > Qualitatively the results look ok - there are definitely some nuclei missing and very few cells that are just too large. Yep, it's pretty good for that the image have a rather bad SNR and are pretty wild along z! Nice!. > I only manually annotated a 128x128x32 patch. I see. I would try to set `train_patch_size` to the largest possible size (e.g. `(32,128,128)`) to avoid boundary effects. Maybe that will help with the probability. But othe rthan that, your network outputs look reasonable!. > Would you consider manually lowering the learning rate after so many epochs?. That will be done automatically during taring (if the loss saturates), so I wouldn't do that.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/36#issuecomment-593626054:612,learn,learning,612,,https://github.com/stardist/stardist/issues/36#issuecomment-593626054,1,['learn'],['learning']
Usability,"> Note that [21] (stardist 2d paper) sits somewhere in between object detection and instance segmentation because the predicted shapes are of relatively high fidelity, but are not pixel-accurate. I think this probably refers to the general idea that the Stardist representation is only perfectly pixel accurate - theoretically, and for arbitrarily high resolution - with an infinite number of rays. In the discrete case, it's only pixel accurate if your number of rays equals the number of boundary pixels of the cell (assuming it is unambiguously clear where this boundary is to begin with). You're ultimately representing an object whose boundary is a rough discrete approximation to a smooth continuous surface with a (somewhat sparse, even with 128+ rays) polygon - like an octagon around a circle. Also to clarify - the UNet does not reconstruct the polygons - it just maps the input image to the probability + ray distances feature space. The probability thresholding + non maximum suppression is what ultimately gives the polygons - and then those are reconstructed [in the code you pointed out to me before.](https://github.com/mpicbg-csbd/stardist/issues/33#issuecomment-583421466). One approach to refining the boundary segmentations would be to begin with the Stardist segmentation as an initialization, and then do graph based segmentation to refine boundaries locally [[1]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1542036) [[2]](https://link.springer.com/content/pdf/10.1007%2F3-540-45465-9_88.pdf). I think that would be really cool - but also a whole project unto itself and potentially a lot of work. Depending on how complex the shape you're trying to segment, it's probably not worth it.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-604064572:548,clear,clear,548,,https://github.com/stardist/stardist/issues/33#issuecomment-604064572,1,['clear'],['clear']
Usability,"> OMP_NUM_THREADS doesnt work for me. What does that mean? If there is still an issue with that, please add some info to #54 . > i checked my output after training for 400 epochs and the network doesnt learn anything my output is completely blank. Can you provide more information, e.g. share the training notebook (with its outputs)? It's impossible to give you an answer without knowing at least some details .",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/55#issuecomment-625775775:202,learn,learn,202,,https://github.com/stardist/stardist/issues/55#issuecomment-625775775,1,['learn'],['learn']
Usability,"> Since the pretrained HE model uses this dataset, did you generate single label masks during training?. Yes, we simply max projected the labels. . > And in doing so wouldn't one drop crucial neighborhood information?. What do you mean by that?. > Would you see hindrances to just cutting labels where they overlap? There would be obviously an information loss, however, one would incorporate neighborhood information during training?. Yes, one could as well just remove the overlapping areas. We however didn't really look into the effects of doing so systematically yet.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/140#issuecomment-834236329:113,simpl,simply,113,,https://github.com/stardist/stardist/issues/140#issuecomment-834236329,1,['simpl'],['simply']
Usability,"> Sorry, too tired apparently :). @maweigert No problem, I hadn't made it too clear that it wasn't my data :). > Did you try to play around with the grid size (e.g. setting it to (4,4))?. Yeah I could try reducing the resolution, so far I was using (2,2) because that seemed like a reasonable choice considering the data and detail of segmentation I want to recover. I guess I'm also a bit curious regarding how precise the polygon reconstructions could get in principle with optimised training and whether there'd be a clear way of thinking about it. In practical terms for now the segmentation is nicely reliable and of sufficient quality so I'll stick to that for the data of my collaborator!. Thanks a lot for the thoughts everyone and feel free to close the issue :)",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-604961417:78,clear,clear,78,,https://github.com/stardist/stardist/issues/33#issuecomment-604961417,2,['clear'],['clear']
Usability,"> Thanks for the excellent job with StarDist. I love this tool. Thanks, we appreciate it!. > I recently purchased a RTX 3080 ... Only CUDA 11.1 can run properly on the RTX 3080.; > I have spent my weekend to try to solve the problem but I could not find any solution. It's a known issue, e.g. see this: https://lambdalabs.com/blog/install-tensorflow-and-pytorch-on-rtx-30-series/. Quote from that article:. Right now, getting these libraries to work with 30XX GPUs requires manual compilation or NVIDIA docker containers. It seems that you have several sub-optimal options: You can either go through the trouble of compiling TensorFlow yourself (I wouldn't do it), get one of those solutions to work (e.g. Lambda Stack, Docker container), or simply wait until the latest version of TensorFlow supports RTX 30 series GPUs.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-740151822:742,simpl,simply,742,,https://github.com/stardist/stardist/issues/104#issuecomment-740151822,1,['simpl'],['simply']
Usability,"> This is working locally now. (Needs the changes in [bioimage-io/core-bioimage-io-python#142](https://github.com/bioimage-io/core-bioimage-io-python/pull/142)). Thanks, I updated this locally. > Also, I can only run one of the tests successfully at a time, because tensorflow does not properly clear the gpu. Do you get `tensorflow.python.framework.errors_impl.FailedPreconditionError: 2 root error(s) found.`?. I think that's not the reason, rather TensorFlow is in a bad state after running `model.export_TF` once. This is a known issue, and I currently don't know how to workaround that.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-968756455:295,clear,clear,295,,https://github.com/stardist/stardist/pull/171#issuecomment-968756455,1,['clear'],['clear']
Usability,"> We actually found that we don't have GPU available for Tensorflow after our build. What do you mean? Are you referring to [this problem](https://github.com/NVIDIA/nvidia-docker/issues/1034)?. > we suspect this is because we used NVIDIA_DRIVER_VERSION=455 (not listed as supported in the README).; > Will the updated version of tensorflow work with this driver version?. The variable `NVIDIA_DRIVER_VERSION` is simply used to indicate the name of a package to be installed. If the `docker build` command completed successfully, then it was likely not the issue.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/108#issuecomment-754113033:412,simpl,simply,412,,https://github.com/stardist/stardist/issues/108#issuecomment-754113033,1,['simpl'],['simply']
Usability,> Why not simply do this?. bc it might be nice to have the same cmap across multiple runs of a script (e.g. when generating figures etc),MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/273#issuecomment-2187332696:10,simpl,simply,10,,https://github.com/stardist/stardist/pull/273#issuecomment-2187332696,1,['simpl'],['simply']
Usability,"> `If you open a new topic, please provide a clear and concise description to understand and ideally reproduce the issue you're having (e.g. including a code snippet, Python script, or Jupyter notebook).`. [code.zip](https://github.com/stardist/stardist/files/6434116/code.zip); We cut out the training code and test code according to the training steps provided, and the rest of the parameters are kept as defaults. We directly use this training file to train and test, and the result seems to be wrong.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/138#issuecomment-833459146:45,clear,clear,45,,https://github.com/stardist/stardist/issues/138#issuecomment-833459146,1,['clear'],['clear']
Usability,"> `n_tiles` looks like exactly what I need. My image isn't that big ~1024x1014x512 voxels. Ok, for that size that indeed should be the way to go!. > (running on the CPU on my laptop - I'm happy to say though that with like 10 minutes of training on my CPU I already get pretty darn good results!. Happy to hear that! Although 10mins on a CPU is like 1 epoch? I would never expect that to work :). > I may stick with the 256 rays but use lots of tiles at prediction time?. Yes, that'd be an option. Alternatively you could use less rays (I practically never use more than ~96). And again, in the future the RAM limitations should be taken care of by the label tiling (there is already a branch `big` for that). . Cheers and thanks for the feedback!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/36#issuecomment-593572037:738,feedback,feedback,738,,https://github.com/stardist/stardist/issues/36#issuecomment-593572037,1,['feedback'],['feedback']
Usability,"> a bit confusing. I agree, we should change it a some point (the naming you suggest indeed makes a bit more sense). ; . > Specifically, why do you use `use_gpu = False and ...` which will always evaluate to `False` ?. This is to ensure that `gputools` is available, once the user decides to set `use_gpu`. Note, that this is a bit of a power-user setting (if you want to speed up training), since installing `gputools`/`pyopencl` can be a bit involved on some systems. This is why it is deactivated by default. . Hope that clears it up a bit and thanks for the feedback! . M",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/75#issuecomment-661022071:524,clear,clears,524,,https://github.com/stardist/stardist/issues/75#issuecomment-661022071,2,"['clear', 'feedback']","['clears', 'feedback']"
Usability,"> the median size object is [ 37.5 156. 160.5]. Before or after downscaling the input images?. > I increased the grid and the unet to 3 .; > After learning, when I apply to predictions on new images the issues is that the Z is right (because below 64) but the X and Y are way too small because of the FOV of [ 64 128 128] .; > ; > Hence it can only see object below that shape : so many small nucleus detected on 1 single nucleus; > Hence my question is how to use Stardist when trained on downscale object onto bigger images with full scale (50, 1440, 1920) ?. It is difficult to understand what you mean. Ideally show us the code/notebook.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/118#issuecomment-785909768:147,learn,learning,147,,https://github.com/stardist/stardist/issues/118#issuecomment-785909768,1,['learn'],['learning']
Usability,"After many tries, best time I can get is 75 seconds (from 97 seconds in my very first message) with `OMP_NUM_THREADS=40`.; What remains concerning is that in this case, I get ; ```python; NMS: Suppressed polyhedra:; NMS: # inner: 191728 / 358813 (53.43 %); NMS: # kernel: 153518 / 358813 (42.78 %); NMS: # render: 6041 / 358813 (1.68 %); NMS: # total: 351287 / 358813 (97.90 %); NMS: # keeping 7526 / 358813 polyhedra (2.10 %); ```; but switching `gravity` off (still with `OMP_NUM_THREADS=40`), I get 85 seconds and almost no `inner` calls (as expected); ```python; NMS: # inner: 1683 / 358813 (0.47 %); NMS: # kernel: 343563 / 358813 (95.75 %); NMS: # render: 6041 / 358813 (1.68 %); NMS: # total: 351287 / 358813 (97.90 %); NMS: # keeping 7526 / 358813 polyhedra (2.10 %); ```; so the difference between `gravity` on and off is small. `inner` calls could simply be very long for my machine for whatever reasons...; I'll gladly take the 20% improvement in time though, thank you very much! :)",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/277#issuecomment-2236669913:858,simpl,simply,858,,https://github.com/stardist/stardist/issues/277#issuecomment-2236669913,1,['simpl'],['simply']
Usability,"Because the unsophisticated solution to the problem was to just simply restart the whole computer. It still takes almost 10 hours, but I think that's because of my big stacks (100 slices, 1024x1024 pixel). With `use_gpu =True` it takes around 80s per epoch now.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/120#issuecomment-786042728:64,simpl,simply,64,,https://github.com/stardist/stardist/issues/120#issuecomment-786042728,1,['simpl'],['simply']
Usability,But it is still counter intuitive to me because you have the config.json file in the same folder as the model weights so for the prediction step it should know the hyperparameters I used for the training right?,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-659548054:24,intuit,intuitive,24,,https://github.com/stardist/stardist/issues/68#issuecomment-659548054,1,['intuit'],['intuitive']
Usability,Can you clarify which part of the instructions didn't work or wasn't clear?; https://github.com/stardist/stardist#apple-silicon,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/218#issuecomment-1426873946:69,clear,clear,69,,https://github.com/stardist/stardist/issues/218#issuecomment-1426873946,1,['clear'],['clear']
Usability,"Check, whether the validation stacks `X_val, Y_val` are somehow look different than the training data (e.g. normalisation). How did you generate the validation data, via the notebook? There seems to be clearly something wrong with them...",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/53#issuecomment-625132058:202,clear,clearly,202,,https://github.com/stardist/stardist/issues/53#issuecomment-625132058,1,['clear'],['clearly']
Usability,"Coming back to this after a while. I was going through the stardist 3d paper (as someone in the lab was planning to use it!) and read this statement:. > Note that [21] (stardist 2d paper) sits somewhere in between object detection and instance segmentation because the predicted shapes are of relatively high fidelity, but are not pixel-accurate. What would be your explanation for why segmentations are not necessarily pixel accurate? I guess for me it's not super clear how the output of a u-net behaves when needing to precisely reconstruct the ray lengths. > Hard to see why it should deviate so much for the images you show, which should be rather easy to segment...would have too look further into it (maybe cells are too large, gridsize too small etc). At the end the segmentations I'm getting are not bad at all, but since that project involves cell shape characterisations I was wondering whether there would be some nice trick to improve the accuracies of the boundary. For the images I showed I used the default settings from the notebook, i.e. the shown image resolution (objects of around 70 pixels in diameter) and `grid=(2,2)`. . Cheers!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-604001939:466,clear,clear,466,,https://github.com/stardist/stardist/issues/33#issuecomment-604001939,1,['clear'],['clear']
Usability,Did you follow the [Troubleshooting](https://github.com/mpicbg-csbd/stardist/blob/master/README.md#macos) guidelines?,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659300697:106,guid,guidelines,106,,https://github.com/stardist/stardist/issues/74#issuecomment-659300697,1,['guid'],['guidelines']
Usability,"Do I understand max projection correctly: for every single nucleus you have a single label mask resulting in many labelmasks Y_1 ... Y_N for only one input image X? If thats right - in order to lower overall cost - the network still will give all segmentations for the image in the best case. However, wouldn't it be helpful to the network to learn the constraining neighbors of each nucleus at a time? Or is this implicitly also captured if one has many label matrices corresponding to one image?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/140#issuecomment-834275557:343,learn,learn,343,,https://github.com/stardist/stardist/issues/140#issuecomment-834275557,1,['learn'],['learn']
Usability,"Great to hear, thanks for the feedback!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/78#issuecomment-839779331:30,feedback,feedback,30,,https://github.com/stardist/stardist/issues/78#issuecomment-839779331,1,['feedback'],['feedback']
Usability,Great! Thanks again for the feedback!,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/17#issuecomment-534933526:28,feedback,feedback,28,,https://github.com/stardist/stardist/issues/17#issuecomment-534933526,1,['feedback'],['feedback']
Usability,"Hello,. I'm facing the same issue, all my data can not fit in RAM (I have 3D images and total training set > > 100Go). I've been trying `keras.utils.Sequence` as suggested by @maweigert, my idea is to simply replace a list of numpy arrays with a Sequence class that loads images on the fly. ```; from tensorflow.keras.utils import Sequence; from tifffile import imread. class DataLoader(Sequence):; ; def __init__(self, path_list, mode):; self.x, self.mode = path_list, mode; ; #retro compatibility with numpy; self.ndim=3; if mode == ""Y"":; self.dtype = np.int; else:; self.dtype = np.float; ; def __len__(self):; return len(self.x). def __getitem__(self, idx):; ; filename = self.x[idx]; if self.mode == ""X"":; return imread(filename) / 255; else:; return imread(filename); ```. I'm actually able to launch a training with batch size of 1 without specifying anisotropy and things seem to run ok (note that I had to add `dtype` and `ndim` to make things work and that I have some problems in my pipeline as explained bellow). However some part of my training pipeline are broken:. - when running this to compute anisotropy `extents = calculate_extents(Y)` I first get this warning; ```_asarray.py (83): Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray```; and after a few seconds of computation the code fails with this error message :; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-14-36ac39a79e87> in <module>; ----> 1 extents = calculate_extents(Y); 2 anisotropy = tuple(np.max(extents) / extents); 3 print('empirical anisotropy of labeled objects = %s' % str(anisotropy)). /work/.cache/poetry/kc-segmentation-DJpFP61h-py3.7/lib/python3.7/site-packages/stardist/utils.py in calculate_extents(lbl, func); 165 n in ",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880580061:201,simpl,simply,201,,https://github.com/stardist/stardist/issues/57#issuecomment-880580061,1,['simpl'],['simply']
Usability,"Hey Martin, thanks for your quick response :); This would be an example:. ![image](https://user-images.githubusercontent.com/12528388/73953738-27380880-4901-11ea-8382-7f9e9ebf62ed.png). The cell on the right should be star convex, however the segmentation boundary is a rather smoothed version of the cell outline... Here's another example of a more round cell:. ![image](https://user-images.githubusercontent.com/12528388/73955948-7e8ba800-4904-11ea-9de4-5dd4a5fb89a2.png). While in the previous example the boundary is simply very smooth, here it seems to be slightly off. This data is actually taken from http://celltrackingchallenge.net/2d-datasets (GFP-GOWT1 mouse stem cells), as it looks very similar to mine. I trained the network on their training data with the default settings from the training notebook plus 128 rays.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-582973907:521,simpl,simply,521,,https://github.com/stardist/stardist/issues/33#issuecomment-582973907,1,['simpl'],['simply']
Usability,"Hey,. Thank you for your feedback. 1. Access to cluster GPUs is handled by IBM LSF (Load Sharing Facility) batch system preventing usage of a GPU by multiple users. The above error is thrown when I initialise the model according to ; ```python; In[11]: model = StarDist2D(conf, name='stardist', basedir='models'); ````; in [notebook 2](https://github.com/mpicbg-csbd/stardist/blob/master/examples/2D/2_training.ipynb) so GPU memory should not be full. 2. `CUDA_VISIBLE_DEVICES` is handled by the LSF batch system. However, I also manually assigned the `CUDA_VISIBLE_DEVICES` and the error persists. If I am not mistaking, the above error is expected if we run >1 ""independent processes"" on a single GPU that is in Exclusive Process mode. Isn't this exactly what `StarDist` does and why we use . ```python; In [10]: if use_gpu:; from csbdeep.utils.tf import limit_gpu_memory; # adjust as necessary: limit GPU memory to be used by TensorFlow to leave some to OpenCL-based computations; limit_gpu_memory(0.8); ```; ?. Best,; D",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/102#issuecomment-728158838:25,feedback,feedback,25,,https://github.com/stardist/stardist/issues/102#issuecomment-728158838,1,['feedback'],['feedback']
Usability,"Hi ,. >Sorry for the late reply. no worries ðŸ˜Š. >You are using the scripts branch?. Actually, I'm not... I simply created a conda env with a yml file (see above) , specifying version `stardist==0.8.2` (not specifying a the script branch). I tried the `stardist-predict3d -h` and I saw it was there :; ```; usage: stardist-predict3d [-h] -i INPUT [INPUT ...] [-o OUTDIR]; [--outname OUTNAME [OUTNAME ...]] -m MODEL; [--axes AXES] [--n_tiles N_TILES N_TILES]; [--pnorm PNORM PNORM] [--prob_thresh PROB_THRESH]; [--nms_thresh NMS_THRESH] [-v]. Prediction script for a 3D stardist model, usage: stardist-predict -i; input.tif -m model_folder_or_pretrained_name -o output_folder. optional arguments:; -h, --help show this help message and exit; -i INPUT [INPUT ...], --input INPUT [INPUT ...]; input file (tiff) (default: None); -o OUTDIR, --outdir OUTDIR; output directory (default: .); --outname OUTNAME [OUTNAME ...]; output file name (tiff) (default: {img}.stardist.tif); -m MODEL, --model MODEL; model folder / pretrained model to use (default: None); --axes AXES axes to use for the input, e.g. 'XYC' (default: None); --n_tiles N_TILES N_TILES; number of tiles to use for prediction (default: None); --pnorm PNORM PNORM pmin/pmax to use for normalization (default: [3,; 99.8]); --prob_thresh PROB_THRESH; prob_thresh for model (if not given use model default); (default: None); --nms_thresh NMS_THRESH; nms_thresh for model (if not given use model default); (default: None); -v, --verbose; ```; but I can't make it work ðŸ˜‘",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/191#issuecomment-1141976465:106,simpl,simply,106,,https://github.com/stardist/stardist/issues/191#issuecomment-1141976465,1,['simpl'],['simply']
Usability,"Hi @Cocomolch4000 ,. This is a good point - we've just updated the Readme to reflect this. Thanks for the feedback!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/139#issuecomment-833649844:106,feedback,feedback,106,,https://github.com/stardist/stardist/issues/139#issuecomment-833649844,1,['feedback'],['feedback']
Usability,"Hi @GFleishman,. Thanks for all the work! Two things that I think are still worth pondering:. 1) The `max_distance` cutoff for the spatial data structure should ideally be chosen such that no potential intersecting polygon is missed. Some options are ; * (Mean) `max_distance= 2*(np.mean(dist)+np.std(dist))` what is used in the PR, but might miss some pairs ; * (Max) `max_distance= 2*np.max(dist)`, preferable as it won't miss any pairs but which leads to order of magnitude more pairs. . There likely won't be any difference between (Mean) and (Max) in practice (as your initial comparison showed), but I am still a bit wary of not having that guarantee. . 2) Memory: I tried prediction on a small crop (with ~250k candidates) of the test data above:; * without kdtree: 15.4 GB, 97s ; * with kdtree (Mean): 15.5 GB, 98s ; * with kdtree (Max): 39.2 GB, 121s . So using (Max) will clearly take a hit on memory...",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-606352621:882,clear,clearly,882,,https://github.com/stardist/stardist/pull/40#issuecomment-606352621,1,['clear'],['clearly']
Usability,"Hi @Nal44, . It's hard to see what your problem is - could you simply share the training notebook with its outputs? (open your `2_training.ipynb` and then `File > Download as > HTML`)",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/66#issuecomment-664440851:63,simpl,simply,63,,https://github.com/stardist/stardist/issues/66#issuecomment-664440851,1,['simpl'],['simply']
Usability,"Hi @ajinkya-kulkarni, I agree that `tqdm.auto` loads a nicer progress bar in Jupyter notebooks, but it also has the disadvantage that the progress bar isn't preserved in the notebook output. I.e. one cannot see how many iterations / long it took when the notebook isn't ""live"" anymore. Or has this recently changed?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/237#issuecomment-1637921675:61,progress bar,progress bar,61,,https://github.com/stardist/stardist/pull/237#issuecomment-1637921675,2,['progress bar'],['progress bar']
Usability,"Hi @esgomezm, I don't think it's possible to do this is as a simple post-processing script.; We need a separate Java library and ideally multi-threading to make this fast. PS: I don't think the pre-processing script for StarDist in DeepImageJ is correct, at least when you use it with our pre-trained model. (We do a percentile-based normalization and don't clip values to the 0..1 range.). Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/29#issuecomment-561167747:61,simpl,simple,61,,https://github.com/stardist/stardist/issues/29#issuecomment-561167747,1,['simpl'],['simple']
Usability,"Hi @esgomezm,. > We are actually trying to export the model using TF2.x (the idea is to test it with the new java library in Fiji + deepImageJ). Is the new Java library for deep learning ready yet? What is the name and where can I find it?. > Is it the export being forced inside StarDist to have a model compatible with TF1.15?. Yes, the model export in the CSBDeep and StarDist Python packages was always meant for use in Fiji, which has only ever supported TensorFlow 1.x. > I'm using the bioimageio library (version 0.5.8) with TF2.11 to export TF and Keras models, and everything works perfectly. Using the bioimageio library where to export the model? And ""everything works perfectly"" where?. > A potential solution (probably you have a better one), could be to avoid those imports if TF version > 2.3? Until version 2.3, the compatibility with CSBDeep & StarDist is ensured, but for later versions is not unless the plugins are updated. Also, my feeling is that the export fails with TF>2.3. I really need to understand the current situation first... I don't even know how to export the model such that it will work with the new Java library.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-1453562117:178,learn,learning,178,,https://github.com/stardist/stardist/issues/68#issuecomment-1453562117,1,['learn'],['learning']
Usability,"Hi @imand500 ,. Sorry for the late reply. . > Since the model is not saved as h5 and it's really dependent on the configuration you made. If a pretrained models is loaded; ```python ; model = StarDist2D.from_pretrained(""2D_versatile_he""); ```; it is actually downloaded and stored as a normal stardist model in your keras cache directory. You can find the location like so:; ```python; print(model.logdir); ```; So you could simply copy that folder somewhere, create a new model from that folder and then continue training with your own data. Hope that helps,. M",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/127#issuecomment-830683343:425,simpl,simply,425,,https://github.com/stardist/stardist/issues/127#issuecomment-830683343,1,['simpl'],['simply']
Usability,"Hi @javierpastorfernandez ,. Yes, stardist will convert the input images to `float32` before applying the augmenter, but does not normalize them. So simply converting them back to `int` at the beginning of your augmenter function should work. . Hope that helps,; M. PS: We also recently made public a [small augmentation library](https://github.com/stardist/augmend) that we typically use with stardist and which might be of interest.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/137#issuecomment-830686786:149,simpl,simply,149,,https://github.com/stardist/stardist/issues/137#issuecomment-830686786,1,['simpl'],['simply']
Usability,"Hi @m-albert,. > While in the previous example the boundary is simply very smooth, here it seems to be slightly off. Hard to see why it should deviate so much for the images you show, which should be rather easy to segment...would have too look further into it (maybe cells are too large, gridsize too small etc) . > This might be important when dealing with small objects. Not sure decreasing the step size will help with my boundaries but I'll retrain and give it a try, potentially being off at the boundaries could confuse the distance predictions. Thanks (and @GFleishman too) for bringing that up! Indeed the stardist calculations are a bit rough for small objects and we never bothered to refine them correctly. Inspired by this thread I took another look at them: Instead of decreasing the stepsize (which would probably be too slow) one can [directly compute the ""overshoot"" distance after the label switches](https://github.com/mpicbg-csbd/stardist/blob/dev/stardist/lib/stardist2d.cpp#L74). That way, distances for small objects should be now more correct. I've put it in the `dev` branch, so you can try it yourself:. `pip install git+https://github.com/mpicbg-csbd/stardist.git@dev`. Let me know if that helps and thanks for all the feedback and input!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-583762427:63,simpl,simply,63,,https://github.com/stardist/stardist/issues/33#issuecomment-583762427,2,"['feedback', 'simpl']","['feedback', 'simply']"
Usability,"Hi @maweigert,. Right now I'm working on a different strategy from the software standpoint that will solve the extra time cost for using Max and the extra memory from having to store the larger lists of neighbors (potential suppression candidates). My first implementation in the PR was a good demonstration of kdtree, but definitely suboptimal w.r.t. what can be done with the Python C-API, which I'm learning on the fly so things have gone through various stages of kludge. The idea now is that I can call the sklearn kdtree object, and it's query_radius method, from within the C++ code itself - enabling a separate query for each polygon candidate just before the inner loop of NMS begins. That way, points which are ""suppressed early"" will never be queried against the tree - reducing total query time even if the distance threshold is large. The second benefit is that the code now only has to store one neighbor list at a time, thus reducing the total RAM used. This is definitely the best way to built it _algorithmically_ but I still don't have the Python C-API down perfectly so implementing this today has been tricky. I also implemented what I mentioned before - applying the probability threshold for each tile separately during prediction. For the whole image, it cut max RAM down from ~400GB to ~50GB, and that was including the neighbors list from kdtree. So, if I can get the tile based threshold combined with the embedded python approach to the kdtree, I think I can get the whole zebrafish image segmented in like 10-15 minutes for something like 30GB RAM. Of course, the tile based prob threshold breaks a bunch of stuff, so I'll leave it to you guys to decide if you want to move in that direction generally or to just have my commit somewhere accessible on the repo so if others run into RAM issues there's some path to cutting it down without changing so many internal dependencies between functions. This is all WIP but I'm hoping to finish it soon. Thanks,; Greg",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-606357837:402,learn,learning,402,,https://github.com/stardist/stardist/pull/40#issuecomment-606357837,1,['learn'],['learning']
Usability,"Hi @maweigert,. for some context: we are currently setting up a user-friendly workflow for users to train stardist and we stumbled over this line.; Thanks to your explanation I understand what the `use_gpu` variable does now, but I still find the line. ```; # Use OpenCL-based computations for data generator during training (requires 'gputools'); use_gpu = False and gputools_available(); ```; a bit confusing. Specifically, why do you use `use_gpu = False and ...` which will always evaluate to `False` ? ; Wouldn't `use_gpu = gputools_available()` be sufficient?; I would assume that `gputools_available` returns `True` if the additional dependency is available and `False` otherwise. One more point: the variable name `use_gpu` could maybe be renamed to `use_gpu_for_data_processing` or so; because now it implies that `use_gpu = False` means the gpu is not used at all (which I know is not the case, but doesn't become quite clear from the notebook). Sorry for being rather nit-picky here, but if this confuses us it will likely be confusing for users as well.; In any case, we will probably adapt the notebook a bit for users and just make these changes there, but we were wondering why you were choosing to do it like this in the first case.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/75#issuecomment-661014557:64,user-friendly,user-friendly,64,,https://github.com/stardist/stardist/issues/75#issuecomment-661014557,2,"['clear', 'user-friendly']","['clear', 'user-friendly']"
Usability,"Hi @pedgomgal1 ,. > first congrats for the complete and accesible pipeline that you provide us, people with very few expertise in machine/deep learning. Thanks! :). > With the IoU graph that I have attached. If you look at the accuracy metrics in the left graph, you see that almost all of them are basically zero, i.e. your model a totally wrong output. This might be the reason for the dying kernel too. Did you have a look at `Y_val_pred `? Does that look reasonable? . So I would first try to see what is going on.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/51#issuecomment-622454317:143,learn,learning,143,,https://github.com/stardist/stardist/issues/51#issuecomment-622454317,1,['learn'],['learning']
Usability,"Hi @qinghongwan, thanks for reporting! It's a simple bug that I've [just fixed](https://github.com/stardist/stardist/commit/0993844dba2bfee55b6b14045798d19e4be1519a) in the `dev` branch. You can install this branch via `pip install git+https://github.com/stardist/stardist@dev` before we make a new proper release (no schedule for that yet). However, you might run into [installation issues](https://github.com/stardist/stardist#installation-1).",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/154#issuecomment-885661470:46,simpl,simple,46,,https://github.com/stardist/stardist/issues/154#issuecomment-885661470,1,['simpl'],['simple']
Usability,"Hi @romainGuiet,. You should be able to get the original shape by simply linearly upscaling the probability and distance prediction by the grid parameter (i.e. two fold along each axis if `grid=(2,2)`). . Your planning to have the full pipeline (with polygon rendering) as part of a Fiji plugin? . Cheers,; M",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/28#issuecomment-557546384:66,simpl,simply,66,,https://github.com/stardist/stardist/issues/28#issuecomment-557546384,1,['simpl'],['simply']
Usability,"Hi @seismonastic ,. > Is the model in `3D_demo` a pre-trained model for 3D images?. No, that is simply the model of the 3D demo notebook when trained for a long time. A proper pre-trained model in 3D is not yet available unfortunately (many more things to consider). . Cheers,; M",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/31#issuecomment-622037189:96,simpl,simply,96,,https://github.com/stardist/stardist/issues/31#issuecomment-622037189,1,['simpl'],['simply']
Usability,"Hi @tboo,. > Just had a first climpse at StarDist 2D and it's performance is outstanding. Really happy with it :). Glad you like it! :). > I simply zipped the respective model folder I generated with the notebooks. You need to call `model.export_TF()`, which will create a file `TF_SavedModel.zip` in the respective model directory. You then have to point the Fiji plugin to `TF_SavedModel.zip` (make sure that tensorflow version of the exporting code and of Fiji are the same). Let us know if that works! . Cheers,; Martin",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/42#issuecomment-606856098:141,simpl,simply,141,,https://github.com/stardist/stardist/issues/42#issuecomment-606856098,1,['simpl'],['simply']
Usability,"Hi @uschmidt83 , Hi @esgomezm . My current workaround is to use the **postprocessing.txt** from DeepImageJ to generate the ROIs.; But it's far from being as good as the StarDist roiset output ðŸ˜¢ . Briefly : **Find Maximas > Voronois + Threshold > Math + BinaryMorphoy >Analyse Particles = ROIs**; ![image](https://user-images.githubusercontent.com/8309560/70048285-32517e00-15ca-11ea-973a-43923cae0a46.png). I leave here a [simple code on gist](https://gist.github.com/romainGuiet/250edffa7d07604fb6e575d51abf86cf) if someone else needs it!. Best,; Romain",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/29#issuecomment-561184774:423,simpl,simple,423,,https://github.com/stardist/stardist/issues/29#issuecomment-561184774,1,['simpl'],['simple']
Usability,"Hi @uschmidt83 ,. Thanks for clarifying all that! ; Indeed I did not notice this flag was exclusively for the data generation for training step. Just the fact that there was a variable `use_gpu` set to `False` led me to wrong conclusions. Perhaps a more explicit naming can help to avoid confusions like this in the future, something like `use_gpu_for_data_gen`. > No, this is intentional. The and gputools_available() part acts as a [""guard""](https://en.wikipedia.org/wiki/Guard_(computer_science)) to always disable this flag when gputools is not installed. I see, so it is up to the user to actively change it if they want to use it for data generation. Thanks, I believe things are clear for me now.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/222#issuecomment-1445371166:686,clear,clear,686,,https://github.com/stardist/stardist/issues/222#issuecomment-1445371166,1,['clear'],['clear']
Usability,"Hi Greg, . thanks a lot for this contribution - it already looks very promising! . > I evaluated NMS time over a wide number of polygon candidates. Here are the results (note the log_10 scale on the vertical axis):. Thats a nice speed up! The 6Mio candidates are for the whole stack, I assume? So using `predict_big` doesn't have any effect on the prediction time?. I'm slightly worried about the additional memory footprint that a full kdtree of all candidates might bring - did you try measuring the peak memory usage? Additionally I suspect there is quite some heavy differences even for different kdtree implementations e.g. there are `KDTree` and `BallTree` from `scikit-learn`. Here's a short test I did with random points in 3D and those different implementation ; ""ckdtree"" -> `scipy.spatial.ckdtree` ; ""kdtree"" -> `sklearn.neighbors.KDTree`; ""ball"" -> `sklearn.neighbors.BallTree`. <img width=""850"" alt=""Screenshot 2020-03-19 at 18 10 04"" src=""https://user-images.githubusercontent.com/11042162/77096353-85333200-6a0f-11ea-9f97-926c8bd22cd9.png"">; ; shows that i) there are quite some difference esp. for memory, and ii) the overall memory footprint is pretty high (e.g. 18GB for a mere 30k points for ckdtree). So having a similar plot as above but for memory consumption in your example would be nice and instructive. Additionally moving to `BallTree` might make things more efficient...",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-601317246:676,learn,learn,676,,https://github.com/stardist/stardist/pull/40#issuecomment-601317246,1,['learn'],['learn']
Usability,"Hi Martin,. Here is the training file in HTML,; Please let me know if you need anything else. Thanks. PS : we adapted the code to work form arrays not TIF images. Anthony Lagnado, Ph.D.| Research Fellow | Department of Physiology and Biomedical Engineering | 507-538-1524; Mayo Clinic, 200 First Street SW, Rochester, MN 55905. From: Martin Weigert [mailto:notifications@github.com]; Sent: Monday, July 27, 2020 9:46 AM; To: mpicbg-csbd/stardist; Cc: Lagnado, Anthony B., Ph.D.; Mention; Subject: [EXTERNAL] Re: [mpicbg-csbd/stardist] Grid and model data informations and others (#66). Hi @Nal44<https://github.com/Nal44>,. It's hard to see what your problem is - could you simply share the training notebook with its outputs? (open your 2_training.ipynb and then File > Download as > HTML). â€”; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/mpicbg-csbd/stardist/issues/66#issuecomment-664440851>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/APP3D56PCOPD7VP7SGMASBTR5WHK7ANCNFSM4ODRGWBA>.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/66#issuecomment-664526462:674,simpl,simply,674,,https://github.com/stardist/stardist/issues/66#issuecomment-664526462,1,['simpl'],['simply']
Usability,"Hi Martin,. I think you are right about scikit-learn kdtree outperforming scipy cKDTree. While working on another change to the package, I was led to trying the scikit-learn kdtree. For the same dataset considered above, the query was a few minutes faster than scipy cKDTree, but more importantly - due to the underlying data types used - the query _results_ consumed significantly less RAM. For both packages I've found that the tree structure itself takes a negligible amount of RAM - I'm not sure exactly how this is implemented under the hood, but I suppose each node in the tree could be as small as a single pointer to where in the array the points are split. If that's the case you really only need something like 3*log_2(N) references (3 axes and N points). However it's implemented, empirically it's quite small. The query results however can be quite large, in the abstract it's a list of lists. The outer list is of length N (for N points) and the ith inner list is of length equal to the number of points within radius distance of the ith point. So this could be on the order of ~1000 * N long ints - that's something like 25GB without considering any extra metadata overhead. Scipy represents this object as a Python list of Python lists of Python ints. Python ints contain a cumbersome amount of metadata, so I don't think they're a great data type here. Scikit-learn on the other hand represents this as a Numpy array with `dtype = 'O'`, that is an array of objects. The objects are 1D Numpy arrays themselves with `dtype = np.int64`. This seems to be overall more space efficient. I'd love to select `np.int32` instead, since that's more than adequate, but it does not seem to be possible without modifying the `Scikit-learn.neighbors.kdtree` code (maybe still worth doing in the long run). So - combining the small query speed up and the better space efficiency, I've added code to use scikit-learn kdtree in a new commit.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-605328480:47,learn,learn,47,,https://github.com/stardist/stardist/pull/40#issuecomment-605328480,5,['learn'],['learn']
Usability,"Hi Martin,. Yes, the 6Mio candidates are for prediction on the whole stack. I did try `predict_big` on this volume. What I noticed was that the memory footprint went way down. Using [Uwe's recommended parameters](https://github.com/mpicbg-csbd/stardist/issues/36#issuecomment-594974089) I was able to run the job with a single cpu (# cores requested and maximum amount of RAM available are mixed on our cluster; each core comes with 15GB RAM). However the remaining runtime predicted by tqdm after a few iterations had accumulated was something like 36 hours - so I cancelled it and gave up on it. Uwe reminded me that with one core, I likely had a maximum of 2 threads, but I did not go back and try `predict_big` with more cores. Do you think `predict_big` with say 32 cores (64 threads) could finish in less than an hour? If so I will give it another try. For different kdtree implementations - I did look through [this article written by the scikit-learn kdtree and balltree developer](https://jakevdp.github.io/blog/2013/04/29/benchmarking-nearest-neighbor-searches-in-python/), but I didn't think there was enough differences in his graphs to justify introducing a new dependency (I don't think scikit-learn was a dependency before?) and I also preferred something with an underlying C++ implementation, as there are more big speed ups available if the kdtree data structure is available for query directly within the NMS algorithm. Unfortunately the only RAM measurements I have are the maximum and average usage (over the full run, prediction and NMS) for prediction on the full volumes both w/ and w/o the kdtree. The command in both cases was:. `pred, det = model.predict_instances(image_norm, verbose=True, n_tiles=(4, 8, 2))`; On a volume with 860x1676x301 voxels and 6.32e6 polygon candidates. w/o kdtree max: 434.439GB; w/ kdtree max: 430.482GB. w/o kdtree avg: 221.672GB; w/ kdtree avg: 297.733GB. I'm thinking that the kdtree and the query results for this many candidates took somethi",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-601381585:953,learn,learn,953,,https://github.com/stardist/stardist/pull/40#issuecomment-601381585,1,['learn'],['learn']
Usability,"Hi Michael,. > 1. My data (example data provided here: https://drive.google.com/drive/folders/17q11-hAJjCs72YFbsVKoGve3e5nzXyJf?usp=sharing) appears to be the wrong shape. Any ideas for a simple fix?. I can reproduce that one of your images is opened with seemingly wrong shape in Python. I don't know why that happens, but I've found that simply re-saving the image as a Tiff file from Fiji solves the problem, i.e. it then loads correctly in Python. > 2. I tried running prediction anyway, and find that I get an out of memory error.; >; > Is this because my tensorflow/CUDA is not communicating with the GPU?. No, this is also related to the wrong image shape and should go away.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/62#issuecomment-644116825:188,simpl,simple,188,,https://github.com/stardist/stardist/issues/62#issuecomment-644116825,2,['simpl'],"['simple', 'simply']"
Usability,"Hi Uwe, . loading directly in a shell does not throw the error. It happens only when I use snakemake which is bizarre. This might be related to another error I get which is that the libiomp5.so is already initialised ( OMP: Error #15: Initializing libiomp5.so, but found libomp.so already initialized). I wonder whether due to this conflict the model is accessed simultaneously by two process leading to the error. . Thanks for looking into this, ; Marc . Dr. Marc Bickle ; Technology Development Studio ; Max Planck Institute of Molecular Cell Biology and Genetics ; Pfotenhauerstrasse 108 ; 01307 Dresden Germany . Phone: +49 (0)172 536 5517 . From: ""Uwe Schmidt"" <notifications@github.com> ; To: ""mpicbg-csbd/stardist"" <stardist@noreply.github.com> ; Cc: ""Marc Bickle"" <bickle@mpi-cbg.de>, ""Author"" <author@noreply.github.com> ; Sent: Sunday, October 25, 2020 11:36:57 PM ; Subject: Re: [mpicbg-csbd/stardist] loading 2D_versatile_fluo error (#93) . Hi, the error message indicates a problem with loading the weights from the HDF5 file. . First, to rule out Snakemake as an issue, did you try to simply load the pre-trained model in a Python shell? For example, does this throw the same error? ; $ python -c ""from stardist.models import StarDist2D; StarDist2D.from_pretrained('2D_versatile_fluo')"" . If that's the case, then I'd try to use a different/newer version of the HDF5 library h5py . . Best, ; Uwe . â€” ; You are receiving this because you authored the thread. ; Reply to this email directly, [ https://github.com/mpicbg-csbd/stardist/issues/93#issuecomment-716223889 | view it on GitHub ] , or [ https://github.com/notifications/unsubscribe-auth/AAU56KJ4GEUFX4BPHFYPRBLSMSSATANCNFSM4SZXDKIQ | unsubscribe ] .",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/93#issuecomment-716329806:1099,simpl,simply,1099,,https://github.com/stardist/stardist/issues/93#issuecomment-716329806,1,['simpl'],['simply']
Usability,"Hi Uwe,; It's entirely possible it's specific to our implementation then. What I meant by unstable is that I had obtained a few times an unusual loss curve with an additional peak half way through the number of epoch. See below. This does not match any changes in learning rate either. <img width=""910"" alt=""Screenshot 2021-02-24 at 22 12 44"" src=""https://user-images.githubusercontent.com/21193399/109297054-4d408b80-7829-11eb-9aac-f0e9788e5907.png"">. But either way, we've now upgraded it to TF2.x and implemented the default settings as close to yours as possible, as well as the exact ways to do augmentation. And this seems to perform much better and not give the phantom masks or the unusual loss curves. Thanks a lot for your help. I really appreciate it.; Best,. Romain",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/116#issuecomment-786602735:264,learn,learning,264,,https://github.com/stardist/stardist/issues/116#issuecomment-786602735,1,['learn'],['learning']
Usability,"Hi again - I need to revisit this discussion. I've now moved to my cluster and am trying to run on the full image:; `pred, det = model.predict_instances(image_norm, n_tiles=(2, 16, 8))`; The image is not huge: ~900x1600x300 voxels; but there are _many_ cells, something on the order of 10^5. Whichever part of predict_instances that has the tqdm progress bar finishes in a reasonable amount of time, about 30 minutes, but whatever is after is taking several hours. Due to some network connectivity issues I haven't been able to get through a complete run yet. I'm guessing the loop monitored by tqdm is model prediction over the tiles, and what comes after is NMS and ""polyhedron_to_label""? I guess NMS would scale with the number of voxels, but polyhedron_to_label would scale with the number of labels? Do you have any estimate for how long something like my dataset might take?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/36#issuecomment-594086237:346,progress bar,progress bar,346,,https://github.com/stardist/stardist/issues/36#issuecomment-594086237,1,['progress bar'],['progress bar']
Usability,"Hi guys,. Finally I found time to try out something (see below). It seems to work and I attached the steps and my comments.; Please let me know what you think about. Thanks again for your help. Carlo. ## StarDist-GPU Windows 10 Installation Steps for NVIDIA RTX 3080. Source: https://medium.com/@dun.chwong/the-simple-guide-deep-learning-with-rtx-3090-cuda-cudnn-tensorflow-keras-pytorch-e88a2a8249bc. ##### Used packages: . 1. cuda_11.0.2_win10_network. . 2. cudnn-11.0-windows-x64-v8.0.5.39. . 3. tf-nightly-gpu 2.5.0.dev20201212. ##### Installation steps:. 1. Install CUDA 11 and cudnn-11.0 as described https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html. . 2. Verify CUDA and cudnn installation by doing the following:. â€‹	. - Open Windows *Command Prompt*. . - Type:. `$ nvidia-smi`. . Output:. ```; Sat Dec 12 23:24:40 2020; +-----------------------------------------------------------------------------+; | NVIDIA-SMI 460.79 Driver Version: 460.79 CUDA Version: 11.2 |; |-------------------------------+----------------------+----------------------+; | GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |; | | | MIG M. |; |===============================+======================+======================|; | 0 GeForce RTX 3080 WDDM | 00000000:68:00.0 On | N/A |; | 53% 35C P2 104W / 340W | 6182MiB / 10240MiB | 0% Default |; | | | N/A |; +-------------------------------+----------------------+----------------------+; ; +-----------------------------------------------------------------------------+; | Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | 0 N/A N/A 1464 C+G Insufficient Permissions N/A |; | 0 N/A N/A 4652 C+G ...zilla Firefox\firefox.exe N/A |; | 0 N/A N/A 6760 C+G Insufficient Permissions N/A |; | 0 N/A N/A 7700 C+G ...bbwe\Microsoft.Photos.exe N/A |; | 0 N/A",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/104#issuecomment-744024006:311,simpl,simple-guide-deep-learning-with-rtx-,311,,https://github.com/stardist/stardist/issues/104#issuecomment-744024006,2,"['guid', 'simpl']","['guide-microsoft-windows', 'simple-guide-deep-learning-with-rtx-']"
Usability,"Hi!. Thanks, I made sure now that all the stardist modules are the same versions. Unfortunately, tensorflow is still not configured properly on the computing cluster, crashing with errors that say:; ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; Aborted; ```. with while trying to do very simple print statements, so this is something I need to solve with our sysadmin. I am loading the trained module onto my local machine (OSX, Python3.6) and using it to predict on similar images. I can now load the cluster-trained (stardist 0.41) model onto my machine:; ```; model = StarDist3D(None, name='modelname',; basedir=path.join(trainingdir,'models')); labels, details = model.predict_instances(test_img[); ```. The kernel runs for a minute or so and then silently crashes without any error message, and then restarts silently (no warning messages on the bash shell that spyder is running in either). <img width=""804"" alt=""Screen Shot 2020-02-21 at 3 09 51 PM"" src=""https://user-images.githubusercontent.com/5126258/75041128-3cde2e00-54bc-11ea-9233-cd8212b14a3b.png"">. --. The training/labels are 3D images of clusters of cells. One mid-level slice looks like this, with 1Âµm slices, about 20Âµm:. <img width=""851"" alt=""Screen Shot 2020-02-21 at 3 02 38 PM"" src=""https://user-images.githubusercontent.com/5126258/75040633-4c10ac00-54bb-11ea-8f3a-f853a9fda51b.png"">. ---. On my local machine, I am running: tensorflow 1.15.0, Python 3.6.; On the cluster, I was running: tf 1.14.0, Python 3.6. Thank you!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/35#issuecomment-589670082:357,simpl,simple,357,,https://github.com/stardist/stardist/issues/35#issuecomment-589670082,1,['simpl'],['simple']
Usability,"Hi, . Could you provide a minimal example that shows both things failing? E.g. a simple way could be to have the data generator create random numpy arrays instead of reading files in `__getitem__`",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/57#issuecomment-880585932:81,simpl,simple,81,,https://github.com/stardist/stardist/issues/57#issuecomment-880585932,1,['simpl'],['simple']
Usability,"Hi, . Thanks for using stardist! . We haven't yet implemented multi-gpu training. In theory it should not be too hard to incorporate it though (something along the lines of this [discussion](https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66458)). You could try it by simply checking out the code and wrapping the model accordingly at the respective place [here](https://github.com/mpicbg-csbd/stardist/blob/master/stardist/models/model3d.py#L301). Let me know how it goes!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/67#issuecomment-650382889:290,simpl,simply,290,,https://github.com/stardist/stardist/issues/67#issuecomment-650382889,1,['simpl'],['simply']
Usability,"Hi, thanks for using StarDist. > As StarDist need numpy, its installation failed as numpy is not yet available. Yes, thanks for reminding me about that. Since `numpy` is typically installed, most people don't notice. > I fix my issue by installing numpy before my other requirements. Yes, that's currently the only way. > however I suggest to add numpy in the requirements of StarDist, I think it is missing. It's not that simple. Actually, we implicitly require `numpy` through `csbdeep`.; The real issue is that we need `numpy` to even build the wheel (package), i.e. our `setup.py` imports from `numpy`. I've [just added](https://github.com/mpicbg-csbd/stardist/commit/9bd7ff911bca5e87cbe70a8549b9c746d6d1c245) a `pyproject.toml`, which will hopefully fix this issue in the next release. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/71#issuecomment-657642225:423,simpl,simple,423,,https://github.com/stardist/stardist/issues/71#issuecomment-657642225,1,['simpl'],['simple']
Usability,"Hi, the error message indicates a problem with loading the weights from the HDF5 file. First, to rule out Snakemake as an issue, did you try to simply load the pre-trained model in a Python shell? For example, does this throw the same error?; ```; $ python -c ""from stardist.models import StarDist2D; StarDist2D.from_pretrained('2D_versatile_fluo')""; ```. If that's the case, then I'd try to use a different/newer version of the HDF5 library `h5py`. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/93#issuecomment-716223889:144,simpl,simply,144,,https://github.com/stardist/stardist/issues/93#issuecomment-716223889,1,['simpl'],['simply']
Usability,"Hi, we're glad that it works for you!. You can convert the Keras model to TensorFlow's [SavedModel format](https://www.tensorflow.org/guide/saved_model) like this:. ```python; model = StarDist2D(None, 'stardist', basedir='models'); model.export_TF(); ```. The created Zip file will contain the `.pb` file and the weights. Please also see the options of `model.export_TF()`, whose defaults are chosen such that an exported model will work in our Fiji plugin (e.g., it will upsample the output if any `grid` > 1).",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/61#issuecomment-643163493:134,guid,guide,134,,https://github.com/stardist/stardist/issues/61#issuecomment-643163493,1,['guid'],['guide']
Usability,"Hi,. > My question is if I can run the whole example script by colab on a better PC and get a model I can export to Fiji?. Yes, basically. We have an [example notebook that runs StarDist 2D on Colab](https://colab.research.google.com/github/mpicbg-csbd/stardist/blob/master/extras/stardist_example_2D_colab.ipynb). Otherwise, you can also try the StarDist notebooks provided by [ZeroCostDL4Mic](https://github.com/HenriquesLab/ZeroCostDL4Mic). > We have one in our imaging facility, but I don't think that we are allowed to install a complete python environment and so on. I would suggest to use your own computer if you're (starting to get) serious about using deep learning in your facility. Best,; Uwe",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/115#issuecomment-782405792:667,learn,learning,667,,https://github.com/stardist/stardist/issues/115#issuecomment-782405792,1,['learn'],['learning']
Usability,I actually was able to fix the issue. If anyone else runs into this simply add the line: ; import qupath.lib.regions.RegionRequest; under the 'manage imports' heading within the script and it worked for me.,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/63#issuecomment-643507577:68,simpl,simply,68,,https://github.com/stardist/stardist/issues/63#issuecomment-643507577,1,['simpl'],['simply']
Usability,"I don't understand why that should be a problem. Please be more descriptive - what are you trying to do exactly and what error do you observe? . Besides (from the issue template):. `If you open a new topic, please provide a clear and concise description to understand and ideally reproduce the issue you're having`",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/168#issuecomment-922922737:224,clear,clear,224,,https://github.com/stardist/stardist/issues/168#issuecomment-922922737,1,['clear'],['clear']
Usability,I think the next steps are:; - also generate the config and macro for stardist postprocessing with the exporter so that the model can be run in deepimagej; - create the python functionality to run a stardist bioimageio model + stardist postprocessing (I think I would just add this to bioimageio.core for simplicity); - check that we can use the model in deepimagej and python; - release a new stardist version so that the functionality is available; - update the zero-cost notebook to use this functionality to export stardist bioimageio models. I will start working on the first point later.,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-973972757:305,simpl,simplicity,305,,https://github.com/stardist/stardist/pull/171#issuecomment-973972757,1,['simpl'],['simplicity']
Usability,"I've made some significant improvements by switching to the scikit-learn version of KDTree (good suggestion!) and changing the way the KDTree is used within NMS. To keep things clean, I'm going to delete my existing fork, create a new one, put the final version of the changes there, and submit them as (hopefully) a single commit.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-606123282:67,learn,learn,67,,https://github.com/stardist/stardist/pull/40#issuecomment-606123282,1,['learn'],['learn']
Usability,"IMHO, I didn't find it explicit in the DocString and went all the way to csbdeep to finally realize that this might be the case. At which point I tried to load it and had erased the previous model so it bugged XD . Training a model is very time consuming and simply running a single command might destroy it so IMHO it is a valuable addition to the readme ^^ But I am on the repeat things over school ^^",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/95#issuecomment-716247005:259,simpl,simply,259,,https://github.com/stardist/stardist/issues/95#issuecomment-716247005,1,['simpl'],['simply']
Usability,"Many thanks @uschmidt83!!!! I had some suspicious about it and you have cleared this doubt. I am trying this new approach, thank you again. Best wishes,; Pedro",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/51#issuecomment-623462144:72,clear,cleared,72,,https://github.com/stardist/stardist/issues/51#issuecomment-623462144,1,['clear'],['cleared']
Usability,"Morning @uschmidt83 !. > Is the new Java library for deep learning ready yet? What is the name and where can I find it?. Yes, here it is: https://github.com/bioimage-io/model-runner-java. > Using the bioimageio library where to export the model? And ""everything works perfectly"" where?. Here is an example where we are using it: https://github.com/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/U-Net_2D_Multilabel_ZeroCostDL4Mic.ipynb; And here you can find the model already: https://bioimage.io/#/?type=all&tags=deepimagej,bacillus-subtilis&id=10.5281%2Fzenodo.7261974. > I really need to understand the current situation first... I don't even know how to export the model such that it will work with the new Java library. The bioimageio.core library will export it directly in a compatible format (as the model I linked in the previous lines). In the upcoming days, we will update deepImageJ with the java model runner so all the TF2 models can also be deployed in Fiji. I can let you know when the plugin is ready if you want.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/68#issuecomment-1455823190:58,learn,learning,58,,https://github.com/stardist/stardist/issues/68#issuecomment-1455823190,1,['learn'],['learning']
Usability,"Not grid=4; I was referencing this previous discussion: https://github.com/mpicbg-csbd/stardist/issues/33#issuecomment-583421466; Yeah, 32^3 with grid=4 would be 2^3 patches right?! Definitely too small :). Qualitatively the results look ok - there are definitely some nuclei missing and very few cells that are just too large. The shape of nuclei is fine, they're all roughly elliptical, but exact boundary matching isn't important to me, if I get 0.6 IOU or better for the majority of cells then I will be very happy. I don't really know what magnitude of prob_loss would be considered good, I was just looking at the trend. It's so jumpy, it seemed that I had reached the best I could with the data I have - which is rather noisy and was even quite difficult to segment by hand in many places. Qualitatively things look ok (here's a 128x128x128 patch, z-voxels are 2x bigger than x/y voxels hence z dimension looking larger in viewer):. ![Screen Shot 2020-03-02 at 3 19 11 PM](https://user-images.githubusercontent.com/8507206/75714224-4d9b5a80-5c99-11ea-9b99-5aad450022bf.png); ![Screen Shot 2020-03-02 at 3 19 42 PM](https://user-images.githubusercontent.com/8507206/75714226-4d9b5a80-5c99-11ea-9d26-28ca0ff1a3cd.png). I only manually annotated a 128x128x32 patch in a different part of the brain than what's shown above, so it at least generalizes ok to different ROIs of the acquisition and different z-depths. Would you consider manually lowering the learning rate after so many epochs? Or any other recommendations for getting the prob_loss even lower?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/36#issuecomment-593599163:1459,learn,learning,1459,,https://github.com/stardist/stardist/issues/36#issuecomment-593599163,1,['learn'],['learning']
Usability,"Ok, thanks for letting us know. Btw, I got TensorFlow with GPU support working (also on Windows) by simply installing [this conda environment](https://github.com/CSBDeep/CSBDeep/tree/master/extras#conda-environment). It will automatically install the necessary CUDA and cuDNN libraries. After installing this environment, activate it, and then install StarDist via pip.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/121#issuecomment-789241256:100,simpl,simply,100,,https://github.com/stardist/stardist/issues/121#issuecomment-789241256,1,['simpl'],['simply']
Usability,"Ok, there was indeed an offending non-ascii character. Thanks for the feedback!. If you install the latest version (`stardist 0.3.4`) it should be fixed.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/18#issuecomment-535047059:70,feedback,feedback,70,,https://github.com/stardist/stardist/issues/18#issuecomment-535047059,1,['feedback'],['feedback']
Usability,"Perhaps the README just needs to be updated that only macOS 12 is supported on arm64?; I'm not sure how this behaves on x86, but Apple is pretty clear about tensorflow arm64 being macOS 12 and up.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/174#issuecomment-1282938048:145,clear,clear,145,,https://github.com/stardist/stardist/issues/174#issuecomment-1282938048,1,['clear'],['clear']
Usability,"Regarding the deepImageJ issues: we will need to wait for feedback from @esgomezm or @carlosuc3m. > Is this something that has to be fixed in `bioimage.io.core`? (We typically use [save_tiff_imagej_compatible](https://github.com/CSBDeep/CSBDeep/blob/b0d2f5f344ebe65a9b4c3007f4567fe74268c813/csbdeep/io/__init__.py#L15-L46) from csbdeep to save tiff files such that ImageJ will read them correctly.). This is fixed already and included in the latest release (0.4.10). > > > As discussed before christmas, the idea would be that you apply this macro automatically in deepimagej if you recognise that you have a stardist model, e.g. by checking if the key `config.stardist` exists.; > ; > Is this already considered in the forthcoming DeepImageJ update?. We decided to not tackle the automatic running for now and just apply the macro by hand for the use-case.; We'll tackle automatically running it later.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-1034212624:58,feedback,feedback,58,,https://github.com/stardist/stardist/pull/171#issuecomment-1034212624,1,['feedback'],['feedback']
Usability,"Sure, for your use case it would make sense to do it the way you described, sorry if the code was confusing and thx again for the feedback!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/12#issuecomment-512993337:130,feedback,feedback,130,,https://github.com/stardist/stardist/issues/12#issuecomment-512993337,1,['feedback'],['feedback']
Usability,Thank you for the reply @maweigert. ; I was using the function to test some morphological operations (not deep learning) based on distances. For my operations I needed the exact l2-distance to the pixel and I was confused why I always got integer distances.,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/12#issuecomment-512886685:111,learn,learning,111,,https://github.com/stardist/stardist/issues/12#issuecomment-512886685,1,['learn'],['learning']
Usability,"Thank you for your quick reply! . > I suggest you do the prediction in Python to get the label image, which you can then import in Fiji with the [3D Roi Manager](https://imagejdocu.tudor.lu/plugin/stacks/3d_roi_manager/start) (installation instructions [here](https://imagejdocu.tudor.lu/plugin/stacks/3d_ij_suite/start#installation)). This should work well, as long as you don't have (strongly) overlapping predictions or it's not important to you to get the complete shapes for all objects. This sounds really good. I do have quite overlapping nuclei though unfortunately. Apologies if this is a bit of a starter question but would I just save the label image as a tiff file and then open it in FIJI, and then segment it with the 3D ROI viewer? . > It's not that simple and even if it worked, this won't get you 3D ROIs in Fiji. Our [StarDist Fiji plugin](https://imagej.net/StarDist) currently only works in 2D because it's not that easy to support 3D... but we hope to get this done eventually. Ah I thought it wouldn't be simple. The error message I was getting was that the input tensor had to be 5 dimensional, and was wondering whether the mismatch was due to tensor organisation convention of NHWC not including a third spatial dimension. . The images you have from paintera look really beautiful, I think that is my aim here, but perhaps FIJI's 3d viewer of the 3d ROIs would get something similar?. Thanks again!!. Michael Schwimmer",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/37#issuecomment-597304169:765,simpl,simple,765,,https://github.com/stardist/stardist/issues/37#issuecomment-597304169,2,['simpl'],['simple']
Usability,"Thanks for adding the notebook @uschmidt83. I think this is sufficient to show how to use the stardist bioimageio models from python for now. I went through the shape spec now and this should all be correct, but pending https://github.com/bioimage-io/core-bioimage-io-python/pull/163.; I have also added the halo paramer to the exported model, see comment above. Regarding deemimagej support I am still waiting for the feedback from @esgomezm on https://github.com/bioimage-io/core-bioimage-io-python/pull/151. I will follow up on this tomorrow.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-979145489:419,feedback,feedback,419,,https://github.com/stardist/stardist/pull/171#issuecomment-979145489,1,['feedback'],['feedback']
Usability,"Thanks for the excellent model and well curated codebase! StarDist turned out to be way easier to use than other cell segmentation models. I was wondering, is there already a way for me to set up transfer learning? I'm perfectly willing to hack it together myself, if it doesn't require too much domain knowledge. My lab is primarily interested in cell classification on HE images.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/126#issuecomment-1302127675:205,learn,learning,205,,https://github.com/stardist/stardist/issues/126#issuecomment-1302127675,1,['learn'],['learning']
Usability,"Thanks for the replies, @maweigert and @uschmidt83 . > For training, a majority of cells should be fully included in the annotated trainings stacks - I fear there is no real way around that. Sorry, I may not have been as clear as I intended. We have a working model that has been created with thicker images, and the problem occurs when applying the model on thin image-stacks. However, it seems like I am not able to reproduce the error (as in crashing the process) with the data I currently have access to, but I can't get any predictions with thinner images. I performed an example run with a representative image (XY-cropped) and a duplicate image with one z-slice removed, i.e. from 5 to 4 z-slices. I have included my output below with verbose=True. All in all, it seems that for some reason StarDist is not able to create any candidates for the thinner stack. At the thickness of 4 z-layers it is expected that the largest cells do not neatly fit into the stack, but there should be plenty of smaller cells that I would think should still create candidates. So I would presume there has to be a reason for why none are created?. @uschmidt83 ; > I don't know how your data looks like, but have you tried padding the image such that the Z axis is big enough (as a trivial workaround)?. I did try that some months back by adding empty slices but it caused bloating of the labels and was not viable. But now that I think about it there probably is a better way for padding the image, such as duplicating the edge layers to provide continuity in intensities. **OUTPUT**:. Holidic_2018-12-19_180224-2; Model = DAPI20x ; Image dims = (4, 944, 2008). predicting instances with prob_thresh = 0.01 and nms_thresh = 0.4; found 0 candidates; non-maximum suppression...; Non Maximum Suppression (3D) ++++ ; NMS: n_polys = 0 ; NMS: n_rays = 128 ; NMS: n_faces = 252 ; NMS: thresh = 0.400 ; NMS: use_bbox = 1 ; NMS: using OpenMP with 16 thread(s); NMS: precompute volumes, bounding boxes, etc; precompute done",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/133#issuecomment-831993257:221,clear,clear,221,,https://github.com/stardist/stardist/issues/133#issuecomment-831993257,1,['clear'],['clear']
Usability,The `inner` test is simply a sphere intersection so it is negligible and I would have expected that the overall time would be reduced. Maybe all those threads are competing too much as you suspected. You could try running your script with `OMP_NUM_THREADS=8 python script.py` and see whether that helps.,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/277#issuecomment-2236558174:20,simpl,simply,20,,https://github.com/stardist/stardist/issues/277#issuecomment-2236558174,1,['simpl'],['simply']
Usability,"The code can be found in the ZCDL4M notebook, section 5.3: https://github.com/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/BioImage.io%20notebooks/StarDist_2D_ZeroCostDL4Mic_BioImageModelZoo_export.ipynb. This is the resume:; 1. Get the ""raw model"" by concatenating the outputs (we can skip this one, it would be ok on deepImageJ):. ```python; # Load the model; model = StarDist2D(None, name=QC_model_name, basedir=QC_model_path); thres, nms = model.thresholds. # Check minimum size: it is [8,8] for the 2D XY plane; depth = model.config.unet_n_depth; pool = model.config.unet_pool; MinimumSize = [pool[0]**(depth+1), pool[1]**(depth+1)]. # Concatenate model output to use pydeepimagej and the StarDist macro; input = model.keras_model.inputs[0]; single_output = Concatenate()([model.keras_model.output[0], model.keras_model.output[1]]); new_model = Model(input, single_output). dij_config = BioImageModelZooConfig(new_model, MinimumSize); .; .; .; # Add weights information; dij_config.add_weights_formats(new_model, 'TensorFlow', ; parent=""keras_hdf5"",; tf_version=tf.__version__); dij_config.add_weights_formats(new_model, 'KerasHDF5',; tf_version=tf.__version__); ```; 2. Use `pydeepimagej` library to export the TensorFlow and keras models: https://github.com/deepimagej/pydeepimagej/blob/5aaf0e71f9b04df591d5ca596f0af633a7e024f5/pydeepimagej/yaml/create_config.py#L251. **Concerning the upsampling and downsamplings:**; The model we export does the downsampling inside as deepImageJ deals with inputs-outputs of different sizes. For the concatenation, we can skip concatenating them. The only problem is that doing so, the model won't be compatible with CSBDeep. If we make the model compatible with CSBDeep, then the TF model would be different from the keras one, so here is maybe a potential solution (?): could we use the parent key in the specs @constantinpape @oeway to link the TF and keras models?? architecture is slightly different but the weights are the same.",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-968831030:228,resume,resume,228,,https://github.com/stardist/stardist/pull/171#issuecomment-968831030,1,['resume'],['resume']
Usability,"This is working locally now. (Needs the changes in https://github.com/bioimage-io/core-bioimage-io-python/pull/142); Also, I can only run one of the tests successfully at a time, because tensorflow does not properly clear the gpu.; Do you maybe have a helper function for that in CSBDeep @maweigert, @uschmidt83?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/171#issuecomment-966429985:216,clear,clear,216,,https://github.com/stardist/stardist/pull/171#issuecomment-966429985,1,['clear'],['clear']
Usability,"Which comment?. > The result of your changes is good, i.e. having a subfolder `bioimageio` with the rdf.yaml and other files. I think it's cleaner that way. > I think there is a simpler way of achieving this without using the tmpfile, see my comment in the code. Which comment?",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/187#issuecomment-1075297878:178,simpl,simpler,178,,https://github.com/stardist/stardist/pull/187#issuecomment-1075297878,1,['simpl'],['simpler']
Usability,"`If you open a new topic, please provide a clear and concise description to understand and ideally reproduce the issue you're having (e.g. including a code snippet, Python script, or Jupyter notebook).`",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/138#issuecomment-833452120:43,clear,clear,43,,https://github.com/stardist/stardist/issues/138#issuecomment-833452120,1,['clear'],['clear']
Usability,i checked my output after training for 400 epochs and the network doesnt learn anything my output is completely blank,MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/55#issuecomment-625765345:73,learn,learn,73,,https://github.com/stardist/stardist/issues/55#issuecomment-625765345,1,['learn'],['learn']
Usability,"lpful, I think you're probably right that this is the relevant code to modify for my data. However, this loopy python code is not practical to actually run on my data, where I have several hundred cells per image. It seems the default for my local installation is to run the compiled cpp version:; https://github.com/mpicbg-csbd/stardist/blob/38454feca61804049a3afae34add804e0b91517c/stardist/lib/stardist2d.cpp#L53-L59. I'd like to modify this and rebuild - but I'm unsure how to do it, since the entire build was part of the `pip install stardist` and there is no documentation on building from source. @maweigert @uschmidt83 Can you guys provide any information about how to rebuild the cpp libraries in stardist? I'm using MacOS and gcc-9, but will need to repeat this process later on red hat linux. Edit: This turned out to be easy. I just made my modifications to the cpp code and reran `python setup.py sdist` from the source directory, then `python setup.py install` For MacOSX you need `CC=gcc-9 CXX=g++-9 python setup.py install`. Since you've been so helpful, I'd like to try and help with your issue as well - but since it occurs primarily on reconstructions of the test data polygons, it's probably something different. Can you rule out that it's just a phenomenon of the data, i.e. different intensity characteristics at the boundaries of training/testing data - so the model just learned to produce more smooth/conservative boundaries?. Edit: I forced the python versions to run with a 4 in the denominator for the step size; it is significantly slower than the CPP code and not practically useful, but it does definitely solve the roughness problem for small label areas:. ![Screen Shot 2020-02-07 at 11 17 10 AM](https://user-images.githubusercontent.com/8507206/74045936-a5e18400-499b-11ea-93cf-340bef4e5d6f.png). <img width=""868"" alt=""Screen Shot 2020-02-07 at 11 17 12 AM"" src=""https://user-images.githubusercontent.com/8507206/74045949-aaa63800-499b-11ea-9c76-3ef96e31b905.png"">",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-583469917:1426,learn,learned,1426,,https://github.com/stardist/stardist/issues/33#issuecomment-583469917,1,['learn'],['learned']
Usability,"ong as it fits in my GPU or should I do something smarter?; - try to make bigger train_patch_size : I'm a bit uncomfortable setting batch_size lower than 4 (or even 8) to be honest, do you perform gradient accumulation under the hood? Maybe I'm just freaking out for nothing but don't you feel that a batch_size of 2 is a bit too small to get a nice gradient descent?. Also I wanted to ask you about something else. ; At the moment I don't have nice labels that have been curated by humans (I might find some time to do this but later), but I have good enough binary masks that allows me to train algorithms like 3DUnets.; My problem is that some cells are close to each other, so my initial labels are not very good at differientiating adjacent cells (which creates big objects that don't look like cells at all) and same goes with the resulting UNet segmentation, that's why I decided to switch to StarDist3D.; First results from StarDist seem already much better at creating nice round cells, but in order to train StarDist I simply separated my binary mask using connex component : so my ground truth for StarDist never really shows adjacent cells (as they are merged into one same label).; Because of this I don't really trust the automated nms_thresh search function (labels are wrong so it's going to be hard for the search). So I played a bit with the nms_thresh to visually inspect the results but I'd like to understand better what 0.4 or 0.05 means for example. Does 0.4 means that I would tolerate that 40% of the volume of two cells (or a proxy of the volume) are actually overlapping? In terms of physics and biology, I don't expect my cells to be able to overlap at all, they can touch each other but that's it. Does this physics rule mean that setting nms_thresh to 0 makes sense? Or does the nms_thresh represents something completely different? ( I tried to have a look at the source code but I can't read C++!). Hope my questions make sense. Thanks again for your help,. Best,; Seb",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691334428:1306,simpl,simply,1306,,https://github.com/stardist/stardist/issues/87#issuecomment-691334428,1,['simpl'],['simply']
Usability,"redicted object is fully contained in at least one of the blocks. This is not important for the CNN prediction, but the non-maximum suppression step. > 3- context : I don't really understand the doc here `Amount of image context on all sides of a block, which is discarded.` What is the point of discarding some pixels on all sides? Are they discarded after the forward pass (for the NMS algorithm) or before (like a padding)?. No, this is just because the CNN prediction is less accurate at the image boundary. It is discarded after the NMS step. > 4- `Also, it must hold that: min_overlap + 2*context < block_size.` I would easily understand that ` 2*context < block_size` in order not to discard everything, but why does `min_overlap` appears here? Would you have a link to a visual explanation how you define the different blocks? I think this could help me understand. After the context is discarded on all sides, blocks still have to overlap at least `min_overlap` pixels. As you say, a diagram would make this clear. Note that the `model.predict_instances_big` is relatively new and really meant for people who have huge images. It's not a mainstream function. > 5- all my 3D stacks don't have the same sizes, should I change the above parameters according to each sizes or one set should work for all?. The parameters can be the the same and really only depend on the CNN architecture (context) and the object sizes (min_overlap). However, the stack must obviously be bigger than the block size. > 6- Overall would you have some advise to improve my pipeline?. Like I said, try using `model.predict_instances` instead and save yourself the headache ;). > I'm working on a docker container of my own and installed the latest tensorflow version, I was getting this error when instantiating the StarDist3D`MemoryError: clEnqueueWriteBuffer failed: MEM_OBJECT_ALLOCATION_FAILURE` it was taking all my 12Go of GPU Memory (I have an RTX 2080 Ti). Yes, TensorFlow unfortunately grabs all the GPU memo",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/87#issuecomment-691208486:3667,clear,clear,3667,,https://github.com/stardist/stardist/issues/87#issuecomment-691208486,1,['clear'],['clear']
Usability,"tion on building from source. I think the package gets built when installing with pip, on the Mac I had to follow these instructions by @maweigert: https://github.com/mpicbg-csbd/stardist/issues/21. I also have stardist installed on a Ubuntu 18 machine for the training and there was no problem. Otherwise, you could speed up the python loops using `numba.jit` e.g. directly from your jupyter notebook like this:. ```python; from stardist.geometry import geom2d; from numba import jit. def _py_star_dist_modified(a, n_rays=32):; # (np.isscalar(n_rays) and 0 < int(n_rays)) or _raise(ValueError()); n_rays = int(n_rays); a = a.astype(np.uint16,copy=False); dst = np.empty(a.shape+(n_rays,),np.float32). for i in range(a.shape[0]):; for j in range(a.shape[1]):; value = a[i,j]; if value == 0:; dst[i,j] = 0; else:; st_rays = np.float32((2*np.pi) / n_rays); for k in range(n_rays):; phi = np.float32(k*st_rays); dy = np.cos(phi)#/100.; dx = np.sin(phi)#/100.; x, y = np.float32(0), np.float32(0). while True:; x += dx; y += dy; ii = int(round(i+x)); jj = int(round(j+y)); if (ii < 0 or ii >= a.shape[0] or; jj < 0 or jj >= a.shape[1] or; value != a[ii,jj]):; dist = np.sqrt(x*x + y*y); dst[i,j,k] = dist; break; return dst. geom2d._py_star_dist = jit(_py_star_dist_modified); ```. This version performs about the same as the cpp one:; ```; %timeit relabel_image_stardist(test_lbl, n_rays=32, mode='python'); 452 Âµs Â± 7.97 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each); %timeit relabel_image_stardist(test_lbl, n_rays=32, mode='cpp'); 493 Âµs Â± 8.6 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each); ```. > Can you rule out that it's just a phenomenon of the data, i.e. different intensity characteristics at the boundaries of training/testing data - so the model just learned to produce more smooth/conservative boundaries?. Hmm you're right that I cannot rule out that it's a training thing. Will definitely train again with a dataset in which I have many shapes represented. Thanks!",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/33#issuecomment-583505792:1995,learn,learned,1995,,https://github.com/stardist/stardist/issues/33#issuecomment-583505792,1,['learn'],['learned']
Usability,"ub.com/mpicbg-csbd/stardist/issues/36#issuecomment-594974089) I was able to run the job with a single cpu (# cores requested and maximum amount of RAM available are mixed on our cluster; each core comes with 15GB RAM). However the remaining runtime predicted by tqdm after a few iterations had accumulated was something like 36 hours - so I cancelled it and gave up on it. Uwe reminded me that with one core, I likely had a maximum of 2 threads, but I did not go back and try `predict_big` with more cores. Do you think `predict_big` with say 32 cores (64 threads) could finish in less than an hour? If so I will give it another try. For different kdtree implementations - I did look through [this article written by the scikit-learn kdtree and balltree developer](https://jakevdp.github.io/blog/2013/04/29/benchmarking-nearest-neighbor-searches-in-python/), but I didn't think there was enough differences in his graphs to justify introducing a new dependency (I don't think scikit-learn was a dependency before?) and I also preferred something with an underlying C++ implementation, as there are more big speed ups available if the kdtree data structure is available for query directly within the NMS algorithm. Unfortunately the only RAM measurements I have are the maximum and average usage (over the full run, prediction and NMS) for prediction on the full volumes both w/ and w/o the kdtree. The command in both cases was:. `pred, det = model.predict_instances(image_norm, verbose=True, n_tiles=(4, 8, 2))`; On a volume with 860x1676x301 voxels and 6.32e6 polygon candidates. w/o kdtree max: 434.439GB; w/ kdtree max: 430.482GB. w/o kdtree avg: 221.672GB; w/ kdtree avg: 297.733GB. I'm thinking that the kdtree and the query results for this many candidates took something like 70GB RAM. One thing that I haven't looked at in the code - it seems like the biggest expansion of data in the method is from raw data --> prediction; e.g. in my case I'm using 128 rays, so I need (129 * 2) more RAM re",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/pull/40#issuecomment-601381585:1208,learn,learn,1208,,https://github.com/stardist/stardist/pull/40#issuecomment-601381585,1,['learn'],['learn']
Usability,"works/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py"", line 1144, in run_setup; run_setup(setup_script, args); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 253, in run_setup; raise; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py"", line 35, in __exit__; self.gen.throw(type, value, traceback); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 166, in save_modules; saved_exc.resume(); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 141, in resume; six.reraise(type, exc, self._tb); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 154, in save_modules; yield saved; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 195, in setup_context; yield; File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 250, in run_setup; _execfile(setup_script, ns); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/sandbox.py"", line 45, in _execfile; exec(code, globals, locals); File ""/var/folders/lz/h4nxm8jd64qdhlr57n1y_yx80000gq/T/easy_install-d8fAD9/numpy-1.19.0/setup.py"", line 30, in <module>; re.MULTILINE | re.DOTALL).groups()[0]; RuntimeError: Python version >= 3.6 required.; ----------------------------------------; ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command ou",MatchSource.ISSUE_COMMENT,stardist,stardist,0.9.1,https://github.com/stardist/stardist/issues/74#issuecomment-659307440:6078,resume,resume,6078,,https://github.com/stardist/stardist/issues/74#issuecomment-659307440,1,['resume'],['resume']
